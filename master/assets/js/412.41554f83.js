(window.webpackJsonp=window.webpackJsonp||[]).push([[412],{798:function(s,a,t){"use strict";t.r(a);var r=t(43),n=Object(r.a)({},(function(){var s=this,a=s.$createElement,t=s._self._c||a;return t("ContentSlotsDistributor",{attrs:{"slot-key":s.$parent.slotKey}},[t("h1",{attrs:{id:"spark-load"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#spark-load"}},[s._v("#")]),s._v(" Spark Load")]),s._v(" "),t("p",[s._v("Spark load 通过外部的 Spark 资源实现对导入数据的预处理，提高 Doris 大数据量的导入性能并且节省 Doris 集群的计算资源。主要用于初次迁移，大数据量导入 Doris 的场景。")]),s._v(" "),t("p",[s._v("Spark load 是一种异步导入方式，用户需要通过 MySQL 协议创建 Spark 类型导入任务，并通过 "),t("code",[s._v("SHOW LOAD")]),s._v(" 查看导入结果。")]),s._v(" "),t("h2",{attrs:{id:"适用场景"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#适用场景"}},[s._v("#")]),s._v(" 适用场景")]),s._v(" "),t("ul",[t("li",[s._v("源数据在 Spark 可以访问的存储系统中，如 HDFS。")]),s._v(" "),t("li",[s._v("数据量在 几十 GB 到 TB 级别。")])]),s._v(" "),t("h2",{attrs:{id:"名词解释"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#名词解释"}},[s._v("#")]),s._v(" 名词解释")]),s._v(" "),t("ol",[t("li",[s._v("Frontend（FE）：Doris 系统的元数据和调度节点。在导入流程中主要负责导入任务的调度工作。")]),s._v(" "),t("li",[s._v("Backend（BE）：Doris 系统的计算和存储节点。在导入流程中主要负责数据写入及存储。")]),s._v(" "),t("li",[s._v("Spark ETL：在导入流程中主要负责数据的 ETL 工作，包括全局字典构建（BITMAP类型）、分区、排序、聚合等。")]),s._v(" "),t("li",[s._v("Broker：Broker 为一个独立的无状态进程。封装了文件系统接口，提供 Doris 读取远端存储系统中文件的能力。")]),s._v(" "),t("li",[s._v("全局字典： 保存了数据从原始值到编码值映射的数据结构，原始值可以是任意数据类型，而编码后的值为整型；全局字典主要应用于精确去重预计算的场景。")])]),s._v(" "),t("h2",{attrs:{id:"基本原理"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#基本原理"}},[s._v("#")]),s._v(" 基本原理")]),s._v(" "),t("h3",{attrs:{id:"基本流程"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#基本流程"}},[s._v("#")]),s._v(" 基本流程")]),s._v(" "),t("p",[s._v("用户通过 MySQL 客户端提交 Spark 类型导入任务，FE记录元数据并返回用户提交成功。")]),s._v(" "),t("p",[s._v("Spark load 任务的执行主要分为以下5个阶段。")]),s._v(" "),t("ol",[t("li",[s._v("FE 调度提交 ETL 任务到 Spark 集群执行。")]),s._v(" "),t("li",[s._v("Spark 集群执行 ETL 完成对导入数据的预处理。包括全局字典构建（BITMAP类型）、分区、排序、聚合等。")]),s._v(" "),t("li",[s._v("ETL 任务完成后，FE 获取预处理过的每个分片的数据路径，并调度相关的 BE 执行 Push 任务。")]),s._v(" "),t("li",[s._v("BE 通过 Broker 读取数据，转化为 Doris 底层存储格式。")]),s._v(" "),t("li",[s._v("FE 调度生效版本，完成导入任务。")])]),s._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("                 +\n                 | 0. User create spark load job\n            +----v----+\n            |   FE    |---------------------------------+\n            +----+----+                                 |\n                 | 3. FE send push tasks                |\n                 | 5. FE publish version                |\n    +------------+------------+                         |\n    |            |            |                         |\n+---v---+    +---v---+    +---v---+                     |\n|  BE   |    |  BE   |    |  BE   |                     |1. FE submit Spark ETL job\n+---^---+    +---^---+    +---^---+                     |\n    |4. BE push with broker   |                         |\n+---+---+    +---+---+    +---+---+                     |\n|Broker |    |Broker |    |Broker |                     |\n+---^---+    +---^---+    +---^---+                     |\n    |            |            |                         |\n+---+------------+------------+---+ 2.ETL +-------------v---------------+\n|               HDFS              +-------\x3e       Spark cluster         |\n|                                 <-------+                             |\n+---------------------------------+       +-----------------------------+\n\n")])])]),t("h2",{attrs:{id:"全局字典"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#全局字典"}},[s._v("#")]),s._v(" 全局字典")]),s._v(" "),t("h3",{attrs:{id:"适用场景-2"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#适用场景-2"}},[s._v("#")]),s._v(" 适用场景")]),s._v(" "),t("p",[s._v("目前Doris中Bitmap列是使用类库"),t("code",[s._v("Roaringbitmap")]),s._v("实现的，而"),t("code",[s._v("Roaringbitmap")]),s._v("的输入数据类型只能是整型，因此如果要在导入流程中实现对于Bitmap列的预计算，那么就需要将输入数据的类型转换成整型。")]),s._v(" "),t("p",[s._v("在Doris现有的导入流程中，全局字典的数据结构是基于Hive表实现的，保存了原始值到编码值的映射。")]),s._v(" "),t("h3",{attrs:{id:"构建流程"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#构建流程"}},[s._v("#")]),s._v(" 构建流程")]),s._v(" "),t("ol",[t("li",[s._v("读取上游数据源的数据，生成一张hive临时表，记为"),t("code",[s._v("hive_table")]),s._v("。")]),s._v(" "),t("li",[s._v("从"),t("code",[s._v("hive_table")]),s._v("中抽取待去重字段的去重值，生成一张新的hive表，记为"),t("code",[s._v("distinct_value_table")]),s._v("。")]),s._v(" "),t("li",[s._v("新建一张全局字典表，记为"),t("code",[s._v("dict_table")]),s._v("；一列为原始值，一列为编码后的值。")]),s._v(" "),t("li",[s._v("将"),t("code",[s._v("distinct_value_table")]),s._v("与"),t("code",[s._v("dict_table")]),s._v("做left join，计算出新增的去重值集合，然后对这个集合使用窗口函数进行编码，此时去重列原始值就多了一列编码后的值，最后将这两列的数据写回"),t("code",[s._v("dict_table")]),s._v("。")]),s._v(" "),t("li",[s._v("将"),t("code",[s._v("dict_table")]),s._v("与"),t("code",[s._v("hive_table")]),s._v("做join，完成"),t("code",[s._v("hive_table")]),s._v("中原始值替换成整型编码值的工作。")]),s._v(" "),t("li",[t("code",[s._v("hive_table")]),s._v("会被下一步数据预处理的流程所读取，经过计算后导入到Doris中。")])]),s._v(" "),t("h2",{attrs:{id:"数据预处理（dpp）"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#数据预处理（dpp）"}},[s._v("#")]),s._v(" 数据预处理（DPP）")]),s._v(" "),t("h3",{attrs:{id:"基本流程-2"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#基本流程-2"}},[s._v("#")]),s._v(" 基本流程")]),s._v(" "),t("ol",[t("li",[s._v("从数据源读取数据，上游数据源可以是HDFS文件，也可以是Hive表。")]),s._v(" "),t("li",[s._v("对读取到的数据进行字段映射，表达式计算以及根据分区信息生成分桶字段"),t("code",[s._v("bucket_id")]),s._v("。")]),s._v(" "),t("li",[s._v("根据Doris表的rollup元数据生成RollupTree。")]),s._v(" "),t("li",[s._v("遍历RollupTree，进行分层的聚合操作，下一个层级的rollup可以由上一个层的rollup计算得来。")]),s._v(" "),t("li",[s._v("每次完成聚合计算后，会对数据根据"),t("code",[s._v("bucket_id")]),s._v("进行分桶然后写入HDFS中。")]),s._v(" "),t("li",[s._v("后续broker会拉取HDFS中的文件然后导入Doris Be中。")])]),s._v(" "),t("h2",{attrs:{id:"基本操作"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#基本操作"}},[s._v("#")]),s._v(" 基本操作")]),s._v(" "),t("h3",{attrs:{id:"配置-etl-集群"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#配置-etl-集群"}},[s._v("#")]),s._v(" 配置 ETL 集群")]),s._v(" "),t("p",[s._v("Spark作为一种外部计算资源在Doris中用来完成ETL工作，未来可能还有其他的外部资源会加入到Doris中使用，如Spark/GPU用于查询，HDFS/S3用于外部存储，MapReduce用于ETL等，因此我们引入resource management来管理Doris使用的这些外部资源。")]),s._v(" "),t("p",[s._v("提交 Spark 导入任务之前，需要配置执行 ETL 任务的 Spark 集群。")]),s._v(" "),t("p",[s._v("语法：")]),s._v(" "),t("div",{staticClass:"language-sql extra-class"},[t("pre",{pre:!0,attrs:{class:"language-sql"}},[t("code",[t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("-- create spark resource")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("CREATE")]),s._v(" EXTERNAL RESOURCE resource_name\nPROPERTIES\n"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("\n  "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("type")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" spark"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n  spark_conf_key "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" spark_conf_value"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n  working_dir "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" path"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n  broker "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" broker_name"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n  broker"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("property_key "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" property_value\n"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("-- drop spark resource")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("DROP")]),s._v(" RESOURCE resource_name\n\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("-- show resources")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("SHOW")]),s._v(" RESOURCES\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("SHOW")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("PROC")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"/resources"')]),s._v("\n\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("-- privileges")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("GRANT")]),s._v(" USAGE_PRIV "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("ON")]),s._v(" RESOURCE resource_name "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("TO")]),s._v(" user_identity\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("GRANT")]),s._v(" USAGE_PRIV "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("ON")]),s._v(" RESOURCE resource_name "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("TO")]),s._v(" ROLE role_name\n\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("REVOKE")]),s._v(" USAGE_PRIV "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("ON")]),s._v(" RESOURCE resource_name "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("FROM")]),s._v(" user_identity\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("REVOKE")]),s._v(" USAGE_PRIV "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("ON")]),s._v(" RESOURCE resource_name "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("FROM")]),s._v(" ROLE role_name\n")])])]),t("h4",{attrs:{id:"创建资源"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#创建资源"}},[s._v("#")]),s._v(" 创建资源")]),s._v(" "),t("p",[t("code",[s._v("resource_name")]),s._v(" 为 Doris 中配置的 Spark 资源的名字。")]),s._v(" "),t("p",[t("code",[s._v("PROPERTIES")]),s._v(" 是 Spark 资源相关参数，如下：")]),s._v(" "),t("ul",[t("li",[t("p",[t("code",[s._v("type")]),s._v("：资源类型，必填，目前仅支持 spark。")])]),s._v(" "),t("li",[t("p",[s._v("Spark 相关参数如下：")]),s._v(" "),t("ul",[t("li",[t("code",[s._v("spark.master")]),s._v(": 必填，目前支持yarn，spark://host:port。")]),s._v(" "),t("li",[t("code",[s._v("spark.submit.deployMode")]),s._v(":  Spark 程序的部署模式，必填，支持 cluster，client 两种。")]),s._v(" "),t("li",[t("code",[s._v("spark.hadoop.yarn.resourcemanager.address")]),s._v(": master为yarn时必填。")]),s._v(" "),t("li",[t("code",[s._v("spark.hadoop.fs.defaultFS")]),s._v(": master为yarn时必填。")]),s._v(" "),t("li",[s._v("其他参数为可选，参考http://spark.apache.org/docs/latest/configuration.html")])])]),s._v(" "),t("li",[t("p",[t("code",[s._v("working_dir")]),s._v(": ETL 使用的目录。spark作为ETL资源使用时必填。例如：hdfs://host:port/tmp/doris。")])]),s._v(" "),t("li",[t("p",[t("code",[s._v("broker")]),s._v(": broker 名字。spark作为ETL资源使用时必填。需要使用"),t("code",[s._v("ALTER SYSTEM ADD BROKER")]),s._v(" 命令提前完成配置。")]),s._v(" "),t("ul",[t("li",[t("code",[s._v("broker.property_key")]),s._v(": broker读取ETL生成的中间文件时需要指定的认证信息等。")])])])]),s._v(" "),t("p",[s._v("示例：")]),s._v(" "),t("div",{staticClass:"language-sql extra-class"},[t("pre",{pre:!0,attrs:{class:"language-sql"}},[t("code",[t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("-- yarn cluster 模式")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("CREATE")]),s._v(" EXTERNAL RESOURCE "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"spark0"')]),s._v("\nPROPERTIES\n"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("\n  "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"type"')]),s._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"spark"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n  "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"spark.master"')]),s._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"yarn"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n  "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"spark.submit.deployMode"')]),s._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"cluster"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n  "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"spark.jars"')]),s._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"xxx.jar,yyy.jar"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n  "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"spark.files"')]),s._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"/tmp/aaa,/tmp/bbb"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n  "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"spark.executor.memory"')]),s._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"1g"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n  "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"spark.yarn.queue"')]),s._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"queue0"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n  "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"spark.hadoop.yarn.resourcemanager.address"')]),s._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"127.0.0.1:9999"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n  "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"spark.hadoop.fs.defaultFS"')]),s._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"hdfs://127.0.0.1:10000"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n  "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"working_dir"')]),s._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"hdfs://127.0.0.1:10000/tmp/doris"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n  "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"broker"')]),s._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"broker0"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n  "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"broker.username"')]),s._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"user0"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n  "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"broker.password"')]),s._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"password0"')]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("-- spark standalone client 模式")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("CREATE")]),s._v(" EXTERNAL RESOURCE "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"spark1"')]),s._v("\nPROPERTIES\n"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("\n  "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"type"')]),s._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"spark"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n  "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"spark.master"')]),s._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"spark://127.0.0.1:7777"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n  "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"spark.submit.deployMode"')]),s._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"client"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n  "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"working_dir"')]),s._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"hdfs://127.0.0.1:10000/tmp/doris"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n  "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"broker"')]),s._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"broker1"')]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n")])])]),t("h4",{attrs:{id:"查看资源"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#查看资源"}},[s._v("#")]),s._v(" 查看资源")]),s._v(" "),t("p",[s._v("普通账户只能看到自己有USAGE_PRIV使用权限的资源。")]),s._v(" "),t("p",[s._v("root和admin账户可以看到所有的资源。")]),s._v(" "),t("h4",{attrs:{id:"资源权限"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#资源权限"}},[s._v("#")]),s._v(" 资源权限")]),s._v(" "),t("p",[s._v("资源权限通过GRANT REVOKE来管理，目前仅支持USAGE_PRIV使用权限。")]),s._v(" "),t("p",[s._v("可以将USAGE_PRIV权限赋予某个用户或者某个角色，角色的使用与之前一致。")]),s._v(" "),t("div",{staticClass:"language-sql extra-class"},[t("pre",{pre:!0,attrs:{class:"language-sql"}},[t("code",[t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("-- 授予spark0资源的使用权限给用户user0")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("GRANT")]),s._v(" USAGE_PRIV "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("ON")]),s._v(" RESOURCE "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"spark0"')]),s._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("TO")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"user0"')]),t("span",{pre:!0,attrs:{class:"token variable"}},[s._v('@"%"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("-- 授予spark0资源的使用权限给角色role0")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("GRANT")]),s._v(" USAGE_PRIV "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("ON")]),s._v(" RESOURCE "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"spark0"')]),s._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("TO")]),s._v(" ROLE "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"role0"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("-- 授予所有资源的使用权限给用户user0")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("GRANT")]),s._v(" USAGE_PRIV "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("ON")]),s._v(" RESOURCE "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("*")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("TO")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"user0"')]),t("span",{pre:!0,attrs:{class:"token variable"}},[s._v('@"%"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("-- 授予所有资源的使用权限给角色role0")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("GRANT")]),s._v(" USAGE_PRIV "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("ON")]),s._v(" RESOURCE "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("*")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("TO")]),s._v(" ROLE "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"role0"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("-- 撤销用户user0的spark0资源使用权限")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("REVOKE")]),s._v(" USAGE_PRIV "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("ON")]),s._v(" RESOURCE "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"spark0"')]),s._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("FROM")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"user0"')]),t("span",{pre:!0,attrs:{class:"token variable"}},[s._v('@"%"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n")])])]),t("h3",{attrs:{id:"配置-spark-客户端"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#配置-spark-客户端"}},[s._v("#")]),s._v(" 配置 SPARK 客户端")]),s._v(" "),t("p",[s._v("FE底层通过执行spark-submit的命令去提交spark任务，因此需要为FE配置spark客户端，建议使用2.4.5或以上的spark2官方版本，"),t("a",{attrs:{href:"https://archive.apache.org/dist/spark/",target:"_blank",rel:"noopener noreferrer"}},[s._v("spark下载地址"),t("OutboundLink")],1),s._v("，下载完成后，请按步骤完成以下配置。")]),s._v(" "),t("h4",{attrs:{id:"配置-spark-home-环境变量"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#配置-spark-home-环境变量"}},[s._v("#")]),s._v(" 配置 SPARK_HOME 环境变量")]),s._v(" "),t("p",[s._v("将spark客户端放在FE同一台机器上的目录下，并在FE的配置文件配置"),t("code",[s._v("spark_home_default_dir")]),s._v("项指向此目录，此配置项默认为FE根目录下的 "),t("code",[s._v("lib/spark2x")]),s._v("路径，此项不可为空。")]),s._v(" "),t("h4",{attrs:{id:"配置-spark-依赖包"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#配置-spark-依赖包"}},[s._v("#")]),s._v(" 配置 SPARK 依赖包")]),s._v(" "),t("p",[s._v("将spark客户端下的jars文件夹内所有jar包归档打包成一个zip文件，并在FE的配置文件配置"),t("code",[s._v("spark_resource_path")]),s._v("项指向此zip文件，若此配置项为空，则FE会尝试寻找FE根目录下的"),t("code",[s._v("lib/spark2x/jars/spark-2x.zip")]),s._v("文件，若没有找到则会报文件不存在的错误。")]),s._v(" "),t("p",[s._v("当提交spark load任务时，会将归档好的依赖文件上传至远端仓库，默认仓库路径挂在"),t("code",[s._v("working_dir/{cluster_id}")]),s._v("目录下，并以"),t("code",[s._v("__spark_repository__{resource_name}")]),s._v("命名，表示集群内的一个resource对应一个远端仓库，远端仓库目录结构参考如下:")]),s._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("__spark_repository__spark0/\n    |-__archive_1.0.0/\n    |        |-__lib_990325d2c0d1d5e45bf675e54e44fb16_spark-dpp-1.0.0-jar-with-dependencies.jar\n    |        |-__lib_7670c29daf535efe3c9b923f778f61fc_spark-2x.zip\n    |-__archive_1.1.0/\n    |        |-__lib_64d5696f99c379af2bee28c1c84271d5_spark-dpp-1.1.0-jar-with-dependencies.jar\n    |        |-__lib_1bbb74bb6b264a270bc7fca3e964160f_spark-2x.zip\n    |-__archive_1.2.0/\n    |        |-...\n")])])]),t("p",[s._v("除了spark依赖(默认以"),t("code",[s._v("spark-2x.zip")]),s._v("命名)，FE还会上传DPP的依赖包至远端仓库，若此次spark load提交的所有依赖文件都已存在远端仓库，那么就不需要在上传依赖，省下原来每次重复上传大量文件的时间。")]),s._v(" "),t("h3",{attrs:{id:"配置-yarn-客户端"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#配置-yarn-客户端"}},[s._v("#")]),s._v(" 配置 YARN 客户端")]),s._v(" "),t("p",[s._v("FE底层通过执行yarn命令去获取正在运行的application的状态以及杀死application，因此需要为FE配置yarn客户端，建议使用2.5.2或以上的hadoop2官方版本，"),t("a",{attrs:{href:"https://archive.apache.org/dist/hadoop/common/",target:"_blank",rel:"noopener noreferrer"}},[s._v("hadoop下载地址"),t("OutboundLink")],1),s._v("，下载完成后，请按步骤完成以下配置。")]),s._v(" "),t("h4",{attrs:{id:"配置-yarn-可执行文件路径"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#配置-yarn-可执行文件路径"}},[s._v("#")]),s._v(" 配置 YARN 可执行文件路径")]),s._v(" "),t("p",[s._v("将下载好的yarn客户端放在FE同一台机器的目录下，并在FE配置文件配置"),t("code",[s._v("yarn_client_path")]),s._v("项指向yarn的二进制可执行文件，默认为FE根目录下的"),t("code",[s._v("lib/yarn-client/hadoop/bin/yarn")]),s._v("路径。")]),s._v(" "),t("p",[s._v("(可选) 当FE通过yarn客户端去获取application的状态或者杀死application时，默认会在FE根目录下的"),t("code",[s._v("lib/yarn-config")]),s._v("路径下生成执行yarn命令所需的配置文件，此路径可通过在FE配置文件配置"),t("code",[s._v("yarn_config_dir")]),s._v("项修改，目前生成的配置文件包括"),t("code",[s._v("core-site.xml")]),s._v("和"),t("code",[s._v("yarn-site.xml")]),s._v("。")]),s._v(" "),t("h3",{attrs:{id:"创建导入"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#创建导入"}},[s._v("#")]),s._v(" 创建导入")]),s._v(" "),t("p",[s._v("语法：")]),s._v(" "),t("div",{staticClass:"language-sql extra-class"},[t("pre",{pre:!0,attrs:{class:"language-sql"}},[t("code",[t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("LOAD")]),s._v(" LABEL load_label\n    "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("data_desc"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("WITH")]),s._v(" RESOURCE resource_name \n    "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("resource_properties"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("PROPERTIES "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("key1"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("value1"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n\n"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("*")]),s._v(" load_label:\n\tdb_name"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("label_name\n\n"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("*")]),s._v(" data_desc:\n    "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("DATA")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("INFILE")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'file_path'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("NEGATIVE"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("INTO")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("TABLE")]),s._v(" tbl_name\n    "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("PARTITION")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("p1"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" p2"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("COLUMNS")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("TERMINATED")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("BY")]),s._v(" separator "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("col1"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("COLUMNS")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("FROM")]),s._v(" PATH "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("AS")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("col2"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("SET")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("k1"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("f1"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("xx"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" k2"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("f2"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("xx"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("WHERE")]),s._v(" predicate"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n    \n    "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("DATA")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("FROM")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("TABLE")]),s._v(" hive_external_tbl\n    "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("NEGATIVE"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("INTO")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("TABLE")]),s._v(" tbl_name\n    "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("PARTITION")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("p1"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" p2"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("SET")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("k1"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("f1"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("xx"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" k2"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("f2"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("xx"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("WHERE")]),s._v(" predicate"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n\n"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("*")]),s._v(" resource_properties:\n    "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("key2"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("value2"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])])]),t("p",[s._v("示例1：上游数据源为hdfs文件的情况")]),s._v(" "),t("div",{staticClass:"language-sql extra-class"},[t("pre",{pre:!0,attrs:{class:"language-sql"}},[t("code",[t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("LOAD")]),s._v(" LABEL db1"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("label1\n"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("DATA")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("INFILE")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"hdfs://abc.com:8888/user/palo/test/ml/file1"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("INTO")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("TABLE")]),s._v(" tbl1\n    "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("COLUMNS")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("TERMINATED")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("BY")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('","')]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("tmp_c1"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("tmp_c2"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("SET")]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("\n        id"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("tmp_c2"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n        name"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("tmp_c1\n    "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("DATA")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("INFILE")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"hdfs://abc.com:8888/user/palo/test/ml/file2"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("INTO")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("TABLE")]),s._v(" tbl2\n    "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("COLUMNS")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("TERMINATED")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("BY")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('","')]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("col1"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" col2"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("where")]),s._v(" col1 "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("WITH")]),s._v(" RESOURCE "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'spark0'")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"spark.executor.memory"')]),s._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"2g"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"spark.shuffle.compress"')]),s._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"true"')]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\nPROPERTIES\n"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"timeout"')]),s._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"3600"')]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n\n")])])]),t("p",[s._v("示例2：上游数据源是hive表的情况")]),s._v(" "),t("div",{staticClass:"language-sql extra-class"},[t("pre",{pre:!0,attrs:{class:"language-sql"}},[t("code",[s._v("step "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(":新建hive外部表\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("CREATE")]),s._v(" EXTERNAL "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("TABLE")]),s._v(" hive_t1\n"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("\n    k1 "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("INT")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n    K2 "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("SMALLINT")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n    k3 "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("varchar")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("50")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n    uuid "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("varchar")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("100")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("ENGINE")]),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("hive\nproperties\n"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"database"')]),s._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"tmp"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"table"')]),s._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"t1"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"hive.metastore.uris"')]),s._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"thrift://0.0.0.0:8080"')]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n\nstep "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),s._v(": 提交"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("load")]),s._v("命令，要求导入的 doris 表中的列必须在 hive 外部表中存在。\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("LOAD")]),s._v(" LABEL db1"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("label1\n"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("DATA")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("FROM")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("TABLE")]),s._v(" hive_t1\n    "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("INTO")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("TABLE")]),s._v(" tbl1\n    "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("SET")]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("\n\t\tuuid"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("bitmap_dict"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("uuid"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("WITH")]),s._v(" RESOURCE "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'spark0'")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"spark.executor.memory"')]),s._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"2g"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"spark.shuffle.compress"')]),s._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"true"')]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\nPROPERTIES\n"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"timeout"')]),s._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"3600"')]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n\n")])])]),t("p",[s._v("创建导入的详细语法执行 "),t("code",[s._v("HELP SPARK LOAD")]),s._v(" 查看语法帮助。这里主要介绍 Spark load 的创建导入语法中参数意义和注意事项。")]),s._v(" "),t("h4",{attrs:{id:"label"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#label"}},[s._v("#")]),s._v(" Label")]),s._v(" "),t("p",[s._v("导入任务的标识。每个导入任务，都有一个在单 database 内部唯一的 Label。具体规则与 "),t("code",[s._v("Broker Load")]),s._v(" 一致。")]),s._v(" "),t("h4",{attrs:{id:"数据描述类参数"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#数据描述类参数"}},[s._v("#")]),s._v(" 数据描述类参数")]),s._v(" "),t("p",[s._v("目前支持的数据源有CSV和hive table。其他规则与 "),t("code",[s._v("Broker Load")]),s._v(" 一致。")]),s._v(" "),t("h4",{attrs:{id:"导入作业参数"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#导入作业参数"}},[s._v("#")]),s._v(" 导入作业参数")]),s._v(" "),t("p",[s._v("导入作业参数主要指的是 Spark load 创建导入语句中的属于 "),t("code",[s._v("opt_properties")]),s._v("部分的参数。导入作业参数是作用于整个导入作业的。规则与 "),t("code",[s._v("Broker Load")]),s._v(" 一致。")]),s._v(" "),t("h4",{attrs:{id:"spark资源参数"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#spark资源参数"}},[s._v("#")]),s._v(" Spark资源参数")]),s._v(" "),t("p",[s._v("Spark资源需要提前配置到 Doris系统中并且赋予用户USAGE_PRIV权限后才能使用 Spark load。")]),s._v(" "),t("p",[s._v("当用户有临时性的需求，比如增加任务使用的资源而修改 Spark configs，可以在这里设置，设置仅对本次任务生效，并不影响 Doris 集群中已有的配置。")]),s._v(" "),t("div",{staticClass:"language-sql extra-class"},[t("pre",{pre:!0,attrs:{class:"language-sql"}},[t("code",[t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("WITH")]),s._v(" RESOURCE "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'spark0'")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("\n  "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"spark.driver.memory"')]),s._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"1g"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n  "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"spark.executor.memory"')]),s._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"3g"')]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])])]),t("h4",{attrs:{id:"数据源为hive表时的导入"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#数据源为hive表时的导入"}},[s._v("#")]),s._v(" 数据源为hive表时的导入")]),s._v(" "),t("p",[s._v("目前如果期望在导入流程中将hive表作为数据源，那么需要先新建一张类型为hive的外部表，\n然后提交导入命令时指定外部表的表名即可。")]),s._v(" "),t("h4",{attrs:{id:"导入流程构建全局字典"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#导入流程构建全局字典"}},[s._v("#")]),s._v(" 导入流程构建全局字典")]),s._v(" "),t("p",[s._v("适用于doris表聚合列的数据类型为bitmap类型。\n在load命令中指定需要构建全局字典的字段即可，格式为："),t("code",[s._v("doris字段名称=bitmap_dict(hive表字段名称)")]),s._v("\n需要注意的是目前只有在上游数据源为hive表时才支持全局字典的构建。")]),s._v(" "),t("h3",{attrs:{id:"查看导入"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#查看导入"}},[s._v("#")]),s._v(" 查看导入")]),s._v(" "),t("p",[s._v("Spark load 导入方式同 Broker load 一样都是异步的，所以用户必须将创建导入的 Label 记录，并且在"),t("strong",[s._v("查看导入命令中使用 Label 来查看导入结果")]),s._v("。查看导入命令在所有导入方式中是通用的，具体语法可执行 "),t("code",[s._v("HELP SHOW LOAD")]),s._v(" 查看。")]),s._v(" "),t("p",[s._v("示例：")]),s._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v('mysql> show load order by createtime desc limit 1\\G\n*************************** 1. row ***************************\n         JobId: 76391\n         Label: label1\n         State: FINISHED\n      Progress: ETL:100%; LOAD:100%\n          Type: SPARK\n       EtlInfo: unselected.rows=4; dpp.abnorm.ALL=15; dpp.norm.ALL=28133376\n      TaskInfo: cluster:cluster0; timeout(s):10800; max_filter_ratio:5.0E-5\n      ErrorMsg: N/A\n    CreateTime: 2019-07-27 11:46:42\n  EtlStartTime: 2019-07-27 11:46:44\n EtlFinishTime: 2019-07-27 11:49:44\n LoadStartTime: 2019-07-27 11:49:44\nLoadFinishTime: 2019-07-27 11:50:16\n           URL: http://1.1.1.1:8089/proxy/application_1586619723848_0035/\n    JobDetails: {"ScannedRows":28133395,"TaskNumber":1,"FileNumber":1,"FileSize":200000}\n')])])]),t("p",[s._v("返回结果集中参数意义可以参考 Broker load。不同点如下：")]),s._v(" "),t("ul",[t("li",[t("p",[s._v("State")]),s._v(" "),t("p",[s._v("导入任务当前所处的阶段。任务提交之后状态为 PENDING，提交 Spark ETL 之后状态变为 ETL，ETL 完成之后 FE 调度 BE 执行 push 操作状态变为 LOADING，push 完成并且版本生效后状态变为 FINISHED。")]),s._v(" "),t("p",[s._v("导入任务的最终阶段有两个：CANCELLED 和 FINISHED，当 Load job 处于这两个阶段时导入完成。其中 CANCELLED 为导入失败，FINISHED 为导入成功。")])]),s._v(" "),t("li",[t("p",[s._v("Progress")]),s._v(" "),t("p",[s._v("导入任务的进度描述。分为两种进度：ETL 和 LOAD，对应了导入流程的两个阶段 ETL 和 LOADING。")]),s._v(" "),t("p",[s._v("LOAD 的进度范围为：0~100%。")]),s._v(" "),t("p",[t("code",[s._v("LOAD 进度 = 当前已完成所有replica导入的tablet个数 / 本次导入任务的总tablet个数 * 100%")])]),s._v(" "),t("p",[t("strong",[s._v("如果所有导入表均完成导入，此时 LOAD 的进度为 99%")]),s._v(" 导入进入到最后生效阶段，整个导入完成后，LOAD 的进度才会改为 100%。")]),s._v(" "),t("p",[s._v("导入进度并不是线性的。所以如果一段时间内进度没有变化，并不代表导入没有在执行。")])]),s._v(" "),t("li",[t("p",[s._v("Type")]),s._v(" "),t("p",[s._v("导入任务的类型。Spark load 为 SPARK。")])]),s._v(" "),t("li",[t("p",[s._v("CreateTime/EtlStartTime/EtlFinishTime/LoadStartTime/LoadFinishTime")]),s._v(" "),t("p",[s._v("这几个值分别代表导入创建的时间，ETL 阶段开始的时间，ETL 阶段完成的时间，LOADING 阶段开始的时间和整个导入任务完成的时间。")])]),s._v(" "),t("li",[t("p",[s._v("JobDetails")]),s._v(" "),t("p",[s._v("显示一些作业的详细运行状态，ETL 结束的时候更新。包括导入文件的个数、总大小（字节）、子任务个数、已处理的原始行数等。")]),s._v(" "),t("p",[t("code",[s._v('{"ScannedRows":139264,"TaskNumber":1,"FileNumber":1,"FileSize":940754064}')])])]),s._v(" "),t("li",[t("p",[s._v("URL")]),s._v(" "),t("p",[s._v("可复制输入到浏览器，跳转至相应application的web界面")])])]),s._v(" "),t("h3",{attrs:{id:"查看-spark-launcher-提交日志"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#查看-spark-launcher-提交日志"}},[s._v("#")]),s._v(" 查看 spark launcher 提交日志")]),s._v(" "),t("p",[s._v("有时用户需要查看spark任务提交过程中产生的详细日志，日志默认保存在FE根目录下"),t("code",[s._v("log/spark_launcher_log")]),s._v("路径下，并以"),t("code",[s._v("spark_launcher_{load_job_id}_{label}.log")]),s._v("命名，日志会在此目录下保存一段时间，当FE元数据中的导入信息被清理时，相应的日志也会被清理，默认保存时间为3天。")]),s._v(" "),t("h3",{attrs:{id:"取消导入"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#取消导入"}},[s._v("#")]),s._v(" 取消导入")]),s._v(" "),t("p",[s._v("当 Spark load 作业状态不为 CANCELLED 或 FINISHED 时，可以被用户手动取消。取消时需要指定待取消导入任务的 Label 。取消导入命令语法可执行 "),t("code",[s._v("HELP CANCEL LOAD")]),s._v("查看。")]),s._v(" "),t("h2",{attrs:{id:"相关系统配置"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#相关系统配置"}},[s._v("#")]),s._v(" 相关系统配置")]),s._v(" "),t("h3",{attrs:{id:"fe-配置"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#fe-配置"}},[s._v("#")]),s._v(" FE 配置")]),s._v(" "),t("p",[s._v("下面配置属于 Spark load 的系统级别配置，也就是作用于所有 Spark load 导入任务的配置。主要通过修改 "),t("code",[s._v("fe.conf")]),s._v("来调整配置值。")]),s._v(" "),t("ul",[t("li",[t("p",[t("code",[s._v("enable_spark_load")])]),s._v(" "),t("p",[s._v("开启 Spark load 和创建 resource 功能。默认为 false，关闭此功能。")])]),s._v(" "),t("li",[t("p",[t("code",[s._v("spark_load_default_timeout_second")])]),s._v(" "),t("p",[s._v("任务默认超时时间为259200秒（3天）。")])]),s._v(" "),t("li",[t("p",[t("code",[s._v("spark_home_default_dir")])]),s._v(" "),t("p",[s._v("spark客户端路径 ("),t("code",[s._v("fe/lib/spark2x")]),s._v(") 。")])]),s._v(" "),t("li",[t("p",[t("code",[s._v("spark_resource_path")])]),s._v(" "),t("p",[s._v("打包好的spark依赖文件路径（默认为空）。")])]),s._v(" "),t("li",[t("p",[t("code",[s._v("spark_launcher_log_dir")])]),s._v(" "),t("p",[s._v("spark客户端的提交日志存放的目录（"),t("code",[s._v("fe/log/spark_launcher_log")]),s._v("）。")])]),s._v(" "),t("li",[t("p",[t("code",[s._v("yarn_client_path")])]),s._v(" "),t("p",[s._v("yarn二进制可执行文件路径 ("),t("code",[s._v("fe/lib/yarn-client/hadoop/bin/yarn")]),s._v(") 。")])]),s._v(" "),t("li",[t("p",[t("code",[s._v("yarn_config_dir")])]),s._v(" "),t("p",[s._v("yarn配置文件生成路径 ("),t("code",[s._v("fe/lib/yarn-config")]),s._v(") 。")])])]),s._v(" "),t("h2",{attrs:{id:"最佳实践"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#最佳实践"}},[s._v("#")]),s._v(" 最佳实践")]),s._v(" "),t("h3",{attrs:{id:"应用场景"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#应用场景"}},[s._v("#")]),s._v(" 应用场景")]),s._v(" "),t("p",[s._v("使用 Spark load 最适合的场景就是原始数据在文件系统（HDFS）中，数据量在 几十 GB 到 TB 级别。小数据量还是建议使用 Stream load 或者 Broker load。")]),s._v(" "),t("h2",{attrs:{id:"常见问题"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#常见问题"}},[s._v("#")]),s._v(" 常见问题")]),s._v(" "),t("ul",[t("li",[s._v("使用Spark load时没有在spark客户端的spark-env.sh配置"),t("code",[s._v("HADOOP_CONF_DIR")]),s._v("环境变量。")])]),s._v(" "),t("p",[s._v("如果"),t("code",[s._v("HADOOP_CONF_DIR")]),s._v("环境变量没有设置，会报 "),t("code",[s._v("When running with master 'yarn' either HADOOP_CONF_DIR or YARN_CONF_DIR must be set in the environment.")]),s._v(" 错误。")]),s._v(" "),t("ul",[t("li",[s._v("使用Spark load时"),t("code",[s._v("spark_home_default_dir")]),s._v("配置项没有指定spark客户端根目录。")])]),s._v(" "),t("p",[s._v("提交Spark job时用到spark-submit命令，如果"),t("code",[s._v("spark_home_default_dir")]),s._v("设置错误，会报 "),t("code",[s._v('Cannot run program "xxx/bin/spark-submit": error=2, No such file or directory')]),s._v(" 错误。")]),s._v(" "),t("ul",[t("li",[s._v("使用Spark load时"),t("code",[s._v("spark_resource_path")]),s._v("配置项没有指向打包好的zip文件。")])]),s._v(" "),t("p",[s._v("如果"),t("code",[s._v("spark_resource_path")]),s._v("没有设置正确，会报"),t("code",[s._v("File xxx/jars/spark-2x.zip does not exist")]),s._v(" 错误。")]),s._v(" "),t("ul",[t("li",[s._v("使用Spark load时"),t("code",[s._v("yarn_client_path")]),s._v("配置项没有指定yarn的可执行文件。")])]),s._v(" "),t("p",[s._v("如果"),t("code",[s._v("yarn_client_path")]),s._v("没有设置正确，会报"),t("code",[s._v("yarn client does not exist in path: xxx/yarn-client/hadoop/bin/yarn")]),s._v(" 错误")])])}),[],!1,null,null,null);a.default=n.exports}}]);