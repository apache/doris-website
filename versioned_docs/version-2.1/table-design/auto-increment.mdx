---
{
    "title": "Auto-Increment Column",
    "language": "en"
}
---

<!-- 
Licensed to the Apache Software Foundation (ASF) under one
or more contributor license agreements.  See the NOTICE file
distributed with this work for additional information
regarding copyright ownership.  The ASF licenses this file
to you under the Apache License, Version 2.0 (the
"License"); you may not use this file except in compliance
with the License.  You may obtain a copy of the License at

  http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing,
software distributed under the License is distributed on an
"AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
KIND, either express or implied.  See the License for the
specific language governing permissions and limitations
under the License.
-->

In Doris, the auto increment column is a feature that automatically generates a unique numeric value, commonly used to create unique identifiers for each row of data, such as primary keys. Each time a new record is inserted, the auto increment column automatically assigns an incrementing value, eliminating the need for manually specifying the number. By using Doris's auto increment column, data uniqueness and consistency are ensured, simplifying the data insertion process, reducing human error, and improving data management efficiency. This makes the auto increment column an ideal choice for scenarios requiring unique identifiers, such as user IDs and more.

## Functionality

For tables with an auto-increment column, Doris processes data writes as follows:

- **Auto-Population (Column Excluded)**:
  If the written data does not include the auto-increment column, Doris generates and populates unique values for this column.

- **Partial Specification (Column Included)**:

  - **Null Values**: Doris replaces null values in the written data with system-generated unique values.

  - **Non-Null Values**: User-provided values remain unchanged.
  
  :::caution Attention
  User-provided non-null values can disrupt the uniqueness of the auto-increment column.
  :::
  
### Uniqueness

Doris guarantees **table-wide uniqueness** for values it generates in the auto-increment column. However:

- **Guaranteed Uniqueness**: This applies only to system-generated values.

- **User-Provided Values**: Doris does not validate or enforce uniqueness for values specified by users in the auto-increment column. This may result in duplicate entries.

### Density

Auto-increment values generated by Doris are generally **dense** but with some considerations:

- **Potential Gaps**: Gaps may appear due to performance optimizations. Each backend node (BE) pre-allocates a block of unique values for efficiency, and these blocks do not overlap between nodes.

- **Non-Chronological Values**: Doris does not guarantee that values generated in later writes are larger than those from earlier writes.

  :::info Note
  Auto-increment values cannot be used to infer the chronological order of writes.
  :::
  
## Syntax

To use auto-increment columns, you need to add the `AUTO_INCREMENT` attribute to the corresponding column during table creation ([CREATE-TABLE](../sql-manual/sql-statements/table-and-view/table/CREATE-TABLE)). To manually specify the starting value for an auto-increment column, you can do so by using the `AUTO_INCREMENT(start_value)` statement when creating the table. If not specified, the default starting value is 1.

### Examples

1. **Creating a duplicate table with an auto-increment column as the key column**

   ```sql
   CREATE TABLE `demo`.`tbl` (
           `id` BIGINT NOT NULL AUTO_INCREMENT,
           `value` BIGINT NOT NULL
   ) ENGINE=OLAP
   DUPLICATE KEY(`id`)
   DISTRIBUTED BY HASH(`id`) BUCKETS 10;
   ```

2. **Creating a duplicate table with an auto-increment column as the key column, and setting the starting value to 100**

   ```sql
   CREATE TABLE `demo`.`tbl` (
         `id` BIGINT NOT NULL AUTO_INCREMENT(100),
         `value` BIGINT NOT NULL
   ) ENGINE=OLAP
   DUPLICATE KEY(`id`)
   DISTRIBUTED BY HASH(`id`) BUCKETS 10;
   ```

3. **Creating a duplicate table with an auto-increment column as one of the value columns**

   ```sql
   CREATE TABLE `demo`.`tbl` (
       `uid` BIGINT NOT NULL,
       `name` BIGINT NOT NULL,
       `id` BIGINT NOT NULL AUTO_INCREMENT,
       `value` BIGINT NOT NULL
   ) ENGINE=OLAP
   DUPLICATE KEY(`uid`, `name`)
   DISTRIBUTED BY HASH(`uid`) BUCKETS 10;
   ```

4. **Creating a unique table with an auto-increment column as the key column**

   ```sql
   CREATE TABLE `demo`.`tbl` (
         `id` BIGINT NOT NULL AUTO_INCREMENT,
         `name` varchar(65533) NOT NULL,
         `value` int(11) NOT NULL
   ) ENGINE=OLAP
   UNIQUE KEY(`id`)
   DISTRIBUTED BY HASH(`id`) BUCKETS 10
   PROPERTIES (
          "enable_unique_key_merge_on_write" = "true"
   );
   ```

5. **Creating a unique table with an auto-increment column as one of the value columns**

   ```sql
   CREATE TABLE `demo`.`tbl` (
         `text` varchar(65533) NOT NULL,
         `id` BIGINT NOT NULL AUTO_INCREMENT,
   ) ENGINE=OLAP
   UNIQUE KEY(`text`)
   DISTRIBUTED BY HASH(`text`) BUCKETS 10
   PROPERTIES (
          "enable_unique_key_merge_on_write" = "true"
   );
   ```

### Constraints and Limitations

- Auto-increment columns can only be used in Duplicate or Unique model tables.

- A table can have only one auto-increment column.

- The auto-increment column must be of type `BIGINT` and cannot be `NULL`.

- The manually specified starting value for an auto-increment column must be 0 or greater.

## Usage

### Loading

1. **Create the table to be imported**

    ```sql
    CREATE TABLE `demo`.`tbl` (
        `id` BIGINT NOT NULL AUTO_INCREMENT,
        `name` varchar(65533) NOT NULL,
        `value` int(11) NOT NULL
    ) ENGINE=OLAP
    UNIQUE KEY(`id`)
    DISTRIBUTED BY HASH(`id`) BUCKETS 10
    PROPERTIES (
        "enable_unique_key_merge_on_write" = "true"
    );
    ```

2. Automatically populate the auto-increment column through import


   <Tabs>
     <TabItem value="Insert using Insert Into" label="Insert using Insert Into" default>
       <div>
   When using the INSERT INTO statement to import data without specifying the auto-increment column id, the id column will be automatically populated with generated values.

   ```sql
   insert into tbl(name, value) values("Bob", 10), ("Alice", 20), ("Jack", 30);
   Query OK, 3 rows affected (0.09 sec)
   {'label':'label_183babcb84ad4023_a2d6266ab73fb5aa', 'status':'VISIBLE', 'txnId':'7'}

   select * from tbl order by id;
   +------+-------+-------+
   | id   | name  | value |
   +------+-------+-------+
   |    1 | Bob   |    10 |
   |    2 | Alice |    20 |
   |    3 | Jack  |    30 |
   +------+-------+-------+
   3 rows in set (0.05 sec)
   ```

   When specifying the auto-increment column `id` during an INSERT INTO import, the null values in that column will be replaced with the generated values.

   ```sql
   insert into tbl(id, name, value) values(null, "Doris", 60), (null, "Nereids", 70);
   Query OK, 2 rows affected (0.07 sec)
   {'label':'label_9cb0c01db1a0402c_a2b8b44c11ce4703', 'status':'VISIBLE', 'txnId':'10'}

   select * from tbl order by id;
   +------+---------+-------+
   | id   | name    | value |
   +------+---------+-------+
   |    1 | Bob     |    10 |
   |    2 | Alice   |    20 |
   |    3 | Jack    |    30 |
   |    4 | Tom     |    40 |
   |    5 | John    |    50 |
   |    6 | Doris   |    60 |
   |    7 | Nereids |    70 |
   +------+---------+-------+
   7 rows in set (0.04 sec)
   ```
       </div>

     </TabItem>
     <TabItem value="Insert using Stream Load" label="Insert using Stream Load" default>
       <div>
   When using Stream Load to import the file test.csv without specifying the auto-increment column `id`, the `id` column will be automatically populated with generated values.

   ```
   ## create load csv file
   cat test.csv:
   Tom,40
   John,50
   
   ## stream load
   curl --location-trusted -u user:passwd -H "columns:name,value" -H "column_separator:," -T ./test.csv http://{host}:{port}/api/{db}/tbl/_stream_load

   ## check loading result
   mysql> select * from tbl order by id;
   +------+-------+-------+
   | id   | name  | value |
   +------+-------+-------+
   |    1 | Bob   |    10 |
   |    2 | Alice |    20 |
   |    3 | Jack  |    30 |
   |    4 | Tom   |    40 |
   |    5 | John  |    50 |
   +------+-------+-------+
   5 rows in set (0.04 sec)
   ```
       </div>
     </TabItem>
   </Tabs>


### Partial Update

When performing a partial update on a merge-on-write Unique table with an auto-increment column:

<Tabs>
  <TabItem value="Auto-increment column is the key column" label="Auto-increment column is the key column" default>
    <div>
    When performing partial column updates on a merge-on-write Unique table that contains an auto-increment column, if the auto-increment column is the key column, the key column must be explicitly specified by the user during partial column updates. The target columns for the partial update must include the auto-increment column. The import behavior in this case is the same as a regular partial column update.

    1. **Create the table to be imported and import the test data**

       ```sql
       CREATE TABLE `demo`.`tbl2` (
           `id` BIGINT NOT NULL AUTO_INCREMENT,
            `name` varchar(65533) NOT NULL,
            `value` int(11) NOT NULL DEFAULT "0"
       ) ENGINE=OLAP
       UNIQUE KEY(`id`)
       DISTRIBUTED BY HASH(`id`) BUCKETS 10
       PROPERTIES (
           "enable_unique_key_merge_on_write" = "true"
       );
       Query OK, 0 rows affected (0.03 sec)

       insert into tbl2(id, name, value) values(1, "Bob", 10), (2, "Alice", 20), (3, "Jack", 30);
       Query OK, 3 rows affected (0.14 sec)
       {'label':'label_5538549c866240b6_bce75ef323ac22a0', 'status':'VISIBLE', 'txnId':'1004'}

       select * from tbl2 order by id;
       +------+-------+-------+
       | id   | name  | value |
       +------+-------+-------+
       |    1 | Bob   |    10 |
       |    2 | Alice |    20 |
       |    3 | Jack  |    30 |
       +------+-------+-------+
       3 rows in set (0.08 sec)
       ```

    2. **Enable partial column update feature and disable strict mode**

       ```
       set enable_unique_key_partial_update=true;
       Query OK, 0 rows affected (0.01 sec)

       set enable_insert_strict=false;
       Query OK, 0 rows affected (0.00 sec)
       ```

    3. **Update data using the partial column update feature**

       ``` 
       insert into tbl2(id, name) values(1, "modified"), (4, "added");
       Query OK, 2 rows affected (0.06 sec)
       {'label':'label_3e68324cfd87457d_a6166cc0a878cfdc', 'status':'VISIBLE', 'txnId':'1005'}

       select * from tbl2 order by id;
       +------+----------+-------+
       | id   | name     | value |
       +------+----------+-------+
       |    1 | modified |    10 |
       |    2 | Alice    |    20 |
       |    3 | Jack     |    30 |
       |    4 | added    |     0 |
       +------+----------+-------+
       4 rows in set (0.04 sec)
       ```

    </div>
  </TabItem>
  <TabItem value="Auto-increment column is the value column" label="Auto-increment column is the value column" default>
    <div>
    When the auto-increment column is a value column, if the user does not specify a value for the auto-increment column, its value will be filled in from the existing rows in the table. If the user specifies the auto-increment column, any null values in that column will be replaced with the generated value, while non-null values will remain unchanged. The data will then be inserted into the table using the partial column update semantics.

    1. **Create the table to be imported and insert test data**

        ```sql
        CREATE TABLE `demo`.`tbl3` (
            `id` BIGINT NOT NULL,
            `name` varchar(100) NOT NULL,
            `score` BIGINT NOT NULL,
            `aid` BIGINT NOT NULL AUTO_INCREMENT
        ) ENGINE=OLAP
        UNIQUE KEY(`id`)
        DISTRIBUTED BY HASH(`id`) BUCKETS 1
        PROPERTIES (
            "enable_unique_key_merge_on_write" = "true"
        );
        Query OK, 0 rows affected (0.16 sec)

        insert into tbl3(id, name, score) values(1, "Doris", 100), (2, "Nereids", 200), (3, "Bob", 300);
        Query OK, 3 rows affected (0.28 sec)
        {'label':'label_c52b2c246e244dda_9b91ee5e27a31f9b', 'status':'VISIBLE', 'txnId':'2003'}

        select * from tbl3 order by id;
        +------+---------+-------+------+
        | id   | name    | score | aid  |
        +------+---------+-------+------+
        |    1 | Doris   |   100 |    0 |
        |    2 | Nereids |   200 |    1 |
        |    3 | Bob     |   300 |    2 |
        +------+---------+-------+------+
        3 rows in set (0.13 sec)
        ```

    2. **Enable the partial column update feature and disable strict mode**

        ```sql
        set enable_unique_key_partial_update=true;
        Query OK, 0 rows affected (0.00 sec)

        set enable_insert_strict=false;
        Query OK, 0 rows affected (0.00 sec)
        ```

    3. **Insert data using the partial column update feature**
    
        ```sql
        insert into tbl3(id, score) values(1, 999), (2, 888);
        Query OK, 2 rows affected (0.07 sec)
        {'label':'label_dfec927d7a4343ca_9f9ade581391de97', 'status':'VISIBLE', 'txnId':'2004'}

        select * from tbl3 order by id;
        +------+---------+-------+------+
        | id   | name    | score | aid  |
        +------+---------+-------+------+
        |    1 | Doris   |   999 |    0 |
        |    2 | Nereids |   888 |    1 |
        |    3 | Bob     |   300 |    2 |
        +------+---------+-------+------+
        3 rows in set (0.06 sec)

        insert into tbl3(id, aid) values(1, 1000), (3, 500);
        Query OK, 2 rows affected (0.07 sec)
        {'label':'label_b26012959f714f60_abe23c87a06aa0bf', 'status':'VISIBLE', 'txnId':'2005'}

        select * from tbl3 order by id;
        +------+---------+-------+------+
        | id   | name    | score | aid  |
        +------+---------+-------+------+
        |    1 | Doris   |   999 | 1000 |
        |    2 | Nereids |   888 |    1 |
        |    3 | Bob     |   300 |  500 |
        +------+---------+-------+------+
        3 rows in set (0.06 sec)
        ```
    </div>
  </TabItem>
</Tabs>


## Usage Scenarios

### Dictionary Encoding

Using bitmaps for audience analysis in user profiling involves creating a user dictionary, where each user is assigned a unique integer as their dictionary value. Aggregating these dictionary values can improve the performance of bitmap operations.

For example, in an offline UV (Unique Visitors) and PV (Page Views) analysis scenario, consider a detailed user behavior table:

1. **Create the User Behavior Table**

    ```sql
    CREATE TABLE `demo`.`dwd_dup_tbl` (
        `user_id` varchar(50) NOT NULL,
        `dim1` varchar(50) NOT NULL,
        `dim2` varchar(50) NOT NULL,
        `dim3` varchar(50) NOT NULL,
        `dim4` varchar(50) NOT NULL,
        `dim5` varchar(50) NOT NULL,
        `visit_time` DATE NOT NULL
    ) ENGINE=OLAP
    DUPLICATE KEY(`user_id`)
    DISTRIBUTED BY HASH(`user_id`) BUCKETS 32;
    ```

2. **Create the Dictionary Table Using Auto-Increment Column**

    ```sql
    CREATE TABLE `demo`.`dictionary_tbl` (
        `user_id` varchar(50) NOT NULL,
        `aid` BIGINT NOT NULL AUTO_INCREMENT
    ) ENGINE=OLAP
    UNIQUE KEY(`user_id`)
    DISTRIBUTED BY HASH(`user_id`) BUCKETS 32
    PROPERTIES (
        "enable_unique_key_merge_on_write" = "true"
    );
    ```

3. **Import Data into the Dictionary Table**

    Import `user_id` from the existing data into the dictionary table to establish a mapping of user_id to integer values:

    ```sql
    insert into dictionary_tbl(user_id)
    select user_id from dwd_dup_tbl group by user_id;
    ```

    Alternatively, use the following method to only import the incremental `user_id` data into the dictionary table:

    ```sql
    insert into dictionary_tbl(user_id)
    select dwd_dup_tbl.user_id from dwd_dup_tbl left join dictionary_tbl
    on dwd_dup_tbl.user_id = dictionary_tbl.user_id where dwd_dup_tbl.visit_time > '2023-12-10' and dictionary_tbl.user_id is NULL;
    ```

    In real-world scenarios, you can also use the Flink connector to write the data to Doris.

4. **Create an Aggregation Table to Store Aggregated Results**

    Assuming dim1, dim3, and dim5 are the dimensions we are interested in for statistics, we can create the following aggregation table:

    ```sql
    CREATE TABLE `demo`.`dws_agg_tbl` (
        `dim1` varchar(50) NOT NULL,
        `dim3` varchar(50) NOT NULL,
        `dim5` varchar(50) NOT NULL,
        `user_id_bitmap` BITMAP BITMAP_UNION NOT NULL,
        `pv` BIGINT SUM NOT NULL 
    ) ENGINE=OLAP
    AGGREGATE KEY(`dim1`,`dim3`,`dim5`)
    DISTRIBUTED BY HASH(`dim1`) BUCKETS 32;
    ```

    After performing the aggregation, insert the results into the aggregation table:

    ```sql
    insert into dws_agg_tbl
    select dwd_dup_tbl.dim1, dwd_dup_tbl.dim3, dwd_dup_tbl.dim5, BITMAP_UNION(TO_BITMAP(dictionary_tbl.aid)), COUNT(1)
    from dwd_dup_tbl INNER JOIN dictionary_tbl on dwd_dup_tbl.user_id = dictionary_tbl.user_id
    group by dwd_dup_tbl.dim1, dwd_dup_tbl.dim3, dwd_dup_tbl.dim5;
    ```

5. **Query UV and PV Results**
 
    Use the following query to retrieve UV and PV:

    ```sql
    select dim1, dim3, dim5, bitmap_count(user_id_bitmap) as uv, pv from dws_agg_tbl;
    ```


### Efficient Pagination

Pagination is often required when displaying data on a page. Traditional pagination usually involves using `LIMIT`, `OFFSET`, and `ORDER BY` in SQL queries. For example, consider the following business table designed for display:

- **Regular Paging Query Method**

    Create a pagination table:

    ```sql
        CREATE TABLE `demo`.`records_tbl` (
        `user_id` int(11) NOT NULL COMMENT "",
        `name` varchar(26) NOT NULL COMMENT "",
        `address` varchar(41) NOT NULL COMMENT "",
        `city` varchar(11) NOT NULL COMMENT "",
        `nation` varchar(16) NOT NULL COMMENT "",
        `region` varchar(13) NOT NULL COMMENT "",
        `phone` varchar(16) NOT NULL COMMENT "",
        `mktsegment` varchar(11) NOT NULL COMMENT ""
    ) DUPLICATE KEY (`user_id`, `name`)
    DISTRIBUTED BY HASH(`user_id`) BUCKETS 10;
    ```

    Execute a paging query, with the example showing 100 rows per page:

    ```sql
    -- Display the data on the first page
    select * from records_tbl order by user_id, name limit 100;

    -- Display the data on the second page
    select * from records_tbl order by user_id, name limit 100 offset 100;
    ```
  
- **Optimize Paging Queries with Auto-Increment Column**

    When performing deep paging queries (with a large offset), even if the actual data needed is small, this method still reads all the data into memory for full sorting before proceeding with further processing, which can be inefficient. By using an auto-increment column to give each row a unique value, the maximum `unique_value` from the previous page's results can be used to filter out large amounts of data efficiently in the query. This allows for more efficient paging by using `where unique_value > max_value limit rows_per_page` to push down predicates.

    Using the same example business table, we can add an auto-increment column to the table to assign a unique identifier to each row:

    ```sql
    CREATE TABLE `demo`.`records_tbl2` (
        `user_id` int(11) NOT NULL COMMENT "",
        `name` varchar(26) NOT NULL COMMENT "",
        `address` varchar(41) NOT NULL COMMENT "",
        `city` varchar(11) NOT NULL COMMENT "",
        `nation` varchar(16) NOT NULL COMMENT "",
        `region` varchar(13) NOT NULL COMMENT "",
        `phone` varchar(16) NOT NULL COMMENT "",
        `mktsegment` varchar(11) NOT NULL COMMENT "",
        `unique_value` BIGINT NOT NULL AUTO_INCREMENT
    ) DUPLICATE KEY (`user_id`, `name`)
    DISTRIBUTED BY HASH(`user_id`) BUCKETS 10;
    ```

    Execute the paging query, with the example showing 100 rows per page:

    ```sql
    -- Display the data on the first page
    select * from records_tbl2 order by unique_value limit 100;

    -- By programmatically recording the maximum value in unique_value from the returned results, 
    -- assuming it is 99, you can query the data on the second page using the following method.
    select * from records_tbl2 where unique_value > 99 order by unique_value limit 100;

    ```

    If you need to query data from a later page and it's inconvenient to directly access the maximum `unique_value` from the previous page’s data, for example, to directly retrieve the content from page 101, you can use the following query:

    ```sql
    select user_id, name, address, city, nation, region, phone, mktsegment
    from records_tbl2, (select unique_value as max_value from records_tbl2 order by unique_value limit 1 offset 9999) as previous_data
    where records_tbl2.unique_value > previous_data.max_value
    order by unique_value limit 100;
    ```
