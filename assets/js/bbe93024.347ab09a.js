"use strict";(self.webpackChunkdoris_website=self.webpackChunkdoris_website||[]).push([["249287"],{573365:function(e,n,a){a.r(n),a.d(n,{default:()=>h,frontMatter:()=>r,metadata:()=>s,assets:()=>d,toc:()=>o,contentTitle:()=>l});var s=JSON.parse('{"id":"data-operate/import/import-way/routine-load-manual","title":"Routine Load","description":"Doris can continuously consume data from Kafka Topic through the Routine Load method. After submitting a Routine Load job,","source":"@site/versioned_docs/version-2.1/data-operate/import/import-way/routine-load-manual.md","sourceDirName":"data-operate/import/import-way","slug":"/data-operate/import/import-way/routine-load-manual","permalink":"/docs/2.1/data-operate/import/import-way/routine-load-manual","draft":false,"unlisted":false,"tags":[],"version":"2.1","lastUpdatedAt":1770477659000,"frontMatter":{"title":"Routine Load","language":"en","description":"Doris can continuously consume data from Kafka Topic through the Routine Load method. After submitting a Routine Load job,"},"sidebar":"docs","previous":{"title":"Broker Load","permalink":"/docs/2.1/data-operate/import/import-way/broker-load-manual"},"next":{"title":"Insert Into Select","permalink":"/docs/2.1/data-operate/import/import-way/insert-into-manual"}}'),i=a("785893"),t=a("250065");let r={title:"Routine Load",language:"en",description:"Doris can continuously consume data from Kafka Topic through the Routine Load method. After submitting a Routine Load job,"},l=void 0,d={},o=[{value:"Usage Scenarios",id:"usage-scenarios",level:2},{value:"Supported Data Sources",id:"supported-data-sources",level:3},{value:"Supported Data File Formats",id:"supported-data-file-formats",level:3},{value:"Usage Limitations",id:"usage-limitations",level:3},{value:"Basic Principles",id:"basic-principles",level:2},{value:"Auto Resume",id:"auto-resume",level:3},{value:"FAQ",id:"faq",level:3},{value:"Quick Start",id:"quick-start",level:2},{value:"Create Job",id:"create-job",level:3},{value:"Viewing Status",id:"viewing-status",level:3},{value:"Pausing Jobs",id:"pausing-jobs",level:3},{value:"Resuming Jobs",id:"resuming-jobs",level:3},{value:"Modifying Jobs",id:"modifying-jobs",level:3},{value:"Canceling Jobs",id:"canceling-jobs",level:3},{value:"Reference Manual",id:"reference-manual",level:2},{value:"Load Commands",id:"load-commands",level:3},{value:"Load Parameter Description",id:"load-parameter-description",level:3},{value:"Load Configuration Parameters",id:"load-configuration-parameters",level:3},{value:"Load Status",id:"load-status",level:3},{value:"Load example",id:"load-example",level:2},{value:"Setting the Maximum Error Tolerance",id:"setting-the-maximum-error-tolerance",level:3},{value:"Consuming Data from a Specified Offset",id:"consuming-data-from-a-specified-offset",level:3},{value:"Specifying the Consumer Group&#39;s group.id and client.id",id:"specifying-the-consumer-groups-groupid-and-clientid",level:3},{value:"Setting load filtering conditions",id:"setting-load-filtering-conditions",level:3},{value:"Loading specified partition data",id:"loading-specified-partition-data",level:3},{value:"Setting Time Zone for load",id:"setting-time-zone-for-load",level:3},{value:"Setting merge_type",id:"setting-merge_type",level:3},{value:"Load with column mapping and derived column calculation",id:"load-with-column-mapping-and-derived-column-calculation",level:3},{value:"Load with enclosed data",id:"load-with-enclosed-data",level:3},{value:"JSON Format Load",id:"json-format-load",level:3},{value:"Loading Complex Data Types",id:"loading-complex-data-types",level:3},{value:"Kafka Security Authentication",id:"kafka-security-authentication",level:3},{value:"Single-task Loading to Multiple Tables",id:"single-task-loading-to-multiple-tables",level:3},{value:"Strict Mode Load",id:"strict-mode-load",level:3},{value:"Connect to the SASL Kafka service",id:"connect-to-the-sasl-kafka-service",level:2},{value:"More Details",id:"more-details",level:2}];function c(e){let n={a:"a",admonition:"admonition",annotation:"annotation",code:"code",h2:"h2",h3:"h3",img:"img",li:"li",math:"math",mi:"mi",mo:"mo",mrow:"mrow",ol:"ol",p:"p",pre:"pre",semantics:"semantics",span:"span",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.a)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.p,{children:"Doris can continuously consume data from Kafka Topic through the Routine Load method. After submitting a Routine Load job, Doris will continuously run the load job, generating real-time loading tasks to constantly consume messages from the specified Topic in the Kafka cluster."}),"\n",(0,i.jsx)(n.p,{children:"Routine Load is a streaming load job that supports Exactly-Once semantics, ensuring that data is neither lost nor duplicated."}),"\n",(0,i.jsx)(n.h2,{id:"usage-scenarios",children:"Usage Scenarios"}),"\n",(0,i.jsx)(n.h3,{id:"supported-data-sources",children:"Supported Data Sources"}),"\n",(0,i.jsx)(n.p,{children:"Routine Load supports consuming data from Kafka clusters."}),"\n",(0,i.jsx)(n.h3,{id:"supported-data-file-formats",children:"Supported Data File Formats"}),"\n",(0,i.jsx)(n.p,{children:"Routine Load supports consuming data in CSV and JSON formats."}),"\n",(0,i.jsx)(n.p,{children:"When loading CSV format, it is necessary to clearly distinguish between null values and empty strings:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["Null values need to be represented with ",(0,i.jsx)(n.code,{children:"\\n"}),". For example, ",(0,i.jsx)(n.code,{children:"a,\\n,b"})," indicates that the middle column is a null value."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["Empty strings can be represented by leaving the data field empty. For example, ",(0,i.jsx)(n.code,{children:"a,,b"})," indicates that the middle column is an empty string."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"usage-limitations",children:"Usage Limitations"}),"\n",(0,i.jsx)(n.p,{children:"When using Routine Load to consume data from Kafka, there are the following limitations:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"The supported message formats are CSV and JSON text formats. Each message in CSV should be on a separate line, and the line should not end with a newline character."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["By default, it supports Kafka versions 0.10.0.0 and above. If you need to use a Kafka version below 0.10.0.0 (such as 0.9.0, 0.8.2, 0.8.1, 0.8.0), you need to modify the BE configuration by setting the value of ",(0,i.jsx)(n.code,{children:"kafka_broker_version_fallback"})," to the compatible older version, or directly set the value of ",(0,i.jsx)(n.code,{children:"property.broker.version.fallback"})," when creating the Routine Load. However, using an older version may mean that some new features of Routine Load, such as setting the offset of Kafka partitions based on time, may not be available."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"basic-principles",children:"Basic Principles"}),"\n",(0,i.jsx)(n.p,{children:"Routine Load continuously consumes data from Kafka Topics and writes it into Doris."}),"\n",(0,i.jsx)(n.p,{children:"When a Routine Load job is created in Doris, it generates a resident import job that consists of several import tasks:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Load Job: A Routine Load Job is a resident import job that continuously consumes data from the data source."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Load Task: An import job is broken down into several import tasks for actual consumption, with each task being an independent transaction."}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"The specific import process of Routine Load is shown in the following diagram:"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"Routine Load",src:a(113865).Z+"",width:"2560",height:"1280"})}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"The Client submits a request to create a Routine Load job to the FE, and the FE generates a resident import job (Routine Load Job) through the Routine Load Manager."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"The FE splits the Routine Load Job into several Routine Load Tasks through the Job Scheduler, which are then scheduled by the Task Scheduler and distributed to BE nodes."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"On the BE, after a Routine Load Task is completed, it submits the transaction to the FE and updates the Job's metadata."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"After a Routine Load Task is submitted, it continues to generate new Tasks or retries timed-out Tasks."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"The newly generated Routine Load Tasks continue to be scheduled by the Task Scheduler in a continuous cycle."}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"auto-resume",children:"Auto Resume"}),"\n",(0,i.jsx)(n.p,{children:"To ensure high availability of jobs, an auto-resume mechanism has been introduced. In the event of an unexpected pause, the Routine Load Scheduler thread will attempt to auto-resume the job. For unexpected Kafka outages or other scenarios where the system is unable to function, the auto-resume mechanism ensures that once Kafka is restored, the routine load job can continue running normally without manual intervention."}),"\n",(0,i.jsx)(n.p,{children:"Situations where auto-resume will not occur:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"The user manually executes the PAUSE ROUTINE LOAD command."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"There are issues with data quality."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Situations where resuming is not possible, such as when a database table is deleted."}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Apart from these three situations, other paused jobs will attempt to resume automatically."}),"\n",(0,i.jsx)(n.h3,{id:"faq",children:"FAQ"}),"\n",(0,i.jsx)(n.p,{children:"Auto-resume may encounter some issues during cluster restarts or upgrades. Before version 2.1.7, there was a high probability that tasks would not automatically resume after being paused due to cluster restarts or upgrades. Since version 2.1.7, the likelihood of tasks not resuming automatically after such events has decreased."}),"\n",(0,i.jsx)(n.h2,{id:"quick-start",children:"Quick Start"}),"\n",(0,i.jsx)(n.h3,{id:"create-job",children:"Create Job"}),"\n",(0,i.jsxs)(n.p,{children:["In Doris, you can create persistent Routine Load  tasks using the ",(0,i.jsx)(n.code,{children:"CREATE ROUTINE LOAD"})," command. For detailed syntax, please refer to ",(0,i.jsx)(n.a,{href:"../../../sql-manual/sql-statements/data-modification/load-and-export/CREATE-ROUTINE-LOAD",children:"CREATE ROUTINE LOAD"}),". Routine Load supports consuming data in CSV and JSON formats."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Loading CSV Data"})}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Loading data sample"}),"\n",(0,i.jsx)(n.p,{children:"In Kafka, there is the following sample data:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-SQL",children:"kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test-routine-load-csv --from-beginning\n1,Emily,25\n2,Benjamin,35\n3,Olivia,28\n4,Alexander,60\n5,Ava,17\n6,William,69\n7,Sophia,32\n8,James,64\n9,Emma,37\n10,Liam,64\n"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Creating table"}),"\n",(0,i.jsx)(n.p,{children:"In Doris, create the table for loading with the following syntax:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'CREATE TABLE testdb.test_routineload_tbl(\n    user_id            BIGINT       NOT NULL COMMENT "User ID",\n    name               VARCHAR(20)           COMMENT "User Name",\n    age                INT                   COMMENT "User Age"\n)\nDUPLICATE KEY(user_id)\nDISTRIBUTED BY HASH(user_id) BUCKETS 10;\n'})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Creating the Routine Load job"}),"\n",(0,i.jsxs)(n.p,{children:["In Doris, use the ",(0,i.jsx)(n.code,{children:"CREATE ROUTINE LOAD"})," command to create the load job:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'CREATE ROUTINE LOAD testdb.example_routine_load_csv ON test_routineload_tbl\nCOLUMNS TERMINATED BY ",",\nCOLUMNS(user_id, name, age)\nFROM KAFKA(\n    "kafka_broker_list" = "192.168.88.62:9092",\n    "kafka_topic" = "test-routine-load-csv",\n    "property.kafka_default_offsets" = "OFFSET_BEGINNING"\n);\n'})}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Loading"})," ",(0,i.jsx)(n.strong,{children:"JSON"})," ",(0,i.jsx)(n.strong,{children:"Data"})]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Loading sample data"}),"\n",(0,i.jsx)(n.p,{children:"In Kafka, there is the following sample data:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test-routine-load-json --from-beginning\n"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Creating table"}),"\n",(0,i.jsx)(n.p,{children:"In Doris, create the table for loading with the following syntax:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'CREATE TABLE testdb.test_routineload_tbl(\n    user_id            BIGINT       NOT NULL COMMENT "User ID",\n    name               VARCHAR(20)           COMMENT "User Name",\n    age                INT                   COMMENT "User Age"\n)\nDUPLICATE KEY(user_id)\nDISTRIBUTED BY HASH(user_id) BUCKETS 10;\n'})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Creating the Routine Load job"}),"\n",(0,i.jsxs)(n.p,{children:["In Doris, use the ",(0,i.jsx)(n.code,{children:"CREATE ROUTINE LOAD"})," command to create the job:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'CREATE ROUTINE LOAD testdb.example_routine_load_json ON test_routineload_tbl\nCOLUMNS(user_id, name, age)\nPROPERTIES(\n    "format"="json",\n    "jsonpaths"="[\\"$.user_id\\",\\"$.name\\",\\"$.age\\"]"\n)\nFROM KAFKA(\n    "kafka_broker_list" = "192.168.88.62:9092"\n);\n'})}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.admonition,{title:"Note",type:"info",children:(0,i.jsxs)(n.p,{children:["If you need to load the JSON object at the root node of a JSON file, the jsonpaths should be specified as ",(0,i.jsxs)(n.span,{className:"katex",children:[(0,i.jsx)(n.span,{className:"katex-mathml",children:(0,i.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(n.semantics,{children:[(0,i.jsxs)(n.mrow,{children:[(0,i.jsx)(n.mi,{mathvariant:"normal",children:"."}),(0,i.jsx)(n.mo,{separator:"true",children:","}),(0,i.jsx)(n.mi,{children:"e"}),(0,i.jsx)(n.mi,{mathvariant:"normal",children:"."}),(0,i.jsx)(n.mi,{children:"g"}),(0,i.jsx)(n.mi,{mathvariant:"normal",children:"."}),(0,i.jsx)(n.mo,{separator:"true",children:","}),(0,i.jsx)(n.mi,{mathvariant:"normal",children:"\u2018"}),(0,i.jsx)(n.mi,{children:"P"}),(0,i.jsx)(n.mi,{children:"R"}),(0,i.jsx)(n.mi,{children:"O"}),(0,i.jsx)(n.mi,{children:"P"}),(0,i.jsx)(n.mi,{children:"E"}),(0,i.jsx)(n.mi,{children:"R"}),(0,i.jsx)(n.mi,{children:"T"}),(0,i.jsx)(n.mi,{children:"I"}),(0,i.jsx)(n.mi,{children:"E"}),(0,i.jsx)(n.mi,{children:"S"}),(0,i.jsx)(n.mo,{stretchy:"false",children:"("}),(0,i.jsx)(n.mi,{mathvariant:"normal",children:'"'}),(0,i.jsx)(n.mi,{children:"j"}),(0,i.jsx)(n.mi,{children:"s"}),(0,i.jsx)(n.mi,{children:"o"}),(0,i.jsx)(n.mi,{children:"n"}),(0,i.jsx)(n.mi,{children:"p"}),(0,i.jsx)(n.mi,{children:"a"}),(0,i.jsx)(n.mi,{children:"t"}),(0,i.jsx)(n.mi,{children:"h"}),(0,i.jsx)(n.mi,{children:"s"}),(0,i.jsx)(n.mi,{mathvariant:"normal",children:'"'}),(0,i.jsx)(n.mo,{children:"="}),(0,i.jsx)(n.mi,{mathvariant:"normal",children:'"'})]}),(0,i.jsx)(n.annotation,{encoding:"application/x-tex",children:'., e.g., `PROPERTIES("jsonpaths"="'})]})})}),(0,i.jsxs)(n.span,{className:"katex-html","aria-hidden":"true",children:[(0,i.jsxs)(n.span,{className:"base",children:[(0,i.jsx)(n.span,{className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,i.jsx)(n.span,{className:"mord",children:"."}),(0,i.jsx)(n.span,{className:"mpunct",children:","}),(0,i.jsx)(n.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,i.jsx)(n.span,{className:"mord mathnormal",children:"e"}),(0,i.jsx)(n.span,{className:"mord",children:"."}),(0,i.jsx)(n.span,{className:"mord mathnormal",style:{marginRight:"0.03588em"},children:"g"}),(0,i.jsx)(n.span,{className:"mord",children:"."}),(0,i.jsx)(n.span,{className:"mpunct",children:","}),(0,i.jsx)(n.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,i.jsx)(n.span,{className:"mord",children:"\u2018"}),(0,i.jsx)(n.span,{className:"mord mathnormal",style:{marginRight:"0.13889em"},children:"PROPERT"}),(0,i.jsx)(n.span,{className:"mord mathnormal",style:{marginRight:"0.07847em"},children:"I"}),(0,i.jsx)(n.span,{className:"mord mathnormal",style:{marginRight:"0.05764em"},children:"ES"}),(0,i.jsx)(n.span,{className:"mopen",children:"("}),(0,i.jsx)(n.span,{className:"mord",children:'"'}),(0,i.jsx)(n.span,{className:"mord mathnormal",style:{marginRight:"0.05724em"},children:"j"}),(0,i.jsx)(n.span,{className:"mord mathnormal",children:"so"}),(0,i.jsx)(n.span,{className:"mord mathnormal",children:"n"}),(0,i.jsx)(n.span,{className:"mord mathnormal",children:"p"}),(0,i.jsx)(n.span,{className:"mord mathnormal",children:"a"}),(0,i.jsx)(n.span,{className:"mord mathnormal",children:"t"}),(0,i.jsx)(n.span,{className:"mord mathnormal",children:"h"}),(0,i.jsx)(n.span,{className:"mord mathnormal",children:"s"}),(0,i.jsx)(n.span,{className:"mord",children:'"'}),(0,i.jsx)(n.span,{className:"mspace",style:{marginRight:"0.2778em"}}),(0,i.jsx)(n.span,{className:"mrel",children:"="}),(0,i.jsx)(n.span,{className:"mspace",style:{marginRight:"0.2778em"}})]}),(0,i.jsxs)(n.span,{className:"base",children:[(0,i.jsx)(n.span,{className:"strut",style:{height:"0.6944em"}}),(0,i.jsx)(n.span,{className:"mord",children:'"'})]})]})]}),'.")`"']})}),"\n",(0,i.jsx)(n.h3,{id:"viewing-status",children:"Viewing Status"}),"\n",(0,i.jsx)(n.p,{children:"In Doris, you can check the status of Routine Load jobs and tasks using the following methods:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Load Jobs: Used to view information about load tasks, such as the target table, number of subtasks, load delay status, load configuration, and load results."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Load Tasks: Used to view the status of individual load tasks, including task ID, transaction status, task status, execution start time, and BE (Backend) node assignment."}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"01 Viewing Running Jobs"})}),"\n",(0,i.jsxs)(n.p,{children:["You can use the ",(0,i.jsx)(n.code,{children:"SHOW ROUTINE LOAD"})," command to check the status of jobs. The ",(0,i.jsx)(n.code,{children:"SHOW ROUTINE LOAD"})," command provides information about the current job, including the target table, load delay status, load configuration, and error messages."]}),"\n",(0,i.jsxs)(n.p,{children:["For example, to view the status of the ",(0,i.jsx)(n.code,{children:"testdb.example_routine_load_csv"})," job, you can run the following command:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'mysql> SHOW ROUTINE LOAD FOR testdb.example_routine_load\\G\n*************************** 1. row ***************************\n                  Id: 12025\n                Name: example_routine_load\n          CreateTime: 2024-01-15 08:12:42\n           PauseTime: NULL\n             EndTime: NULL\n              DbName: default_cluster:testdb\n           TableName: test_routineload_tbl\n        IsMultiTable: false\n               State: RUNNING\n      DataSourceType: KAFKA\n      CurrentTaskNum: 1\n       JobProperties: {"max_batch_rows":"200000","timezone":"America/New_York","send_batch_parallelism":"1","load_to_single_tablet":"false","column_separator":"\',\'","line_delimiter":"\\n","current_concurrent_number":"1","delete":"*","partial_columns":"false","merge_type":"APPEND","exec_mem_limit":"2147483648","strict_mode":"false","jsonpaths":"","max_batch_interval":"10","max_batch_size":"104857600","fuzzy_parse":"false","partitions":"*","columnToColumnExpr":"user_id,name,age","whereExpr":"*","desired_concurrent_number":"5","precedingFilter":"*","format":"csv","max_error_number":"0","max_filter_ratio":"1.0","json_root":"","strip_outer_array":"false","num_as_string":"false"}\nDataSourceProperties: {"topic":"test-topic","currentKafkaPartitions":"0","brokerList":"192.168.88.62:9092"}\n    CustomProperties: {"kafka_default_offsets":"OFFSET_BEGINNING","group.id":"example_routine_load_73daf600-884e-46c0-a02b-4e49fdf3b4dc"}\n           Statistic: {"receivedBytes":28,"runningTxns":[],"errorRows":0,"committedTaskNum":3,"loadedRows":3,"loadRowsRate":0,"abortedTaskNum":0,"errorRowsAfterResumed":0,"totalRows":3,"unselectedRows":0,"receivedBytesRate":0,"taskExecuteTimeMs":30069}\n            Progress: {"0":"2"}\n                 Lag: {"0":0}\nReasonOfStateChanged:\n        ErrorLogUrls:\n            OtherMsg:\n                User: root\n             Comment:\n1 row in set (0.00 sec)\n'})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"02 Viewing Running Tasks"})}),"\n",(0,i.jsxs)(n.p,{children:["You can use the ",(0,i.jsx)(n.code,{children:"SHOW ROUTINE LOAD TASK"})," command to check the status of load tasks. The ",(0,i.jsx)(n.code,{children:"SHOW ROUTINE LOAD TASK"})," command provides information about the individual tasks under a specific load job, including task ID, transaction status, task status, execution start time, and BE ID."]}),"\n",(0,i.jsxs)(n.p,{children:["For example, to view the task status of the ",(0,i.jsx)(n.code,{children:"example_routine_load_csv"})," job, you can run the following command:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'mysql> SHOW ROUTINE LOAD TASK WHERE jobname = \'example_routine_load_csv\';\n+-----------------------------------+-------+-----------+-------+---------------------+---------------------+---------+-------+----------------------+\n| TaskId                            | TxnId | TxnStatus | JobId | CreateTime          | ExecuteStartTime    | Timeout | BeId  | DataSourceProperties |\n+-----------------------------------+-------+-----------+-------+---------------------+---------------------+---------+-------+----------------------+\n| 8cf47e6a68ed4da3-8f45b431db50e466 | 195   | PREPARE   | 12177 | 2024-01-15 12:20:41 | 2024-01-15 12:21:01 | 20      | 10429 | {"4":1231,"9":2603}  |\n| f2d4525c54074aa2-b6478cf8daaeb393 | 196   | PREPARE   | 12177 | 2024-01-15 12:20:41 | 2024-01-15 12:21:01 | 20      | 12109 | {"1":1225,"6":1216}  |\n| cb870f1553864250-975279875a25fab6 | -1    | NULL      | 12177 | 2024-01-15 12:20:52 | NULL                | 20      | -1    | {"2":7234,"7":4865}  |\n| 68771fd8a1824637-90a9dac2a7a0075e | -1    | NULL      | 12177 | 2024-01-15 12:20:52 | NULL                | 20      | -1    | {"3":1769,"8":2982}  |\n| 77112dfea5e54b0a-a10eab3d5b19e565 | 197   | PREPARE   | 12177 | 2024-01-15 12:21:02 | 2024-01-15 12:21:02 | 20      | 12098 | {"0":3000,"5":2622}  |\n+-----------------------------------+-------+-----------+-------+---------------------+---------------------+---------+-------+----------------------+\n'})}),"\n",(0,i.jsx)(n.h3,{id:"pausing-jobs",children:"Pausing Jobs"}),"\n",(0,i.jsxs)(n.p,{children:["You can pause an load job using the ",(0,i.jsx)(n.a,{href:"../../../sql-manual/sql-statements/data-modification/load-and-export/PAUSE-ROUTINE-LOAD",children:"PAUSE ROUTINE LOAD"})," command. When a job is paused, it enters the PAUSED state, but the load job is not terminated and can be resumed using the RESUME ROUTINE LOAD command."]}),"\n",(0,i.jsxs)(n.p,{children:["To pause the ",(0,i.jsx)(n.code,{children:"testdb.example_routine_load_csv"})," load job, you can use the following command:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"PAUSE ROUTINE LOAD FOR testdb.example_routine_load_csv;\n"})}),"\n",(0,i.jsx)(n.h3,{id:"resuming-jobs",children:"Resuming Jobs"}),"\n",(0,i.jsxs)(n.p,{children:["You can resume a paused load job using the ",(0,i.jsx)(n.a,{href:"../../../sql-manual/sql-statements/data-modification/load-and-export/RESUME-ROUTINE-LOAD",children:"RESUME ROUTINE LOAD"})," command."]}),"\n",(0,i.jsxs)(n.p,{children:["To resume the ",(0,i.jsx)(n.code,{children:"testdb.example_routine_load_csv"})," job, you can use the following command:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"RESUME ROUTINE LOAD FOR testdb.example_routine_load_csv;\n"})}),"\n",(0,i.jsx)(n.h3,{id:"modifying-jobs",children:"Modifying Jobs"}),"\n",(0,i.jsxs)(n.p,{children:["You can modify a created loading job using the ",(0,i.jsx)(n.a,{href:"../../../sql-manual/sql-statements/data-modification/load-and-export/ALTER-ROUTINE-LOAD",children:"ALTER ROUTINE LOAD"})," command. Before modifying the job, you need to pause it using the",(0,i.jsx)(n.code,{children:" PAUSE ROUTINE LOAD"})," command, and after making the modifications, you can resume it using the ",(0,i.jsx)(n.code,{children:"RESUME ROUTINE LOAD"})," command."]}),"\n",(0,i.jsxs)(n.p,{children:["To modify the ",(0,i.jsx)(n.code,{children:"desired_concurrent_number"})," parameter for the job and update the Kafka topic information, you can use the following command:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'ALTER ROUTINE LOAD FOR testdb.example_routine_load_csv\nPROPERTIES(\n    "desired_concurrent_number" = "3"\n)\nFROM KAFKA(\n    "kafka_broker_list" = "192.168.88.60:9092",\n    "kafka_topic" = "test-topic"\n);\n'})}),"\n",(0,i.jsx)(n.h3,{id:"canceling-jobs",children:"Canceling Jobs"}),"\n",(0,i.jsxs)(n.p,{children:["You can stop and delete a Routine Load job using the ",(0,i.jsx)(n.a,{href:"../../../sql-manual/sql-statements/data-modification/load-and-export/STOP-ROUTINE-LOAD",children:"STOP ROUTINE LOAD"})," command. Once deleted, the load job cannot be recovered and cannot be viewed using the ",(0,i.jsx)(n.code,{children:"SHOW ROUTINE LOAD"})," command."]}),"\n",(0,i.jsxs)(n.p,{children:["To stop and delete the ",(0,i.jsx)(n.code,{children:"testdb.example_routine_load_csv"})," load job, you can use the following command:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"STOP ROUTINE LOAD FOR testdb.example_routine_load_csv;\n"})}),"\n",(0,i.jsx)(n.h2,{id:"reference-manual",children:"Reference Manual"}),"\n",(0,i.jsx)(n.h3,{id:"load-commands",children:"Load Commands"}),"\n",(0,i.jsx)(n.p,{children:"The syntax for creating a Routine Load persistent load job is as follows:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'CREATE ROUTINE LOAD [<db_name>.]<job_name> [ON <tbl_name>]\n[merge_type]\n[load_properties]\n[job_properties]\nFROM KAFKA [data_source_properties]\n[COMMENT "<comment>"]\n'})}),"\n",(0,i.jsx)(n.p,{children:"The modules for creating a loading job are explained as follows:"}),"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Module"}),(0,i.jsx)(n.th,{children:"Description"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"db_name"}),(0,i.jsx)(n.td,{children:"Specifies the name of the database for creating the loading task."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"job_name"}),(0,i.jsx)(n.td,{children:"Specifies the name of the created loading job. The job name must be unique within the same database."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"tbl_name"}),(0,i.jsx)(n.td,{children:"Specifies the name of the table to be loaded. This parameter is optional. If not specified, the dynamic table mode will be used, where Kafka data should contain the table name information."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"merge_type"}),(0,i.jsxs)(n.td,{children:["Specifies the data merge type. The default value is APPEND. Possible merge_type options are: ",(0,i.jsxs)(n.ul,{children:[(0,i.jsx)(n.li,{children:"APPEND: Append load mode"}),(0,i.jsx)(n.li,{children:"MERGE: Merge load mode"}),(0,i.jsx)(n.li,{children:"DELETE: load data as delete records"})]})]})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"load_properties"}),(0,i.jsxs)(n.td,{children:["Describes the load properties, including:",(0,i.jsxs)(n.ul,{children:[(0,i.jsx)(n.li,{children:"column_spearator clause"}),(0,i.jsx)(n.li,{children:"columns_mapping clause"}),(0,i.jsx)(n.li,{children:"preceding_filter clause"}),(0,i.jsx)(n.li,{children:"where_predicates clause"}),(0,i.jsx)(n.li,{children:"partitions clause"}),(0,i.jsx)(n.li,{children:"delete_on clause"}),(0,i.jsx)(n.li,{children:"order_by clause"})]})]})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"job_properties"}),(0,i.jsx)(n.td,{children:"Specifies the general load parameters for Routine Load."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"data_source_properties"}),(0,i.jsx)(n.td,{children:"Describes the properties of Kafka data source."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"comment"}),(0,i.jsx)(n.td,{children:"Describes any additional comments for the loading job."})]})]})]}),"\n",(0,i.jsx)(n.h3,{id:"load-parameter-description",children:"Load Parameter Description"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"01 FE Configuration Parameters"})}),"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Parameter Name"}),(0,i.jsx)(n.th,{children:"Default Value"}),(0,i.jsx)(n.th,{children:"Dynamic Configuration"}),(0,i.jsx)(n.th,{children:"FE Master Exclusive Configuration"}),(0,i.jsx)(n.th,{children:"Description"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"max_routine_load_task_concurrent_num"}),(0,i.jsx)(n.td,{children:"256"}),(0,i.jsx)(n.td,{children:"Yes"}),(0,i.jsx)(n.td,{children:"Yes"}),(0,i.jsx)(n.td,{children:"Limits the maximum number of concurrent subtasks for Routine Load jobs. It is recommended to maintain the default value. If set too high, it may lead to excessive concurrent tasks, consuming cluster resources."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"max_routine_load_task_num_per_be"}),(0,i.jsx)(n.td,{children:"1024"}),(0,i.jsx)(n.td,{children:"Yes"}),(0,i.jsx)(n.td,{children:"Yes"}),(0,i.jsxs)(n.td,{children:["The maximum number of concurrent Routine Load tasks allowed per BE. ",(0,i.jsx)(n.code,{children:"max_routine_load_task_num_per_be"})," should be less than ",(0,i.jsx)(n.code,{children:"routine_load_thread_pool_size"}),"."]})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"max_routine_load_job_num"}),(0,i.jsx)(n.td,{children:"100"}),(0,i.jsx)(n.td,{children:"Yes"}),(0,i.jsx)(n.td,{children:"Yes"}),(0,i.jsx)(n.td,{children:"Limits the maximum number of Routine Load jobs, including NEED_SCHEDULED, RUNNING, and PAUSE."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"max_tolerable_backend_down_num"}),(0,i.jsx)(n.td,{children:"0"}),(0,i.jsx)(n.td,{children:"Yes"}),(0,i.jsx)(n.td,{children:"Yes"}),(0,i.jsx)(n.td,{children:"If any BE is down, Routine Load cannot automatically recover. Under certain conditions, Doris can reschedule PAUSED tasks to RUNNING state. A value of 0 means that rescheduling is only allowed when all BE nodes are alive."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"period_of_auto_resume_min"}),(0,i.jsx)(n.td,{children:"5 (minutes)"}),(0,i.jsx)(n.td,{children:"Yes"}),(0,i.jsx)(n.td,{children:"Yes"}),(0,i.jsx)(n.td,{children:"The period for automatically resuming Routine Load."})]})]})]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"02 BE Configuration Parameters"})}),"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Parameter Name"}),(0,i.jsx)(n.th,{children:"Default Value"}),(0,i.jsx)(n.th,{children:"Dynamic Configuration"}),(0,i.jsx)(n.th,{children:"Description"})]})}),(0,i.jsx)(n.tbody,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"max_consumer_num_per_group"}),(0,i.jsx)(n.td,{children:"3"}),(0,i.jsx)(n.td,{children:"Yes"}),(0,i.jsx)(n.td,{children:"The maximum number of consumers that can be generated for a subtask to consume data. For Kafka data sources, a consumer may consume one or more Kafka partitions. If a task needs to consume 6 Kafka partitions, it will generate 3 consumers, each consuming 2 partitions. If there are only 2 partitions, it will generate only 2 consumers, each consuming 1 partition."})]})})]}),"\n",(0,i.jsx)(n.h3,{id:"load-configuration-parameters",children:"Load Configuration Parameters"}),"\n",(0,i.jsxs)(n.p,{children:["When creating a Routine Load job, you can specify the load configuration parameters for different modules using the ",(0,i.jsx)(n.code,{children:"CREATE ROUTINE LOAD"})," command."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"tbl_name Clause"})}),"\n",(0,i.jsx)(n.p,{children:"Specifies the name of the table to be loaded. This parameter is optional."}),"\n",(0,i.jsxs)(n.p,{children:["If not specified, the dynamic table mode is used, which requires the data in Kafka to contain the table name information. Currently, only extracting the table name from the Value field of Kafka is supported. The format should be as follows, using JSON as an example: ",(0,i.jsx)(n.code,{children:'table_name|{"col1": "val1", "col2": "val2"}'}),", where ",(0,i.jsx)(n.code,{children:"tbl_name"})," is the table name and ",(0,i.jsx)(n.code,{children:"|"})," is used as the separator between the table name and the table data. The same format applies to CSV data, such as ",(0,i.jsx)(n.code,{children:"table_name|val1,val2,val3"}),". Note that the ",(0,i.jsx)(n.code,{children:"table_name"})," here must be consistent with the table name in Doris, otherwise the load will fail. Note that dynamic tables do not support the column_mapping configuration described later."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"merge_type Clause"})}),"\n",(0,i.jsx)(n.p,{children:"The merge_type module specifies the type of data merging. There are three options for merge_type:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"APPEND: Append load mode."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"MERGE: Merge load mode. Only applicable to Unique Key models. It needs to be used together with the [DELETE ON] module to mark the Delete Flag column."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"DELETE: All loaded data is data that needs to be deleted."}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"load_properties Clause"})}),"\n",(0,i.jsx)(n.p,{children:"The load_properties module describes the properties of the loaded data using the following syntax:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"[COLUMNS TERMINATED BY <column_separator>,]\n[COLUMNS (<column1_name>[, <column2_name>, <column_mapping>, ...]),]\n[WHERE <where_expr>,]\n[PARTITION(<partition1_name>, [<partition2_name>, <partition3_name>, ...]),]\n[DELETE ON <delete_expr>,]\n[ORDER BY <order_by_column1>[, <order_by_column2>, <order_by_column3>, ...]]\n"})}),"\n",(0,i.jsx)(n.p,{children:"The specific parameters for each module are as follows:"}),"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Submodule"}),(0,i.jsx)(n.th,{children:"Parameter"}),(0,i.jsx)(n.th,{children:"Description"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"COLUMNS TERMINATED BY"}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"<column_separator>"})}),(0,i.jsxs)(n.td,{children:["Specifies the column delimiter, defaulting to ",(0,i.jsx)(n.code,{children:"\\t"}),". For example, to specify a comma as the delimiter, use ",(0,i.jsx)(n.code,{children:'COLUMNS TERMINATED BY ","'}),". When handling empty values, note the following:",(0,i.jsxs)(n.ul,{children:[(0,i.jsxs)(n.li,{children:["Null values should be represented as ",(0,i.jsx)(n.code,{children:"\\n"}),". For example, ",(0,i.jsx)(n.code,{children:"a,\\n,b"})," represents a null value in the middle column."]}),(0,i.jsxs)(n.li,{children:["Empty strings (",(0,i.jsx)(n.code,{children:"''"}),") are treated as empty values. For example, ",(0,i.jsx)(n.code,{children:"a,,b"})," represents an empty string in the middle column."]})]})]})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"COLUMNS"}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"<column_name>"})}),(0,i.jsxs)(n.td,{children:["Specifies the corresponding column names. For example, to specify the load columns as ",(0,i.jsx)(n.code,{children:"(k1, k2, k3)"}),", use ",(0,i.jsx)(n.code,{children:"COLUMNS(k1, k2, k3)"}),". The COLUMNS clause can be omitted in the following cases:",(0,i.jsxs)(n.ul,{children:[(0,i.jsx)(n.li,{children:"When the columns in the CSV match the table columns one by one."}),(0,i.jsx)(n.li,{children:"When the key columns in JSON have the same names as the table columns."})]})]})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"<column_mapping>"})}),(0,i.jsxs)(n.td,{children:["During the load process, column mapping can be used to filter and transform columns. For example, if the target column needs to perform a derived calculation based on a column in the data source (e.g., the target column k4 is calculated as k3 + 1 based on the k3 column), you can use ",(0,i.jsx)(n.code,{children:"COLUMNS(k1, k2, k3, k4 = k3 + 1)"}),". For more details, refer to the ",(0,i.jsx)(n.a,{href:"../../../data-operate/import/load-data-convert",children:"Data Conversion"})," documentation."]})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"WHERE"}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"<where_expr>"})}),(0,i.jsxs)(n.td,{children:["Specifies the condition to filter the loaded data source. For example, to load only data where age > 30, use ",(0,i.jsx)(n.code,{children:"WHERE age > 30"}),"."]})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"PARTITION"}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"<partition_name>"})}),(0,i.jsxs)(n.td,{children:["Specifies which partitions in the target table to load. If not specified, it will automatically load into the corresponding partitions. For example, to load partitions p1 and p2 of the target table, use ",(0,i.jsx)(n.code,{children:"PARTITION(p1, p2)"}),"."]})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"DELETE ON"}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"<delete_expr>"})}),(0,i.jsxs)(n.td,{children:["In the MERGE load mode, using delete_expr to mark which columns need to be deleted. For example, to delete columns where age > 30 during the MERGE process, use ",(0,i.jsx)(n.code,{children:"DELETE ON age > 30"}),"."]})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"ORDER BY"}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"<order_by_column>"})}),(0,i.jsxs)(n.td,{children:["Only effective for Unique Key models. Specifies the Sequence Column in the loaded data to ensure the order of the data. For example, when loading into a Unique Key table and specifying create_time as the Sequence Column, use ",(0,i.jsx)(n.code,{children:"ORDER BY create_time"}),". For more information on Sequence Columns in Unique Key models, refer to the ",(0,i.jsx)(n.a,{href:"../../../data-operate/update/update-of-unique-model",children:"Data Update/Sequence Columns"})]})]})]})]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"job_properties Clause"})}),"\n",(0,i.jsx)(n.p,{children:"The job_properties clause is used to specify the properties of a Routine Load job when creating it. The syntax is as follows:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'PROPERTIES ("<key1>" = "<value1>"[, "<key2>" = "<value2>" ...])\n'})}),"\n",(0,i.jsx)(n.p,{children:"Here are the available parameters for the job_properties clause:"}),"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Parameter"}),(0,i.jsx)(n.th,{children:"Description"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"desired_concurrent_number"}),(0,i.jsx)(n.td,{children:(0,i.jsxs)(n.ul,{children:[(0,i.jsx)(n.li,{children:"Default value: 256"}),(0,i.jsx)(n.li,{children:"Description: Specifies the desired concurrency for a single load subtask (load task). It modifies the expected number of load subtasks for a Routine Load job. The actual concurrency during the load process may not be equal to the desired concurrency. The actual concurrency is determined based on factors such as the number of nodes in the cluster, the load on the cluster, and the characteristics of the data source. The actual number of loading subtasks can be calculated using the following formula:"}),(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"min(topic_partition_num, desired_concurrent_number, max_routine_load_task_concurrent_num)"})})," ",(0,i.jsx)(n.li,{children:"where:"}),(0,i.jsx)(n.li,{children:"topic_partition_num: The number of partitions in the Kafka topic"}),(0,i.jsx)(n.li,{children:"desired_concurrent_number: The parameter value set"}),(0,i.jsx)(n.li,{children:"max_routine_load_task_concurrent_num: The parameter for setting the maximum task parallelism for Routine Load in the FE"})]})})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"max_batch_interval"}),(0,i.jsx)(n.td,{children:"The maximum running time for each subtask, in seconds. Must be greater than 0, with a default value of 60s. max_batch_interval/max_batch_rows/max_batch_size together form the execution threshold for subtasks. If any of these parameters reaches the threshold, the load subtask ends and a new one is generated."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"max_batch_rows"}),(0,i.jsx)(n.td,{children:"The maximum number of rows read by each subtask. Must be greater than or equal to 200,000. The default value is 20,000,000. max_batch_interval/max_batch_rows/max_batch_size together form the execution threshold for subtasks. If any of these parameters reaches the threshold, the load subtask ends and a new one is generated."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"max_batch_size"}),(0,i.jsx)(n.td,{children:"The maximum number of bytes read by each subtask. The unit is bytes, and the range is from 100MB to 10GB. The default value is 1G. max_batch_interval/max_batch_rows/max_batch_size together form the execution threshold for subtasks. If any of these parameters reaches the threshold, the load subtask ends and a new one is generated."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"max_error_number"}),(0,i.jsxs)(n.td,{children:["The maximum number of error rows allowed within a sampling window. Must be greater than or equal to 0. The default value is 0, which means no error rows are allowed. The sampling window is ",(0,i.jsx)(n.code,{children:"max_batch_rows * 10"}),". If the number of error rows within the sampling window exceeds ",(0,i.jsx)(n.code,{children:"max_error_number"}),", the regular job will be paused and manual intervention is required to check for data quality issues using the ",(0,i.jsx)(n.a,{href:"../../../sql-manual/sql-statements/data-modification/load-and-export/SHOW-ROUTINE-LOAD",children:"SHOW ROUTINE LOAD"})," command and ",(0,i.jsx)(n.code,{children:"ErrorLogUrls"}),". Rows filtered out by the WHERE condition are not counted as error rows."]})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"strict_mode"}),(0,i.jsxs)(n.td,{children:["Whether to enable strict mode. The default value is disabled. Strict mode applies strict filtering to type conversions during the load process. If enabled, non-null original data that results in a NULL after type conversion will be filtered out. The filtering rules in strict mode are as follows:",(0,i.jsxs)(n.ul,{children:[(0,i.jsx)(n.li,{children:"Derived columns (generated by functions) are not affected by strict mode."}),(0,i.jsxs)(n.li,{children:["If a column's type needs to be converted, any data with an incorrect data type will be filtered out. You can check the filtered columns due to data type errors in the ",(0,i.jsx)(n.code,{children:"ErrorLogUrls"})," of ",(0,i.jsx)(n.a,{href:"../../../sql-manual/sql-statements/data-modification/load-and-export/SHOW-ROUTINE-LOAD",children:"SHOW ROUTINE LOAD"}),"."]}),(0,i.jsxs)(n.li,{children:["For columns with range restrictions, if the original data can be successfully converted but falls outside the declared range, strict mode does not affect it. For example, if the type is decimal(1,0) and the original data is 10, it can be converted but is not within the range declared for the column. Strict mode does not affect this type of data. For more details, see ",(0,i.jsx)(n.a,{href:"../../../data-operate/import/handling-messy-data#strict-mode",children:"Strict Mode"}),"."]})]})]})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"timezone"}),(0,i.jsx)(n.td,{children:"Specifies the time zone used by the load job. The default is to use the session's timezone parameter. This parameter affects the results of all timezone-related functions involved in the load."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"format"}),(0,i.jsx)(n.td,{children:"Specifies the data format for the load. The default is CSV, and JSON format is supported."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"jsonpaths"}),(0,i.jsx)(n.td,{children:"When the data format is JSON, jsonpaths can be used to specify the JSON paths to extract data from nested structures. It is a JSON array of strings, where each string represents a JSON path."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"json_root"}),(0,i.jsxs)(n.td,{children:["When importing JSON format data, you can specify the root node of the JSON data through json_root. Doris will extract and parse elements from the root node. Default is empty. For example, specify the JSON root node with: ",(0,i.jsx)(n.code,{children:'"json_root" = "$.RECORDS"'})]})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"strip_outer_array"}),(0,i.jsxs)(n.td,{children:["When importing JSON format data, if strip_outer_array is true, it indicates that the JSON data is presented as an array, and each element in the data will be treated as a row. Default value is false. Typically, JSON data in Kafka might be represented as an array with square brackets ",(0,i.jsx)(n.code,{children:"[]"})," in the outermost layer. In this case, you can specify ",(0,i.jsx)(n.code,{children:'"strip_outer_array" = "true"'})," to consume Topic data in array mode. For example, the following data will be parsed into two rows: ",(0,i.jsx)(n.code,{children:'[{"user_id":1,"name":"Emily","age":25},{"user_id":2,"name":"Benjamin","age":35}]'})]})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"send_batch_parallelism"}),(0,i.jsxs)(n.td,{children:["Used to set the parallelism of sending batch data. If the parallelism value exceeds the ",(0,i.jsx)(n.code,{children:"max_send_batch_parallelism_per_job"})," in BE configuration, the coordinating BE will use the value of ",(0,i.jsx)(n.code,{children:"max_send_batch_parallelism_per_job"}),"."]})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"load_to_single_tablet"}),(0,i.jsx)(n.td,{children:"Supports importing data to only one tablet in the corresponding partition per task. Default value is false. This parameter can only be set when importing data to OLAP tables with random bucketing."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"partial_columns"}),(0,i.jsxs)(n.td,{children:["Specifies whether to enable partial column update feature. Default value is false. This parameter can only be set when the table model is Unique and uses Merge on Write. Multi-table streaming does not support this parameter. For details, refer to ",(0,i.jsx)(n.a,{href:"../../../data-operate/update/update-of-unique-model",children:"Partial Column Update"})]})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"max_filter_ratio"}),(0,i.jsxs)(n.td,{children:["The maximum allowed filter ratio within the sampling window. Must be between 0 and 1 inclusive. Default value is 1.0, indicating any error rows can be tolerated. The sampling window is ",(0,i.jsx)(n.code,{children:"max_batch_rows * 10"}),". If the ratio of error rows to total rows within the sampling window exceeds ",(0,i.jsx)(n.code,{children:"max_filter_ratio"}),", the routine job will be suspended and require manual intervention to check data quality issues. Rows filtered by WHERE conditions are not counted as error rows."]})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"enclose"}),(0,i.jsx)(n.td,{children:'Specifies the enclosing character. When CSV data fields contain line or column separators, a single-byte character can be specified as an enclosing character for protection to prevent accidental truncation. For example, if the column separator is "," and the enclosing character is "\'", the data "a,\'b,c\'" will have "b,c" parsed as one field.'})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"escape"}),(0,i.jsx)(n.td,{children:'Specifies the escape character. Used to escape characters in fields that are identical to the enclosing character. For example, if the data is "a,\'b,\'c\'", the enclosing character is "\'", and you want "b,\'c" to be parsed as one field, you need to specify a single-byte escape character, such as "", and modify the data to "a,\'b,\'c\'".'})]})]})]}),"\n",(0,i.jsx)(n.p,{children:"These parameters can be used to customize the behavior of a Routine Load job according to your specific requirements."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"04 data_source_properties Clause"})}),"\n",(0,i.jsx)(n.p,{children:"When creating a Routine Load job, you can specify the data_source_properties clause to specify properties of the Kafka data source. The syntax is as follows:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'FROM KAFKA ("<key1>" = "<value1>"[, "<key2>" = "<value2>" ...])\n'})}),"\n",(0,i.jsx)(n.p,{children:"The available options for the data_source_properties clause are as follows:"}),"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Parameter"}),(0,i.jsx)(n.th,{children:"Description"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"kafka_broker_list"}),(0,i.jsxs)(n.td,{children:["Specifies the connection information for Kafka brokers. The format is ",(0,i.jsx)(n.code,{children:"<kafka_broker_ip>:<kafka_port>"}),". Multiple brokers are separated by commas. For example, to specify a Broker List with the default port 9092, you can use the following command: ",(0,i.jsx)(n.code,{children:'"kafka_broker_list" = "<broker1_ip>:9092,<broker2_ip>:9092"'})]})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"kafka_topic"}),(0,i.jsx)(n.td,{children:"Specifies the Kafka topic to subscribe to. A load job can only consume one Kafka topic."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"kafka_partitions"}),(0,i.jsx)(n.td,{children:"Specifies the Kafka partitions to subscribe to. If not specified, all partitions are consumed by default."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"kafka_offsets"}),(0,i.jsxs)(n.td,{children:["Specifies the starting consumption offset for Kafka partitions. If a timestamp is specified, consumption starts from the nearest offset equal to or greater than that timestamp. The offset can be a specific offset greater than or equal to 0, or it can use the following formats:",(0,i.jsxs)(n.ul,{children:[(0,i.jsx)(n.li,{children:"OFFSET_BEGINNING: Starts consuming from the position where there is data."}),(0,i.jsx)(n.li,{children:"OFFSET_END: Starts consuming from the end."}),(0,i.jsx)(n.li,{children:'Timestamp format, e.g., "2021-05-22 11:00:00"'}),(0,i.jsxs)(n.li,{children:["If not specified, consumption starts from ",(0,i.jsx)(n.code,{children:"OFFSET_END"})," for all partitions under the topic."]}),(0,i.jsxs)(n.li,{children:["Multiple starting consumption offsets can be specified, separated by commas, such as ",(0,i.jsx)(n.code,{children:'"kafka_offsets" = "101,0,OFFSET_BEGINNING,OFFSET_END"'})," or ",(0,i.jsx)(n.code,{children:'"kafka_offsets" = "2021-05-22 11:00:00,2021-05-22 11:00:00"'})]}),(0,i.jsx)(n.li,{children:"Note that timestamp format cannot be mixed with OFFSET format."})]})]})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"property"}),(0,i.jsxs)(n.td,{children:['Specifies custom Kafka parameters. This is equivalent to the "--property" parameter in the Kafka shell. When the value of a parameter is a file, the keyword "FILE:" needs to be added before the value. For creating a file, you can refer to the ',(0,i.jsx)(n.a,{href:"../../../sql-manual/sql-statements/security/CREATE-FILE",children:"CREATE FILE"})," command documentation. For more supported custom parameters, you can refer to the client-side configuration options in the official ",(0,i.jsx)(n.a,{href:"https://github.com/confluentinc/librdkafka/blob/master/CONFIGURATION.md",children:"CONFIGURATION"})," documentation of librdkafka. For example: ",(0,i.jsx)(n.code,{children:'"property.client.id" = "12345"'}),", ",(0,i.jsx)(n.code,{children:'"property.group.id" = "group_id_0"'}),", ",(0,i.jsx)(n.code,{children:'"property.ssl.ca.location" = "FILE:ca.pem"'})]})]})]})]}),"\n",(0,i.jsxs)(n.p,{children:["By configuring the Kafka property parameter in the ",(0,i.jsx)(n.code,{children:"data_source_properties"}),", you can set up security access options. Currently, Doris supports various Kafka security protocols such as plaintext (default), SSL, PLAIN, and Kerberos."]}),"\n",(0,i.jsx)(n.h3,{id:"load-status",children:"Load Status"}),"\n",(0,i.jsxs)(n.p,{children:["You can check the status of a load job using the ",(0,i.jsx)(n.code,{children:"SHOW ROUTINE LOAD"})," command. The syntax for the command is as follows:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"SHOW [ALL] ROUTINE LOAD [FOR jobName];\n"})}),"\n",(0,i.jsxs)(n.p,{children:["For example, executing ",(0,i.jsx)(n.code,{children:"SHOW ROUTINE LOAD"})," will return a result set similar to the following:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'mysql> SHOW ROUTINE LOAD FOR testdb.example_routine_load\\G\n*************************** 1. row ***************************\n                  Id: 12025\n                Name: example_routine_load\n          CreateTime: 2024-01-15 08:12:42\n           PauseTime: NULL\n             EndTime: NULL\n              DbName: default_cluster:testdb\n           TableName: test_routineload_tbl\n        IsMultiTable: false\n               State: RUNNING\n      DataSourceType: KAFKA\n      CurrentTaskNum: 1\n       JobProperties: {"max_batch_rows":"200000","timezone":"America/New_York","send_batch_parallelism":"1","load_to_single_tablet":"false","column_separator":"\',\'","line_delimiter":"\\n","current_concurrent_number":"1","delete":"*","partial_columns":"false","merge_type":"APPEND","exec_mem_limit":"2147483648","strict_mode":"false","jsonpaths":"","max_batch_interval":"10","max_batch_size":"104857600","fuzzy_parse":"false","partitions":"*","columnToColumnExpr":"user_id,name,age","whereExpr":"*","desired_concurrent_number":"5","precedingFilter":"*","format":"csv","max_error_number":"0","max_filter_ratio":"1.0","json_root":"","strip_outer_array":"false","num_as_string":"false"}\nDataSourceProperties: {"topic":"test-topic","currentKafkaPartitions":"0","brokerList":"192.168.88.62:9092"}\n    CustomProperties: {"kafka_default_offsets":"OFFSET_BEGINNING","group.id":"example_routine_load_73daf600-884e-46c0-a02b-4e49fdf3b4dc"}\n           Statistic: {"receivedBytes":28,"runningTxns":[],"errorRows":0,"committedTaskNum":3,"loadedRows":3,"loadRowsRate":0,"abortedTaskNum":0,"errorRowsAfterResumed":0,"totalRows":3,"unselectedRows":0,"receivedBytesRate":0,"taskExecuteTimeMs":30069}\n            Progress: {"0":"2"}\n                 Lag: {"0":0}\nReasonOfStateChanged:\n        ErrorLogUrls:\n            OtherMsg:\n                User: root\n             Comment:\n1 row in set (0.00 sec)\n'})}),"\n",(0,i.jsx)(n.p,{children:"The columns in the result set provide the following information:"}),"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Column Name"}),(0,i.jsx)(n.th,{children:"Description"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Id"}),(0,i.jsx)(n.td,{children:"The ID of the load job, automatically generated by Doris."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Name"}),(0,i.jsx)(n.td,{children:"The name of the load job."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"CreateTime"}),(0,i.jsx)(n.td,{children:"The time when the job was created."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"PauseTime"}),(0,i.jsx)(n.td,{children:"The time when the job was last paused."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"EndTime"}),(0,i.jsx)(n.td,{children:"The time when the job ended."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"DbName"}),(0,i.jsx)(n.td,{children:"The name of the associated database."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"TableName"}),(0,i.jsx)(n.td,{children:'The name of the associated table. For multi-table scenarios, it is displayed as "multi-table".'})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"IsMultiTbl"}),(0,i.jsx)(n.td,{children:"Indicates whether it is a multi-table load."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"State"}),(0,i.jsxs)(n.td,{children:["The running state of the job, which can have five values: ",(0,i.jsxs)(n.ul,{children:[(0,i.jsx)(n.li,{children:" NEED_SCHEDULE: The job is waiting to be scheduled. After the CREATE ROUTINE LOAD or RESUME ROUTINE LOAD command, the job enters the NEED_SCHEDULE state."}),(0,i.jsx)(n.li,{children:"RUNNING: The job is currently running."}),(0,i.jsx)(n.li,{children:"PAUSED: The job has been paused and can be resumed using the RESUME ROUTINE LOAD command."}),(0,i.jsx)(n.li,{children:"STOPPED: The job has finished and cannot be restarted."}),(0,i.jsx)(n.li,{children:"CANCELLED: The job has been canceled."})]})]})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"DataSourceType"}),(0,i.jsx)(n.td,{children:"The type of data source, which is KAFKA, in this example."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"CurrentTaskNum"}),(0,i.jsx)(n.td,{children:"The current number of subtasks."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"JobProperties"}),(0,i.jsx)(n.td,{children:"Details of the job configuration."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"DataSourceProperties"}),(0,i.jsx)(n.td,{children:"Details of the data source configuration."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"CustomProperties"}),(0,i.jsx)(n.td,{children:"Custom configuration properties."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Statistic"}),(0,i.jsx)(n.td,{children:"Statistics of the job's running status."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Progress"}),(0,i.jsxs)(n.td,{children:["The job's progress. For Kafka data sources, it shows the offset consumed for each partition. For example, ",(0,i.jsx)(n.code,{children:'{"0":"2"}'})," indicates that partition 0 has consumed 2 offsets."]})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Lag"}),(0,i.jsxs)(n.td,{children:["The lag of the job. For Kafka data sources, it shows the consumption lag for each partition. For example, ",(0,i.jsx)(n.code,{children:'{"0":10}'})," indicates a consumption lag of 10 for partition 0."]})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"ReasonOfStateChanged"}),(0,i.jsx)(n.td,{children:"The reason for the state change of jobs."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"ErrorLogUrls"}),(0,i.jsx)(n.td,{children:"The URL(s) to view the filtered low-quality data."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"OtherMsg"}),(0,i.jsx)(n.td,{children:"Other error messages."})]})]})]}),"\n",(0,i.jsx)(n.h2,{id:"load-example",children:"Load example"}),"\n",(0,i.jsx)(n.h3,{id:"setting-the-maximum-error-tolerance",children:"Setting the Maximum Error Tolerance"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Load sample data:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"1,Benjamin,18\n2,Emily,20\n3,Alexander,dirty_data\n"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Create table:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'CREATE TABLE demo.routine_test01 (\n    id       INT             NOT NULL   COMMENT "User ID",\n    name     VARCHAR(30)     NOT NULL   COMMENT "Name",\n    age      INT                        COMMENT "Age"\n)\nDUPLICATE KEY(`id`)\nDISTRIBUTED BY HASH(`id`) BUCKETS 1;\n'})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Load command:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'CREATE ROUTINE LOAD demo.kafka_job01 ON routine_test01\n        COLUMNS TERMINATED BY ","\n        PROPERTIES\n        (\n            "max_filter_ratio"="0.5",\n            "max_error_number" = "100",\n            "strict_mode" = "true"\n        )\n        FROM KAFKA\n        (\n            "kafka_broker_list" = "10.16.10.6:9092",\n            "kafka_topic" = "routineLoad01",\n            "property.kafka_default_offsets" = "OFFSET_BEGINNING"\n        );  \n'})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Load result:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"mysql> select * from routine_test01;\n+------+------------+------+\n| id   | name       | age  |\n+------+------------+------+\n|    1 | Benjamin   |   18 |\n|    2 | Emily      |   20 |\n+------+------------+------+\n2 rows in set (0.01 sec)\n"})}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"consuming-data-from-a-specified-offset",children:"Consuming Data from a Specified Offset"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Load sample data:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"1,Benjamin,18\n2,Emily,20\n3,Alexander,22\n4,Sophia,24\n5,William,26\n6,Charlotte,28\n"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Create table:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'CREATE TABLE demo.routine_test02 (\n    id       INT             NOT NULL   COMMENT "User ID",\n    name     VARCHAR(30)     NOT NULL   COMMENT "Name",\n    age      INT                        COMMENT "Age"\n)\nDUPLICATE KEY(`id`)\nDISTRIBUTED BY HASH(`id`) BUCKETS 1;\n'})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Load command:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'CREATE ROUTINE LOAD demo.kafka_job02 ON routine_test02\n        COLUMNS TERMINATED BY ","\n        FROM KAFKA\n        (\n            "kafka_broker_list" = "10.16.10.6:9092",\n            "kafka_topic" = "routineLoad02",\n            "kafka_partitions" = "0",\n            "kafka_offsets" = "3"\n        );\n'})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Load result:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"mysql> select * from routine_test02;\n+------+--------------+------+\n| id   | name         | age  |\n+------+--------------+------+\n|    4 | Sophia       |   24 |\n|    5 | William      |   26 |\n|    6 | Charlotte    |   28 |\n+------+--------------+------+\n3 rows in set (0.01 sec)\n"})}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"specifying-the-consumer-groups-groupid-and-clientid",children:"Specifying the Consumer Group's group.id and client.id"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Load sample data:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"1,Benjamin,18\n2,Emily,20\n3,Alexander,22\n"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Create table:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'CREATE TABLE demo.routine_test03 (\n    id       INT             NOT NULL   COMMENT "User ID",\n    name     VARCHAR(30)     NOT NULL   COMMENT "Name",\n    age      INT                        COMMENT "Age"\n)\nDUPLICATE KEY(`id`)\nDISTRIBUTED BY HASH(`id`) BUCKETS 1;\n'})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Load command:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'CREATE ROUTINE LOAD demo.kafka_job03 ON routine_test03\n        COLUMNS TERMINATED BY ","\n        FROM KAFKA\n        (\n            "kafka_broker_list" = "10.16.10.6:9092",\n            "kafka_topic" = "routineLoad01",\n            "property.group.id" = "kafka_job03",\n            "property.client.id" = "kafka_client_03",\n            "property.kafka_default_offsets" = "OFFSET_BEGINNING"\n        );  \n'})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Load result:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"mysql> select * from routine_test03;\n+------+------------+------+\n| id   | name       | age  |\n+------+------------+------+\n|    1 | Benjamin   |   18 |\n|    2 | Emily      |   20 |\n|    3 | Alexander  |   22 |\n+------+------------+------+\n3 rows in set (0.01 sec)\n"})}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"setting-load-filtering-conditions",children:"Setting load filtering conditions"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Load sample data:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"1,Benjamin,18\n2,Emily,20\n3,Alexander,22\n4,Sophia,24\n5,William,26\n6,Charlotte,28\n"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Create table:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'CREATE TABLE demo.routine_test04 (\n    id       INT             NOT NULL   COMMENT "User ID",\n    name     VARCHAR(30)     NOT NULL   COMMENT "Name",\n    age      INT                        COMMENT "Age"\n)\nDUPLICATE KEY(`id`)\nDISTRIBUTED BY HASH(`id`) BUCKETS 1;\n'})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Load command:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'CREATE ROUTINE LOAD demo.kafka_job04 ON routine_test04\n        COLUMNS TERMINATED BY ",",\n        WHERE id >= 3\n        FROM KAFKA\n        (\n            "kafka_broker_list" = "10.16.10.6:9092",\n            "kafka_topic" = "routineLoad04",\n            "property.kafka_default_offsets" = "OFFSET_BEGINNING"\n        );  \n'})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Load result:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"mysql> select * from routine_test04;\n+------+--------------+------+\n| id   | name         | age  |\n+------+--------------+------+\n|    4 | Sophia       |   24 |\n|    5 | William      |   26 |\n|    6 | Charlotte    |   28 |\n+------+--------------+------+\n3 rows in set (0.01 sec)\n"})}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"loading-specified-partition-data",children:"Loading specified partition data"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Load sample data:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"1,Benjamin,18,2024-02-04 10:00:00\n2,Emily,20,2024-02-05 11:00:00\n3,Alexander,22,2024-02-06 12:00:00\n"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Create table:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'CREATE TABLE demo.routine_test05 (\n    id      INT            NOT NULL  COMMENT "ID",\n    name    VARCHAR(30)    NOT NULL  COMMENT "Name",\n    age     INT                      COMMENT "Age",\n    date    DATETIME                 COMMENT "Date"\n)\nDUPLICATE KEY(`id`)\nPARTITION BY RANGE(`id`)\n(PARTITION partition_a VALUES [("0"), ("1")),\nPARTITION partition_b VALUES [("1"), ("2")),\nPARTITION partition_c VALUES [("2"), ("3")))\nDISTRIBUTED BY HASH(`id`) BUCKETS 1;\n'})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Load command:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'CREATE ROUTINE LOAD demo.kafka_job05 ON routine_test05\n        COLUMNS TERMINATED BY ",",\n        PARTITION(partition_b)\n        FROM KAFKA\n        (\n            "kafka_broker_list" = "10.16.10.6:9092",\n            "kafka_topic" = "routineLoad05",\n            "property.kafka_default_offsets" = "OFFSET_BEGINNING"\n        );\n'})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Load result:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"mysql> select * from routine_test05;\n+------+----------+------+---------------------+\n| id   | name     | age  | date                |\n+------+----------+------+---------------------+\n|    1 | Benjamin |   18 | 2024-02-04 10:00:00 |\n+------+----------+------+---------------------+\n1 rows in set (0.01 sec)\n"})}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"setting-time-zone-for-load",children:"Setting Time Zone for load"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Load sample data:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"1,Benjamin,18,2024-02-04 10:00:00\n2,Emily,20,2024-02-05 11:00:00\n3,Alexander,22,2024-02-06 12:00:00\n"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Create table:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'CREATE TABLE demo.routine_test06 (\n    id      INT            NOT NULL  COMMENT "id",\n    name    VARCHAR(30)    NOT NULL  COMMENT "name",\n    age     INT                      COMMENT "age",\n    date    DATETIME                 COMMENT "date"\n)\nDUPLICATE KEY(id)\nDISTRIBUTED BY HASH(id) BUCKETS 1;\n'})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Load command:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'CREATE ROUTINE LOAD demo.kafka_job06 ON routine_test06\n        COLUMNS TERMINATED BY ","\n        PROPERTIES\n        (\n            "timezone" = "Asia/Shanghai"\n        )\n        FROM KAFKA\n        (\n            "kafka_broker_list" = "10.16.10.6:9092",\n            "kafka_topic" = "routineLoad06",\n            "property.kafka_default_offsets" = "OFFSET_BEGINNING"\n        );  \n'})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Load result:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"mysql> select * from routine_test06;\n+------+-------------+------+---------------------+\n| id   | name        | age  | date                |\n+------+-------------+------+---------------------+\n|    1 | Benjamin    |   18 | 2024-02-04 10:00:00 |\n|    2 | Emily       |   20 | 2024-02-05 11:00:00 |\n|    3 | Alexander   |   22 | 2024-02-06 12:00:00 |\n+------+-------------+------+---------------------+\n3 rows in set (0.00 sec)\n"})}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"setting-merge_type",children:"Setting merge_type"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Specify merge_type for delete operation"})}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Load sample data:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"3,Alexander,22\n5,William,26\n"})}),"\n",(0,i.jsx)(n.p,{children:"Table data before load:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"mysql> SELECT * FROM routine_test07;\n+------+----------------+------+\n| id   | name           | age  |\n+------+----------------+------+\n|    1 | Benjamin       |   18 |\n|    2 | Emily          |   20 |\n|    3 | Alexander      |   22 |\n|    4 | Sophia         |   24 |\n|    5 | William        |   26 |\n|    6 | Charlotte      |   28 |\n+------+----------------+------+\n"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Create table:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'CREATE TABLE demo.routine_test07 (\n    id      INT            NOT NULL  COMMENT "id",\n    name    VARCHAR(30)    NOT NULL  COMMENT "name",\n    age     INT                      COMMENT "age"\n)\nUNIQUE KEY(id)\nDISTRIBUTED BY HASH(id) BUCKETS 1;\n'})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Load command:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'CREATE ROUTINE LOAD demo.kafka_job07 ON routine_test07\n        WITH DELETE\n        COLUMNS TERMINATED BY ","\n        FROM KAFKA\n        (\n            "kafka_broker_list" = "10.16.10.6:9092",\n            "kafka_topic" = "routineLoad07",\n            "property.kafka_default_offsets" = "OFFSET_BEGINNING"\n        );  \n'})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Load result:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"mysql> SELECT * FROM routine_test07;\n+------+----------------+------+\n| id   | name           | age  |\n+------+----------------+------+\n|    1 | Benjamin       |   18 |\n|    2 | Emily          |   20 |\n|    4 | Sophia         |   24 |\n|    6 | Charlotte      |   28 |\n+------+----------------+------+\n"})}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Specify merge_type for merge operation"})}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Load sample data:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"1,xiaoxiaoli,28\n2,xiaoxiaowang,30\n3,xiaoxiaoliu,32\n4,dadali,34\n5,dadawang,36\n6,dadaliu,38\n"})}),"\n",(0,i.jsx)(n.p,{children:"Table data before load:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"mysql> SELECT * FROM routine_test08;\n+------+----------------+------+\n| id   | name           | age  |\n+------+----------------+------+\n|    1 | Benjamin       |   18 |\n|    2 | Emily          |   20 |\n|    3 | Alexander      |   22 |\n|    4 | Sophia         |   24 |\n|    5 | William        |   26 |\n|    6 | Charlotte      |   28 |\n+------+----------------+------+\n6 rows in set (0.01 sec)\n"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Create table:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'CREATE TABLE demo.routine_test08 (\n    id      INT            NOT NULL  COMMENT "id",\n    name    VARCHAR(30)    NOT NULL  COMMENT "name",\n    age     INT                      COMMENT "age"\n)\nUNIQUE KEY(id)\nDISTRIBUTED BY HASH(id) BUCKETS 1;\n'})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Load command:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'CREATE ROUTINE LOAD demo.kafka_job08 ON routine_test08\n        WITH MERGE\n        COLUMNS TERMINATED BY ",",\n        DELETE ON id = 2\n        FROM KAFKA\n        (\n            "kafka_broker_list" = "10.16.10.6:9092",\n            "kafka_topic" = "routineLoad08",\n            "property.kafka_default_offsets" = "OFFSET_BEGINNING"\n        );   \n'})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Load result:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"mysql> SELECT * FROM routine_test08;\n+------+-------------+------+\n| id   | name        | age  |\n+------+-------------+------+\n|    1 | xiaoxiaoli  |   28 |\n|    3 | xiaoxiaoliu |   32 |\n|    4 | dadali      |   34 |\n|    5 | dadawang    |   36 |\n|    6 | dadaliu     |   38 |\n+------+-------------+------+\n5 rows in set (0.00 sec)\n"})}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Specifying the sequence column to be merged"})}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Load sample data:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"1,xiaoxiaoli,28\n2,xiaoxiaowang,30\n3,xiaoxiaoliu,32\n4,dadali,34\n5,dadawang,36\n6,dadaliu,38\n"})}),"\n",(0,i.jsx)(n.p,{children:"Data in the table before loading:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"mysql> SELECT * FROM routine_test09;\n+------+----------------+------+\n| id   | name           | age  |\n+------+----------------+------+\n|    1 | Benjamin       |   18 |\n|    2 | Emily          |   20 |\n|    3 | Alexander      |   22 |\n|    4 | Sophia         |   24 |\n|    5 | William        |   26 |\n|    6 | Charlotte      |   28 |\n+------+----------------+------+\n6 rows in set (0.01 sec)\n"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Create table"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'CREATE TABLE demo.routine_test08 (\n    id      INT            NOT NULL  COMMENT "id",\n    name    VARCHAR(30)    NOT NULL  COMMENT "name",\n    age     INT                      COMMENT "age",\n)\nUNIQUE KEY(id)\nDISTRIBUTED BY HASH(id) BUCKETS 1\nPROPERTIES (\n    "function_column.sequence_col" = "age"\n);\n'})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Load Command"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'CREATE ROUTINE LOAD demo.kafka_job09 ON routine_test09\n        WITH MERGE \n        COLUMNS TERMINATED BY ",",\n        COLUMNS(id, name, age),\n        DELETE ON id = 2,\n        ORDER BY age\n        PROPERTIES\n        (\n            "desired_concurrent_number"="1",\n            "strict_mode" = "false"\n        )\n        FROM KAFKA\n        (\n            "kafka_broker_list" = "10.16.10.6:9092",\n            "kafka_topic" = "routineLoad09",\n            "property.kafka_default_offsets" = "OFFSET_BEGINNING"\n        );   \n'})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Load result:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"mysql> SELECT * FROM routine_test09;\n+------+-------------+------+\n| id   | name        | age  |\n+------+-------------+------+\n|    1 | xiaoxiaoli  |   28 |\n|    3 | xiaoxiaoliu |   32 |\n|    4 | dadali      |   34 |\n|    5 | dadawang    |   36 |\n|    6 | dadaliu     |   38 |\n+------+-------------+------+\n5 rows in set (0.00 sec)\n"})}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"load-with-column-mapping-and-derived-column-calculation",children:"Load with column mapping and derived column calculation"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Load sample data:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"1,Benjamin,18\n2,Emily,20\n3,Alexander,22\n"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Create table:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'CREATE TABLE demo.routine_test10 (\n    id      INT            NOT NULL  COMMENT "id",\n    name    VARCHAR(30)    NOT NULL  COMMENT "name",\n    age     INT                      COMMENT "age",\n    num     INT                      COMMENT "number"\n)\nDUPLICATE KEY(`id`)\nDISTRIBUTED BY HASH(`id`) BUCKETS 1;\n'})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Load command:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'CREATE ROUTINE LOAD demo.kafka_job10 ON routine_test10\n        COLUMNS TERMINATED BY ",",\n        COLUMNS(id, name, age, num=age*10)\n        FROM KAFKA\n        (\n            "kafka_broker_list" = "10.16.10.6:9092",\n            "kafka_topic" = "routineLoad10",\n            "property.kafka_default_offsets" = "OFFSET_BEGINNING"\n        );  \n'})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Load result:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"mysql> SELECT * FROM routine_test10;\n+------+----------------+------+------+\n| id   | name           | age  | num  |\n+------+----------------+------+------+\n|    1 | Benjamin       |   18 |  180 |\n|    2 | Emily          |   20 |  200 |\n|    3 | Alexander      |   22 |  220 |\n+------+----------------+------+------+\n3 rows in set (0.01 sec)\n"})}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"load-with-enclosed-data",children:"Load with enclosed data"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Load sample data:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'1,"Benjamin",18\n2,"Emily",20\n3,"Alexander",22\n'})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Create table:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'CREATE TABLE demo.routine_test11 (\n    id      INT            NOT NULL  COMMENT "id",\n    name    VARCHAR(30)    NOT NULL  COMMENT "name",\n    age     INT                      COMMENT "age",\n    num     INT                      COMMENT "number"\n)\nDUPLICATE KEY(`id`)\nDISTRIBUTED BY HASH(`id`) BUCKETS 1;\n'})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Load command:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'CREATE ROUTINE LOAD demo.kafka_job11 ON routine_test11\n        COLUMNS TERMINATED BY ","\n        PROPERTIES\n        (\n            "desired_concurrent_number"="1",\n            "enclose" = "\\""\n        )\n        FROM KAFKA\n        (\n            "kafka_broker_list" = "10.16.10.6:9092",\n            "kafka_topic" = "routineLoad12",\n            "property.kafka_default_offsets" = "OFFSET_BEGINNING"\n        );\n'})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Load result:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"mysql> SELECT * FROM routine_test11;\n+------+----------------+------+------+\n| id   | name           | age  | num  |\n+------+----------------+------+------+\n|    1 | Benjamin       |   18 |  180 |\n|    2 | Emily          |   20 |  200 |\n|    3 | Alexander      |   22 |  220 |\n+------+----------------+------+------+\n3 rows in set (0.02 sec)\n"})}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"json-format-load",children:"JSON Format Load"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Load JSON format data in simple mode"})}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Load sample data:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'{ "id" : 1, "name" : "Benjamin", "age":18 }\n{ "id" : 2, "name" : "Emily", "age":20 }\n{ "id" : 3, "name" : "Alexander", "age":22 }\n'})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Create table:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'CREATE TABLE demo.routine_test12 (\n    id      INT            NOT NULL  COMMENT "id",\n    name    VARCHAR(30)    NOT NULL  COMMENT "name",\n    age     INT                      COMMENT "age"\n)\nDUPLICATE KEY(`id`)\nDISTRIBUTED BY HASH(`id`) BUCKETS 1;\n'})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Load command:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'CREATE ROUTINE LOAD demo.kafka_job12 ON routine_test12\n        PROPERTIES\n        (\n            "format" = "json"\n        )\n        FROM KAFKA\n        (\n            "kafka_broker_list" = "10.16.10.6:9092",\n            "kafka_topic" = "routineLoad12",\n            "property.kafka_default_offsets" = "OFFSET_BEGINNING"\n        );  \n'})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Load result:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"mysql> select * from routine_test12;\n+------+----------------+------+\n| id   | name           | age  |\n+------+----------------+------+\n|    1 | Benjamin       |   18 |\n|    2 | Emily          |   20 |\n|    3 | Alexander      |   22 |\n+------+----------------+------+\n3 rows in set (0.02 sec)\n"})}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Load complex JSON format data in match mode"})}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Load sample data"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'{ "name" : "Benjamin", "id" : 1, "num":180 , "age":18 }\n{ "name" : "Emily", "id" : 2, "num":200 , "age":20 }\n{ "name" : "Alexander", "id" : 3, "num":220 , "age":22 }\n'})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Create table:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'CREATE TABLE demo.routine_test13 (\n    id      INT            NOT NULL  COMMENT "id",\n    name    VARCHAR(30)    NOT NULL  COMMENT "name",\n    age     INT                      COMMENT "age",\n    num     INT                      COMMENT "num"\n)\nDUPLICATE KEY(`id`)\nDISTRIBUTED BY HASH(`id`) BUCKETS 1;\n'})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Load command:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'CREATE ROUTINE LOAD demo.kafka_job13 ON routine_test13\n        COLUMNS(name, id, num, age)\n        PROPERTIES\n        (\n            "format" = "json",\n            "jsonpaths" = "[\\"$.name\\",\\"$.id\\",\\"$.num\\",\\"$.age\\"]"\n        )\n        FROM KAFKA\n        (\n            "kafka_broker_list" = "10.16.10.6:9092",\n            "kafka_topic" = "routineLoad13",\n            "property.kafka_default_offsets" = "OFFSET_BEGINNING"\n        );  \n'})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Load result:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"mysql> select * from routine_test13;\n+------+----------------+------+------+\n| id   | name           | age  | num  |\n+------+----------------+------+------+\n|    1 | Benjamin       |   18 |  180 |\n|    2 | Emily          |   20 |  200 |\n|    3 | Alexander      |   22 |  220 |\n+------+----------------+------+------+\n3 rows in set (0.01 sec)\n"})}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Loading data with specified JSON root node"})}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Load sample data"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'{"id": 1231, "source" :{ "id" : 1, "name" : "Benjamin", "age":18 }}\n{"id": 1232, "source" :{ "id" : 2, "name" : "Emily", "age":20 }}\n{"id": 1233, "source" :{ "id" : 3, "name" : "Alexander", "age":22 }}\n'})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Create table:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'CREATE TABLE demo.routine_test14 (\n    id      INT            NOT NULL  COMMENT "id",\n    name    VARCHAR(30)    NOT NULL  COMMENT "name",\n    age     INT                      COMMENT "age"\n)\nDUPLICATE KEY(`id`)\nDISTRIBUTED BY HASH(`id`) BUCKETS 1;\n'})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Load command"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'CREATE ROUTINE LOAD demo.kafka_job14 ON routine_test14\n        PROPERTIES\n        (\n            "format" = "json",\n            "json_root" = "$.source"\n        )\n        FROM KAFKA\n        (\n            "kafka_broker_list" = "10.16.10.6:9092",\n            "kafka_topic" = "routineLoad14",\n            "property.kafka_default_offsets" = "OFFSET_BEGINNING"\n        );  \n'})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Load result"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"mysql> select * from routine_test14;\n+------+----------------+------+\n| id   | name           | age  |\n+------+----------------+------+\n|    1 | Benjamin       |   18 |\n|    2 | Emily          |   20 |\n|    3 | Alexander      |   22 |\n+------+----------------+------+\n3 rows in set (0.01 sec)\n"})}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Loading data with column mapping and derived column calculation"})}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Load sample data:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'{ "id" : 1, "name" : "Benjamin", "age":18 }\n{ "id" : 2, "name" : "Emily", "age":20 }\n{ "id" : 3, "name" : "Alexander", "age":22 }\n'})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Create table:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'CREATE TABLE demo.routine_test15 (\n    id      INT            NOT NULL  COMMENT "id",\n    name    VARCHAR(30)    NOT NULL  COMMENT "name",\n    age     INT                      COMMENT "age",\n    num     INT                      COMMENT "num"\n)\nDUPLICATE KEY(`id`)\nDISTRIBUTED BY HASH(`id`) BUCKETS 1;\n'})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Load command:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'CREATE ROUTINE LOAD demo.kafka_job15 ON routine_test15\n        COLUMNS(id, name, age, num=age*10)\n        PROPERTIES\n        (\n            "format" = "json",\n        )\n        FROM KAFKA\n        (\n            "kafka_broker_list" = "10.16.10.6:9092",\n            "kafka_topic" = "routineLoad15",\n            "property.kafka_default_offsets" = "OFFSET_BEGINNING"\n        );  \n'})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Load result:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"mysql> select * from routine_test15;\n+------+----------------+------+------+\n| id   | name           | age  | num  |\n+------+----------------+------+------+\n|    1 | Benjamin       |   18 |  180 |\n|    2 | Emily          |   20 |  200 |\n|    3 | Alexander      |   22 |  220 |\n+------+----------------+------+------+\n3 rows in set (0.01 sec)\n"})}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"loading-complex-data-types",children:"Loading Complex Data Types"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Load Array Data Type"})}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Load sample data:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'{ "id" : 1, "name" : "Benjamin", "age":18, "array":[1,2,3,4,5]}\n{ "id" : 2, "name" : "Emily", "age":20, "array":[6,7,8,9,10]}\n{ "id" : 3, "name" : "Alexander", "age":22, "array":[11,12,13,14,15]}\n'})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Create table:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'CREATE TABLE demo.routine_test16\n(\n    id      INT             NOT NULL  COMMENT "id",\n    name    VARCHAR(30)     NOT NULL  COMMENT "name",\n    age     INT                       COMMENT "age",\n    array   ARRAY<int(11)>  NULL      COMMENT "test array column"\n)\nDUPLICATE KEY(`id`)\nDISTRIBUTED BY HASH(`id`) BUCKETS 1;\n'})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Load command:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'CREATE ROUTINE LOAD demo.kafka_job16 ON routine_test16\n        PROPERTIES\n        (\n            "format" = "json"\n        )\n        FROM KAFKA\n        (\n            "kafka_broker_list" = "10.16.10.6:9092",\n            "kafka_topic" = "routineLoad16",\n            "property.kafka_default_offsets" = "OFFSET_BEGINNING"\n        );  \n'})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Load result:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"mysql> select * from routine_test16;\n+------+----------------+------+----------------------+\n| id   | name           | age  | array                |\n+------+----------------+------+----------------------+\n|    1 | Benjamin       |   18 | [1, 2, 3, 4, 5]      |\n|    2 | Emily          |   20 | [6, 7, 8, 9, 10]     |\n|    3 | Alexander      |   22 | [11, 12, 13, 14, 15] |\n+------+----------------+------+----------------------+\n3 rows in set (0.00 sec)\n"})}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Loading Map Data Type"})}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Load sample data:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'{ "id" : 1, "name" : "Benjamin", "age":18, "map":{"a": 100, "b": 200}}\n{ "id" : 2, "name" : "Emily", "age":20, "map":{"c": 300, "d": 400}}\n{ "id" : 3, "name" : "Alexander", "age":22, "map":{"e": 500, "f": 600}}\n'})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Create table:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'CREATE TABLE demo.routine_test17 (\n    id      INT                 NOT NULL  COMMENT "id",\n    name    VARCHAR(30)         NOT NULL  COMMENT "name",\n    age     INT                           COMMENT "age",\n    map     Map<STRING, INT>    NULL      COMMENT "test column"\n)\nDUPLICATE KEY(`id`)\nDISTRIBUTED BY HASH(`id`) BUCKETS 1;\n'})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Load command:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'CREATE ROUTINE LOAD demo.kafka_job17 ON routine_test17\n    PROPERTIES\n        (\n            "format" = "json"\n        )\n        FROM KAFKA\n        (\n            "kafka_broker_list" = "10.16.10.6:9092",\n            "kafka_topic" = "routineLoad17",\n            "property.kafka_default_offsets" = "OFFSET_BEGINNING"\n        );  \n'})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Load result:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'mysql> select * from routine_test17;\n+------+----------------+------+--------------------+\n| id   | name           | age  | map                |\n+------+----------------+------+--------------------+\n|    1 | Benjamin       |   18 | {"a":100, "b":200} |\n|    2 | Emily          |   20 | {"c":300, "d":400} |\n|    3 | Alexander      |   22 | {"e":500, "f":600} |\n+------+----------------+------+--------------------+\n3 rows in set (0.01 sec)\n'})}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Loading Bitmap Data Type"})}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Load sample data"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'{ "id" : 1, "name" : "Benjamin", "age":18, "bitmap_id":243}\n{ "id" : 2, "name" : "Emily", "age":20, "bitmap_id":28574}\n{ "id" : 3, "name" : "Alexander", "age":22, "bitmap_id":8573}\n'})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Create table:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'CREATE TABLE demo.routine_test18 (\n    id        INT            NOT NULL      COMMENT "id",\n    name      VARCHAR(30)    NOT NULL      COMMENT "name",\n    age       INT                          COMMENT "age",\n    bitmap_id INT                          COMMENT "test",\n    device_id BITMAP         BITMAP_UNION  COMMENT "test column"\n)\nAGGREGATE KEY (`id`,`name`,`age`,`bitmap_id`)\nDISTRIBUTED BY HASH(`id`) BUCKETS 1;\n'})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Load command:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'CREATE ROUTINE LOAD demo.kafka_job18 ON routine_test18\n        COLUMNS(id, name, age, bitmap_id, device_id=to_bitmap(bitmap_id))\n        PROPERTIES\n        (\n            "format" = "json"\n        )\n        FROM KAFKA\n        (\n            "kafka_broker_list" = "10.16.10.6:9092",\n            "kafka_topic" = "routineLoad18",\n            "property.kafka_default_offsets" = "OFFSET_BEGINNING"\n        );\n'})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Load result:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"mysql> select id, BITMAP_UNION_COUNT(pv) over(order by id) uv from(\n    ->    select id, BITMAP_UNION(device_id) as pv\n    ->    from routine_test18 \n    -> group by id \n    -> ) final;\n+------+------+\n| id   | uv   |\n+------+------+\n|    1 |    1 |\n|    2 |    2 |\n|    3 |    3 |\n+------+------+\n3 rows in set (0.00 sec)\n"})}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Loading HLL Data Type"})}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Loading sample data:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"2022-05-05,10001,Test01,Beijing,windows\n2022-05-05,10002,Test01,Beijing,linux\n2022-05-05,10003,Test01,Beijing,macos\n2022-05-05,10004,Test01,Hebei,windows\n2022-05-06,10001,Test01,Shanghai,windows\n2022-05-06,10002,Test01,Shanghai,linux\n2022-05-06,10003,Test01,Jiangsu,macos\n2022-05-06,10004,Test01,Shaanxi,windows\n"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Create table:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"create table demo.routine_test19 (\n    dt        DATE,\n    id        INT,\n    name      VARCHAR(10),\n    province  VARCHAR(10),\n    os        VARCHAR(10),\n    pv        hll hll_union\n)\nAggregate KEY (dt,id,name,province,os)\ndistributed by hash(id) buckets 10;\n"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Load command:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'CREATE ROUTINE LOAD demo.kafka_job19 ON routine_test19\n        COLUMNS TERMINATED BY ",",\n        COLUMNS(dt, id, name, province, os, pv=hll_hash(id))\n        FROM KAFKA\n        (\n            "kafka_broker_list" = "10.16.10.6:9092",\n            "kafka_topic" = "routineLoad19",\n            "property.kafka_default_offsets" = "OFFSET_BEGINNING"\n        );  \n'})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Load result:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"mysql> select * from routine_test19;\n+------------+-------+----------+----------+---------+------+\n| dt         | id    | name     | province | os      | pv   |\n+------------+-------+----------+----------+---------+------+\n| 2022-05-05 | 10001 | Test01   | Beijing     | windows | NULL |\n| 2022-05-06 | 10001 | Test01   | Shanghai    | windows | NULL |\n| 2022-05-05 | 10002 | Test01   | Beijing     | linux   | NULL |\n| 2022-05-06 | 10002 | Test01   | Shanghai    | linux   | NULL |\n| 2022-05-05 | 10004 | Test01   | Hebei      | windows | NULL |\n| 2022-05-06 | 10004 | Test01   | Shaanxi      | windows | NULL |\n| 2022-05-05 | 10003 | Test01   | Beijing     | macos   | NULL |\n| 2022-05-06 | 10003 | Test01   | Jiangsu     | macos   | NULL |\n+------------+-------+----------+----------+---------+------+\n8 rows in set (0.01 sec)\n\nmysql> SELECT HLL_UNION_AGG(pv) FROM routine_test19;\n+-------------------+\n| hll_union_agg(pv) |\n+-------------------+\n|                 4 |\n+-------------------+\n1 row in set (0.01 sec)\n"})}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"kafka-security-authentication",children:"Kafka Security Authentication"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Loading Kafka Data with SSL Authentication"})}),"\n",(0,i.jsx)(n.p,{children:"Example load command:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-SQL",children:'CREATE ROUTINE LOAD demo.kafka_job20 ON routine_test20\n        PROPERTIES\n        (\n            "format" = "json"\n        )\n        FROM KAFKA\n        (\n            "kafka_broker_list" = "192.168.100.129:9092",\n            "kafka_topic" = "routineLoad21",\n            "property.security.protocol" = "ssl",\n            "property.ssl.ca.location" = "FILE:ca.pem",\n            "property.ssl.certificate.location" = "FILE:client.pem",\n            "property.ssl.key.location" = "FILE:client.key",\n            "property.ssl.key.password" = "ssl_passwd"\n        );  \n'})}),"\n",(0,i.jsx)(n.p,{children:"Parameter descriptions:"}),"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Parameter"}),(0,i.jsx)(n.th,{children:"Description"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"property.security.protocol"}),(0,i.jsx)(n.td,{children:"The security protocol used, in this example it is SSL"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"property.ssl.ca.location"}),(0,i.jsx)(n.td,{children:"The location of the CA (Certificate Authority) certificate"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"property.ssl.certificate.location"}),(0,i.jsx)(n.td,{children:"The location of the Client's public key (required if client authentication is enabled on the Kafka server)"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"property.ssl.key.location"}),(0,i.jsx)(n.td,{children:"The location of the Client's private key (required if client authentication is enabled on the Kafka server)"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"property.ssl.key.password"}),(0,i.jsx)(n.td,{children:"The password for the Client's private key (required if client authentication is enabled on the Kafka server)"})]})]})]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Loading Kafka Data with Kerberos Authentication"})}),"\n",(0,i.jsx)(n.p,{children:"Example load command:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-SQL",children:'CREATE ROUTINE LOAD demo.kafka_job21 ON routine_test21\n        PROPERTIES\n        (\n            "format" = "json"\n        )\n        FROM KAFKA\n        (\n            "kafka_broker_list" = "192.168.100.129:9092",\n            "kafka_topic" = "routineLoad21",\n            "property.security.protocol" = "SASL_PLAINTEXT",\n            "property.sasl.kerberos.service.name" = "kafka",\n            "property.sasl.kerberos.keytab"="/opt/third/kafka/kerberos/kafka_client.keytab",\n            "property.sasl.kerberos.principal" = "clients/stream.dt.local@EXAMPLE.COM"\n        );  \n'})}),"\n",(0,i.jsx)(n.p,{children:"Parameter descriptions:"}),"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Parameter"}),(0,i.jsx)(n.th,{children:"Description"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"property.security.protocol"}),(0,i.jsx)(n.td,{children:"The security protocol used, in this example it is SASL_PLAINTEXT"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"property.sasl.kerberos.service.name"}),(0,i.jsx)(n.td,{children:"Specifies the broker service name, default is Kafka"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"property.sasl.kerberos.keytab"}),(0,i.jsx)(n.td,{children:"The location of the keytab file"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"property.sasl.kerberos.principal"}),(0,i.jsx)(n.td,{children:"Specifies the Kerberos principal"})]})]})]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Loading Kafka Cluster with PLAIN Authentication"})}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Example load command:"}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-SQL",children:'CREATE ROUTINE LOAD demo.kafka_job22 ON routine_test22\n        PROPERTIES\n        (\n            "format" = "json"\n        )\n        FROM KAFKA\n        (\n            "kafka_broker_list" = "192.168.100.129:9092",\n            "kafka_topic" = "routineLoad22",\n            "property.security.protocol"="SASL_PLAINTEXT",\n            "property.sasl.mechanism"="PLAIN",\n            "property.sasl.username"="admin",\n            "property.sasl.password"="admin"\n        );  \n'})}),"\n",(0,i.jsx)(n.p,{children:"Parameter descriptions:"}),"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Parameter"}),(0,i.jsx)(n.th,{children:"Description"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"property.security.protocol"}),(0,i.jsx)(n.td,{children:"The security protocol used, in this example it is SASL_PLAINTEXT"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"property.sasl.mechanism"}),(0,i.jsx)(n.td,{children:"Specifies the SASL authentication mechanism as PLAIN"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"property.sasl.username"}),(0,i.jsx)(n.td,{children:"The username for SASL"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"property.sasl.password"}),(0,i.jsx)(n.td,{children:"The password for SASL"})]})]})]}),"\n",(0,i.jsx)(n.h3,{id:"single-task-loading-to-multiple-tables",children:"Single-task Loading to Multiple Tables"}),"\n",(0,i.jsx)(n.p,{children:'Create a Kafka routine dynamic table load task named "test1" for the "example_db". Specify the column delimiter, group.id, and client.id. Automatically consume all partitions and start subscribing from the available data position (OFFSET_BEGINNING).'}),"\n",(0,i.jsxs)(n.p,{children:['Assuming we need to load data from Kafka into tables "tbl1" and "tbl2" in the "example_db", we create a Routine Load task named "test1". This task will load data from Kafka\'s topic ',(0,i.jsx)(n.code,{children:"my_topic"}),' into both "tbl1" and "tbl2" simultaneously. This way, we can load data from Kafka into two tables using a single routine load task.']}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'CREATE ROUTINE LOAD example_db.test1\nFROM KAFKA\n(\n    "kafka_broker_list" = "broker1:9092,broker2:9092,broker3:9092",\n    "kafka_topic" = "my_topic",\n    "property.kafka_default_offsets" = "OFFSET_BEGINNING"\n);\n'})}),"\n",(0,i.jsxs)(n.p,{children:["Currently, only extracting the table name from the Value field of Kafka is supported. The format should be as follows, using JSON as an example: ",(0,i.jsx)(n.code,{children:'table_name|{"col1": "val1", "col2": "val2"}'}),", where ",(0,i.jsx)(n.code,{children:"tbl_name"})," is the table name and ",(0,i.jsx)(n.code,{children:"|"})," is used as the separator between the table name and the table data. The same format applies to CSV data, such as ",(0,i.jsx)(n.code,{children:"table_name|val1,val2,val3"}),". Note that the ",(0,i.jsx)(n.code,{children:"table_name"})," here must be consistent with the table name in Doris, otherwise the load will fail. Note that dynamic tables do not support the column_mapping configuration described later."]}),"\n",(0,i.jsx)(n.h3,{id:"strict-mode-load",children:"Strict Mode Load"}),"\n",(0,i.jsx)(n.p,{children:'Create a Kafka routine load task named "test1" for the "example_db" and "example_tbl". The load task is set to strict mode.'}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'CREATE ROUTINE LOAD example_db.test1 ON example_tbl\nCOLUMNS(k1, k2, k3, v1, v2, v3 = k1 * 100),\nPRECEDING FILTER k1 = 1,\nWHERE k1 < 100 and k2 like "%doris%"\nPROPERTIES\n(\n    "strict_mode" = "true"\n)\nFROM KAFKA\n(\n    "kafka_broker_list" = "broker1:9092,broker2:9092,broker3:9092",\n    "kafka_topic" = "my_topic"\n);\n'})}),"\n",(0,i.jsx)(n.h2,{id:"connect-to-the-sasl-kafka-service",children:"Connect to the SASL Kafka service"}),"\n",(0,i.jsx)(n.p,{children:"Here we take accessing the StreamNative message service as an example:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'CREATE ROUTINE LOAD example_db.test1 ON example_tbl\nCOLUMNS(user_id, name, age)\nFROM KAFKA (\n"kafka_broker_list" = "pc-xxxx.aws-mec1-test-xwiqv.aws.snio.cloud:9093",\n"kafka_topic" = "my_topic",\n"property.security.protocol" = "SASL_SSL",\n"property.sasl.mechanism" = "PLAIN",\n"property.sasl.username" = "user",\n"property.sasl.password" = "token:eyJhbxxx",\n"property.group.id" = "my_group_id_1",\n"property.client.id" = "my_client_id_1",\n"property.enable.ssl.certificate.verification" = "false"\n);\n'})}),"\n",(0,i.jsxs)(n.p,{children:["Note that if the trusted CA certificate path is not configured on the BE side, you need to set ",(0,i.jsx)(n.code,{children:'"property.enable.ssl.certificate.verification" = "false"'})," to not verify whether the server certificate is credible."]}),"\n",(0,i.jsxs)(n.p,{children:["Otherwise, you need to configure the trusted CA certificate path: ",(0,i.jsx)(n.code,{children:'"property.ssl.ca.location" = "/path/to/ca-cert.pem"'}),"."]}),"\n",(0,i.jsx)(n.h2,{id:"more-details",children:"More Details"}),"\n",(0,i.jsxs)(n.p,{children:["Refer to the SQL manual on ",(0,i.jsx)(n.a,{href:"../../../sql-manual/sql-statements/data-modification/load-and-export/CREATE-ROUTINE-LOAD",children:"Routine Load"}),". You can also enter ",(0,i.jsx)(n.code,{children:"HELP ROUTINE LOAD"})," in the client command line for more help."]})]})}function h(e={}){let{wrapper:n}={...(0,t.a)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},113865:function(e,n,a){a.d(n,{Z:function(){return s}});let s=a.p+"assets/images/routine-load-980e3f00a54145d3354f4f14d9e5d783.png"},250065:function(e,n,a){a.d(n,{Z:function(){return l},a:function(){return r}});var s=a(667294);let i={},t=s.createContext(i);function r(e){let n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);