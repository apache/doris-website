"use strict";(self.webpackChunkdoris_website=self.webpackChunkdoris_website||[]).push([["169612"],{471018:function(e,a,t){t.r(a),t.d(a,{default:()=>m,frontMatter:()=>r,metadata:()=>i,assets:()=>d,toc:()=>s,contentTitle:()=>l});var i=JSON.parse('{"id":"data-operate/import/load-best-practices","title":"Load Best Practices","description":"It is recommended to prioritize using the Duplicate Key model,","source":"@site/versioned_docs/version-3.x/data-operate/import/load-best-practices.md","sourceDirName":"data-operate/import","slug":"/data-operate/import/load-best-practices","permalink":"/docs/3.x/data-operate/import/load-best-practices","draft":false,"unlisted":false,"tags":[],"version":"3.x","lastUpdatedAt":1770477659000,"frontMatter":{"title":"Load Best Practices","language":"en","description":"It is recommended to prioritize using the Duplicate Key model,"},"sidebar":"docs","previous":{"title":"High Concurrency LOAD Optimization(Group Commit)","permalink":"/docs/3.x/data-operate/import/group-commit-manual"},"next":{"title":"Stream Load in Complex Network Environments","permalink":"/docs/3.x/data-operate/import/load-internals/stream-load-in-complex-network"}}'),n=t("785893"),o=t("250065");let r={title:"Load Best Practices",language:"en",description:"It is recommended to prioritize using the Duplicate Key model,"},l=void 0,d={},s=[{value:"Table Model Selection",id:"table-model-selection",level:2},{value:"Partition and Bucket Configuration",id:"partition-and-bucket-configuration",level:2},{value:"Random Bucketing",id:"random-bucketing",level:2},{value:"Batch Loading",id:"batch-loading",level:2},{value:"Partition Loading",id:"partition-loading",level:2},{value:"Large-scale Data Batch Loading",id:"large-scale-data-batch-loading",level:2},{value:"Broker Load Concurrency",id:"broker-load-concurrency",level:2},{value:"Stream Load Concurrency",id:"stream-load-concurrency",level:2}];function c(e){let a={a:"a",h2:"h2",p:"p",...(0,o.a)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(a.h2,{id:"table-model-selection",children:"Table Model Selection"}),"\n",(0,n.jsxs)(a.p,{children:["It is recommended to prioritize using the Duplicate Key model, which offers advantages in both data loading and query performance compared to other models. For more information, please refer to: ",(0,n.jsx)(a.a,{href:"../../table-design/data-model/overview",children:"Data Model"})]}),"\n",(0,n.jsx)(a.h2,{id:"partition-and-bucket-configuration",children:"Partition and Bucket Configuration"}),"\n",(0,n.jsxs)(a.p,{children:["It is recommended to keep the size of a tablet between 1-10GB. Tablets that are too small may lead to poor aggregation performance and increase metadata management overhead; tablets that are too large may hinder replica migration and repair. For details, please refer to: ",(0,n.jsx)(a.a,{href:"../../table-design/data-partitioning/data-distribution",children:"Data Distribution"}),"."]}),"\n",(0,n.jsx)(a.h2,{id:"random-bucketing",children:"Random Bucketing"}),"\n",(0,n.jsxs)(a.p,{children:["When using Random bucketing, you can enable single-tablet loading mode by setting load_to_single_tablet to true. This mode can improve data loading concurrency and throughput while reducing write amplification during large-scale data loading. For details, refer to: ",(0,n.jsx)(a.a,{href:"../../table-design/data-partitioning/data-bucketing#random-bucketing",children:"Random Bucketing"})]}),"\n",(0,n.jsx)(a.h2,{id:"batch-loading",children:"Batch Loading"}),"\n",(0,n.jsxs)(a.p,{children:["Client-side batching: It is recommended to batch data (from several MB to GB in size) on the client side before loading. High-frequency small loads will cause frequent compaction, leading to severe write amplification issues.\nServer-side batching: For high-concurrency small data volume loading, it is recommended to enable ",(0,n.jsx)(a.a,{href:"/docs/3.x/data-operate/import/group-commit-manual",children:"Group Commit"})," to implement batching on the server side."]}),"\n",(0,n.jsx)(a.h2,{id:"partition-loading",children:"Partition Loading"}),"\n",(0,n.jsx)(a.p,{children:"It is recommended to load data from only a few partitions at a time. Loading from too many partitions simultaneously will increase memory usage and may cause performance issues. Each tablet in Doris has an active Memtable in memory, which is flushed to disk when it reaches a certain size. To prevent process OOM, when the active Memtable's memory usage is too high, it will trigger early flushing, resulting in many small files and affecting loading performance."}),"\n",(0,n.jsx)(a.h2,{id:"large-scale-data-batch-loading",children:"Large-scale Data Batch Loading"}),"\n",(0,n.jsx)(a.p,{children:"When dealing with a large number of files or large data volumes, it is recommended to load in batches to avoid high retry costs in case of loading failures and to reduce system resource impact. For Broker Load, it is recommended not to exceed 100GB per batch. For large local data files, you can use Doris's streamloader tool, which automatically performs batch loading."}),"\n",(0,n.jsx)(a.h2,{id:"broker-load-concurrency",children:"Broker Load Concurrency"}),"\n",(0,n.jsx)(a.p,{children:"Compressed files/Parquet/ORC files: It is recommended to split files into multiple smaller files for loading to achieve higher concurrency."}),"\n",(0,n.jsx)(a.p,{children:"Uncompressed CSV and JSON files: Doris will automatically split files and load them concurrently."}),"\n",(0,n.jsxs)(a.p,{children:["For concurrency strategies, please refer to: ",(0,n.jsx)(a.a,{href:"./import-way/broker-load-manual#Related-Configurations",children:"Broker Load Configuration Parameters"})]}),"\n",(0,n.jsx)(a.h2,{id:"stream-load-concurrency",children:"Stream Load Concurrency"}),"\n",(0,n.jsx)(a.p,{children:"It is recommended to keep Stream load concurrency per BE under 128 (controlled by BE's webserver_num_workers parameter). High concurrency may cause webserver thread exhaustion and affect loading performance. Particularly when a single BE's concurrency exceeds 512 (doris_max_remote_scanner_thread_pool_thread_num parameter), it may cause the BE process to hang."})]})}function m(e={}){let{wrapper:a}={...(0,o.a)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},250065:function(e,a,t){t.d(a,{Z:function(){return l},a:function(){return r}});var i=t(667294);let n={},o=i.createContext(n);function r(e){let a=i.useContext(o);return i.useMemo(function(){return"function"==typeof e?e(a):{...a,...e}},[a,e])}function l(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(n):e.components||n:r(e.components),i.createElement(o.Provider,{value:a},e.children)}}}]);