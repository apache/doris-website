"use strict";(self.webpackChunkdoris_website=self.webpackChunkdoris_website||[]).push([["862915"],{461344:function(e,n,t){t.r(n),t.d(n,{default:()=>h,frontMatter:()=>a,metadata:()=>s,assets:()=>d,toc:()=>l,contentTitle:()=>o});var s=JSON.parse('{"id":"data-operate/import/streaming-job","title":"Continuous Load","description":"Doris allows you to create a continuous import task using a Job + TVF approach. After submitting the Job, Doris continuously runs the import job,","source":"@site/docs/data-operate/import/streaming-job.md","sourceDirName":"data-operate/import","slug":"/data-operate/import/streaming-job","permalink":"/docs/dev/data-operate/import/streaming-job","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1770477659000,"frontMatter":{"title":"Continuous Load","language":"en","description":"Doris allows you to create a continuous import task using a Job + TVF approach. After submitting the Job, Doris continuously runs the import job,"},"sidebar":"docs","previous":{"title":"Stream Load in Complex Network Environments","permalink":"/docs/dev/data-operate/import/load-internals/stream-load-in-complex-network"},"next":{"title":"Data Update Overview","permalink":"/docs/dev/data-operate/update/update-overview"}}'),r=t("785893"),i=t("250065");let a={title:"Continuous Load",language:"en",description:"Doris allows you to create a continuous import task using a Job + TVF approach. After submitting the Job, Doris continuously runs the import job,"},o=void 0,d={},l=[{value:"Overview",id:"overview",level:2},{value:"Supported TVFs",id:"supported-tvfs",level:2},{value:"Basic Principles",id:"basic-principles",level:2},{value:"S3",id:"s3",level:3},{value:"Quick Start",id:"quick-start",level:2},{value:"Creating an Import Job",id:"creating-an-import-job",level:3},{value:"Check import status",id:"check-import-status",level:3},{value:"Pause import job",id:"pause-import-job",level:3},{value:"Resume import job",id:"resume-import-job",level:3},{value:"Modify import job",id:"modify-import-job",level:3},{value:"Delete imported jobs",id:"delete-imported-jobs",level:3},{value:"Reference",id:"reference",level:2},{value:"Import command",id:"import-command",level:3},{value:"Importing Parameters",id:"importing-parameters",level:3},{value:"FE Configuration Parameters",id:"fe-configuration-parameters",level:4},{value:"Import Configuration Parameters",id:"import-configuration-parameters",level:4},{value:"Import Status",id:"import-status",level:3},{value:"Job",id:"job",level:4},{value:"Task",id:"task",level:4}];function c(e){let n={a:"a",code:"code",em:"em",h2:"h2",h3:"h3",h4:"h4",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,i.a)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,r.jsx)(n.p,{children:"Doris allows you to create a continuous import task using a Job + TVF approach. After submitting the Job, Doris continuously runs the import job, querying the TVF in real time and writing the data into the Doris table."}),"\n",(0,r.jsx)(n.h2,{id:"supported-tvfs",children:"Supported TVFs"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.a,{href:"/docs/dev/sql-manual/sql-functions/table-valued-functions/s3",children:"S3"})," TVF"]}),"\n",(0,r.jsx)(n.h2,{id:"basic-principles",children:"Basic Principles"}),"\n",(0,r.jsx)(n.h3,{id:"s3",children:"S3"}),"\n",(0,r.jsx)(n.p,{children:"Iterates through the files in the specified directory of S3, splitting each file into a list and writing it to the Doris table in small batches."}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Incremental Read Method"})}),"\n",(0,r.jsx)(n.p,{children:"After creating the task, Doris continuously reads data from the specified path and polls for new files at a fixed frequency."}),"\n",(0,r.jsx)(n.p,{children:"Note: The name of a new file must be lexicographically greater than the name of the last imported file; otherwise, Doris will not treat it as a new file. For example, if files are named file1, file2, and file3, they will be imported sequentially; if a new file named file0 is added later, Doris will not import it because it is lexicographically less than the last imported file, file3."}),"\n",(0,r.jsx)(n.h2,{id:"quick-start",children:"Quick Start"}),"\n",(0,r.jsx)(n.h3,{id:"creating-an-import-job",children:"Creating an Import Job"}),"\n",(0,r.jsx)(n.p,{children:"Assume that files ending in CSV are periodically generated in the S3 directory. You can then create a Job."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-SQL",children:'CREATE JOB my_job \nON STREAMING\nDO \nINSERT INTO db1.tbl1 \nselect * from S3(\n    "uri" = "s3://bucket/*.csv",\n    "s3.access_key" = "<s3_access_key>",\n    "s3.secret_key" = "<s3_secret_key>",\n    "s3.region" = "<s3_region>",\n    "s3.endpoint" = "<s3_endpoint>",\n    "format" = "<format>"\n)\n'})}),"\n",(0,r.jsx)(n.h3,{id:"check-import-status",children:"Check import status"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-SQL",children:'select * from job(type=insert) where ExecuteType = "streaming"\n               Id: 1758538737484\n             Name: my_job1\n          Definer: root\n      ExecuteType: STREAMING\nRecurringStrategy: \\N\n           Status: RUNNING\n       ExecuteSql: INSERT INTO test.`student1`\nSELECT * FROM S3\n(\n    "uri" = "s3://bucket/s3/demo/*.csv",\n    "format" = "csv",\n    "column_separator" = ",",\n    "s3.endpoint" = "s3.ap-southeast-1.amazonaws.com",\n    "s3.region" = "ap-southeast-1",\n    "s3.access_key" = "",\n    "s3.secret_key" = ""\n)\n       CreateTime: 2025-09-22 19:24:51\n SucceedTaskCount: 1\n  FailedTaskCount: 0\nCanceledTaskCount: 0\n          Comment: \\N\n       Properties: \\N\n    CurrentOffset: {"fileName":"s3/demo/test/1.csv"}\n        EndOffset: {"fileName":"s3/demo/test/1.csv"}\n    LoadStatistic: {"scannedRows":20,"loadBytes":425,"fileNumber":2,"fileSize":256}\n         ErrorMsg: \\N\n    JobRuntimeMsg: \\N\n'})}),"\n",(0,r.jsx)(n.h3,{id:"pause-import-job",children:"Pause import job"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-SQL",children:"PAUSE JOB WHERE jobname = <job_name> ;\n"})}),"\n",(0,r.jsx)(n.h3,{id:"resume-import-job",children:"Resume import job"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-SQL",children:"RESUME JOB where jobName = <job_name> ;\n"})}),"\n",(0,r.jsx)(n.h3,{id:"modify-import-job",children:"Modify import job"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-SQL",children:'-- -- Supports modifying Job properties and insert statements\nAlter Job jobName\nPROPERTIES(\n   "session.insert_max_filter_ratio"="0.5" \n)\nINSERT INTO db1.tbl1 \nselect * from S3(\n    "uri" = "s3://bucket/*.csv",\n    "s3.access_key" = "<s3_access_key>",\n    "s3.secret_key" = "<s3_secret_key>",\n    "s3.region" = "<s3_region>",\n    "s3.endpoint" = "<s3_endpoint>",\n    "format" = "<format>"\n)\n'})}),"\n",(0,r.jsx)(n.h3,{id:"delete-imported-jobs",children:"Delete imported jobs"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-SQL",children:"DROP JOB where jobName = <job_name> ;\n"})}),"\n",(0,r.jsx)(n.h2,{id:"reference",children:"Reference"}),"\n",(0,r.jsx)(n.h3,{id:"import-command",children:"Import command"}),"\n",(0,r.jsx)(n.p,{children:"\u521B\u5EFA\u4E00\u4E2A Job + TVF \u5E38\u9A7B\u5BFC\u5165\u4F5C\u4E1A\u8BED\u6CD5\u5982\u4E0B\uFF1A"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-SQL",children:"CREATE JOB <job_name>\nON STREAMING\n[job_properties]\n[ COMMENT <comment> ]\nDO <Insert_Command> \n"})}),"\n",(0,r.jsx)(n.p,{children:"The module description is as follows:"}),"\n",(0,r.jsx)(n.p,{children:"| Module | Description |"}),"\n",(0,r.jsx)(n.p,{children:"| -------------- | ------------------------------------------------------------ |\n| job_name | Task name |\n| job_properties | General import parameters used to specify the Job |\n| comment | Remarks used to describe the Job |\n| Insert_Command | SQL to execute; currently only Insert into table select * from s3() is supported |"}),"\n",(0,r.jsx)(n.h3,{id:"importing-parameters",children:"Importing Parameters"}),"\n",(0,r.jsx)(n.h4,{id:"fe-configuration-parameters",children:"FE Configuration Parameters"}),"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Parameter"}),(0,r.jsx)(n.th,{children:"Default Value"}),(0,r.jsx)(n.th,{})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"max_streaming_job_num"}),(0,r.jsx)(n.td,{children:"1024"}),(0,r.jsx)(n.td,{children:"Maximum number of Streaming jobs"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"job_streaming_task_exec_thread_num"}),(0,r.jsx)(n.td,{children:"10"}),(0,r.jsx)(n.td,{children:"Number of threads used to execute StreamingTasks"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"max_streaming_task_show_count"}),(0,r.jsx)(n.td,{children:"100"}),(0,r.jsx)(n.td,{children:"Maximum number of task execution records kept in memory for a StreamingTask"})]})]})]}),"\n",(0,r.jsx)(n.h4,{id:"import-configuration-parameters",children:"Import Configuration Parameters"}),"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Parameter"}),(0,r.jsx)(n.th,{children:"Default Value"}),(0,r.jsx)(n.th,{children:"Description"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"session.*"}),(0,r.jsx)(n.td,{children:"None"}),(0,r.jsx)(n.td,{children:"Supports configuring all session variables in job_properties. For importing variables, please refer to [Insert Into Select](../../data-operate/import/import-way/insert-into-manual.md#Import Configuration Parameters)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"s3.max_batch_files"}),(0,r.jsx)(n.td,{children:"256"}),(0,r.jsx)(n.td,{children:"Triggers an import write when the cumulative number of files reaches this value."})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"s3.max_batch_bytes"}),(0,r.jsx)(n.td,{children:"10G"}),(0,r.jsx)(n.td,{children:"Triggers an import write when the cumulative data volume reaches this value."})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"max_interval"}),(0,r.jsx)(n.td,{children:"10s"}),(0,r.jsx)(n.td,{children:"The idle scheduling interval when there are no new files or data added upstream."})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"import-status",children:"Import Status"}),"\n",(0,r.jsx)(n.h4,{id:"job",children:"Job"}),"\n",(0,r.jsxs)(n.p,{children:["After a job is successfully submitted, you can execute ",(0,r.jsx)(n.strong,{children:"select * from job(\"insert\") where ExecuteType = 'Streaming'"})," to check the current status of the job."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-SQL",children:'select * from job(type=insert) where ExecuteType = "streaming"\n               Id: 1758538737484\n             Name: my_job1\n          Definer: root\n      ExecuteType: STREAMING\nRecurringStrategy: \\N\n           Status: RUNNING\n       ExecuteSql: INSERT INTO test.`student1`\nSELECT * FROM S3\n(\n    "uri" = "s3://wd-test123/s3/demo/*.csv",\n    "format" = "csv",\n    "column_separator" = ",",\n    "s3.endpoint" = "s3.ap-southeast-1.amazonaws.com",\n    "s3.region" = "ap-southeast-1",\n    "s3.access_key" = "",\n    "s3.secret_key" = ""\n)\n       CreateTime: 2025-09-22 19:24:51\n SucceedTaskCount: 5\n  FailedTaskCount: 0\nCanceledTaskCount: 0\n          Comment: \n       Properties: {"s3.max_batch_files":"2","session.insert_max_filter_ratio":"0.5"}\n    CurrentOffset: {"fileName":"s3/demo/test/1.csv"}\n        EndOffset: {"fileName":"s3/demo/test/1.csv"}\n    LoadStatistic: {"scannedRows":0,"loadBytes":0,"fileNumber":0,"fileSize":0}\n         ErrorMsg: \\N\n'})}),"\n",(0,r.jsx)(n.p,{children:"The specific parameter results are displayed as follows:"}),"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Result Columns"}),(0,r.jsx)(n.th,{children:"Description"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"ID"}),(0,r.jsx)(n.td,{children:"Job ID"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"NAME"}),(0,r.jsx)(n.td,{children:"Job Name"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Definer"}),(0,r.jsx)(n.td,{children:"Job Definer"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"ExecuteType"}),(0,r.jsxs)(n.td,{children:["Job scheduling type: ",(0,r.jsx)(n.em,{children:"ONE_TIME/RECURRING/STREAMING/MANUAL"})]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"RecurringStrategy"}),(0,r.jsx)(n.td,{children:"Recurring strategy. Used in normal Insert operations; empty when ExecuteType=Streaming"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Status"}),(0,r.jsx)(n.td,{children:"Job status"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"ExecuteSql"}),(0,r.jsx)(n.td,{children:"Job's Insert SQL statement"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"CreateTime"}),(0,r.jsx)(n.td,{children:"Job creation time"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"SucceedTaskCount"}),(0,r.jsx)(n.td,{children:"Number of successful tasks"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"FailedTaskCount"}),(0,r.jsx)(n.td,{children:"Number of failed tasks"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"CanceledTaskCount"}),(0,r.jsx)(n.td,{children:"Number of canceled tasks"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Comment"}),(0,r.jsx)(n.td,{children:"Job comment"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Properties"}),(0,r.jsx)(n.td,{children:"Job properties"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"CurrentOffset"}),(0,r.jsxs)(n.td,{children:["Job's current completion offset. Only ",(0,r.jsx)(n.code,{children:"ExecuteType=Streaming"})," has a value."]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"EndOffset"}),(0,r.jsxs)(n.td,{children:["The maximum EndOffset obtained by the Job from the data source. Only ",(0,r.jsx)(n.code,{children:"ExecuteType=Streaming"})," has a value."]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"LoadStatistic"}),(0,r.jsx)(n.td,{children:"Job statistics."})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"ErrorMsg"}),(0,r.jsx)(n.td,{children:"Error messages during Job execution."})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"JobRuntimeMsg"}),(0,r.jsx)(n.td,{children:"Some runtime information for the Job."})]})]})]}),"\n",(0,r.jsx)(n.h4,{id:"task",children:"Task"}),"\n",(0,r.jsxs)(n.p,{children:["You can execute ",(0,r.jsx)(n.code,{children:"select \\* from tasks(type='insert') where jobId='1758534452459'"})," to view the running status of each Task."]}),"\n",(0,r.jsx)(n.p,{children:"Note: Only the latest Task information will be retained."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-SQL",children:'mysql> select * from tasks(type=\'insert\') where jobId=\'1758534452459\'\\G\n*************************** 1. row ***************************\n       TaskId: 1758534723330\n        JobId: 1758534452459\n      JobName: test_streaming_insert_job_name\n        Label: 1758534452459_1758534723330\n       Status: SUCCESS\n     ErrorMsg: \\N\n   CreateTime: 2025-09-22 17:52:55\n    StartTime: \\N\n   FinishTime: \\N\n  TrackingUrl: \\N\nLoadStatistic: {"scannedRows":20,"loadBytes":425,"fileNumber":2,"fileSize":256}\n         User: root\nFirstErrorMsg: \\N\nRunningOffset: {"startFileName":"s3/demo/1.csv","endFileName":"s3/demo/8.csv"}\n'})}),"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Results Columns"}),(0,r.jsx)(n.th,{children:"Description"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"TaskId"}),(0,r.jsx)(n.td,{children:"Task ID"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"JobID"}),(0,r.jsx)(n.td,{children:"JobID"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"JobName"}),(0,r.jsx)(n.td,{children:"Job Name"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Label"}),(0,r.jsx)(n.td,{children:"Label of Insert"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Status"}),(0,r.jsx)(n.td,{children:"Status of Task"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"ErrorMsg"}),(0,r.jsx)(n.td,{children:"Task failure information"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"CreateTime"}),(0,r.jsx)(n.td,{children:"Task creation time"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"StartTime"}),(0,r.jsx)(n.td,{children:"Task start time"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"FinishTime"}),(0,r.jsx)(n.td,{children:"Task completion time"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"TrackingUrl"}),(0,r.jsx)(n.td,{children:"Error URL of Insert"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"LoadStatistic"}),(0,r.jsx)(n.td,{children:"Task statistics"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"User"}),(0,r.jsx)(n.td,{children:"Executor of task"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"FirstErrorMsg"}),(0,r.jsx)(n.td,{children:"Information about the first data quality error in a normal InsertTask"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"RunningOffset"}),(0,r.jsx)(n.td,{children:"Offset information of the current Task synchronization. Only has a value if Job.ExecuteType=Streaming"})]})]})]})]})}function h(e={}){let{wrapper:n}={...(0,i.a)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},250065:function(e,n,t){t.d(n,{Z:function(){return o},a:function(){return a}});var s=t(667294);let r={},i=s.createContext(r);function a(e){let n=s.useContext(i);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),s.createElement(i.Provider,{value:n},e.children)}}}]);