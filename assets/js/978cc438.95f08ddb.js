"use strict";(self.webpackChunkdoris_website=self.webpackChunkdoris_website||[]).push([["260165"],{94418:function(e,n,t){t.r(n),t.d(n,{default:()=>h,frontMatter:()=>o,metadata:()=>a,assets:()=>i,toc:()=>l,contentTitle:()=>d});var a=JSON.parse('{"id":"data-operate/import/data-source/snowflake","title":"Snowflake","description":"During the migration from Snowflake to Doris, object storage is typically used as an intermediate medium. The core process is as follows: First, export data to object storage using Snowflake\'s COPY INTO statement. Then, use Doris\' S3 Load feature to read data from the object storage and load it into Doris. For details, refer to S3 Load.","source":"@site/docs/data-operate/import/data-source/snowflake.md","sourceDirName":"data-operate/import/data-source","slug":"/data-operate/import/data-source/snowflake","permalink":"/docs/dev/data-operate/import/data-source/snowflake","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"title":"Snowflake","language":"en"},"sidebar":"docs","previous":{"title":"S3 Compatible Storage","permalink":"/docs/dev/data-operate/import/data-source/s3-compatible"},"next":{"title":"BigQuery","permalink":"/docs/dev/data-operate/import/data-source/bigquery"}}'),r=t("785893"),s=t("250065");let o={title:"Snowflake",language:"en"},d=void 0,i={},l=[{value:"Considerations",id:"considerations",level:2},{value:"Data type mapping",id:"data-type-mapping",level:2},{value:"1. Create Table",id:"1-create-table",level:2},{value:"2. Export Data from Snowflake",id:"2-export-data-from-snowflake",level:2},{value:"3. Load Data to Doris",id:"3-load-data-to-doris",level:2}];function c(e){let n={a:"a",code:"code",em:"em",h2:"h2",p:"p",pre:"pre",strong:"strong",t:"t",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,s.a)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsxs)(n.p,{children:["During the migration from Snowflake to Doris, object storage is typically used as an intermediate medium. The core process is as follows: First, export data to object storage using Snowflake's ",(0,r.jsx)(n.a,{href:"https://docs.snowflake.com/en/user-guide/data-unload-overview",children:"COPY INTO"})," statement. Then, use Doris' S3 Load feature to read data from the object storage and load it into Doris. For details, refer to ",(0,r.jsx)(n.a,{href:"/docs/dev/data-operate/import/data-source/amazon-s3",children:"S3 Load"}),"."]}),"\n",(0,r.jsx)(n.h2,{id:"considerations",children:"Considerations"}),"\n",(0,r.jsxs)(n.p,{children:["Before migration, select Doris' ",(0,r.jsx)(n.a,{href:"/docs/dev/table-design/data-model/overview",children:"data model"}),", ",(0,r.jsx)(n.a,{href:"/docs/dev/table-design/data-partitioning/dynamic-partitioning",children:"partitioning"}),", and ",(0,r.jsx)(n.a,{href:"/docs/dev/table-design/data-partitioning/data-bucketing",children:"bucketing"})," strategies based on Snowflake's table structure. For more table creation strategies, refer to ",(0,r.jsx)(n.a,{href:"/docs/dev/data-operate/import/load-best-practices",children:"Load Best Practices"}),"."]}),"\n",(0,r.jsx)(n.h2,{id:"data-type-mapping",children:"Data type mapping"}),"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Snowflake"}),(0,r.jsx)(n.th,{children:"Doris"}),(0,r.jsx)(n.th,{children:"Comment"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"NUMBER(p, s)/DECIMAL(p, s)/NUMERIC(p,s)"}),(0,r.jsx)(n.td,{children:"DECIMAL(p, s)"}),(0,r.jsx)(n.td,{})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"INT/INTEGER"}),(0,r.jsx)(n.td,{children:"INT"}),(0,r.jsx)(n.td,{})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"TINYINT/BYTEINT"}),(0,r.jsx)(n.td,{children:"TINYINT"}),(0,r.jsx)(n.td,{})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"SMALLINT"}),(0,r.jsx)(n.td,{children:"SMALLINT"}),(0,r.jsx)(n.td,{})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"BIGINT"}),(0,r.jsx)(n.td,{children:"BIGINT"}),(0,r.jsx)(n.td,{})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"FLOAT/FLOAT4/FLOAT8/DOUBLE/DOUBLE PRECISION/REAL"}),(0,r.jsx)(n.td,{children:"DOUBLE"}),(0,r.jsx)(n.td,{})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"VARCHAR/STRING/TEXT"}),(0,r.jsx)(n.td,{children:"VARCHAR/STRING"}),(0,r.jsx)(n.td,{children:"VARCHAR maximum length is 65535"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"CHAR/CHARACTER/NCHAR"}),(0,r.jsx)(n.td,{children:"CHAR"}),(0,r.jsx)(n.td,{})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"BINARY/VARBINARY"}),(0,r.jsx)(n.td,{children:"STRING"}),(0,r.jsx)(n.td,{})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"BOOLEAN"}),(0,r.jsx)(n.td,{children:"BOOLEAN"}),(0,r.jsx)(n.td,{})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"DATE"}),(0,r.jsx)(n.td,{children:"DATE"}),(0,r.jsx)(n.td,{})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"DATETIME/TIMESTAMP/TIMESTAMP_NTZ"}),(0,r.jsx)(n.td,{children:"DATETIME"}),(0,r.jsx)(n.td,{children:"TIMESTAMP is a configurable alias (default: TIMESTAMP_NTZ)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"TIME"}),(0,r.jsx)(n.td,{children:"STRING"}),(0,r.jsx)(n.td,{children:"Cast to String when exporting from Snowflake"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"VARIANT"}),(0,r.jsx)(n.td,{children:"VARIANT"}),(0,r.jsx)(n.td,{})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"ARRAY"}),(0,r.jsxs)(n.td,{children:["ARRAY",(0,r.jsx)(n.t,{})]}),(0,r.jsx)(n.td,{})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"OBJECT"}),(0,r.jsx)(n.td,{children:"JSON"}),(0,r.jsx)(n.td,{})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"GEOGRAPHY/GEOMETRY"}),(0,r.jsx)(n.td,{children:"STRING"}),(0,r.jsx)(n.td,{})]})]})]}),"\n",(0,r.jsx)(n.h2,{id:"1-create-table",children:"1. Create Table"}),"\n",(0,r.jsx)(n.p,{children:"To migrate a Snowflake table to Doris, first create the Doris table."}),"\n",(0,r.jsx)(n.p,{children:"Assume we have the following table and data in Snowflake:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-sql",children:"CREATE OR REPLACE TABLE sales_data (\n    order_id      INT PRIMARY KEY,\n    customer_name VARCHAR(128),\n    order_date    DATE,\n    amount        DECIMAL(10,2),\n    country       VARCHAR(48)\n) \nCLUSTER BY (order_date);\n\nINSERT INTO sales_data VALUES\n(1, 'Alice', '2025-04-08', 99.99, 'USA'),\n(2, 'Bob', '2025-04-08', 149.50, 'Canada'),\n(3, 'Charlie', '2025-04-09', 75.00, 'UK'),\n(4, 'Diana', '2025-04-10', 200.00, 'Australia');\n"})}),"\n",(0,r.jsx)(n.p,{children:"Based on this structure, create a Doris Primary Key partitioned table aligned with Snowflake's clustering key, partitioned by day:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-sql",children:'CREATE TABLE `sales_data` (\n  order_id      INT,\n  order_date    DATE NOT NULL,\n  customer_name VARCHAR(128),\n  amount        DECIMAL(10,2),\n  country       VARCHAR(48)\n) ENGINE=OLAP\nUNIQUE KEY(`order_id`,`order_date`)\nPARTITION BY RANGE(`order_date`) (\nPARTITION p20250408 VALUES [(\'2025-04-08\'), (\'2025-04-09\')),\nPARTITION p20250409 VALUES [(\'2025-04-09\'), (\'2025-04-10\')),\nPARTITION p20250410 VALUES [(\'2025-04-10\'), (\'2025-04-11\'))\n)\nDISTRIBUTED BY HASH(`order_id`) BUCKETS 16\nPROPERTIES (\n "dynamic_partition.enable" = "true",\n "dynamic_partition.time_unit" = "DAY",\n "dynamic_partition.end" = "5",\n "dynamic_partition.prefix" = "p",\n "dynamic_partition.buckets" = "16",\n "replication_num" = "1"\n);\n'})}),"\n",(0,r.jsx)(n.h2,{id:"2-export-data-from-snowflake",children:"2. Export Data from Snowflake"}),"\n",(0,r.jsxs)(n.p,{children:["2.1. ",(0,r.jsx)(n.strong,{children:"Export to S3 Parquet Files via COPY INTO"})]}),"\n",(0,r.jsxs)(n.p,{children:["Snowflake supports exporting to ",(0,r.jsx)(n.a,{href:"https://docs.snowflake.com/en/user-guide/data-unload-s3",children:"AWS S3"}),"\uFF0C",(0,r.jsx)(n.a,{href:"https://docs.snowflake.com/en/user-guide/data-unload-gcs",children:"GCS"}),"\uFF0C",(0,r.jsx)(n.a,{href:"https://docs.snowflake.com/en/user-guide/data-unload-azure",children:"AZURE"}),"\uFF0C",(0,r.jsx)(n.strong,{children:"Export data partitioned by Doris' partition fields"}),". Example for AWS S3:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"```sql\nCREATE FILE FORMAT my_parquet_format TYPE = parquet;\n\nCREATE OR REPLACE STAGE external_stage\nURL='s3://mybucket/sales_data'\nCREDENTIALS=(AWS_KEY_ID='<ak>' AWS_SECRET_KEY='<sk>')\nFILE_FORMAT = my_parquet_format;\n\nCOPY INTO @external_stage from sales_data PARTITION BY (CAST(order_date AS VARCHAR)) header=true;\n```\n"})}),"\n",(0,r.jsxs)(n.p,{children:["2.2. ",(0,r.jsx)(n.strong,{children:"Verify Exported Files on S3"})]}),"\n",(0,r.jsxs)(n.p,{children:["Exported files are organized into ",(0,r.jsx)(n.strong,{children:"subdirectories by partition"})," on S3:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"![snowflake_s3_out_en](/images/data-operate/snowflake_s3_out_en.png)\n\n![snowflake_s3_out2_en](/images/data-operate/snowflake_s3_out2_en.png)\n"})}),"\n",(0,r.jsx)(n.h2,{id:"3-load-data-to-doris",children:"3. Load Data to Doris"}),"\n",(0,r.jsxs)(n.p,{children:["S3 Load is an asynchronous data load method. After execution, Doris actively pulls data from the data source. The data source supports object storage compatible with the S3 protocol, including (",(0,r.jsx)(n.a,{href:"/docs/dev/data-operate/import/data-source/amazon-s3",children:"AWS S3"}),"\uFF0C",(0,r.jsx)(n.a,{href:"/docs/dev/data-operate/import/data-source/google-cloud-storage",children:"GCS"}),"\uFF0C",(0,r.jsx)(n.a,{href:"/docs/dev/data-operate/import/data-source/azure-storage",children:"AZURE"}),"\uFF0Cetc)"]}),"\n",(0,r.jsxs)(n.p,{children:["This method is suitable for scenarios involving large volumes of data that require asynchronous processing in the background. For data imports that need to be handled synchronously, refer to  ",(0,r.jsx)(n.a,{href:"/docs/dev/data-operate/import/data-source/amazon-s3#load-with-tvf",children:"TVF Load"}),"\u3002"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsxs)(n.em,{children:["Note: For ",(0,r.jsx)(n.strong,{children:"Parquet/ORC format files that contain complex types (Struct/Array/Map)"}),", TVF Load must be used."]})}),"\n",(0,r.jsxs)(n.p,{children:["3.1. ",(0,r.jsx)(n.strong,{children:"Load a Single Partition"})]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-sql",children:'LOAD LABEL sales_data_2025_04_08\n(\n    DATA INFILE("s3://mybucket/sales_data/2025_04_08/*")\n    INTO TABLE sales_data\n    FORMAT AS "parquet"\n    (order_id, order_date, customer_name, amount, country)\n)\nWITH S3\n(\n    "provider" = "S3",\n    "s3.endpoint" = "s3.ap-southeast-1.amazonaws.com",\n    "s3.access_key" = "<ak>",\n    "s3.secret_key" = "<sk>",\n    "s3.region" = "ap-southeast-1"\n);\n'})}),"\n",(0,r.jsxs)(n.p,{children:["3.2. ",(0,r.jsx)(n.strong,{children:"Check Load Status via SHOW LOAD"})]}),"\n",(0,r.jsx)(n.p,{children:"Since S3 Load import is submitted asynchronously, you can check the status of a specific label using SHOW LOAD:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:'mysql> show load where label = "label_sales_data_2025_04_08"\\G\n*************************** 1. row ***************************\n         JobId: 17956078\n         Label: label_sales_data_2025_04_08\n         State: FINISHED\n      Progress: 100.00% (1/1)\n          Type: BROKER\n       EtlInfo: unselected.rows=0; dpp.abnorm.ALL=0; dpp.norm.ALL=2\n      TaskInfo: cluster:s3.ap-southeast-1.amazonaws.com; timeout(s):3600; max_filter_ratio:0.0; priority:NORMAL\n      ErrorMsg: NULL\n    CreateTime: 2025-04-10 17:50:53\n  EtlStartTime: 2025-04-10 17:50:54\n EtlFinishTime: 2025-04-10 17:50:54\n LoadStartTime: 2025-04-10 17:50:54\nLoadFinishTime: 2025-04-10 17:50:54\n           URL: NULL\n    JobDetails: {"Unfinished backends":{"5eec1be8612d4872-91040ff1e7208a4f":[]},"ScannedRows":2,"TaskNumber":1,"LoadBytes":91,"All backends":{"5eec1be8612d4872-91040ff1e7208a4f":[10022]},"FileNumber":1,"FileSize":1620}\n TransactionId: 766228\n  ErrorTablets: {}\n          User: root\n       Comment: \n1 row in set (0.00 sec)\n'})}),"\n",(0,r.jsxs)(n.p,{children:["3.3. ",(0,r.jsx)(n.strong,{children:"Handle Load Errors"})]}),"\n",(0,r.jsx)(n.p,{children:"When there are multiple load tasks, you can use the following statement to query the dates and reasons for data load failures."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-SQL",children:'mysql> show load where state=\'CANCELLED\' and label like "label_test%"\\G\n*************************** 1. row ***************************\n         JobId: 18312384\n         Label: label_test123\n         State: CANCELLED\n      Progress: 100.00% (3/3)\n          Type: BROKER\n       EtlInfo: unselected.rows=0; dpp.abnorm.ALL=4; dpp.norm.ALL=0\n      TaskInfo: cluster:s3.ap-southeast-1.amazonaws.com; timeout(s):14400; max_filter_ratio:0.0; priority:NORMAL\n      ErrorMsg: type:ETL_QUALITY_UNSATISFIED; msg:quality not good enough to cancel\n    CreateTime: 2025-04-15 17:32:59\n  EtlStartTime: 2025-04-15 17:33:02\n EtlFinishTime: 2025-04-15 17:33:02\n LoadStartTime: 2025-04-15 17:33:02\nLoadFinishTime: 2025-04-15 17:33:02\n           URL: http://10.16.10.6:28747/api/_load_error_log?file=__shard_2/error_log_insert_stmt_7602ccd7c3a4854-95307efca7bfe342_7602ccd7c3a4854_95307efca7bfe342\n    JobDetails: {"Unfinished backends":{"7602ccd7c3a4854-95307efca7bfe341":[]},"ScannedRows":4,"TaskNumber":1,"LoadBytes":188,"All backends":{"7602ccd7c3a4854-95307efca7bfe341":[10022]},"FileNumber":3,"FileSize":4839}\n TransactionId: 769213\n  ErrorTablets: {}\n          User: root\n       Comment: \n'})}),"\n",(0,r.jsxs)(n.p,{children:["As shown in the example above, the issue is a ",(0,r.jsx)(n.strong,{children:"data quality error"}),"(ETL_QUALITY_UNSATISFIED). To view the detailed error, you need to visit the URL provided in the result. For example, the data exceeded the defined length of the country column in the table schema:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'[root@VM-10-6-centos ~]$ curl "http://10.16.10.6:28747/api/_load_error_log?file=__shard_2/error_log_insert_stmt_7602ccd7c3a4854-95307efca7bfe342_7602ccd7c3a4854_95307efca7bfe342"\nReason: column_name[country], the length of input is too long than schema. first 32 bytes of input str: [USA] schema length: 1; actual length: 3; . src line []; \nReason: column_name[country], the length of input is too long than schema. first 32 bytes of input str: [Canada] schema length: 1; actual length: 6; . src line []; \nReason: column_name[country], the length of input is too long than schema. first 32 bytes of input str: [UK] schema length: 1; actual length: 2; . src line []; \nReason: column_name[country], the length of input is too long than schema. first 32 bytes of input str: [Australia] schema length: 1; actual length: 9; . src line [];\n'})}),"\n",(0,r.jsxs)(n.p,{children:["For data quality errors, if you want to allow skipping erroneous records, you can set a fault tolerance rate in the Properties section of the S3 Load task. For details, refer to ",(0,r.jsx)(n.a,{href:"/docs/dev/data-operate/import/import-way/broker-load-manual#related-configurations",children:"Import Configuration Parameters"}),"\u3002"]}),"\n",(0,r.jsxs)(n.p,{children:["3.4. ",(0,r.jsx)(n.strong,{children:"Load data for multiple partitions"})]}),"\n",(0,r.jsx)(n.p,{children:"When migrating a large volume of historical data, it is recommended to use a batch load strategy. Each batch corresponds to one or a few partitions in Doris. It is recommended to keep the data size under 100GB per batch to reduce system load and lower the cost of retries in case of load failures."}),"\n",(0,r.jsxs)(n.p,{children:["You can refer to the script ",(0,r.jsx)(n.a,{href:"https://github.com/apache/doris/blob/master/samples/load/shell/s3_load_demo.sh",children:"s3_load_demo.sh"}),", which can poll the partition directory on S3 and submit the S3 Load task to Doris to achieve batch load."]})]})}function h(e={}){let{wrapper:n}={...(0,s.a)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},250065:function(e,n,t){t.d(n,{Z:function(){return d},a:function(){return o}});var a=t(667294);let r={},s=a.createContext(r);function o(e){let n=a.useContext(s);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function d(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),a.createElement(s.Provider,{value:n},e.children)}}}]);