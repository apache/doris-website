"use strict";(self.webpackChunkdoris_website=self.webpackChunkdoris_website||[]).push([["799183"],{897054:function(e,n,s){s.r(n),s.d(n,{default:()=>h,frontMatter:()=>a,metadata:()=>i,assets:()=>d,toc:()=>l,contentTitle:()=>o});var i=JSON.parse('{"id":"lakehouse/storages/hdfs","title":"HDFS","description":"This document describes the parameters required for accessing HDFS. These parameters apply to:","source":"@site/versioned_docs/version-2.1/lakehouse/storages/hdfs.md","sourceDirName":"lakehouse/storages","slug":"/lakehouse/storages/hdfs","permalink":"/docs/2.1/lakehouse/storages/hdfs","draft":false,"unlisted":false,"tags":[],"version":"2.1","frontMatter":{"title":"HDFS","language":"en"},"sidebar":"docs","previous":{"title":"File System","permalink":"/docs/2.1/lakehouse/metastores/filesystem"},"next":{"title":"S3","permalink":"/docs/2.1/lakehouse/storages/s3"}}'),t=s("785893"),r=s("250065");let a={title:"HDFS",language:"en"},o=void 0,d={},l=[{value:"Parameter Overview",id:"parameter-overview",level:2},{value:"Authentication Configuration",id:"authentication-configuration",level:2},{value:"Simple Authentication",id:"simple-authentication",level:3},{value:"Kerberos Authentication",id:"kerberos-authentication",level:3},{value:"HDFS HA Configuration",id:"hdfs-ha-configuration",level:2},{value:"Configuration Files",id:"configuration-files",level:2},{value:"HDFS IO Optimization",id:"hdfs-io-optimization",level:2},{value:"Hedged Read",id:"hedged-read",level:3},{value:"dfs.client.socket-timeout",id:"dfsclientsocket-timeout",level:3},{value:"Debugging HDFS",id:"debugging-hdfs",level:2},{value:"HDFS Client",id:"hdfs-client",level:3}];function c(e){let n={a:"a",blockquote:"blockquote",code:"code",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.a)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.p,{children:"This document describes the parameters required for accessing HDFS. These parameters apply to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Catalog properties"}),"\n",(0,t.jsx)(n.li,{children:"Table Valued Function properties"}),"\n",(0,t.jsx)(n.li,{children:"Broker Load properties"}),"\n",(0,t.jsx)(n.li,{children:"Export properties"}),"\n",(0,t.jsx)(n.li,{children:"Outfile properties"}),"\n",(0,t.jsx)(n.li,{children:"Backup and restore"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"parameter-overview",children:"Parameter Overview"}),"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Property Name"}),(0,t.jsx)(n.th,{children:"Legacy Name"}),(0,t.jsx)(n.th,{children:"Description"}),(0,t.jsx)(n.th,{children:"Default Value"}),(0,t.jsx)(n.th,{children:"Required"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"hdfs.authentication.type"}),(0,t.jsx)(n.td,{children:"hadoop.security.authentication"}),(0,t.jsx)(n.td,{children:"Specifies the authentication type. Optional values are kerberos or simple. If kerberos is selected, the system will use Kerberos authentication to interact with HDFS; if simple is used, it means no authentication is used, suitable for open HDFS clusters. Selecting kerberos requires configuring the corresponding principal and keytab."}),(0,t.jsx)(n.td,{children:"simple"}),(0,t.jsx)(n.td,{children:"No"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"hdfs.authentication.kerberos.principal"}),(0,t.jsx)(n.td,{children:"hadoop.kerberos.principal"}),(0,t.jsx)(n.td,{children:"When the authentication type is kerberos, specify the Kerberos principal. A Kerberos principal is a unique identity string that typically includes service name, hostname, and domain name."}),(0,t.jsx)(n.td,{children:"-"}),(0,t.jsx)(n.td,{children:"No"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"hdfs.authentication.kerberos.keytab"}),(0,t.jsx)(n.td,{children:"hadoop.kerberos.keytab"}),(0,t.jsx)(n.td,{children:"This parameter specifies the keytab file path for Kerberos authentication. The keytab file stores encrypted credentials, allowing the system to authenticate automatically without requiring users to manually enter passwords."}),(0,t.jsx)(n.td,{children:"-"}),(0,t.jsx)(n.td,{children:"No"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"hdfs.impersonation.enabled"}),(0,t.jsx)(n.td,{children:"-"}),(0,t.jsx)(n.td,{children:"If true, enables HDFS impersonation functionality. Uses the proxy user configured in core-site.xml to proxy Doris login users for HDFS operations"}),(0,t.jsx)(n.td,{children:"Not supported yet"}),(0,t.jsx)(n.td,{children:"-"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"hadoop.username"}),(0,t.jsx)(n.td,{children:"-"}),(0,t.jsx)(n.td,{children:"When the authentication type is simple, this user will be used to access HDFS. By default, the Linux system user running the Doris process will be used for access"}),(0,t.jsx)(n.td,{children:"-"}),(0,t.jsx)(n.td,{children:"-"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"hadoop.config.resources"}),(0,t.jsx)(n.td,{children:"-"}),(0,t.jsx)(n.td,{children:"Specifies the HDFS configuration file directory (must include hdfs-site.xml and core-site.xml), using relative paths. The default directory is /plugins/hadoop/conf/ under the (FE/BE) deployment directory (can be changed by modifying hadoop_config_dir in fe.conf/be.conf to change the default path). All FE and BE nodes need to configure the same relative path. Example: hadoop/conf/core-site.xml,hadoop/conf/hdfs-site.xml"}),(0,t.jsx)(n.td,{children:"-"}),(0,t.jsx)(n.td,{children:"-"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"dfs.nameservices"}),(0,t.jsx)(n.td,{children:"-"}),(0,t.jsx)(n.td,{children:"Manually configure HDFS high availability cluster parameters. If using hadoop.config.resources configuration, parameters will be automatically read from hdfs-site.xml. Need to be used with the following parameters: dfs.ha.namenodes.your-nameservice, dfs.namenode.rpc-address.your-nameservice.nn1, dfs.client.failover.proxy.provider, etc."}),(0,t.jsx)(n.td,{children:"-"}),(0,t.jsx)(n.td,{children:"-"})]})]})]}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsx)(n.p,{children:"For versions before 3.1, please use the legacy names."}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"authentication-configuration",children:"Authentication Configuration"}),"\n",(0,t.jsx)(n.p,{children:"HDFS supports two authentication methods:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Simple"}),"\n",(0,t.jsx)(n.li,{children:"Kerberos"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"simple-authentication",children:"Simple Authentication"}),"\n",(0,t.jsx)(n.p,{children:"Simple authentication is suitable for HDFS clusters that have not enabled Kerberos."}),"\n",(0,t.jsx)(n.p,{children:"Using Simple authentication, you can set the following parameters or use the default values directly:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:'"hdfs.authentication.type" = "simple"\n'})}),"\n",(0,t.jsxs)(n.p,{children:["In Simple authentication mode, you can use the ",(0,t.jsx)(n.code,{children:"hadoop.username"})," parameter to specify the username. If not specified, it defaults to the username of the current process."]}),"\n",(0,t.jsx)(n.p,{children:"Examples:"}),"\n",(0,t.jsxs)(n.p,{children:["Using ",(0,t.jsx)(n.code,{children:"lakers"})," username to access HDFS"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:'"hdfs.authentication.type" = "simple",\n"hadoop.username" = "lakers"\n'})}),"\n",(0,t.jsx)(n.p,{children:"Using default system user to access HDFS"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:'"hdfs.authentication.type" = "simple"\n'})}),"\n",(0,t.jsx)(n.h3,{id:"kerberos-authentication",children:"Kerberos Authentication"}),"\n",(0,t.jsx)(n.p,{children:"Kerberos authentication is suitable for HDFS clusters with Kerberos enabled."}),"\n",(0,t.jsx)(n.p,{children:"Using Kerberos authentication, you need to set the following parameters:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:'"hdfs.authentication.type" = "kerberos",\n"hdfs.authentication.kerberos.principal" = "<your_principal>",\n"hdfs.authentication.kerberos.keytab" = "<your_keytab>"\n'})}),"\n",(0,t.jsx)(n.p,{children:"In Kerberos authentication mode, you need to set the Kerberos principal and keytab file path."}),"\n",(0,t.jsxs)(n.p,{children:["Doris will access HDFS with the identity specified by the ",(0,t.jsx)(n.code,{children:"hdfs.authentication.kerberos.principal"})," property, using the keytab specified by keytab to authenticate the Principal."]}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsx)(n.p,{children:"Note:"}),"\n",(0,t.jsx)(n.p,{children:"The keytab file must exist on every FE and BE node with the same path, and the user running the Doris process must have read permissions for the keytab file."}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Example:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:'"hdfs.authentication.type" = "kerberos",\n"hdfs.authentication.kerberos.principal" = "hdfs/hadoop@HADOOP.COM",\n"hdfs.authentication.kerberos.keytab" = "/etc/security/keytabs/hdfs.keytab",\n'})}),"\n",(0,t.jsx)(n.h2,{id:"hdfs-ha-configuration",children:"HDFS HA Configuration"}),"\n",(0,t.jsxs)(n.p,{children:["If HDFS HA mode is enabled, need to configure ",(0,t.jsx)(n.code,{children:"dfs.nameservices"})," related parameters:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:"'dfs.nameservices' = '<your-nameservice>',\n'dfs.ha.namenodes.<your-nameservice>' = '<nn1>,<nn2>',\n'dfs.namenode.rpc-address.<your-nameservice>.<nn1>' = '<nn1_host:port>',\n'dfs.namenode.rpc-address.<your-nameservice>.<nn2>' = '<nn2_host:port>',\n'dfs.client.failover.proxy.provider.<your-nameservice>' = 'org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider',\n"})}),"\n",(0,t.jsx)(n.p,{children:"Example:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:"'dfs.nameservices' = 'nameservice1',\n'dfs.ha.namenodes.nameservice1' = 'nn1,nn2',\n'dfs.namenode.rpc-address.nameservice1.nn1' = '172.21.0.2:8088',\n'dfs.namenode.rpc-address.nameservice1.nn2' = '172.21.0.3:8088',\n'dfs.client.failover.proxy.provider.nameservice1' = 'org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider',\n"})}),"\n",(0,t.jsx)(n.h2,{id:"configuration-files",children:"Configuration Files"}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsx)(n.p,{children:"This feature is supported since version 3.1.0"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["Doris supports specifying the HDFS configuration file directory through the ",(0,t.jsx)(n.code,{children:"hadoop.config.resources"})," parameter."]}),"\n",(0,t.jsxs)(n.p,{children:["The configuration file directory must contain ",(0,t.jsx)(n.code,{children:"hdfs-site.xml"})," and ",(0,t.jsx)(n.code,{children:"core-site.xml"})," files. The default directory is ",(0,t.jsx)(n.code,{children:"/plugins/hadoop_conf/"})," under the (FE/BE) deployment directory. All FE and BE nodes need to configure the same relative path."]}),"\n",(0,t.jsxs)(n.p,{children:["If the configuration files contain the above parameters mentioned in this document, user-explicitly configured parameters take priority. Configuration files can specify multiple files, separated by commas, such as ",(0,t.jsx)(n.code,{children:"hadoop/conf/core-site.xml,hadoop/conf/hdfs-site.xml"}),"."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Examples:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:"-- Multiple configuration files\n'hadoop.config.resources'='hdfs-cluster-1/core-site.xml,hdfs-cluster-1/hdfs-site.xml'\n-- Single configuration file\n'hadoop.config.resources'='hdfs-cluster-2/hdfs-site.xml'\n"})}),"\n",(0,t.jsx)(n.h2,{id:"hdfs-io-optimization",children:"HDFS IO Optimization"}),"\n",(0,t.jsx)(n.p,{children:"In some cases, high HDFS load may cause reading data replicas on HDFS to take a long time, thereby slowing down overall query efficiency. The following introduces some related optimization configurations."}),"\n",(0,t.jsx)(n.h3,{id:"hedged-read",children:"Hedged Read"}),"\n",(0,t.jsx)(n.p,{children:"HDFS Client provides Hedged Read functionality. This feature can start another read thread to read the same data when a read request exceeds a certain threshold without returning, using whichever returns first."}),"\n",(0,t.jsx)(n.p,{children:"Note: This feature may increase the load on the HDFS cluster, please use it judiciously."}),"\n",(0,t.jsx)(n.p,{children:"You can enable this feature in the following way:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:'"dfs.client.hedged.read.threadpool.size" = "128",\n"dfs.client.hedged.read.threshold.millis" = "500"\n'})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.code,{children:"dfs.client.hedged.read.threadpool.size"})}),"\n",(0,t.jsx)(n.p,{children:"Represents the number of threads used for Hedged Read, which are shared by one HDFS Client. Usually, for one HDFS cluster, BE nodes share one HDFS Client."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.code,{children:"dfs.client.hedged.read.threshold.millis"})}),"\n",(0,t.jsx)(n.p,{children:"Read threshold in milliseconds. When a read request exceeds this threshold without returning, it will trigger Hedged Read."}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"After enabling, you can see related parameters in Query Profile:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.code,{children:"TotalHedgedRead"})}),"\n",(0,t.jsx)(n.p,{children:"Number of times Hedged Read was initiated."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.code,{children:"HedgedReadWins"})}),"\n",(0,t.jsx)(n.p,{children:"Number of successful Hedged Read attempts (initiated and returned faster than the original request)."}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Note that the values here are cumulative values for a single HDFS Client, not values for a single query. The same HDFS Client is reused by multiple queries."}),"\n",(0,t.jsx)(n.h3,{id:"dfsclientsocket-timeout",children:"dfs.client.socket-timeout"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.code,{children:"dfs.client.socket-timeout"})," is a client configuration parameter in Hadoop HDFS used to set the socket timeout when the client establishes connections or reads data with DataNode or NameNode, in milliseconds. The default value of this parameter is usually 60,000 milliseconds."]}),"\n",(0,t.jsxs)(n.p,{children:["Reducing this parameter value allows the client to timeout faster and retry or switch to other nodes when encountering network delays, slow DataNode responses, or connection exceptions. This helps reduce wait times and improve system response speed. For example, in some tests, setting ",(0,t.jsx)(n.code,{children:"dfs.client.socket-timeout"})," to a smaller value (such as 5000 milliseconds) can quickly detect DataNode delays or failures, avoiding long waits."]}),"\n",(0,t.jsx)(n.p,{children:"Note:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Setting the timeout too small may cause frequent timeout errors during network fluctuations or high node load, affecting task stability."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"It is recommended to reasonably adjust this parameter value based on actual network environment and system load conditions to balance response speed and system stability."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["This parameter should be set in the client configuration file (such as ",(0,t.jsx)(n.code,{children:"hdfs-site.xml"}),") to ensure the client uses the correct timeout when communicating with HDFS."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["In summary, properly configuring the ",(0,t.jsx)(n.code,{children:"dfs.client.socket-timeout"})," parameter can improve I/O response speed while ensuring system stability and reliability."]}),"\n",(0,t.jsx)(n.h2,{id:"debugging-hdfs",children:"Debugging HDFS"}),"\n",(0,t.jsx)(n.p,{children:"Hadoop environment configuration is complex, and in some cases, connectivity issues and poor access performance may occur. Here are some third-party tools to help users quickly troubleshoot connectivity issues and basic performance problems."}),"\n",(0,t.jsx)(n.h3,{id:"hdfs-client",children:"HDFS Client"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["Java: ",(0,t.jsx)(n.a,{href:"https://github.com/morningman/hdfs-client-java",children:"https://github.com/morningman/hdfs-client-java"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["CPP: ",(0,t.jsx)(n.a,{href:"https://github.com/morningman/hdfs-client-cpp",children:"https://github.com/morningman/hdfs-client-cpp"})]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"These two tools can be used to quickly verify HDFS connectivity and read performance. Most of their Hadoop dependencies are the same as Doris's own Hadoop dependencies, so they can simulate Doris's HDFS access scenarios to the greatest extent."}),"\n",(0,t.jsx)(n.p,{children:"The Java version uses Java to access HDFS and can simulate the logic of Doris FE side accessing HDFS."}),"\n",(0,t.jsx)(n.p,{children:"The CPP version accesses HDFS through C++ calling libhdfs and can simulate the logic of Doris BE side accessing HDFS."}),"\n",(0,t.jsx)(n.p,{children:"For specific usage, please refer to the README in each code repository."})]})}function h(e={}){let{wrapper:n}={...(0,r.a)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},250065:function(e,n,s){s.d(n,{Z:function(){return o},a:function(){return a}});var i=s(667294);let t={},r=i.createContext(t);function a(e){let n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);