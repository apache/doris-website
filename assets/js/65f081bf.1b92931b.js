"use strict";(self.webpackChunkdoris_website=self.webpackChunkdoris_website||[]).push([["757986"],{585874:function(e,n,s){s.r(n),s.d(n,{default:()=>h,frontMatter:()=>o,metadata:()=>r,assets:()=>d,toc:()=>l,contentTitle:()=>a});var r=JSON.parse('{"id":"data-operate/import/import-way/broker-load-manual","title":"Broker Load","description":"Broker Load is initiated from the MySQL API. Doris will actively pull the data from the source based on the information in the LOAD statement.","source":"@site/versioned_docs/version-4.x/data-operate/import/import-way/broker-load-manual.md","sourceDirName":"data-operate/import/import-way","slug":"/data-operate/import/import-way/broker-load-manual","permalink":"/docs/4.x/data-operate/import/import-way/broker-load-manual","draft":false,"unlisted":false,"tags":[],"version":"4.x","lastUpdatedAt":1770477659000,"frontMatter":{"title":"Broker Load","language":"en","description":"Broker Load is initiated from the MySQL API. Doris will actively pull the data from the source based on the information in the LOAD statement."},"sidebar":"docs","previous":{"title":"Stream Load","permalink":"/docs/4.x/data-operate/import/import-way/stream-load-manual"},"next":{"title":"Routine Load","permalink":"/docs/4.x/data-operate/import/import-way/routine-load-manual"}}'),t=s("785893"),i=s("250065");let o={title:"Broker Load",language:"en",description:"Broker Load is initiated from the MySQL API. Doris will actively pull the data from the source based on the information in the LOAD statement."},a=void 0,d={},l=[{value:"Limitations",id:"limitations",level:2},{value:"Basic Principles",id:"basic-principles",level:2},{value:"Quick start",id:"quick-start",level:2},{value:"Prerequisite check",id:"prerequisite-check",level:3},{value:"Create load job",id:"create-load-job",level:3},{value:"Checking import status",id:"checking-import-status",level:2},{value:"Cancelling an Import",id:"cancelling-an-import",level:2},{value:"Choosing Compute Group",id:"choosing-compute-group",level:3},{value:"Reference Manual",id:"reference-manual",level:2},{value:"SQL syntax for broker load",id:"sql-syntax-for-broker-load",level:3},{value:"Related Configurations",id:"related-configurations",level:3},{value:"Common Issues",id:"common-issues",level:2},{value:"Common Errors",id:"common-errors",level:3},{value:"S3 Load URL style",id:"s3-load-url-style",level:3},{value:"S3 Load temporary credentials",id:"s3-load-temporary-credentials",level:3},{value:"HDFS Simple Authentication",id:"hdfs-simple-authentication",level:3},{value:"HDFS Kerberos Authentication",id:"hdfs-kerberos-authentication",level:3},{value:"HDFS HA Mode",id:"hdfs-ha-mode",level:3},{value:"Load with other brokers",id:"load-with-other-brokers",level:3},{value:"Broker Load examples",id:"broker-load-examples",level:2},{value:"Importing TXT Files from HDFS",id:"importing-txt-files-from-hdfs",level:3},{value:"HDFS requires the configuration of NameNode HA (High Availability)",id:"hdfs-requires-the-configuration-of-namenode-ha-high-availability",level:3},{value:"Importing data from HDFS using wildcards to match two batches of files and importing them into two separate tables",id:"importing-data-from-hdfs-using-wildcards-to-match-two-batches-of-files-and-importing-them-into-two-separate-tables",level:3},{value:"Import a batch of data from HDFS using wildcards",id:"import-a-batch-of-data-from-hdfs-using-wildcards",level:3},{value:"Import Parquet format data and specify the FORMAT as <code>parquet</code>",id:"import-parquet-format-data-and-specify-the-format-as-parquet",level:3},{value:"Import the data and extract the partition field from the file path",id:"import-the-data-and-extract-the-partition-field-from-the-file-path",level:3},{value:"Filter the imported data",id:"filter-the-imported-data",level:3},{value:"Import data and extract the time partition field from the file path.",id:"import-data-and-extract-the-time-partition-field-from-the-file-path",level:3},{value:"Use Merge mode for import",id:"use-merge-mode-for-import",level:3},{value:"Specify the &quot;source_sequence&quot; column during import to ensure the order of replacements.",id:"specify-the-source_sequence-column-during-import-to-ensure-the-order-of-replacements",level:3},{value:"Import the specified file format as <code>json</code>, and specify the <code>json_root</code> and jsonpaths accordingly.",id:"import-the-specified-file-format-as-json-and-specify-the-json_root-and-jsonpaths-accordingly",level:3},{value:"Load from other brokers",id:"load-from-other-brokers",level:3},{value:"More Help",id:"more-help",level:2}];function c(e){let n={a:"a",admonition:"admonition",annotation:"annotation",code:"code",h2:"h2",h3:"h3",img:"img",li:"li",math:"math",mi:"mi",mo:"mo",mrow:"mrow",ol:"ol",p:"p",pre:"pre",semantics:"semantics",span:"span",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.a)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.p,{children:"Broker Load is initiated from the MySQL API. Doris will actively pull the data from the source based on the information in the LOAD statement. Broker Load is an asynchronous import method. The progress and result of Broker Load tasks can be viewed by the SHOW LOAD statement."}),"\n",(0,t.jsx)(n.p,{children:"Broker Load is suitable for scenarios where the source data is stored in remote storage systems, such as HDFS, and the data volume is relatively large."}),"\n",(0,t.jsxs)(n.p,{children:["Direct reads from HDFS or S3 can also be imported through HDFS TVF or S3 TVF in the ",(0,t.jsx)(n.a,{href:"../../../lakehouse/file-analysis",children:"Lakehouse/TVF"}),'. The current "Insert Into" based on TVF is a synchronous import, while Broker Load is an asynchronous import method.']}),"\n",(0,t.jsxs)(n.p,{children:["In early versions of Doris, both S3 Load and HDFS Load were implemented by connecting to specific Broker processes using ",(0,t.jsx)(n.code,{children:"WITH BROKER"}),".\nIn newer versions, S3 Load and HDFS Load have been optimized as the most commonly used import methods, and they no longer depend on an additional Broker process, though they still use syntax similar to Broker Load.\nDue to historical reasons and the similarity in syntax, S3 Load, HDFS Load, and Broker Load are collectively referred to as Broker Load."]}),"\n",(0,t.jsx)(n.h2,{id:"limitations",children:"Limitations"}),"\n",(0,t.jsx)(n.p,{children:"Supported data sources:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"S3 protocol"}),"\n",(0,t.jsx)(n.li,{children:"HDFS protocol"}),"\n",(0,t.jsx)(n.li,{children:"Custom protocol (require broker process)"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Supported file path patterns:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Wildcards: ",(0,t.jsx)(n.code,{children:"*"}),", ",(0,t.jsx)(n.code,{children:"?"}),", ",(0,t.jsx)(n.code,{children:"[abc]"}),", ",(0,t.jsx)(n.code,{children:"[a-z]"})]}),"\n",(0,t.jsxs)(n.li,{children:["Range expansion: ",(0,t.jsx)(n.code,{children:"{1..10}"}),", ",(0,t.jsx)(n.code,{children:"{a,b,c}"})]}),"\n",(0,t.jsxs)(n.li,{children:["See ",(0,t.jsx)(n.a,{href:"../../../sql-manual/basic-element/file-path-pattern",children:"File Path Pattern"})," for complete syntax"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Supported data types:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"CSV"}),"\n",(0,t.jsx)(n.li,{children:"JSON"}),"\n",(0,t.jsx)(n.li,{children:"PARQUET"}),"\n",(0,t.jsx)(n.li,{children:"ORC"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Supported compress types:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"PLAIN"}),"\n",(0,t.jsx)(n.li,{children:"GZ"}),"\n",(0,t.jsx)(n.li,{children:"LZO"}),"\n",(0,t.jsx)(n.li,{children:"BZ2"}),"\n",(0,t.jsx)(n.li,{children:"LZ4FRAME"}),"\n",(0,t.jsx)(n.li,{children:"DEFLATE"}),"\n",(0,t.jsx)(n.li,{children:"LZOP"}),"\n",(0,t.jsx)(n.li,{children:"LZ4BLOCK"}),"\n",(0,t.jsx)(n.li,{children:"SNAPPYBLOCK"}),"\n",(0,t.jsx)(n.li,{children:"ZLIB"}),"\n",(0,t.jsx)(n.li,{children:"ZSTD"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"basic-principles",children:"Basic Principles"}),"\n",(0,t.jsx)(n.p,{children:"After a user submits an import task, the Frontend (FE) generates a corresponding plan. Based on the current number of Backend (BE) nodes and the size of the file, the plan is distributed to multiple BE nodes for execution, with each BE node handling a portion of the import data."}),"\n",(0,t.jsx)(n.p,{children:"During execution, the BE nodes pull data from the Broker, perform necessary transformations, and then import the data into the system. Once all BE nodes have completed the import, the FE makes the final determination on whether the import was successful."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Broker Load",src:s(172029).Z+"",width:"2560",height:"1376"})}),"\n",(0,t.jsx)(n.p,{children:"As seen in the diagram, BE nodes rely on Broker processes to read data from corresponding remote storage systems. The introduction of Broker processes primarily aims to accommodate different remote storage systems. Users can develop their own Broker processes according to established standards. These Broker processes, which can be developed using Java, offer better compatibility with various storage systems in the big data ecosystem. The separation of Broker processes from BE nodes ensures error isolation between the two, enhancing the stability of the BE."}),"\n",(0,t.jsx)(n.p,{children:"Currently, BE nodes have built-in support for HDFS and S3 Brokers. Therefore, when importing data from HDFS or S3, there is no need to additionally start a Broker process. However, if a customized Broker implementation is required, the corresponding Broker process needs to be deployed."}),"\n",(0,t.jsx)(n.h2,{id:"quick-start",children:"Quick start"}),"\n",(0,t.jsxs)(n.p,{children:["This section shows a demo for S3 Load.\nFor the specific syntax for usage, please refer to ",(0,t.jsx)(n.a,{href:"../../../sql-manual/sql-statements/data-modification/load-and-export/BROKER-LOAD",children:"BROKER LOAD"})," in the SQL manual."]}),"\n",(0,t.jsx)(n.h3,{id:"prerequisite-check",children:"Prerequisite check"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Grant privileges on the table"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["Broker Load requires ",(0,t.jsx)(n.code,{children:"INSERT"})," privileges on the target table. If there are no ",(0,t.jsx)(n.code,{children:"INSERT"})," privileges, it can be granted to the user through the ",(0,t.jsx)(n.a,{href:"../../../sql-manual/sql-statements/account-management/GRANT-TO",children:"GRANT"})," command."]}),"\n",(0,t.jsxs)(n.ol,{start:"2",children:["\n",(0,t.jsx)(n.li,{children:"S3 authentication and connection info"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Here, we mainly introduce how to import data stored in AWS S3. For importing data from other object storage systems that support the S3 protocol, you can refer to the steps for AWS S3."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["AK and SK: First, you need to find or regenerate your AWS ",(0,t.jsx)(n.code,{children:"Access Keys"}),". You can find instructions on how to generate them in the AWS console under ",(0,t.jsx)(n.code,{children:"My Security Credentials"}),"."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["REGION and ENDPOINT: The REGION can be selected when creating a bucket or viewed in the bucket list. The S3 ENDPOINT for each REGION can be found in the ",(0,t.jsx)(n.a,{href:"https://docs.aws.amazon.com/general/latest/gr/s3.html#s3_region",children:"AWS documentation"}),"."]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"create-load-job",children:"Create load job"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Create a CSV file brokerload_example.csv. The file is stored on S3 and its content is as follows:"}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"1,Emily,25\n2,Benjamin,35\n3,Olivia,28\n4,Alexander,60\n5,Ava,17\n6,William,69\n7,Sophia,32\n8,James,64\n9,Emma,37\n10,Liam,64\n"})}),"\n",(0,t.jsxs)(n.ol,{start:"2",children:["\n",(0,t.jsx)(n.li,{children:"Create Doris table for the load"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Create the imported table in Doris. The SQL statement is as follows:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:'CREATE TABLE testdb.test_brokerload(\n    user_id            BIGINT       NOT NULL COMMENT "user id",\n    name               VARCHAR(20)           COMMENT "name",\n    age                INT                   COMMENT "age"\n)\nDUPLICATE KEY(user_id)\nDISTRIBUTED BY HASH(user_id) BUCKETS 10;\n'})}),"\n",(0,t.jsxs)(n.ol,{start:"3",children:["\n",(0,t.jsx)(n.li,{children:"Use Broker Load to import data from S3. The bucket name and S3 authentication information should be filled in according to the actual situation:"}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:'    LOAD LABEL broker_load_2022_04_01\n    (\n        DATA INFILE("s3://your_bucket_name/brokerload_example.csv")\n        INTO TABLE test_brokerload\n        COLUMNS TERMINATED BY ","\n        FORMAT AS "CSV"\n        (user_id, name, age)\n    )\n    WITH S3\n    (\n        "provider" = "S3",\n        "AWS_ENDPOINT" = "s3.us-west-2.amazonaws.com",\n        "AWS_ACCESS_KEY" = "<your-ak>",\n        "AWS_SECRET_KEY"="<your-sk>",\n        "AWS_REGION" = "us-west-2",\n        "compress_type" = "PLAIN"\n    )\n    PROPERTIES\n    (\n        "timeout" = "3600"\n    );\n'})}),"\n",(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.code,{children:"provider"})," specifies the vendor of the S3 Service.\nSupported S3 Provider list:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:'"S3" (AWS, Amazon Web Services)'}),"\n",(0,t.jsx)(n.li,{children:'"AZURE" (Microsoft Azure)'}),"\n",(0,t.jsx)(n.li,{children:'"GCP" (GCP, Google Cloud Platform)'}),"\n",(0,t.jsx)(n.li,{children:'"OSS" (Alibaba Cloud)'}),"\n",(0,t.jsx)(n.li,{children:'"COS" (Tencent Cloud)'}),"\n",(0,t.jsx)(n.li,{children:'"OBS" (Huawei Cloud)'}),"\n",(0,t.jsx)(n.li,{children:'"BOS" (Baidu Cloud)'}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:'If your service is not in the list (such as MinIO), you can try using "S3" (AWS compatible mode)'}),"\n",(0,t.jsx)(n.h2,{id:"checking-import-status",children:"Checking import status"}),"\n",(0,t.jsxs)(n.p,{children:["Broker Load is an asynchronous import method, and the specific import results can be viewed through the ",(0,t.jsx)(n.a,{href:"../../../sql-manual/sql-statements/data-modification/load-and-export/SHOW-LOAD",children:"SHOW LOAD"})," command."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:'mysql> show load order by createtime desc limit 1\\G;\n*************************** 1. row ***************************\n         JobId: 41326624\n         Label: broker_load_2022_04_01\n         State: FINISHED\n      Progress: ETL:100%; LOAD:100%\n          Type: BROKER\n       EtlInfo: unselected.rows=0; dpp.abnorm.ALL=0; dpp.norm.ALL=27\n      TaskInfo: cluster:N/A; timeout(s):1200; max_filter_ratio:0.1\n      ErrorMsg: NULL\n    CreateTime: 2022-04-01 18:59:06\n  EtlStartTime: 2022-04-01 18:59:11\n EtlFinishTime: 2022-04-01 18:59:11\n LoadStartTime: 2022-04-01 18:59:11\nLoadFinishTime: 2022-04-01 18:59:11\n           URL: NULL\n    JobDetails: {"Unfinished backends":{"5072bde59b74b65-8d2c0ee5b029adc0":[]},"ScannedRows":27,"TaskNumber":1,"All backends":{"5072bde59b74b65-8d2c0ee5b029adc0":[36728051]},"FileNumber":1,"FileSize":5540}\n1 row in set (0.01 sec)\n'})}),"\n",(0,t.jsx)(n.h2,{id:"cancelling-an-import",children:"Cancelling an Import"}),"\n",(0,t.jsxs)(n.p,{children:["When the status of a Broker Load job is not CANCELLED or FINISHED, it can be manually cancelled by the user. To cancel, the user needs to specify the label of the import task to be cancelled. The syntax for the cancel import command can be viewed by executing ",(0,t.jsx)(n.a,{href:"../../../sql-manual/sql-statements/data-modification/load-and-export/CANCEL-LOAD",children:"CANCEL LOAD"}),"."]}),"\n",(0,t.jsx)(n.p,{children:'For example: To cancel the import job with the label "broker_load_2022_04_01" on the DEMO database.'}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:'CANCEL LOAD FROM demo WHERE LABEL = "broker_load_2022_04_01";\n'})}),"\n",(0,t.jsx)(n.h3,{id:"choosing-compute-group",children:"Choosing Compute Group"}),"\n",(0,t.jsx)(n.p,{children:"In the storage-computation separation mode, the priority logic for Broker Load to select a Compute Group is as follows:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["Select the Compute Group specified by the ",(0,t.jsx)(n.code,{children:"use db@cluster statement"}),";"]}),"\n",(0,t.jsxs)(n.li,{children:["Select the Compute Group specified by the user properties ",(0,t.jsx)(n.code,{children:"default_compute_group"}),";"]}),"\n",(0,t.jsx)(n.li,{children:"Select one from the Compute Groups that the current user has permissions to access;"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["In the integrated storage-computation mode, select the Compute Group specified in the user properties ",(0,t.jsx)(n.code,{children:"resource_tags.location"}),";\nif not specified in the user properties, use the Compute Group named ",(0,t.jsx)(n.code,{children:"default"}),";"]}),"\n",(0,t.jsx)(n.h2,{id:"reference-manual",children:"Reference Manual"}),"\n",(0,t.jsx)(n.h3,{id:"sql-syntax-for-broker-load",children:"SQL syntax for broker load"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:'LOAD LABEL load_label\n(\ndata_desc1[, data_desc2, ...]\n[format_properties]\n)\nWITH [S3|HDFS|BROKER broker_name] \n[broker_properties]\n[load_properties]\n[COMMENT "comments"];\n'})}),"\n",(0,t.jsxs)(n.p,{children:["The WITH clause specifies how to access the storage system, and ",(0,t.jsx)(n.code,{children:"broker_properties"})," is the configuration parameter for the access method"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"S3"}),": Storage system using the S3 protocol"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"HDFS"}),": Storage system using the HDFS protocol"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"BROKER broker_name"}),": Storage system using other protocols. You can view the currently available broker_name list through ",(0,t.jsx)(n.code,{children:"SHOW BROKER"}),'. For more information, see "Other Broker Import" in the Common Issues section.']}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"related-configurations",children:"Related Configurations"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Load Properties"})}),"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Property Name"}),(0,t.jsx)(n.th,{children:"Type"}),(0,t.jsx)(n.th,{children:"Default Value"}),(0,t.jsx)(n.th,{children:"Description"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:'"timeout"'}),(0,t.jsx)(n.td,{children:"Long"}),(0,t.jsx)(n.td,{children:"14400"}),(0,t.jsx)(n.td,{children:"Used to specify the timeout for the import in seconds. The configurable range is from 1 second to 259200 seconds."})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:'"max_filter_ratio"'}),(0,t.jsx)(n.td,{children:"Float"}),(0,t.jsx)(n.td,{children:"0.0"}),(0,t.jsx)(n.td,{children:"Used to specify the maximum tolerable ratio of filterable (irregular or otherwise problematic) data, which defaults to zero tolerance. The value range is 0 to 1. If the error rate of the imported data exceeds this value, the import will fail. Irregular data does not include rows filtered out by the where condition."})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:'"strict_mode"'}),(0,t.jsx)(n.td,{children:"Boolean"}),(0,t.jsx)(n.td,{children:"false"}),(0,t.jsx)(n.td,{children:"Used to specify whether to enable strict mode for this import."})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:'"partial_columns"'}),(0,t.jsx)(n.td,{children:"Boolean"}),(0,t.jsx)(n.td,{children:"false"}),(0,t.jsx)(n.td,{children:"Used to specify whether to enable partial column update, the default value is false, this parameter is only available for Unique Key + Merge on Write tables."})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:'"timezone"'}),(0,t.jsx)(n.td,{children:"String"}),(0,t.jsx)(n.td,{children:'"Asia/Shanghai"'}),(0,t.jsx)(n.td,{children:"Used to specify the timezone to be used for this import. This parameter affects the results of all timezone-related functions involved in the import."})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:'"load_parallelism"'}),(0,t.jsx)(n.td,{children:"Integer"}),(0,t.jsx)(n.td,{children:"8"}),(0,t.jsx)(n.td,{children:"Limits the maximum parallel instances on each backend."})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:'"send_batch_parallelism"'}),(0,t.jsx)(n.td,{children:"Integer"}),(0,t.jsx)(n.td,{children:"1"}),(0,t.jsx)(n.td,{children:"The parallelism for sink node to send data, when memtable_on_sink_node is disabled."})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:'"load_to_single_tablet"'}),(0,t.jsx)(n.td,{children:"Boolean"}),(0,t.jsx)(n.td,{children:'"false"'}),(0,t.jsx)(n.td,{children:"Used to specify whether to load data only to a single tablet corresponding to the partition. This parameter is only available when loading to an OLAP table with random bucketing."})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:'"priority"'}),(0,t.jsx)(n.td,{children:'oneof "HIGH", "NORMAL", "LOW"'}),(0,t.jsx)(n.td,{children:'"NORMAL"'}),(0,t.jsx)(n.td,{children:"The priority of the task."})]})]})]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Format Properties"})}),"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Property Name"}),(0,t.jsx)(n.th,{children:"Type"}),(0,t.jsx)(n.th,{children:"Default Value"}),(0,t.jsx)(n.th,{children:"Description"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"skip_lines"})}),(0,t.jsx)(n.td,{children:"Integer"}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"0"})}),(0,t.jsxs)(n.td,{children:["Number of lines to skip at the start of a CSV file. Ignored if using ",(0,t.jsx)(n.code,{children:"csv_with_names"})," or ",(0,t.jsx)(n.code,{children:"csv_with_names_and_types"}),"."]})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"trim_double_quotes"})}),(0,t.jsx)(n.td,{children:"Boolean"}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"false"})}),(0,t.jsxs)(n.td,{children:["If ",(0,t.jsx)(n.code,{children:"true"}),", trims outermost double quotes from each field."]})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"enclose"})}),(0,t.jsx)(n.td,{children:"String"}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:'""'})}),(0,t.jsxs)(n.td,{children:["Enclosure character for fields containing delimiters or newlines. E.g., if delimiter is ",(0,t.jsx)(n.code,{children:","})," and encloser is ",(0,t.jsx)(n.code,{children:"'"}),", then ",(0,t.jsx)(n.code,{children:"'b,c'"})," is parsed as one field."]})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"escape"})}),(0,t.jsx)(n.td,{children:"String"}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:'""'})}),(0,t.jsxs)(n.td,{children:["Escape character to include enclosure characters in field content. E.g., ",(0,t.jsx)(n.code,{children:"'b,\\'c'"})," keeps ",(0,t.jsx)(n.code,{children:"'b,'c'"})," as one field when ",(0,t.jsx)(n.code,{children:"'"})," is the enclosure and ",(0,t.jsx)(n.code,{children:"\\"})," is the escape."]})]})]})]}),"\n",(0,t.jsx)(n.p,{children:"Note: Format properties define how to parse the source file (e.g., delimiters, quote handling) and must be set inside the LOAD clause. Load properties control the execution behavior (e.g., timeout, retries) and must be set outside, in the outer PROPERTIES block."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:'LOAD LABEL s3_load_example (\n    DATA INFILE("s3://bucket/path/file.csv")\n    INTO TABLE users\n    COLUMNS TERMINATED BY ","\n    FORMAT AS "CSV"\n    (user_id, name, age)\n    PROPERTIES (\n        "trim_double_quotes" = "true"  -- format property\n    )\n)\nWITH S3 (\n    ...\n)\nPROPERTIES (\n    "timeout" = "3600"  -- load property\n);\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"fe.conf"})}),"\n",(0,t.jsxs)(n.p,{children:["The following configurations belong to the system-level settings for Broker load, which affect all Broker load import tasks. These configurations can be adjusted by modifying the ",(0,t.jsx)(n.code,{children:"fe.conf "}),"file."]}),"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Session Variable"}),(0,t.jsx)(n.th,{children:"Type"}),(0,t.jsx)(n.th,{children:"Default Value"}),(0,t.jsx)(n.th,{children:"Description"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"min_bytes_per_broker_scanner"}),(0,t.jsx)(n.td,{children:"Long"}),(0,t.jsx)(n.td,{children:"67108864 (64 MB)"}),(0,t.jsx)(n.td,{children:"The minimum amount of data processed by a single BE in a Broker Load job, in bytes."})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"max_bytes_per_broker_scanner"}),(0,t.jsx)(n.td,{children:"Long"}),(0,t.jsx)(n.td,{children:"536870912000 (500 GB)"}),(0,t.jsxs)(n.td,{children:["The maximum amount of data processed by a single BE in a Broker Load job, in bytes. Usually, the maximum amount of data supported by an import job is ",(0,t.jsx)(n.code,{children:"max_bytes_per_broker_scanner * number of BE nodes"}),". If you need to import a larger amount of data, you need to adjust the size of the ",(0,t.jsx)(n.code,{children:"max_bytes_per_broker_scanner"})," parameter appropriately."]})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"max_broker_concurrency"}),(0,t.jsx)(n.td,{children:"Integer"}),(0,t.jsx)(n.td,{children:"10"}),(0,t.jsx)(n.td,{children:"Limits the maximum number of concurrent imports for a job."})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"default_load_parallelism"}),(0,t.jsx)(n.td,{children:"Integer"}),(0,t.jsx)(n.td,{children:"8"}),(0,t.jsx)(n.td,{children:"Maximum number of concurrent instances per BE node"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"broker_load_default_timeout_second"}),(0,t.jsx)(n.td,{children:"14400"}),(0,t.jsx)(n.td,{children:"Default timeout for Broker Load import, in seconds."}),(0,t.jsx)(n.td,{})]})]})]}),"\n",(0,t.jsxs)(n.p,{children:["Note: The ",(0,t.jsx)(n.code,{children:"min_bytes_per_broker_scanner"}),", the ",(0,t.jsx)(n.code,{children:"max_broker_concurrency"}),", the size of the source file and the number of BEs in the current cluster jointly determine the number of concurrent execution instances for this load."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-Plain",children:"Import Concurrency = Math.min(Source File Size / min_bytes_per_broker_scanner, max_broker_concurrency, Current Number of BE Nodes * load_parallelism)\nProcessing Volume per BE for this Import = Source File Size / Import Concurrency\n"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"session variables"})}),"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Session Variable"}),(0,t.jsx)(n.th,{children:"Type"}),(0,t.jsx)(n.th,{children:"Default"}),(0,t.jsx)(n.th,{children:"Description"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"time_zone"}),(0,t.jsx)(n.td,{children:"String"}),(0,t.jsx)(n.td,{children:'"Asia/Shanghai"'}),(0,t.jsx)(n.td,{children:"Default time zone, which will affect the results of time zone related functions in import."})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"send_batch_parallelism"}),(0,t.jsx)(n.td,{children:"Integer"}),(0,t.jsx)(n.td,{children:"1"}),(0,t.jsxs)(n.td,{children:["The concurrency of the sink node sending data, which takes effect only when ",(0,t.jsx)(n.code,{children:"enable_memtable_on_sink_node"})," is set to false."]})]})]})]}),"\n",(0,t.jsx)(n.h2,{id:"common-issues",children:"Common Issues"}),"\n",(0,t.jsx)(n.h3,{id:"common-errors",children:"Common Errors"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsxs)(n.strong,{children:["1. Import Error: ",(0,t.jsx)(n.code,{children:"Scan bytes per broker scanner exceed limit:xxx"})]})}),"\n",(0,t.jsxs)(n.p,{children:["Please refer to the best practices section in the documentation and modify the FE configuration items ",(0,t.jsx)(n.code,{children:"max_bytes_per_broker_scanner"})," and ",(0,t.jsx)(n.code,{children:"max_broker_concurrency."})]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsxs)(n.strong,{children:["2. Import Error: : ",(0,t.jsx)(n.code,{children:"failed to send batch"})," or ",(0,t.jsx)(n.code,{children:"TabletWriter add batch with unknown id"})]})}),"\n",(0,t.jsxs)(n.p,{children:["Appropriately adjust the ",(0,t.jsx)(n.code,{children:"query_timeout"})," and ",(0,t.jsx)(n.code,{children:"streaming_load_rpc_max_alive_time_sec"})," settings."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsxs)(n.strong,{children:["3. Import Error: ",(0,t.jsx)(n.code,{children:"LOAD_RUN_FAIL; msg:Invalid Column Name:xxx"})]})}),"\n",(0,t.jsx)(n.p,{children:"For PARQUET or ORC format data, the column names in the file header must match the column names in the Doris table. For example:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:"(tmp_c1,tmp_c2)\nSET\n(\n    id=tmp_c2,\n    name=tmp_c1\n)\n"})}),"\n",(0,t.jsx)(n.p,{children:"This represents fetching columns named (tmp_c1, tmp_c2) in the parquet or orc file and mapping them to the (id, name) columns in the Doris table. If no set is specified, the columns in the file header will be used for mapping."}),"\n",(0,t.jsx)(n.admonition,{title:"Note",type:"info",children:(0,t.jsx)(n.p,{children:"If ORC files are generated directly using certain Hive versions, the column headers in the ORC file may not be the Hive metadata, but (_col0, _col1, _col2, ...), which may lead to the Invalid Column Name error. In this case, mapping using SET is necessary."})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsxs)(n.strong,{children:["5. Import Error: ",(0,t.jsx)(n.code,{children:"Failed to get S3 FileSystem for bucket is null/empty"})]})}),"\n",(0,t.jsxs)(n.p,{children:["The bucket information is incorrect or does not exist. Or the bucket format is not supported. When creating a bucket name with an underscore using GCS, such as ",(0,t.jsx)(n.code,{children:"s3://gs_bucket/load_tbl"}),", the S3 Client may report an error when accessing GCS. It is recommended not to use underscores when creating buckets."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"6. Import Timeout"})}),"\n",(0,t.jsx)(n.p,{children:"The default timeout for imports is 4 hours. If a timeout occurs, it is not recommended to directly increase the maximum import timeout to solve the problem. If the single import time exceeds the default import timeout of 4 hours, it is best to split the file to be imported and perform multiple imports to solve the problem. Setting an excessively long timeout time can lead to high costs for retrying failed imports."}),"\n",(0,t.jsx)(n.p,{children:"You can calculate the expected maximum import file data volume for the Doris cluster using the following formula:"}),"\n",(0,t.jsx)(n.p,{children:"Expected Maximum Import File Data Volume = 14400s * 10M/s * Number of BEs"}),"\n",(0,t.jsx)(n.p,{children:"For example, if the cluster has 10 BEs:"}),"\n",(0,t.jsx)(n.p,{children:"Expected Maximum Import File Data Volume = 14400s * 10M/s * 10 = 1440000M \u2248 1440G"}),"\n",(0,t.jsx)(n.admonition,{title:"Note",type:"info",children:(0,t.jsx)(n.p,{children:"In general, user environments may not reach speeds of 10M/s, so it is recommended to split files exceeding 500G before importing."})}),"\n",(0,t.jsx)(n.h3,{id:"s3-load-url-style",children:"S3 Load URL style"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["The S3 SDK defaults to using the virtual-hosted style method for accessing objects. However, some object storage systems may not have enabled or supported the virtual-hosted style access. In such cases, we can add the ",(0,t.jsx)(n.code,{children:"use_path_style"})," parameter to force the use of the path style method:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:'  WITH S3\n  (\n        "AWS_ENDPOINT" = "AWS_ENDPOINT",\n        "AWS_ACCESS_KEY" = "AWS_ACCESS_KEY",\n        "AWS_SECRET_KEY"="AWS_SECRET_KEY",\n        "AWS_REGION" = "AWS_REGION",\n        "use_path_style" = "true"\n  )\n'})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"s3-load-temporary-credentials",children:"S3 Load temporary credentials"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Support for accessing all object storage systems that support the S3 protocol using temporary credentials (TOKEN) is available. The usage is as follows:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:'  WITH S3\n  (\n        "AWS_ENDPOINT" = "AWS_ENDPOINT",\n        "AWS_ACCESS_KEY" = "AWS_TEMP_ACCESS_KEY",\n        "AWS_SECRET_KEY" = "AWS_TEMP_SECRET_KEY",\n        "AWS_TOKEN" = "AWS_TEMP_TOKEN",\n        "AWS_REGION" = "AWS_REGION"\n  )\n'})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"hdfs-simple-authentication",children:"HDFS Simple Authentication"}),"\n",(0,t.jsx)(n.p,{children:'Simple authentication refers to the configuration of Hadoop where hadoop.security.authentication is set to "simple".'}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-Plain",children:'(\n    "username" = "user",\n    "password" = ""\n);\n'})}),"\n",(0,t.jsx)(n.p,{children:"The username should be configured as the user to be accessed, and the password can be left blank."}),"\n",(0,t.jsx)(n.h3,{id:"hdfs-kerberos-authentication",children:"HDFS Kerberos Authentication"}),"\n",(0,t.jsx)(n.p,{children:"This authentication method requires the following information:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"hadoop.security.authentication:"})," Specifies the authentication method as Kerberos."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"hadoop.kerberos.principal:"})," Specifies the Kerberos principal."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"hadoop.kerberos.keytab:"})," Specifies the file path of the Kerberos keytab. The file must be an absolute path on the server where the Broker process is located and must be accessible by the Broker process."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"kerberos_keytab_content:"})," Specifies the content of the Kerberos keytab file after being encoded in base64. This can be used as an alternative to the kerberos_keytab configuration."]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Example configuration:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-Plain",children:'(\n    "hadoop.security.authentication" = "kerberos",\n    "hadoop.kerberos.principal" = "doris@YOUR.COM",\n    "hadoop.kerberos.keytab" = "/home/doris/my.keytab"\n)\n(\n    "hadoop.security.authentication" = "kerberos",\n    "hadoop.kerberos.principal" = "doris@YOUR.COM",\n    "kerberos_keytab_content" = "ASDOWHDLAWIDJHWLDKSALDJSDIWALD"\n)\n'})}),"\n",(0,t.jsxs)(n.p,{children:["To use Kerberos authentication, the ",(0,t.jsx)(n.a,{href:"https://web.mit.edu/kerberos/krb5-1.12/doc/admin/conf_files/krb5_conf.html",children:"krb5.conf (opens new window)"})," file is required. The krb5.conf file contains Kerberos configuration information. Typically, the krb5.conf file should be installed in the /etc directory. You can override the default location by setting the KRB5_CONFIG environment variable. An example of the krb5.conf file content is as follows:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-Plain",children:"[libdefaults]\n    default_realm = DORIS.HADOOP\n    default_tkt_enctypes = des3-hmac-sha1 des-cbc-crc\n    default_tgs_enctypes = des3-hmac-sha1 des-cbc-crc\n    dns_lookup_kdc = true\n    dns_lookup_realm = false\n\n[realms]\n    DORIS.HADOOP = {\n        kdc = kerberos-doris.hadoop.service:7005\n    }\n"})}),"\n",(0,t.jsx)(n.h3,{id:"hdfs-ha-mode",children:"HDFS HA Mode"}),"\n",(0,t.jsx)(n.p,{children:"This configuration is used to access HDFS clusters deployed in HA (High Availability) mode."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"dfs.nameservices:"}),' Specifies the name of the HDFS service, which can be customized. For example: "dfs.nameservices" = "my_ha".']}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"dfs.ha.namenodes.xxx:"}),' Customizes the names of the namenodes, with multiple names separated by commas. Here, xxx represents the custom name specified in dfs.nameservices. For example: "dfs.ha.namenodes.my_ha" = "my_nn".']}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"dfs.namenode.rpc-address.xxx.nn:"}),' Specifies the RPC address information for the namenode. In this context, nn represents the namenode name configured in dfs.ha.namenodes.xxx. For example: "dfs.namenode.rpc-address.my_ha.my_nn" = "host:port".']}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"dfs.client.failover.proxy.provider.[nameservice ID]:"})," Specifies the provider for client connections to the namenode. The default is org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider."]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"An example configuration is as follows:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:'(\n    "fs.defaultFS" = "hdfs://my_ha",\n    "dfs.nameservices" = "my_ha",\n    "dfs.ha.namenodes.my_ha" = "my_namenode1, my_namenode2",\n    "dfs.namenode.rpc-address.my_ha.my_namenode1" = "nn1_host:rpc_port",\n    "dfs.namenode.rpc-address.my_ha.my_namenode2" = "nn2_host:rpc_port",\n    "dfs.client.failover.proxy.provider.my_ha" = "org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider"\n)\n'})}),"\n",(0,t.jsx)(n.p,{children:"HA mode can be combined with the previous two authentication methods for cluster access. For example, accessing HA HDFS through simple authentication:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:'(\n    "username"="user",\n    "password"="passwd",\n    "fs.defaultFS" = "hdfs://my_ha",\n    "dfs.nameservices" = "my_ha",\n    "dfs.ha.namenodes.my_ha" = "my_namenode1, my_namenode2",\n    "dfs.namenode.rpc-address.my_ha.my_namenode1" = "nn1_host:rpc_port",\n    "dfs.namenode.rpc-address.my_ha.my_namenode2" = "nn2_host:rpc_port",\n    "dfs.client.failover.proxy.provider.my_ha" = "org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider"\n)\n'})}),"\n",(0,t.jsx)(n.h3,{id:"load-with-other-brokers",children:"Load with other brokers"}),"\n",(0,t.jsxs)(n.p,{children:["The Broker for other remote storage systems is an optional process in the Doris cluster, primarily used to support Doris in reading and writing files and directories on remote storage.\nCurrently, Doris provides Broker implementations for various remote storage systems.\nIn earlier versions, different object storage Brokers were also available, but now it is recommended to use the ",(0,t.jsx)(n.code,{children:"WITH S3"})," method to import data from object storage, and the ",(0,t.jsx)(n.code,{children:"WITH BROKER"})," method is no longer recommended."]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Tencent Cloud CHDFS"}),"\n",(0,t.jsx)(n.li,{children:"Tencent Cloud GFS"}),"\n",(0,t.jsx)(n.li,{children:"JuiceFS"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"The Broker provides services through an RPC service port and operates as a stateless Java process. Its primary responsibility is to encapsulate POSIX-like file operations for remote storage, such as open, pread, pwrite, and more. Additionally, the Broker does not keep track of any other information, which means that all the connection details, file information, and permission details related to the remote storage must be passed to the Broker process through parameters during RPC calls. This ensures that the Broker can correctly read and write files."}),"\n",(0,t.jsx)(n.p,{children:"The Broker serves solely as a data pathway and does not involve any computational tasks, thus requiring minimal memory usage. Typically, a Doris system would deploy one or more Broker processes. Furthermore, Brokers of the same type are grouped together and assigned a unique name (Broker name)."}),"\n",(0,t.jsx)(n.p,{children:"This section primarily focuses on the parameters required by the Broker when accessing different remote storage systems, such as connection information, authentication details, and more. Understanding and correctly configuring these parameters is crucial for successful and secure data exchange between Doris and the remote storage systems."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Broker Information"})}),"\n",(0,t.jsx)(n.p,{children:"The information of the Broker consists of two parts: the name (Broker name) and the authentication information. The usual syntax format is as follows:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:'WITH BROKER "broker_name" \n(\n    "username" = "xxx",\n    "password" = "yyy",\n    "other_prop" = "prop_value",\n    ...\n);\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Broker Name"})}),"\n",(0,t.jsxs)(n.p,{children:["Typically, users need to specify an existing Broker Name through the ",(0,t.jsx)(n.code,{children:'WITH BROKER "broker_name"'})," clause in the operation command. The Broker Name is a name designated by the user when adding a Broker process through the ",(0,t.jsx)(n.code,{children:"ALTER SYSTEM ADD BROKER"})," command. One name usually corresponds to one or more Broker processes. Doris will select an available Broker process based on the name. Users can view the Brokers that currently exist in the cluster through the ",(0,t.jsx)(n.code,{children:"SHOW BROKER"})," command."]}),"\n",(0,t.jsx)(n.admonition,{title:"Note",type:"info",children:(0,t.jsx)(n.p,{children:"The Broker Name is merely a user-defined name and does not represent the type of Broker."})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Authentication Information"}),"\nDifferent Broker types and access methods require different authentication information. The authentication information is usually provided in the Property Map in a Key-Value format after ",(0,t.jsx)(n.code,{children:'WITH BROKER "broker_name"'}),"."]}),"\n",(0,t.jsx)(n.h2,{id:"broker-load-examples",children:"Broker Load examples"}),"\n",(0,t.jsx)(n.h3,{id:"importing-txt-files-from-hdfs",children:"Importing TXT Files from HDFS"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:'LOAD LABEL demo.label_20220402\n(\n    DATA INFILE("hdfs://host:port/tmp/test_hdfs.txt")\n    INTO TABLE `load_hdfs_file_test`\n    COLUMNS TERMINATED BY "\\t"            \n    (id,age,name)\n) \nwith HDFS\n(\n  "fs.defaultFS"="hdfs://host:port",\n  "hadoop.username" = "user"\n)\nPROPERTIES\n(\n    "timeout"="1200",\n    "max_filter_ratio"="0.1"\n);\n'})}),"\n",(0,t.jsx)(n.h3,{id:"hdfs-requires-the-configuration-of-namenode-ha-high-availability",children:"HDFS requires the configuration of NameNode HA (High Availability)"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:'LOAD LABEL demo.label_20220402\n(\n    DATA INFILE("hdfs://hafs/tmp/test_hdfs.txt")\n    INTO TABLE `load_hdfs_file_test`\n    COLUMNS TERMINATED BY "\\t"            \n    (id,age,name)\n) \nwith HDFS\n(\n    "hadoop.username" = "user",\n    "fs.defaultFS"="hdfs://hafs"\uFF0C\n    "dfs.nameservices" = "hafs",\n    "dfs.ha.namenodes.hafs" = "my_namenode1, my_namenode2",\n    "dfs.namenode.rpc-address.hafs.my_namenode1" = "nn1_host:rpc_port",\n    "dfs.namenode.rpc-address.hafs.my_namenode2" = "nn2_host:rpc_port",\n    "dfs.client.failover.proxy.provider.hafs" = "org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider"\n)\nPROPERTIES\n(\n    "timeout"="1200",\n    "max_filter_ratio"="0.1"\n);\n'})}),"\n",(0,t.jsx)(n.h3,{id:"importing-data-from-hdfs-using-wildcards-to-match-two-batches-of-files-and-importing-them-into-two-separate-tables",children:"Importing data from HDFS using wildcards to match two batches of files and importing them into two separate tables"}),"\n",(0,t.jsxs)(n.p,{children:["Broker Load supports wildcards (",(0,t.jsx)(n.code,{children:"*"}),", ",(0,t.jsx)(n.code,{children:"?"}),", ",(0,t.jsx)(n.code,{children:"[...]"}),") and range patterns (",(0,t.jsx)(n.code,{children:"{1..10}"}),") in file paths. For detailed syntax, see ",(0,t.jsx)(n.a,{href:"../../../sql-manual/basic-element/file-path-pattern",children:"File Path Pattern"}),"."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:'LOAD LABEL example_db.label2\n(\n    DATA INFILE("hdfs://host:port/input/file-10*")\n    INTO TABLE `my_table1`\n    PARTITION (p1)\n    COLUMNS TERMINATED BY ","\n    (k1, tmp_k2, tmp_k3)\n    SET (\n        k2 = tmp_k2 + 1,\n        k3 = tmp_k3 + 1\n    ),\n    DATA INFILE("hdfs://host:port/input/file-20*")\n    INTO TABLE `my_table2`\n    COLUMNS TERMINATED BY ","\n    (k1, k2, k3)\n)\nwith HDFS\n(\n  "fs.defaultFS"="hdfs://host:port",\n  "hadoop.username" = "user"\n);\n'})}),"\n",(0,t.jsxs)(n.p,{children:["To import two batches of files matching the wildcards ",(0,t.jsx)(n.code,{children:"file-10*"})," and ",(0,t.jsx)(n.code,{children:"file-20*"})," from HDFS and load them into two separate tables ",(0,t.jsx)(n.code,{children:"my_table1"})," and ",(0,t.jsx)(n.code,{children:"my_table2"}),". In this case, my_table1 specifies that the data should be imported into partition p1, and the values in the second and third columns of the source files should be incremented by 1 before being imported."]}),"\n",(0,t.jsx)(n.h3,{id:"import-a-batch-of-data-from-hdfs-using-wildcards",children:"Import a batch of data from HDFS using wildcards"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:'LOAD LABEL example_db.label3\n(\n    DATA INFILE("hdfs://host:port/user/doris/data/*/*")\n    INTO TABLE `my_table`\n    COLUMNS TERMINATED BY "\\\\x01"\n)\nwith HDFS\n(\n  "fs.defaultFS"="hdfs://host:port",\n  "hadoop.username" = "user"\n);\n'})}),"\n",(0,t.jsx)(n.p,{children:"To specify the delimiter as the commonly used default delimiter for Hive, which is \\x01, and to use the wildcard character * to refer to all files in all directories under the data directory."}),"\n",(0,t.jsxs)(n.h3,{id:"import-parquet-format-data-and-specify-the-format-as-parquet",children:["Import Parquet format data and specify the FORMAT as ",(0,t.jsx)(n.code,{children:"parquet"})]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:'LOAD LABEL example_db.label4\n(\n    DATA INFILE("hdfs://host:port/input/file")\n    INTO TABLE `my_table`\n    FORMAT AS "parquet"\n    (k1, k2, k3)\n)\nwith HDFS\n(\n  "fs.defaultFS"="hdfs://host:port",\n  "hadoop.username" = "user"\n);\n'})}),"\n",(0,t.jsx)(n.p,{children:"The default method is to determine by file extension."}),"\n",(0,t.jsx)(n.h3,{id:"import-the-data-and-extract-the-partition-field-from-the-file-path",children:"Import the data and extract the partition field from the file path"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:'LOAD LABEL example_db.label5\n(\n    DATA INFILE("hdfs://host:port/input/city=beijing/*/*")\n    INTO TABLE `my_table`\n    FORMAT AS "csv"\n    (k1, k2, k3)\n    COLUMNS FROM PATH AS (city, utc_date)\n)\nwith HDFS\n(\n  "fs.defaultFS"="hdfs://host:port",\n  "hadoop.username" = "user"\n);\n'})}),"\n",(0,t.jsxs)(n.p,{children:["The columns in the ",(0,t.jsx)(n.code,{children:"my_table"})," are ",(0,t.jsx)(n.code,{children:"k1"}),", ",(0,t.jsx)(n.code,{children:"k2"}),", ",(0,t.jsx)(n.code,{children:"k3"}),", ",(0,t.jsx)(n.code,{children:"city"}),", and ",(0,t.jsx)(n.code,{children:"utc_date"}),"."]}),"\n",(0,t.jsxs)(n.p,{children:["The directory ",(0,t.jsx)(n.code,{children:"hdfs://hdfs_host:hdfs_port/user/doris/data/input/dir/city=beijing"})," contains the following files:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-Plain",children:"hdfs://hdfs_host:hdfs_port/input/city=beijing/utc_date=2020-10-01/0000.csv\nhdfs://hdfs_host:hdfs_port/input/city=beijing/utc_date=2020-10-02/0000.csv\nhdfs://hdfs_host:hdfs_port/input/city=tianji/utc_date=2020-10-03/0000.csv\nhdfs://hdfs_host:hdfs_port/input/city=tianji/utc_date=2020-10-04/0000.csv\n"})}),"\n",(0,t.jsxs)(n.p,{children:["The file only contains three columns of data:",(0,t.jsx)(n.code,{children:"k1"}),",",(0,t.jsx)(n.code,{children:"k2"}),", and ",(0,t.jsx)(n.code,{children:"k3"}),". The other two columns,",(0,t.jsx)(n.code,{children:"city"})," and ",(0,t.jsx)(n.code,{children:"utc_date"}),", will be extracted from the file path."]}),"\n",(0,t.jsx)(n.h3,{id:"filter-the-imported-data",children:"Filter the imported data"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:'LOAD LABEL example_db.label6\n(\n    DATA INFILE("hdfs://host:port/input/file")\n    INTO TABLE `my_table`\n    (k1, k2, k3)\n    SET (\n        k2 = k2 + 1\n    )\n    PRECEDING FILTER k1 = 1\n    WHERE k1 > k2\n)\nwith HDFS\n(\n  "fs.defaultFS"="hdfs://host:port",\n  "hadoop.username" = "user"\n);\n'})}),"\n",(0,t.jsx)(n.p,{children:"Only the rows where k1 = 1 in the original data and k1 > k2 after transformation will be imported."}),"\n",(0,t.jsx)(n.h3,{id:"import-data-and-extract-the-time-partition-field-from-the-file-path",children:"Import data and extract the time partition field from the file path."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:'LOAD LABEL example_db.label7\n(\n    DATA INFILE("hdfs://host:port/user/data/*/test.txt") \n    INTO TABLE `tbl12`\n    COLUMNS TERMINATED BY ","\n    (k2,k3)\n    COLUMNS FROM PATH AS (data_time)\n    SET (\n        data_time=str_to_date(data_time, \'%Y-%m-%d %H%%3A%i%%3A%s\')\n    )\n)\nwith HDFS\n(\n  "fs.defaultFS"="hdfs://host:port",\n  "hadoop.username" = "user"\n);\n'})}),"\n",(0,t.jsx)(n.admonition,{title:"Tip",type:"tip",children:(0,t.jsx)(n.p,{children:'The time contains "%3A". In HDFS paths, colons ":" are not allowed, so all colons are replaced with "%3A".'})}),"\n",(0,t.jsx)(n.p,{children:"There are the following files under the path:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-Plain",children:"/user/data/data_time=2020-02-17 00%3A00%3A00/test.txt\n/user/data/data_time=2020-02-18 00%3A00%3A00/test.txt\n"})}),"\n",(0,t.jsx)(n.p,{children:"The table structure is as follows:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:'CREATE TABLE IF NOT EXISTS tbl12 (\n    data_time DATETIME,\n    k2        INT,\n    k3        INT\n) DISTRIBUTED BY HASH(data_time) BUCKETS 10\nPROPERTIES (\n    "replication_num" = "3"\n);\n'})}),"\n",(0,t.jsx)(n.h3,{id:"use-merge-mode-for-import",children:"Use Merge mode for import"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:'LOAD LABEL example_db.label8\n(\n    MERGE DATA INFILE("hdfs://host:port/input/file")\n    INTO TABLE `my_table`\n    (k1, k2, k3, v2, v1)\n    DELETE ON v2 > 100\n)\nwith HDFS\n(\n  "fs.defaultFS"="hdfs://host:port",\n  "hadoop.username"="user"\n)\nPROPERTIES\n(\n    "timeout" = "3600",\n    "max_filter_ratio" = "0.1"\n);\n'})}),"\n",(0,t.jsx)(n.p,{children:'To use Merge mode for import, the "my_table" must be a Unique Key table. When the value of the "v2" column in the imported data is greater than 100, that row will be considered a deletion row. The timeout for the import task is 3600 seconds, and an error rate of up to 10% is allowed.'}),"\n",(0,t.jsx)(n.h3,{id:"specify-the-source_sequence-column-during-import-to-ensure-the-order-of-replacements",children:'Specify the "source_sequence" column during import to ensure the order of replacements.'}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:'LOAD LABEL example_db.label9\n(\n    DATA INFILE("hdfs://host:port/input/file")\n    INTO TABLE `my_table`\n    COLUMNS TERMINATED BY ","\n    (k1,k2,source_sequence,v1,v2)\n    ORDER BY source_sequence\n) \nwith HDFS\n(\n  "fs.defaultFS"="hdfs://host:port",\n  "hadoop.username"="user"\n);\nThe "my_table" must be a Unique Key model table and have a specified Sequence column. The data will maintain its order based on the values in the "source_sequence" column in the source data.\n'})}),"\n",(0,t.jsxs)(n.h3,{id:"import-the-specified-file-format-as-json-and-specify-the-json_root-and-jsonpaths-accordingly",children:["Import the specified file format as ",(0,t.jsx)(n.code,{children:"json"}),", and specify the ",(0,t.jsx)(n.code,{children:"json_root"})," and jsonpaths accordingly."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:'LOAD LABEL example_db.label10\n(\n    DATA INFILE("hdfs://host:port/input/file.json")\n    INTO TABLE `my_table`\n    FORMAT AS "json"\n    PROPERTIES(\n      "json_root" = "$.item",\n      "jsonpaths" = "[\\"$.id\\", \\"$.city\\", \\"$.code\\"]"\n    )       \n)\nwith HDFS\n(\n  "fs.defaultFS"="hdfs://host:port",\n  "hadoop.username"="user"\n);\n'})}),"\n",(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.code,{children:"jsonpaths"})," can also be used in conjunction with the column list and ",(0,t.jsx)(n.code,{children:"SET (column_mapping)"})," :"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:'LOAD LABEL example_db.label10\n(\n    DATA INFILE("hdfs://host:port/input/file.json")\n    INTO TABLE `my_table`\n    FORMAT AS "json"\n    (id, code, city)\n    SET (id = id * 10)\n    PROPERTIES(\n      "json_root" = "$.item",\n      "jsonpaths" = "[\\"$.id\\", \\"$.city\\", \\"$.code\\"]"\n    )       \n)\nwith HDFS\n(\n  "fs.defaultFS"="hdfs://host:port",\n  "hadoop.username"="user"\n);\n'})}),"\n",(0,t.jsx)(n.admonition,{title:"Note",type:"info",children:(0,t.jsxs)(n.p,{children:["If you need to load the JSON object at the root node of a JSON file, the jsonpaths should be specified as ",(0,t.jsxs)(n.span,{className:"katex",children:[(0,t.jsx)(n.span,{className:"katex-mathml",children:(0,t.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,t.jsxs)(n.semantics,{children:[(0,t.jsxs)(n.mrow,{children:[(0,t.jsx)(n.mi,{mathvariant:"normal",children:"."}),(0,t.jsx)(n.mo,{separator:"true",children:","}),(0,t.jsx)(n.mi,{children:"e"}),(0,t.jsx)(n.mi,{mathvariant:"normal",children:"."}),(0,t.jsx)(n.mi,{children:"g"}),(0,t.jsx)(n.mi,{mathvariant:"normal",children:"."}),(0,t.jsx)(n.mo,{separator:"true",children:","}),(0,t.jsx)(n.mi,{mathvariant:"normal",children:"\u2018"}),(0,t.jsx)(n.mi,{children:"P"}),(0,t.jsx)(n.mi,{children:"R"}),(0,t.jsx)(n.mi,{children:"O"}),(0,t.jsx)(n.mi,{children:"P"}),(0,t.jsx)(n.mi,{children:"E"}),(0,t.jsx)(n.mi,{children:"R"}),(0,t.jsx)(n.mi,{children:"T"}),(0,t.jsx)(n.mi,{children:"I"}),(0,t.jsx)(n.mi,{children:"E"}),(0,t.jsx)(n.mi,{children:"S"}),(0,t.jsx)(n.mo,{stretchy:"false",children:"("}),(0,t.jsx)(n.mi,{mathvariant:"normal",children:'"'}),(0,t.jsx)(n.mi,{children:"j"}),(0,t.jsx)(n.mi,{children:"s"}),(0,t.jsx)(n.mi,{children:"o"}),(0,t.jsx)(n.mi,{children:"n"}),(0,t.jsx)(n.mi,{children:"p"}),(0,t.jsx)(n.mi,{children:"a"}),(0,t.jsx)(n.mi,{children:"t"}),(0,t.jsx)(n.mi,{children:"h"}),(0,t.jsx)(n.mi,{children:"s"}),(0,t.jsx)(n.mi,{mathvariant:"normal",children:'"'}),(0,t.jsx)(n.mo,{children:"="}),(0,t.jsx)(n.mi,{mathvariant:"normal",children:'"'})]}),(0,t.jsx)(n.annotation,{encoding:"application/x-tex",children:'., e.g., `PROPERTIES("jsonpaths"="'})]})})}),(0,t.jsxs)(n.span,{className:"katex-html","aria-hidden":"true",children:[(0,t.jsxs)(n.span,{className:"base",children:[(0,t.jsx)(n.span,{className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,t.jsx)(n.span,{className:"mord",children:"."}),(0,t.jsx)(n.span,{className:"mpunct",children:","}),(0,t.jsx)(n.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,t.jsx)(n.span,{className:"mord mathnormal",children:"e"}),(0,t.jsx)(n.span,{className:"mord",children:"."}),(0,t.jsx)(n.span,{className:"mord mathnormal",style:{marginRight:"0.03588em"},children:"g"}),(0,t.jsx)(n.span,{className:"mord",children:"."}),(0,t.jsx)(n.span,{className:"mpunct",children:","}),(0,t.jsx)(n.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,t.jsx)(n.span,{className:"mord",children:"\u2018"}),(0,t.jsx)(n.span,{className:"mord mathnormal",style:{marginRight:"0.13889em"},children:"PROPERT"}),(0,t.jsx)(n.span,{className:"mord mathnormal",style:{marginRight:"0.07847em"},children:"I"}),(0,t.jsx)(n.span,{className:"mord mathnormal",style:{marginRight:"0.05764em"},children:"ES"}),(0,t.jsx)(n.span,{className:"mopen",children:"("}),(0,t.jsx)(n.span,{className:"mord",children:'"'}),(0,t.jsx)(n.span,{className:"mord mathnormal",style:{marginRight:"0.05724em"},children:"j"}),(0,t.jsx)(n.span,{className:"mord mathnormal",children:"so"}),(0,t.jsx)(n.span,{className:"mord mathnormal",children:"n"}),(0,t.jsx)(n.span,{className:"mord mathnormal",children:"p"}),(0,t.jsx)(n.span,{className:"mord mathnormal",children:"a"}),(0,t.jsx)(n.span,{className:"mord mathnormal",children:"t"}),(0,t.jsx)(n.span,{className:"mord mathnormal",children:"h"}),(0,t.jsx)(n.span,{className:"mord mathnormal",children:"s"}),(0,t.jsx)(n.span,{className:"mord",children:'"'}),(0,t.jsx)(n.span,{className:"mspace",style:{marginRight:"0.2778em"}}),(0,t.jsx)(n.span,{className:"mrel",children:"="}),(0,t.jsx)(n.span,{className:"mspace",style:{marginRight:"0.2778em"}})]}),(0,t.jsxs)(n.span,{className:"base",children:[(0,t.jsx)(n.span,{className:"strut",style:{height:"0.6944em"}}),(0,t.jsx)(n.span,{className:"mord",children:'"'})]})]})]}),'.")`"']})}),"\n",(0,t.jsx)(n.h3,{id:"load-from-other-brokers",children:"Load from other brokers"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Alibaba Cloud OSS"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:'(\n    "fs.oss.accessKeyId" = "",\n    "fs.oss.accessKeySecret" = "",\n    "fs.oss.endpoint" = ""\n)\n'})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"JuiceFS"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:'(\n    "fs.defaultFS" = "jfs://xxx/",\n    "fs.jfs.impl" = "io.juicefs.JuiceFileSystem",\n    "fs.AbstractFileSystem.jfs.impl" = "io.juicefs.JuiceFS",\n    "juicefs.meta" = "xxx",\n    "juicefs.access-log" = "xxx"\n)\n'})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"GCS"}),"\n",(0,t.jsxs)(n.p,{children:["When using a Broker to access GCS, the Project ID is required, while other parameters are optional. Please refer to the ",(0,t.jsx)(n.a,{href:"https://github.com/GoogleCloudDataproc/hadoop-connectors/blob/branch-2.2.x/gcs/CONFIGURATION.md",children:"GCS Config"})," for all parameter configurations."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:'(\n    "fs.gs.project.id" = "Your Project ID",\n    "fs.AbstractFileSystem.gs.impl" = "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS",\n    "fs.gs.impl" = "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem",\n)\n'})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"more-help",children:"More Help"}),"\n",(0,t.jsxs)(n.p,{children:["For more detailed syntax and best practices for using  ",(0,t.jsx)(n.a,{href:"../../../sql-manual/sql-statements/data-modification/load-and-export/BROKER-LOAD",children:"Broker Load"})," , please refer to the Broker Load command manual. You can also enter HELP BROKER LOAD in the MySQL client command line to obtain more help information."]})]})}function h(e={}){let{wrapper:n}={...(0,i.a)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},172029:function(e,n,s){s.d(n,{Z:function(){return r}});let r=s.p+"assets/images/broker-load-ad8c8c6f730de2066090371965f1ffd5.png"},250065:function(e,n,s){s.d(n,{Z:function(){return a},a:function(){return o}});var r=s(667294);let t={},i=r.createContext(t);function o(e){let n=r.useContext(i);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),r.createElement(i.Provider,{value:n},e.children)}}}]);