"use strict";(self.webpackChunkdoris_website=self.webpackChunkdoris_website||[]).push([["105061"],{898425:function(e,n,s){s.r(n),s.d(n,{default:()=>h,frontMatter:()=>o,metadata:()=>r,assets:()=>d,toc:()=>l,contentTitle:()=>i});var r=JSON.parse('{"id":"sql-manual/sql-statements/cluster-management/compute-management/CREATE-RESOURCE","title":"CREATE RESOURCE","description":"Description","source":"@site/versioned_docs/version-3.x/sql-manual/sql-statements/cluster-management/compute-management/CREATE-RESOURCE.md","sourceDirName":"sql-manual/sql-statements/cluster-management/compute-management","slug":"/sql-manual/sql-statements/cluster-management/compute-management/CREATE-RESOURCE","permalink":"/docs/3.x/sql-manual/sql-statements/cluster-management/compute-management/CREATE-RESOURCE","draft":false,"unlisted":false,"tags":[],"version":"3.x","frontMatter":{"title":"CREATE RESOURCE","language":"en"},"sidebar":"docs","previous":{"title":"SHOW BROKER","permalink":"/docs/3.x/sql-manual/sql-statements/cluster-management/instance-management/SHOW-BROKER"},"next":{"title":"ALTER RESOURCE","permalink":"/docs/3.x/sql-manual/sql-statements/cluster-management/compute-management/ALTER-RESOURCE"}}'),a=s("785893"),t=s("250065");let o={title:"CREATE RESOURCE",language:"en"},i=void 0,d={},l=[{value:"Description",id:"description",level:2},{value:"Syntax",id:"syntax",level:2},{value:"Parameters",id:"parameters",level:2},{value:"Examples",id:"examples",level:2}];function c(e){let n={a:"a",code:"code",h2:"h2",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.a)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.h2,{id:"description",children:"Description"}),"\n",(0,a.jsx)(n.p,{children:"This statement is used to create a resource. Only the root or admin user can create resources. Currently supports Spark, ODBC, S3 external resources.\nIn the future, other external resources may be added to Doris for use, such as Spark/GPU for query, HDFS/S3 for external storage, MapReduce for ETL, etc."}),"\n",(0,a.jsx)(n.h2,{id:"syntax",children:"Syntax"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-sql",children:'CREATE [EXTERNAL] RESOURCE "<resource_name>"\nPROPERTIES (\n   `<property>`\n    [ , ... ]\n);\n'})}),"\n",(0,a.jsx)(n.h2,{id:"parameters",children:"Parameters"}),"\n",(0,a.jsxs)(n.p,{children:["1.",(0,a.jsx)(n.code,{children:"<type>"})]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.code,{children:"<property>"})," format as ",(0,a.jsx)(n.code,{children:"<key>"})," = ",(0,a.jsx)(n.code,{children:"<value>"}),", and the specific available values for ",(0,a.jsx)(n.code,{children:"<key>"})," are as follows:"]}),"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",(0,a.jsxs)(n.table,{children:[(0,a.jsx)(n.thead,{children:(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.th,{children:"Parameter"}),(0,a.jsx)(n.th,{children:"Description"}),(0,a.jsx)(n.th,{children:"Required"})]})}),(0,a.jsx)(n.tbody,{children:(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.code,{children:"<type>"})}),(0,a.jsx)(n.td,{children:"Specify the type of resource, supported: spark/odbc_catalog/s3/jdbc/hdfs/hms/es."}),(0,a.jsx)(n.td,{children:"Y"})]})})]}),"\n",(0,a.jsxs)(n.p,{children:["Depending on the ",(0,a.jsx)(n.code,{children:"<type>"}),", the parameters of PROPERTIES vary. Please refer to the example for details."]}),"\n",(0,a.jsx)(n.h2,{id:"examples",children:"Examples"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"1. Create a Spark resource named spark0 in yarn cluster mode."})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-sql",children:'CREATE EXTERNAL RESOURCE "spark0"\nPROPERTIES\n(\n  "type" = "spark",\n  "spark.master" = "yarn",\n  "spark.submit.deployMode" = "cluster",\n  "spark.jars" = "xxx.jar,yyy.jar",\n  "spark.files" = "/tmp/aaa,/tmp/bbb",\n  "spark.executor.memory" = "1g",\n  "spark.yarn.queue" = "queue0",\n  "spark.hadoop.yarn.resourcemanager.address" = "127.0.0.1:9999",\n  "spark.hadoop.fs.defaultFS" = "hdfs://127.0.0.1:10000",\n  "working_dir" = "hdfs://127.0.0.1:10000/tmp/doris",\n  "broker" = "broker0",\n  "broker.username" = "user0",\n  "broker.password" = "password0"\n);\n'})}),"\n",(0,a.jsx)(n.p,{children:"Spark related parameters are as follows:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"spark.master: Required, currently supports yarn, spark://host:port."}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"spark.submit.deployMode: The deployment mode of the Spark program, required, supports both cluster and client."}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"spark.hadoop.yarn.resourcemanager.address: Required when master is yarn."}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"spark.hadoop.fs.defaultFS: Required when master is yarn."}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["Other parameters are optional, refer to ",(0,a.jsx)(n.a,{href:"http://spark.apache.org/docs/latest/configuration.html",children:"here"})]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"Working_dir and broker need to be specified when Spark is used for ETL. described as follows:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"working_dir: The directory used by the ETL. Required when spark is used as an ETL resource. For example: hdfs://host:port/tmp/doris."}),"\n",(0,a.jsxs)(n.li,{children:["broker: broker name. Required when spark is used as an ETL resource. Configuration needs to be done in advance using the ",(0,a.jsx)(n.code,{children:"ALTER SYSTEM ADD BROKER"})," command."]}),"\n",(0,a.jsx)(n.li,{children:"broker.property_key: The authentication information that the broker needs to specify when reading the intermediate file generated by ETL."}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"2. Create an ODBC resource"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-sql",children:'CREATE EXTERNAL RESOURCE `oracle_odbc`\nPROPERTIES (\n"type" = "odbc_catalog",\n"host" = "192.168.0.1",\n"port" = "8086",\n"user" = "test",\n"password" = "test",\n"database" = "test",\n"odbc_type" = "oracle",\n"driver" = "Oracle 19 ODBC driver"\n);\n'})}),"\n",(0,a.jsx)(n.p,{children:"The relevant parameters of ODBC are as follows:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"hosts: IP address of the external database"}),"\n",(0,a.jsx)(n.li,{children:"driver: The driver name of the ODBC appearance, which must be the same as the Driver name in be/conf/odbcinst.ini."}),"\n",(0,a.jsx)(n.li,{children:"odbc_type: the type of the external database, currently supports oracle, mysql, postgresql"}),"\n",(0,a.jsx)(n.li,{children:"user: username of the foreign database"}),"\n",(0,a.jsx)(n.li,{children:"password: the password information of the corresponding user"}),"\n",(0,a.jsx)(n.li,{children:"charset: connection charset"}),"\n",(0,a.jsx)(n.li,{children:"There is also support for implementing custom parameters per ODBC Driver, see the description of the corresponding ODBC Driver"}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"3. Create S3 resource"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-sql",children:'CREATE RESOURCE "remote_s3"\nPROPERTIES\n(\n   "type" = "s3",\n   "s3.endpoint" = "bj.s3.com",\n   "s3.region" = "bj",\n   "s3.access_key" = "bbb",\n   "s3.secret_key" = "aaaa",\n   -- the followings are optional\n   "s3.connection.maximum" = "50",\n   "s3.connection.request.timeout" = "3000",\n   "s3.connection.timeout" = "1000"\n);\n'})}),"\n",(0,a.jsxs)(n.p,{children:["If S3 resource is used for ",(0,a.jsx)(n.a,{href:"../../../../table-design/tiered-storage/overview",children:"cold hot separation"}),", we should add more required fields."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-sql",children:'CREATE RESOURCE "remote_s3"\nPROPERTIES\n(\n   "type" = "s3",\n   "s3.endpoint" = "bj.s3.com",\n   "s3.region" = "bj",\n   "s3.access_key" = "bbb",\n   "s3.secret_key" = "aaaa",\n   -- required by cooldown\n   "s3.root.path" = "/path/to/root",\n   "s3.bucket" = "test-bucket"\n);\n'})}),"\n",(0,a.jsx)(n.p,{children:"S3 related parameters are as follows:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Required parameters\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"s3.endpoint"}),": s3 endpoint"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"s3.region"}),":s3 region"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"s3.root.path"}),": s3 root directory"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"s3.access_key"}),": s3 access key"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"s3.secret_key"}),": s3 secret key"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"s3.bucket"}),"\uFF1As3 bucket"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["optional parameter\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"s3.connection.maximum"}),": the maximum number of s3 connections, the default is 50"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"s3.connection.request.timeout"}),": s3 request timeout, in milliseconds, the default is 3000"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"s3.connection.timeout"}),": s3 connection timeout, in milliseconds, the default is 1000"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"Doris also supported `AWS Assume Role` for creating S3 Resource , please refer to [AWS intergration](../../../../admin-manual/auth/integrations/aws-authentication-and-authorization.md#assumed-role-authentication).\n"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"4. Create JDBC resource"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-sql",children:'CREATE RESOURCE mysql_resource PROPERTIES (\n   "type"="jdbc",\n   "user"="root",\n   "password"="123456",\n   "jdbc_url" = "jdbc:mysql://127.0.0.1:3316/doris_test?useSSL=false",\n   "driver_url" = "https://doris-community-test-1308700295.cos.ap-hongkong.myqcloud.com/jdbc_driver/mysql-connector-java-8.0.25.jar",\n"driver_class" = "com.mysql.cj.jdbc.Driver"\n);\n'})}),"\n",(0,a.jsx)(n.p,{children:"JDBC related parameters are as follows:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"user\uFF1AThe username used to connect to the database"}),"\n",(0,a.jsx)(n.li,{children:"password\uFF1AThe password used to connect to the database"}),"\n",(0,a.jsx)(n.li,{children:"jdbc_url: The identifier used to connect to the specified database"}),"\n",(0,a.jsx)(n.li,{children:"driver_url: The url of JDBC driver package"}),"\n",(0,a.jsx)(n.li,{children:"driver_class: The class of JDBC driver"}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"5. Create HDFS resource"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-sql",children:'CREATE RESOURCE hdfs_resource PROPERTIES (\n   "type"="hdfs",\n   "hadoop.username"="user",\n   "dfs.nameservices" = "my_ha",\n   "dfs.ha.namenodes.my_ha" = "my_namenode1, my_namenode2",\n   "dfs.namenode.rpc-address.my_ha.my_namenode1" = "nn1_host:rpc_port",\n   "dfs.namenode.rpc-address.my_ha.my_namenode2" = "nn2_host:rpc_port",\n   "dfs.client.failover.proxy.provider.my_ha" = "org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider"\n);\n'})}),"\n",(0,a.jsx)(n.p,{children:"HDFS related parameters are as follows:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"fs.defaultFS: namenode address and port"}),"\n",(0,a.jsx)(n.li,{children:"hadoop.username: hdfs username"}),"\n",(0,a.jsx)(n.li,{children:"dfs.nameservices: if hadoop enable HA, please set fs nameservice. See hdfs-site.xml"}),"\n",(0,a.jsx)(n.li,{children:"dfs.ha.namenodes.[nameservice ID]\uFF1Aunique identifiers for each NameNode in the nameservice. See hdfs-site.xml"}),"\n",(0,a.jsx)(n.li,{children:"dfs.namenode.rpc-address.[nameservice ID].[name node ID]`\uFF1Athe fully-qualified RPC address for each NameNode to listen on. See hdfs-site.xml"}),"\n",(0,a.jsx)(n.li,{children:"dfs.client.failover.proxy.provider.[nameservice ID]\uFF1Athe Java class that HDFS clients use to contact the Active NameNode, usually it is org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider"}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"6. Create HMS resource"})}),"\n",(0,a.jsxs)(n.p,{children:["HMS resource is used to create ",(0,a.jsx)(n.a,{href:"../../../../lakehouse/datalake-analytics/hive",children:"hms catalog"})]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-sql",children:"CREATE RESOURCE hms_resource PROPERTIES (\n   'type'='hms',\n   'hive.metastore.uris' = 'thrift://127.0.0.1:7004',\n   'dfs.nameservices'='HANN',\n   'dfs.ha.namenodes.HANN'='nn1,nn2',\n   'dfs.namenode.rpc-address.HANN.nn1'='nn1_host:rpc_port',\n   'dfs.namenode.rpc-address.HANN.nn2'='nn2_host:rpc_port',\n   'dfs.client.failover.proxy.provider.HANN'='org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider'\n);\n"})}),"\n",(0,a.jsx)(n.p,{children:"HMS related parameters are as follows:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"hive.metastore.uris: hive metastore server address\nOptional:"}),"\n",(0,a.jsx)(n.li,{children:"dfs.*: If hive data is on hdfs, HDFS resource parameters should be added, or copy hive-site.xml into fe/conf."}),"\n",(0,a.jsxs)(n.li,{children:["s3.*: If hive data is on s3, S3 resource parameters should be added. If using ",(0,a.jsx)(n.a,{href:"https://www.aliyun.com/product/bigdata/dlf",children:"Aliyun Data Lake Formation"}),", copy hive-site.xml into fe/conf."]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"7. Create ES resource"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-sql",children:'CREATE RESOURCE es_resource PROPERTIES (\n   "type"="es",\n   "hosts"="http://127.0.0.1:29200",\n   "nodes_discovery"="false",\n   "enable_keyword_sniff"="true"\n);\n'})}),"\n",(0,a.jsx)(n.p,{children:"ES related parameters are as follows:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"hosts: ES Connection Address, maybe one or more node, load-balance is also accepted"}),"\n",(0,a.jsx)(n.li,{children:"user: username for ES"}),"\n",(0,a.jsx)(n.li,{children:"password: password for the user"}),"\n",(0,a.jsx)(n.li,{children:"enable_docvalue_scan: whether to enable ES/Lucene column storage to get the value of the query field, the default is true"}),"\n",(0,a.jsx)(n.li,{children:"enable_keyword_sniff: Whether to probe the string segmentation type text.fields in ES, query by keyword (the default is true, false matches the content after the segmentation)"}),"\n",(0,a.jsx)(n.li,{children:"nodes_discovery: Whether or not to enable ES node discovery, the default is true. In network isolation, set this parameter to false. Only the specified node is connected"}),"\n",(0,a.jsx)(n.li,{children:"http_ssl_enabled: Whether ES cluster enables https access mode, the current FE/BE implementation is to trust all"}),"\n"]})]})}function h(e={}){let{wrapper:n}={...(0,t.a)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},250065:function(e,n,s){s.d(n,{Z:function(){return i},a:function(){return o}});var r=s(667294);let a={},t=r.createContext(a);function o(e){let n=r.useContext(t);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),r.createElement(t.Provider,{value:n},e.children)}}}]);