"use strict";(self.webpackChunkdoris_website=self.webpackChunkdoris_website||[]).push([["358079"],{100196:function(e,t,n){n.r(t),n.d(t,{default:()=>d,frontMatter:()=>i,metadata:()=>a,assets:()=>c,toc:()=>l,contentTitle:()=>r});var a=JSON.parse('{"id":"lakehouse/best-practices/doris-maxcompute","title":"From MaxCompute to Doris","description":"This document explains how to quickly import data from Alibaba Cloud MaxCompute into Apache Doris using the MaxCompute Catalog.","source":"@site/docs/lakehouse/best-practices/doris-maxcompute.md","sourceDirName":"lakehouse/best-practices","slug":"/lakehouse/best-practices/doris-maxcompute","permalink":"/docs/dev/lakehouse/best-practices/doris-maxcompute","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"title":"From MaxCompute to Doris","language":"en"},"sidebar":"docs","previous":{"title":"Integration with Aliyun DLF Rest Catalog","permalink":"/docs/dev/lakehouse/best-practices/doris-dlf-paimon"},"next":{"title":"Generating TPC-H on Hive/Iceberg","permalink":"/docs/dev/lakehouse/best-practices/tpch"}}'),s=n("785893"),o=n("250065");let i={title:"From MaxCompute to Doris",language:"en"},r=void 0,c={},l=[{value:"Environment Preparation",id:"environment-preparation",level:2},{value:"01 Enable MaxCompute Open Storage API",id:"01-enable-maxcompute-open-storage-api",level:3},{value:"02 Enable MaxCompute Permissions",id:"02-enable-maxcompute-permissions",level:3},{value:"03 Confirm Doris and MaxCompute Network Environment",id:"03-confirm-doris-and-maxcompute-network-environment",level:3},{value:"Import MaxCompute Data",id:"import-maxcompute-data",level:2},{value:"01 Create Catalog",id:"01-create-catalog",level:3},{value:"02 Import TPCH Dataset",id:"02-import-tpch-dataset",level:3},{value:"03 Import Github Event Dataset",id:"03-import-github-event-dataset",level:3}];function p(e){let t={a:"a",code:"code",h2:"h2",h3:"h3",p:"p",pre:"pre",...(0,o.a)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsxs)(t.p,{children:["This document explains how to quickly import data from Alibaba Cloud MaxCompute into Apache Doris using the ",(0,s.jsx)(t.a,{href:"/docs/dev/lakehouse/catalogs/maxcompute-catalog",children:"MaxCompute Catalog"}),"."]}),"\n",(0,s.jsx)(t.p,{children:"This document is based on Apache Doris version 2.1.9."}),"\n",(0,s.jsx)(t.h2,{id:"environment-preparation",children:"Environment Preparation"}),"\n",(0,s.jsx)(t.h3,{id:"01-enable-maxcompute-open-storage-api",children:"01 Enable MaxCompute Open Storage API"}),"\n",(0,s.jsxs)(t.p,{children:["In the left navigation bar of the ",(0,s.jsx)(t.a,{href:"https://maxcompute.console.aliyun.com/",children:"MaxCompute Console"})," -> ",(0,s.jsx)(t.code,{children:"Tenant Management"})," -> ",(0,s.jsx)(t.code,{children:"Tenant Properties"})," -> Turn on the ",(0,s.jsx)(t.code,{children:"Open Storage (Storage API) switch"}),"."]}),"\n",(0,s.jsx)(t.h3,{id:"02-enable-maxcompute-permissions",children:"02 Enable MaxCompute Permissions"}),"\n",(0,s.jsx)(t.p,{children:"Doris uses AK/SK to access MaxCompute services. Please ensure that the IAM user corresponding to the AK/SK has the following roles or permissions for the corresponding MaxCompute services:"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-json",children:'{\n    "Statement": [{\n            "Action": ["odps:List",\n                "odps:Usage"],\n            "Effect": "Allow",\n            "Resource": ["acs:odps:*:regions/*/quotas/pay-as-you-go"]}],\n    "Version": "1"\n}\n'})}),"\n",(0,s.jsx)(t.h3,{id:"03-confirm-doris-and-maxcompute-network-environment",children:"03 Confirm Doris and MaxCompute Network Environment"}),"\n",(0,s.jsx)(t.p,{children:"It is strongly recommended that the Doris cluster and MaxCompute service are in the same VPC and ensure that the correct security group is set."}),"\n",(0,s.jsx)(t.p,{children:"The examples in this document are tested in the same VPC network environment."}),"\n",(0,s.jsx)(t.h2,{id:"import-maxcompute-data",children:"Import MaxCompute Data"}),"\n",(0,s.jsx)(t.h3,{id:"01-create-catalog",children:"01 Create Catalog"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-sql",children:'CREATE CATALOG mc PROPERTIES (\n  "type" = "max_compute",\n  "mc.default.project" = "xxx",\n  "mc.access_key" = "AKxxxxx",\n  "mc.secret_key" = "SKxxxxx",\n  "mc.endpoint" = "xxxxx"\n);\n'})}),"\n",(0,s.jsx)(t.p,{children:"Support Schema Level (3.1.3+):"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-sql",children:'CREATE CATALOG mc PROPERTIES (\n  "type" = "max_compute",\n  "mc.default.project" = "xxx",\n  "mc.access_key" = "AKxxxxx",\n  "mc.secret_key" = "SKxxxxx",\n  "mc.endpoint" = "xxxxx",\n  \'mc.enable.namespace.schema\' = \'true\'\n);\n'})}),"\n",(0,s.jsxs)(t.p,{children:["Please refer to the ",(0,s.jsx)(t.a,{href:"/docs/dev/lakehouse/catalogs/maxcompute-catalog",children:"MaxCompute Catalog"})," documentation for details."]}),"\n",(0,s.jsx)(t.h3,{id:"02-import-tpch-dataset",children:"02 Import TPCH Dataset"}),"\n",(0,s.jsxs)(t.p,{children:["We use the TPCH 100 dataset from the public datasets in MaxCompute as an example (data has already been imported into MaxCompute), and use the ",(0,s.jsx)(t.code,{children:"CREATE TABLE AS SELECT"})," statement to import MaxCompute data into Doris."]}),"\n",(0,s.jsxs)(t.p,{children:["This dataset contains 7 tables. The largest table, ",(0,s.jsx)(t.code,{children:"lineitem"}),", has 16 columns and 600,037,902 rows. It occupies about 30GB of disk space."]}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-sql",children:"-- switch catalog\nSWITCH internal;\n-- create database\nCREATE DATABASE tpch_100g;\n-- ingest data\nCREATE TABLE tpch_100g.lineitem AS SELECT * FROM mc.selectdb_test.lineitem;\nCREATE TABLE tpch_100g.nation AS SELECT * FROM mc.selectdb_test.nation;\nCREATE TABLE tpch_100g.orders AS SELECT * FROM mc.selectdb_test.orders;\nCREATE TABLE tpch_100g.part AS SELECT * FROM mc.selectdb_test.part;\nCREATE TABLE tpch_100g.partsupp AS SELECT * FROM mc.selectdb_test.partsupp;\nCREATE TABLE tpch_100g.region AS SELECT * FROM mc.selectdb_test.region;\nCREATE TABLE tpch_100g.supplier AS SELECT * FROM mc.selectdb_test.supplier;\n"})}),"\n",(0,s.jsx)(t.p,{children:"In a Doris cluster with a single BE of 16C 64G specification, the above operations take about 6-7 minutes to execute serially."}),"\n",(0,s.jsx)(t.h3,{id:"03-import-github-event-dataset",children:"03 Import Github Event Dataset"}),"\n",(0,s.jsxs)(t.p,{children:["We use the Github Event dataset from the public datasets in MaxCompute as an example (data has already been imported into MaxCompute), and use the ",(0,s.jsx)(t.code,{children:"CREATE TABLE AS SELECT"})," statement to import MaxCompute data into Doris."]}),"\n",(0,s.jsxs)(t.p,{children:["Here we select data from the ",(0,s.jsx)(t.code,{children:"dwd_github_events_odps"})," table for the 365 partitions from '2015-01-01' to '2016-01-01'. The data has 32 columns and 212,786,803 rows. It occupies about 10GB of disk space."]}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-sql",children:"-- switch catalog\nSWITCH internal;\n-- create database\nCREATE DATABASE github_events;\n-- ingest data\nCREATE TABLE github_events.dwd_github_events_odps\nAS SELECT * FROM mc.github_events.dwd_github_events_odps\nWHERE ds BETWEEN '2015-01-01' AND '2016-01-01';\n"})}),"\n",(0,s.jsx)(t.p,{children:"In a Doris cluster with a single BE of 16C 64G specification, the above operation takes about 2 minutes."})]})}function d(e={}){let{wrapper:t}={...(0,o.a)(),...e.components};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(p,{...e})}):p(e)}},250065:function(e,t,n){n.d(t,{Z:function(){return r},a:function(){return i}});var a=n(667294);let s={},o=a.createContext(s);function i(e){let t=a.useContext(o);return a.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),a.createElement(o.Provider,{value:t},e.children)}}}]);