"use strict";(self.webpackChunkdoris_website=self.webpackChunkdoris_website||[]).push([["846881"],{184784:function(e,n,t){t.r(n),t.d(n,{default:()=>h,frontMatter:()=>a,metadata:()=>r,assets:()=>l,toc:()=>c,contentTitle:()=>d});var r=JSON.parse('{"id":"ecosystem/spark-load","title":"Spark Load","description":"Spark Load uses external Spark resources to pre-process the imported data, improve the import performance of Doris large","source":"@site/docs/ecosystem/spark-load.md","sourceDirName":"ecosystem","slug":"/ecosystem/spark-load","permalink":"/docs/dev/ecosystem/spark-load","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"title":"Spark Load","language":"en"},"sidebar":"docs","previous":{"title":"Hive HLL UDF","permalink":"/docs/dev/ecosystem/hive-hll-udf"},"next":{"title":"Install Error","permalink":"/docs/dev/faq/install-faq"}}'),s=t("785893"),i=t("250065");let a={title:"Spark Load",language:"en"},d="Spark Load",l={},c=[{value:"Applicable scenarios",id:"applicable-scenarios",level:2},{value:"Basic Principles",id:"basic-principles",level:2},{value:"Basic Process",id:"basic-process",level:3},{value:"Global dictionary",id:"global-dictionary",level:3},{value:"Applicable scenarios",id:"applicable-scenarios-1",level:4},{value:"Construction process",id:"construction-process",level:4},{value:"Hive Bitmap UDF",id:"hive-bitmap-udf",level:4},{value:"Quick start",id:"quick-start",level:3},{value:"Reference manual",id:"reference-manual",level:2},{value:"Spark Load client",id:"spark-load-client",level:3},{value:"Directory structure",id:"directory-structure",level:4},{value:"Startup script parameters",id:"startup-script-parameters",level:4},{value:"Cancel load",id:"cancel-load",level:3},{value:"Configuration parameters",id:"configuration-parameters",level:3},{value:"General configuration",id:"general-configuration",level:4},{value:"Task Configuration",id:"task-configuration",level:4},{value:"Spark parameter configuration",id:"spark-parameter-configuration",level:4},{value:"Hadoop parameter configuration",id:"hadoop-parameter-configuration",level:4},{value:"Environment parameter configuration",id:"environment-parameter-configuration",level:4},{value:"Load example",id:"load-example",level:2},{value:"Load Bitmap type data",id:"load-bitmap-type-data",level:3}];function o(e){let n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.a)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"spark-load",children:"Spark Load"})}),"\n",(0,s.jsx)(n.p,{children:"Spark Load uses external Spark resources to pre-process the imported data, improve the import performance of Doris large\ndata volumes and save the computing resources of the Doris\ncluster. It is mainly used for the initial migration and the scenario of importing large data volumes into Doris."}),"\n",(0,s.jsx)(n.p,{children:"Spark Load uses the resources of the Spark cluster to sort the data to be imported. Doris BE directly writes files,\nwhich can greatly reduce the resource usage of the Doris\ncluster. It has a good effect on reducing the resource usage and load of the Doris cluster for the migration of\nhistorical massive data."}),"\n",(0,s.jsx)(n.p,{children:"Users need to create and execute import tasks through the Spark Load client. The execution status of the task will be\noutput to the console, and the import results can also be viewed through SHOW LOAD."}),"\n",(0,s.jsx)(n.admonition,{title:"CAUTION",type:"caution",children:(0,s.jsxs)(n.p,{children:["This feature is experimental and is currently only available in the master branch.\nThe current version only supports storage computing coupled clusters.\nIf you encounter any problems during use, please provide feedback through the mailing group, ",(0,s.jsx)(n.a,{href:"https://github.com/apache/doris/issues",children:"GitHub Issue"}),", etc."]})}),"\n",(0,s.jsx)(n.h2,{id:"applicable-scenarios",children:"Applicable scenarios"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"The source data is in a storage system accessible to Spark, such as HDFS."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"The data volume is at the level of tens of GB to TB."}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.admonition,{title:"CAUTION",type:"caution",children:(0,s.jsx)(n.p,{children:"For unique key models, only tables in merge-on-read mode are currently supported."})}),"\n",(0,s.jsx)(n.h2,{id:"basic-principles",children:"Basic Principles"}),"\n",(0,s.jsx)(n.h3,{id:"basic-process",children:"Basic Process"}),"\n",(0,s.jsx)(n.p,{children:"The execution of the Spark Load task is mainly divided into the following 5 stages:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"The user writes a configuration file to configure the source file/table to be read, as well as the target table and\nother information"}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"The Spark Load client creates an import job to FE and starts a transaction, and FE returns the target table metadata\nto the client"}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"The Spark Load client submits the ETL task to the Spark cluster for execution."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"The Spark cluster executes ETL to complete the preprocessing of the imported data, including global dictionary\nconstruction (Bitmap type), partitioning, sorting, aggregation, etc."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"After the ETL task is completed, the Spark Load client synchronizes the preprocessed data path of each shard to FE,\nand schedules the relevant BE to execute the Push task."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"BE reads the data and converts it into the Doris underlying storage format."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"FE schedules the effective version to complete the import task."}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"global-dictionary",children:"Global dictionary"}),"\n",(0,s.jsx)(n.h4,{id:"applicable-scenarios-1",children:"Applicable scenarios"}),"\n",(0,s.jsx)(n.p,{children:"Currently, the Bitmap column in Doris is implemented using the class library Roaringbitmap, and the input data type of\nRoaringbitmap can only be integer. Therefore, if you want to implement pre-calculation of the Bitmap column in the\nimport process, you need to convert the input data type into an integer."}),"\n",(0,s.jsx)(n.p,{children:"In Doris's existing import process, the data structure of the global dictionary is based on the Hive table, which saves\nthe mapping from the original value to the encoded value."}),"\n",(0,s.jsx)(n.h4,{id:"construction-process",children:"Construction process"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Read the data from the upstream data source and generate a Hive temporary table, recorded as hive_table."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Extract the deduplication values of the fields to be deduplicated from hive_table and generate a new Hive table,\nrecorded as distinct_value_table."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Create a new global dictionary table, recorded as dict_table, with one column for the original value and one column\nfor the encoded value."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Do a Left Join of distinct_value_table and dict_table to calculate the newly added set of deduplicated values, and\nthen use the window function to encode this set. At this time, the original value of the deduplicated column has an\nadditional column of encoded values, and finally write the data of these two columns back to\ndict_table."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Join dict_table with hive_table to complete the work of replacing the original values \u200B\u200Bin hive_table with integer\nencoded values."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"hive_table will be read by the next step of data preprocessing and imported into Doris after calculation.\nData Preprocessing (DPP)\nBasic Process"}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Read data from the data source. The upstream data source can be an HDFS file or a Hive table."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Perform field mapping, expression calculation, and generate bucket field bucket_id based on partition information for\nthe read data."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Generate RollupTree based on the Rollup metadata of the Doris table."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Traverse the RollupTree and perform hierarchical aggregation operations. The Rollup of the next level can be\ncalculated from the Rollup of the previous level."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"After each aggregation calculation, the data will be bucketed according to bucket_id and written to HDFS."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"The subsequent Broker will pull the files in HDFS and import them into Doris Be."}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"hive-bitmap-udf",children:"Hive Bitmap UDF"}),"\n",(0,s.jsx)(n.p,{children:"Spark supports importing the Bitmap data generated by Hive directly into Doris. See hive-bitmap-udf document for details."}),"\n",(0,s.jsx)(n.h3,{id:"quick-start",children:"Quick start"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Read data from file"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Target table structure"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sql",children:'CREATE TABLE IF NOT EXISTS tbl_test_spark_load (\n                                                  c_int int(11) NULL,\n   c_char char(15) NULL,\n   c_varchar varchar(100) NULL,\n   c_bool boolean NULL,\n   c_tinyint tinyint(4) NULL,\n   c_smallint smallint(6) NULL,\n   c_bigint bigint(20) NULL,\n   c_largeint largeint(40) NULL,\n   c_float float NULL,\n   c_double double NULL,\n   c_decimal decimal(6, 3) NULL,\n   c_decimalv3 decimal(6, 3) NULL,\n   c_date date NULL,\n   c_datev2 date NULL,\n   c_datetime datetime NULL,\n   c_datetimev2 datetime NULL\n   )\nDISTRIBUTED BY HASH(c_int) BUCKETS 1\nPROPERTIES (\n  "replication_num" = "1"\n)\n'})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Write configuration files"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-json",children:'  {\n  "feAddresses": "127.0.0.1:8030",\n  "label": "spark-load-test-file",\n  "user": "root",\n  "password": "",\n  "database": "test",\n  "workingDir": "hdfs://hadoop:8020/spark-load",\n  "loadTasks": {\n    "tbl_test_spark_load": {\n      "type": "file",\n      "paths": [\n        "hdfs://hadoop:8020/data/data.txt"\n      ],\n      "format": "csv",\n      "fieldSep": ",",\n      "columns": "c_int,c_char,c_varchar,c_bool,c_tinyint,c_smallint,c_bigint,c_largeint,c_float,c_double,c_decimal,c_decimalv3,c_date,c_datev2,c_datetime,c_datetimev2"\n    }\n  },\n  "spark": {\n    "sparkHome": "/opt/spark",\n    "master": "yarn",\n    "deployMode": "cluster",\n    "properties": {\n      "spark.executor.memory": "2G",\n      "spark.executor.cores": 1,\n      "spark.num.executor": 4\n    }\n  },\n  "hadoopProperties": {\n    "fs.defaultFS": "hdfs://hadoop:8020",\n    "hadoop.username": "hadoop"\n  }\n} \n'})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Start Spark Load job"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"    $ cd spark-load-dir\n    $ sh./bin/spark-load.sh - c config.json\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"View job execution results"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sql",children:'mysql\n> show load;\n+-------+-----------------------+-----------+---------------+---------+---------+-----------------------------------------------------+----------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------------------------------------------------------------------------------------------+---------------+--------------+------+---------+\n| JobId | Label | State | Progress | Type | EtlInfo | TaskInfo | ErrorMsg | CreateTime | EtlStartTime | EtlFinishTime | LoadStartTime | LoadFinishTime | URL | JobDetails | TransactionId | ErrorTablets | User | Comment |--+---------------------+---------------------+--- ------------------+---------------------------------- -------------------------------------------------- -------------------------------------------------- ------+---------------+--------------+------+----- ----+ | 38104 | spark-load-test-hvie | FINISHED | 100.00% (0/0) | INGESTION | NULL | cluster:N/A; timeout(s):86400; max_filter_ratio:0.0 | NULL | 2024-08-16 14 :47:22 | 2024-08-16 14:47:22 | 2024-08-16 14:47:58 | 2024-08-16 14:47:58 | 2024-08-16 14:48:01 | app-1723790846300 | {"Unfinished backends":{"0-0":[]},"ScannedRows":0,"TaskNumber":1,"LoadBytes":0,"All backends":{"0-0":[-1 ]},"FileNumber":0,"FileSize":0} | 27024 | {} | root | |\n+-------+----------------- ------+----------+---------------+-----------+---- -----+-------------------------------------------- ---------+----------+---------------------+------- ------------+---------------------+------------- --------+---------------------+------------------- --+----------------------------------------------- -------------------------------------------------- ----------------------------------------+--------- ------+--------------+------+---------+\n'})}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"reference-manual",children:"Reference manual"}),"\n",(0,s.jsx)(n.h3,{id:"spark-load-client",children:"Spark Load client"}),"\n",(0,s.jsx)(n.h4,{id:"directory-structure",children:"Directory structure"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"\u251C\u2500\u2500 app\n\u2502 \u2514\u2500\u2500 spark-load-dpp-1.0-SNAPSHOT.jar\n\u251C\u2500\u2500 bin\n\u2502 \u2514\u2500\u2500 spark-load.sh\n\u2514\u2500\u2500 lib\n"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"app"}),": application package of spark dpp"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"bin"}),": spark load startup script"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"lib"}),": other dependent packages"]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"startup-script-parameters",children:"Startup script parameters"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"--config"}),"|",(0,s.jsx)(n.code,{children:"-c"}),": specify the configuration file address"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"--recovery"}),"|",(0,s.jsx)(n.code,{children:"-r"}),": whether to start in recovery mode"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"cancel-load",children:"Cancel load"}),"\n",(0,s.jsxs)(n.p,{children:["When the Spark Load job status is not ",(0,s.jsx)(n.code,{children:"CANCELLED"})," or ",(0,s.jsx)(n.code,{children:"FINISHED"}),", it can be manually canceled by the user."]}),"\n",(0,s.jsxs)(n.p,{children:["The user can cancel the load task by ending the process of the Spark Load startup script, or by executing the ",(0,s.jsx)(n.code,{children:"CANCEL LOAD"})," command in Doris."]}),"\n",(0,s.jsxs)(n.p,{children:["Through CANCEL When canceling LOAD, you need to specify the Label of the import task to be canceled. To view the cancel\nimport command syntax, execute ",(0,s.jsx)(n.code,{children:"HELP CANCEL LOAD"}),"."]}),"\n",(0,s.jsx)(n.h3,{id:"configuration-parameters",children:"Configuration parameters"}),"\n",(0,s.jsx)(n.h4,{id:"general-configuration",children:"General configuration"}),"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Name"}),(0,s.jsx)(n.th,{children:"Required"}),(0,s.jsx)(n.th,{children:"Default value"}),(0,s.jsx)(n.th,{children:"Parameter description"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"feAddresses"}),(0,s.jsx)(n.td,{children:"yes"}),(0,s.jsx)(n.td,{children:"-"}),(0,s.jsx)(n.td,{children:"Doris FE HTTP address, format: fe_ip:http_port, [fe_ip:http_port]"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"label"}),(0,s.jsx)(n.td,{children:"yes"}),(0,s.jsx)(n.td,{children:"-"}),(0,s.jsx)(n.td,{children:"Load job label"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"user"}),(0,s.jsx)(n.td,{children:"yes"}),(0,s.jsx)(n.td,{children:"-"}),(0,s.jsx)(n.td,{children:"Doris Username"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Password"}),(0,s.jsx)(n.td,{children:"Yes"}),(0,s.jsx)(n.td,{children:"-"}),(0,s.jsx)(n.td,{children:"Doris Password"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Database"}),(0,s.jsx)(n.td,{children:"Yes"}),(0,s.jsx)(n.td,{children:"-"}),(0,s.jsx)(n.td,{children:"Doris Database Name"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"WorkingDir"}),(0,s.jsx)(n.td,{children:"Yes"}),(0,s.jsx)(n.td,{children:"-"}),(0,s.jsx)(n.td,{children:"Spark Load Working Path"})]})]})]}),"\n",(0,s.jsx)(n.h4,{id:"task-configuration",children:"Task Configuration"}),"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Name"}),(0,s.jsx)(n.th,{children:"Suboption-1"}),(0,s.jsx)(n.th,{children:"Suboption-2"}),(0,s.jsx)(n.th,{children:"Required"}),(0,s.jsx)(n.th,{children:"Default value"}),(0,s.jsx)(n.th,{children:"Description"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"loadTasks"}),(0,s.jsx)(n.td,{}),(0,s.jsx)(n.td,{}),(0,s.jsx)(n.td,{children:"Yes"}),(0,s.jsx)(n.td,{children:"-"}),(0,s.jsx)(n.td,{children:"Import task job"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{}),(0,s.jsx)(n.td,{children:"Target table name"}),(0,s.jsx)(n.td,{}),(0,s.jsx)(n.td,{children:"Yes"}),(0,s.jsx)(n.td,{children:"-"}),(0,s.jsx)(n.td,{children:"Imported Doris table name"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{}),(0,s.jsx)(n.td,{}),(0,s.jsx)(n.td,{children:"type"}),(0,s.jsx)(n.td,{children:"Yes"}),(0,s.jsx)(n.td,{children:"-"}),(0,s.jsx)(n.td,{children:"Task type: file - Read file task, hive - Read Hive table task"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{}),(0,s.jsx)(n.td,{}),(0,s.jsx)(n.td,{children:"paths"}),(0,s.jsx)(n.td,{children:"Yes"}),(0,s.jsx)(n.td,{children:"-"}),(0,s.jsx)(n.td,{children:"File path array, only valid for read file task (type=file)"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{}),(0,s.jsx)(n.td,{}),(0,s.jsx)(n.td,{children:"format"}),(0,s.jsx)(n.td,{children:"Yes"}),(0,s.jsx)(n.td,{children:"-"}),(0,s.jsx)(n.td,{children:"File type, supported types: csv, parquet, orc, only valid for read file task (type=file)"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{}),(0,s.jsx)(n.td,{}),(0,s.jsx)(n.td,{children:"fieldSep"}),(0,s.jsx)(n.td,{children:"No"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"\\t"})}),(0,s.jsx)(n.td,{children:"Column delimiter, only valid for read file task (type=file) and file type is csv (format=csv)"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{}),(0,s.jsx)(n.td,{}),(0,s.jsx)(n.td,{children:"lineDelim"}),(0,s.jsx)(n.td,{children:"No"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"\\n"})}),(0,s.jsx)(n.td,{children:"Row delimiter, only valid for read file task (type=file) and file type is csv (format=csv)"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{}),(0,s.jsx)(n.td,{}),(0,s.jsx)(n.td,{children:"hiveMetastoreUris"}),(0,s.jsx)(n.td,{children:"Yes"}),(0,s.jsx)(n.td,{children:"-"}),(0,s.jsx)(n.td,{children:"Hive Metadata service address"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{}),(0,s.jsx)(n.td,{}),(0,s.jsx)(n.td,{children:"hiveDatabase"}),(0,s.jsx)(n.td,{children:"Yes"}),(0,s.jsx)(n.td,{children:"-"}),(0,s.jsx)(n.td,{children:"Hive database name"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{}),(0,s.jsx)(n.td,{}),(0,s.jsx)(n.td,{children:"hiveTable"}),(0,s.jsx)(n.td,{children:"Yes"}),(0,s.jsx)(n.td,{children:"-"}),(0,s.jsx)(n.td,{children:"Hive data table name"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{}),(0,s.jsx)(n.td,{}),(0,s.jsx)(n.td,{children:"columns"}),(0,s.jsx)(n.td,{children:"No"}),(0,s.jsx)(n.td,{children:"Target table columns"}),(0,s.jsx)(n.td,{children:"Source data column names, valid only for reading file tasks (type=file)"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{}),(0,s.jsx)(n.td,{}),(0,s.jsx)(n.td,{children:"columnMappings"}),(0,s.jsx)(n.td,{children:"No"}),(0,s.jsx)(n.td,{children:"-"}),(0,s.jsx)(n.td,{children:"Column mapping"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{}),(0,s.jsx)(n.td,{}),(0,s.jsx)(n.td,{children:"where"}),(0,s.jsx)(n.td,{children:"No"}),(0,s.jsx)(n.td,{children:"-"}),(0,s.jsx)(n.td,{children:"Filter conditions"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{}),(0,s.jsx)(n.td,{}),(0,s.jsx)(n.td,{children:"targetPartitions"}),(0,s.jsx)(n.td,{children:"No"}),(0,s.jsx)(n.td,{children:"-"}),(0,s.jsx)(n.td,{children:"Target import partition"})]})]})]}),"\n",(0,s.jsx)(n.h4,{id:"spark-parameter-configuration",children:"Spark parameter configuration"}),"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Name"}),(0,s.jsx)(n.th,{children:"Suboption"}),(0,s.jsx)(n.th,{children:"Required"}),(0,s.jsx)(n.th,{children:"Default value"}),(0,s.jsx)(n.th,{children:"Description"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"spark"}),(0,s.jsx)(n.td,{}),(0,s.jsx)(n.td,{children:"Yes"}),(0,s.jsx)(n.td,{children:"-"}),(0,s.jsx)(n.td,{children:"Import task job"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{}),(0,s.jsx)(n.td,{children:"sparkHome"}),(0,s.jsx)(n.td,{children:"Yes"}),(0,s.jsx)(n.td,{children:"-"}),(0,s.jsx)(n.td,{children:"Spark deployment path"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{}),(0,s.jsx)(n.td,{children:"master"}),(0,s.jsx)(n.td,{children:"Yes"}),(0,s.jsx)(n.td,{children:"-"}),(0,s.jsx)(n.td,{children:"Spark Master, supported types are: yarn, standalone, local"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{}),(0,s.jsx)(n.td,{children:"deployMode"}),(0,s.jsx)(n.td,{children:"No"}),(0,s.jsx)(n.td,{children:"client"}),(0,s.jsx)(n.td,{children:"Spark deployment mode, supported types are: cluster, client"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{}),(0,s.jsx)(n.td,{children:"properties"}),(0,s.jsx)(n.td,{children:"Yes"}),(0,s.jsx)(n.td,{children:"-"}),(0,s.jsx)(n.td,{children:"Spark job properties"})]})]})]}),"\n",(0,s.jsx)(n.h4,{id:"hadoop-parameter-configuration",children:"Hadoop parameter configuration"}),"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Name"}),(0,s.jsx)(n.th,{children:"Required"}),(0,s.jsx)(n.th,{children:"Default value"}),(0,s.jsx)(n.th,{children:"Parameter description"})]})}),(0,s.jsx)(n.tbody,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"hadoop"}),(0,s.jsx)(n.td,{children:"Yes"}),(0,s.jsx)(n.td,{children:"-"}),(0,s.jsx)(n.td,{children:"Hadoop configuration, including HDFS-related and Yarn configuration"})]})})]}),"\n",(0,s.jsx)(n.h4,{id:"environment-parameter-configuration",children:"Environment parameter configuration"}),"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Name"}),(0,s.jsx)(n.th,{children:"Required"}),(0,s.jsx)(n.th,{children:"Default value"}),(0,s.jsx)(n.th,{children:"Parameter description"})]})}),(0,s.jsx)(n.tbody,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"env"}),(0,s.jsx)(n.td,{children:"No"}),(0,s.jsx)(n.td,{children:"-"}),(0,s.jsx)(n.td,{children:"Environment variables"})]})})]}),"\n",(0,s.jsx)(n.h2,{id:"load-example",children:"Load example"}),"\n",(0,s.jsx)(n.h3,{id:"load-bitmap-type-data",children:"Load Bitmap type data"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Load by building a global dictionary"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Hive table"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-hiveql",children:"CREATE TABLE IF NOT EXISTS hive_t1\n(\n   k1INT,\n   K2   SMALLINT,\n   k3   VARCHAR(50),\n   uuid VARCHAR(100)\n) STORED AS TEXTFILE \n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Doris table"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sql",children:'CREATE TABLE IF NOT EXISTS doris_t1 ( \n    k1 INT, \n    K2 SMALLINT, \n    k3 VARCHAR(50), \n    uuid BITMAP \n) ENGINE=OLAP \nDUPLICATE KEY (k1) \nDISTRIBUTED BY HASH(k1) BUCKETS 1 \nPROPERTIES ( "replication_num" = "1" ) \n'})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Configuration file"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-json",children:'  {\n  "feAddresses": "127.0.0.1:8030",\n  "label": "spark-load-test-bitmap-dict",\n  "user": "root",\n  "password": "",\n  "database": "test",\n  "workingDir": "hdfs://hadoop:8020/spark-load",\n  "loadTasks": {\n    "doris_t1": {\n      "type": "hive",\n      "hiveMetastoreUris": "thrift://hadoop:9083",\n      "hiveDatabase": "test",\n      "hiveTable": "hive_t1",\n      "columnMappings": [\n        "uuid=bitmap_dict(uuid)"\n      ]\n    }\n  },\n  "spark": {\n    "sparkHome": "/opt/spark",\n    "master": "yarn",\n    "deployMode": "cluster",\n    "properties": {\n      "spark.executor.cores": "1",\n      "spark.executor.memory": "2GB",\n      "spark.executor.instances": "1"\n    }\n  },\n  "hadoopProperties": {\n    "fs.defaultFS": "hdfs://hadoop:8020",\n    "hadoop.username": "hadoop"\n  }\n}\n'})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Load Hive Binary data after processing with Bitmap UDF"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Hive table"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-hiveql",children:"CREATE TABLE IF NOT EXISTS hive_t1 (\nk1 INT, \nK2 SMALLINT, \nk3 VARCHAR(50), \nuuid VARCHAR(100) \n) STORED AS TEXTFILE \n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Doris table"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sql",children:'CREATE TABLE IF NOT EXISTS doris_t1\n(\n    k1 INT,\n    K2 SMALLINT,\n    k3 VARCHAR(50),\n    uuid BITMAP\n) ENGINE=OLAP DUPLICATE KEY(k1)\nDISTRIBUTED BY HASH(k1) BUCKETS 1\nPROPERTIES\n(\n    "replication_num" = "1"\n) \n'})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Configuration file"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-json",children:'{\n  "feAddresses": "127.0.0.1:8030",\n  "label": "spark-load-test-bitmap-binary",\n  "user": "root",\n  "password": "",\n  "database": "test",\n  "workingDir": "hdfs: //hadoop:8020/spark-load",\n  "loadTasks": {\n    "doris_tb1": {\n      "type": "hive",\n      "hiveMetastoreUris": "thrift://hadoop:9083",\n      "hiveDatabase": "test",\n      "hiveTable": "hive_t1",\n      "columnMappings": [\n        "uuid=binary_bitmap(uuid)"\n      ]\n    }\n  },\n  "spark": {\n    "sparkHome": "/opt/spark",\n    "master": "yarn",\n    "deployMode": "cluster",\n    "properties": {\n      "spark.executor.cores": "1",\n      "spark.executor.memory": "2GB ",\n      "spark.executor.instances": "1"\n    }\n  },\n  "hadoopProperties": {\n    "fs.defaultFS": "hdfs://hadoop:8020",\n    "hadoop.username": "hadoop"\n  }\n}\n'})}),"\n"]}),"\n"]}),"\n"]}),"\n"]})]})}function h(e={}){let{wrapper:n}={...(0,i.a)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(o,{...e})}):o(e)}},250065:function(e,n,t){t.d(n,{Z:function(){return d},a:function(){return a}});var r=t(667294);let s={},i=r.createContext(s);function a(e){let n=r.useContext(i);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function d(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),r.createElement(i.Provider,{value:n},e.children)}}}]);