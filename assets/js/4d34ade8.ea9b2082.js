"use strict";(self.webpackChunkdoris_website=self.webpackChunkdoris_website||[]).push([["120772"],{726682:function(e,n,i){i.r(n),i.d(n,{default:()=>h,frontMatter:()=>a,metadata:()=>t,assets:()=>l,toc:()=>o,contentTitle:()=>d});var t=JSON.parse('{"id":"ai/vector-search/behind-index","title":"Optimizations Behind Performance","description":"Early versions of Apache Doris focused on online analytical processing (OLAP),","source":"@site/versioned_docs/version-4.x/ai/vector-search/behind-index.md","sourceDirName":"ai/vector-search","slug":"/ai/vector-search/behind-index","permalink":"/docs/4.x/ai/vector-search/behind-index","draft":false,"unlisted":false,"tags":[],"version":"4.x","lastUpdatedAt":1770477659000,"frontMatter":{"title":"Optimizations Behind Performance","language":"en","description":"Early versions of Apache Doris focused on online analytical processing (OLAP),"},"sidebar":"docs","previous":{"title":"Performance Testing and Analysis","permalink":"/docs/4.x/ai/vector-search/performance"},"next":{"title":"Lakehouse Overview","permalink":"/docs/4.x/lakehouse/lakehouse-overview"}}'),r=i("785893"),s=i("250065");let a={title:"Optimizations Behind Performance",language:"en",description:"Early versions of Apache Doris focused on online analytical processing (OLAP),"},d=void 0,l={},o=[{value:"Indexing Stage",id:"indexing-stage",level:2},{value:"Multi-Level Sharding",id:"multi-level-sharding",level:3},{value:"High-Performance Index Building",id:"high-performance-index-building",level:3},{value:"Parallel, High-Quality Index Construction",id:"parallel-high-quality-index-construction",level:4},{value:"SIMD",id:"simd",level:4},{value:"Querying Stage",id:"querying-stage",level:2},{value:"Prepare Statement",id:"prepare-statement",level:3},{value:"Index Only Scan",id:"index-only-scan",level:3},{value:"Virtual Columns for CSE",id:"virtual-columns-for-cse",level:3},{value:"Scan Parallelism Optimization",id:"scan-parallelism-optimization",level:3},{value:"Global TopN Delayed Materialization",id:"global-topn-delayed-materialization",level:3}];function c(e){let n={a:"a",blockquote:"blockquote",code:"code",h2:"h2",h3:"h3",h4:"h4",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.a)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.p,{children:"Early versions of Apache Doris focused on online analytical processing (OLAP), primarily for reporting and aggregation workloads\u2014typical queries being multi-table JOIN and GROUP BY. In 2.x, Doris added text search via inverted indexes and introduced the Variant type for efficient JSON handling. In 3.x, storage-compute separation enabled leveraging object storage to significantly reduce storage costs. In 4.x, Doris steps into the AI era by introducing vector indexes and hybrid search (vector + text), positioning Doris as an enterprise AI analytics platform. This document explains how Doris implements vector indexing in 4.x and the engineering efforts made to reach state-of-the-art performance."}),"\n",(0,r.jsx)(n.p,{children:"We divide vector indexing into two stages: indexing and querying. The indexing stage focuses on 1) data sharding, 2) efficiently building high-quality indexes, and 3) index management. The querying stage has a single goal: improve query performance\u2014eliminating redundant computation and unnecessary IO while optimizing concurrency."}),"\n",(0,r.jsx)(n.h2,{id:"indexing-stage",children:"Indexing Stage"}),"\n",(0,r.jsx)(n.p,{children:"Indexing performance is strongly tied to index hyperparameters: higher index quality typically means longer build time. Thanks to optimizations in the ingestion path, Doris can maintain high index quality while improving ingestion throughput."}),"\n",(0,r.jsx)(n.p,{children:"On the 768D 10M dataset, Apache Doris achieves industry-leading ingestion performance."}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"alt text",src:i(736446).Z+"",width:"1364",height:"434"})}),"\n",(0,r.jsx)(n.h3,{id:"multi-level-sharding",children:"Multi-Level Sharding"}),"\n",(0,r.jsx)(n.p,{children:"Internal tables in Apache Doris are inherently distributed. During query and ingestion, users interact with a single logical table, while the Doris kernel creates the required number of physical tablets based on the table definition. During ingestion, data is routed to the appropriate BE tablet by partition and bucket keys. Multiple tablets together form the logical table seen by users. Each ingestion request forms a transaction, creating a rowset (versioning unit) on the corresponding tablet. Each rowset contains several segments, and the segment is the actual data carrier; ANN indexes operate at the segment granularity."}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"Hierarchy from table to shards",src:i(760235).Z+"",width:"5440",height:"3136"})}),"\n",(0,r.jsx)(n.p,{children:"Vector indexes (e.g., HNSW) rely on key hyperparameters that directly determine index quality and query performance, and are typically tuned for specific data scales. Apache Doris\u2019s multi-level sharding decouples \u201Cindex parameters\u201D from the \u201Cfull table data scale\u201D: users need not rebuild indexes as total data grows, but only tune parameters based on per-batch ingestion size. From our tests, HNSW suggested parameters under different batch sizes are:"}),"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"batch_size"}),(0,r.jsx)(n.th,{children:"max_degree"}),(0,r.jsx)(n.th,{children:"ef_construction"}),(0,r.jsx)(n.th,{children:"ef_search"}),(0,r.jsx)(n.th,{children:"recall@100"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"250000"}),(0,r.jsx)(n.td,{children:"100"}),(0,r.jsx)(n.td,{children:"200"}),(0,r.jsx)(n.td,{children:"50"}),(0,r.jsx)(n.td,{children:"89%"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"250000"}),(0,r.jsx)(n.td,{children:"100"}),(0,r.jsx)(n.td,{children:"200"}),(0,r.jsx)(n.td,{children:"100"}),(0,r.jsx)(n.td,{children:"93%"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"250000"}),(0,r.jsx)(n.td,{children:"100"}),(0,r.jsx)(n.td,{children:"200"}),(0,r.jsx)(n.td,{children:"150"}),(0,r.jsx)(n.td,{children:"95%"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"250000"}),(0,r.jsx)(n.td,{children:"100"}),(0,r.jsx)(n.td,{children:"200"}),(0,r.jsx)(n.td,{children:"200"}),(0,r.jsx)(n.td,{children:"98%"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"500000"}),(0,r.jsx)(n.td,{children:"120"}),(0,r.jsx)(n.td,{children:"240"}),(0,r.jsx)(n.td,{children:"50"}),(0,r.jsx)(n.td,{children:"91%"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"500000"}),(0,r.jsx)(n.td,{children:"120"}),(0,r.jsx)(n.td,{children:"240"}),(0,r.jsx)(n.td,{children:"100"}),(0,r.jsx)(n.td,{children:"94%"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"500000"}),(0,r.jsx)(n.td,{children:"120"}),(0,r.jsx)(n.td,{children:"240"}),(0,r.jsx)(n.td,{children:"150"}),(0,r.jsx)(n.td,{children:"96%"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"500000"}),(0,r.jsx)(n.td,{children:"120"}),(0,r.jsx)(n.td,{children:"240"}),(0,r.jsx)(n.td,{children:"200"}),(0,r.jsx)(n.td,{children:"99%"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"1000000"}),(0,r.jsx)(n.td,{children:"150"}),(0,r.jsx)(n.td,{children:"300"}),(0,r.jsx)(n.td,{children:"50"}),(0,r.jsx)(n.td,{children:"90%"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"1000000"}),(0,r.jsx)(n.td,{children:"150"}),(0,r.jsx)(n.td,{children:"300"}),(0,r.jsx)(n.td,{children:"100"}),(0,r.jsx)(n.td,{children:"93%"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"1000000"}),(0,r.jsx)(n.td,{children:"150"}),(0,r.jsx)(n.td,{children:"300"}),(0,r.jsx)(n.td,{children:"150"}),(0,r.jsx)(n.td,{children:"96%"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"1000000"}),(0,r.jsx)(n.td,{children:"150"}),(0,r.jsx)(n.td,{children:"300"}),(0,r.jsx)(n.td,{children:"200"}),(0,r.jsx)(n.td,{children:"98%"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:"In short, focus on \u201Cper-batch ingestion size\u201D and choose proper index parameters to maintain quality and stable query behavior."}),"\n",(0,r.jsx)(n.h3,{id:"high-performance-index-building",children:"High-Performance Index Building"}),"\n",(0,r.jsx)(n.h4,{id:"parallel-high-quality-index-construction",children:"Parallel, High-Quality Index Construction"}),"\n",(0,r.jsx)(n.p,{children:"Doris accelerates index builds with two-level parallelism: cluster-level parallelism across BE nodes, and intra-node multithreaded distance computation on grouped batch data. Beyond speed, Doris improves index quality via in-memory batching: when the total vector count is fixed but batching is too fine (frequent incremental builds), graph structures become sparser and recall drops. For example, on 768D10M, building in 10 batches may reach ~99% recall, while 100 batches may drop to ~95%. In-memory batching balances memory usage and graph quality under the same hyperparameters, avoiding quality degradation from over-batching."}),"\n",(0,r.jsx)(n.h4,{id:"simd",children:"SIMD"}),"\n",(0,r.jsx)(n.p,{children:"The core cost in ANN index building is large-scale distance computation\u2014a CPU-bound workload. Doris centralizes this work on BE nodes, implemented in C++, and leverages Faiss\u2019s automatic and manual vectorization optimizations. For L2 distance, Faiss uses compiler pragmas to trigger auto-vectorization:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-cpp",children:"FAISS_PRAGMA_IMPRECISE_FUNCTION_BEGIN\nfloat fvec_L2sqr(const float* x, const float* y, size_t d) {\n    size_t i; float res = 0;\n    FAISS_PRAGMA_IMPRECISE_LOOP\n    for (i = 0; i < d; i++) {\n        const float tmp = x[i] - y[i];\n        res += tmp * tmp;\n    }\n    return res;\n}\nFAISS_PRAGMA_IMPRECISE_FUNCTION_END\n"})}),"\n",(0,r.jsxs)(n.p,{children:["With ",(0,r.jsx)(n.code,{children:"FAISS_PRAGMA_IMPRECISE_*"}),", compilers auto-vectorize:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-cpp",children:'#define FAISS_PRAGMA_IMPRECISE_LOOP \\\n    _Pragma("clang loop vectorize(enable) interleave(enable)")\n'})}),"\n",(0,r.jsxs)(n.p,{children:["Faiss also applies explicit SIMD in ",(0,r.jsx)(n.code,{children:"#ifdef SSE3/AVX2/AVX512F"})," blocks using ",(0,r.jsx)(n.code,{children:"_mm*"}),"/",(0,r.jsx)(n.code,{children:"_mm256*"}),"/",(0,r.jsx)(n.code,{children:"_mm512*"}),", combined with ",(0,r.jsx)(n.code,{children:"ElementOpL2/ElementOpIP"})," and dimension-specialized ",(0,r.jsx)(n.code,{children:"fvec_op_ny_D{1,2,4,8,12}"})," to:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Process multiple samples per iteration (e.g., 8/16) and perform register-level transpose to improve memory access locality;"}),"\n",(0,r.jsxs)(n.li,{children:["Use FMA (e.g., ",(0,r.jsx)(n.code,{children:"_mm512_fmadd_ps"}),") to fuse multiply-add and reduce instruction count;"]}),"\n",(0,r.jsx)(n.li,{children:"Do horizontal sums to produce scalars efficiently;"}),"\n",(0,r.jsx)(n.li,{children:"Handle tail elements via masked reads for non-aligned sizes.\nThese optimizations reduce instruction and memory costs and significantly boost indexing throughput."}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"querying-stage",children:"Querying Stage"}),"\n",(0,r.jsxs)(n.p,{children:["Search is latency sensitive. At tens of millions of records with high concurrency, P99 latency typically needs to be under 500 ms\u2014raising the bar for the optimizer, execution engine, and index implementation. Out-of-the-box tests show Doris reaches performance comparable to mainstream dedicated vector databases. The chart below compares Doris against other systems on Performance768D10M; peer data comes from Zilliz\u2019s open-source ",(0,r.jsx)(n.a,{href:"https://github.com/zilliztech/VectorDBBench",children:"VectorDBBench"}),"."]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"alt text",src:i(284682).Z+"",width:"1280",height:"612"})}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsx)(n.p,{children:"Note: The chart includes a subset of out-of-the-box results. OpenSearch and Elastic Cloud can improve query performance by optimizing the number of index files."}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"prepare-statement",children:"Prepare Statement"}),"\n",(0,r.jsx)(n.p,{children:"In the traditional path, Doris runs full optimization (parsing, semantic analysis, RBO, CBO) for every SQL. While essential for general OLAP, this adds overhead for simple, highly repetitive search queries. Doris 4.0 extends Prepare Statement beyond point lookups to all SQL types, including vector search:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["Separate compile and execute\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Prepare performs parsing, semantics, and optimization once, producing a reusable Logical Plan."}),"\n",(0,r.jsx)(n.li,{children:"Execute binds parameters at runtime and runs the pre-built plan, skipping the optimizer entirely."}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["Plan cache\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Reuse is determined by SQL fingerprint (normalized SQL + schema version)."}),"\n",(0,r.jsx)(n.li,{children:"Different parameter values with the same structure reuse the cached plan, avoiding re-optimization."}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["Schema version check\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Validate schema version at execution to ensure correctness."}),"\n",(0,r.jsx)(n.li,{children:"No change \u2192 reuse; changed \u2192 invalidate and re-prepare."}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["Speedup by skipping optimizer\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Execute no longer runs RBO/CBO; optimizer time is nearly eliminated."}),"\n",(0,r.jsx)(n.li,{children:"Template-heavy vector queries benefit with significantly lower end-to-end latency."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"index-only-scan",children:"Index Only Scan"}),"\n",(0,r.jsx)(n.p,{children:"Doris implements vector indexes as external (pluggable) indexes, which simplifies management and supports asynchronous builds, but introduces performance challenges such as avoiding redundant computation and IO. ANN indexes can return distances in addition to row IDs. Doris leverages this by short-circuiting distance expressions within the Scan operator via \u201Cvirtual columns,\u201D and the Ann Index Only Scan fully eliminates distance-related read IO.\nIn the naive flow, Scan pushes predicates to the index, the index returns row IDs, and Scan then reads data pages and computes expressions before returning N rows upstream."}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"alt text",src:i(788513).Z+"",width:"2526",height:"688"})}),"\n",(0,r.jsx)(n.p,{children:"With Index Only Scan applied, the flow becomes:"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"alt text",src:i(957273).Z+"",width:"1726",height:"594"})}),"\n",(0,r.jsxs)(n.p,{children:["For example, ",(0,r.jsx)(n.code,{children:"SELECT l2_distance_approximate(embedding, [...]) AS dist FROM tbl ORDER BY dist LIMIT 100;"})," executes without touching data files."]}),"\n",(0,r.jsxs)(n.p,{children:["Beyond Ann TopN Search, Range Search and Compound Search adopt similar optimizations. Range Search is more nuanced: whether the index returns ",(0,r.jsx)(n.code,{children:"dist"})," depends on the comparator. Below lists query types related to Ann Index Only Scan and whether Index Scan applies:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-SQL",children:"-- Sql1: Range + proj\n-- Index returns dist; no need to recompute dist\n-- Virtual column for CSE avoids dist recomputation in proj\n-- IndexScan: True\nselect id, dist(embedding, [...]) from tbl where dist <= 10;\n\n-- Sql2: Range + no-proj\n-- Index returns dist; no need to recompute\n-- IndexScan: True\nselect id from tbl where dist <= 10 order by id limit N;\n\n-- Sql3: Range + proj + no-dist-from index\n-- Index cannot return dist (only updates rowid map)\n-- proj requires dist \u2192 embedding must be reread\n-- IndexScan: False\nselect id, dist(embedding, [...]) from tbl where dist > 10;\n\n-- Sql4: Range + proj + no-dist-from index\n-- Index cannot return dist, but proj does not need dist \u2192 embedding not reread\n-- IndexScan: True\nselect id from tbl where dist > 10;\n\n-- Sql5: TopN\n-- Index returns dist; virtual slot for CSE uploads dist to proj\n-- embedding column not read\n-- IndexScan: True\nselect id[, dist(embedding, [...])] from tbl order by dist(embedding, [...]) asc limit N;\n\n-- Sql6: TopN + IndexFilter\n-- 1) comment not read (inverted index already optimizes this)\n-- 2) embedding not read (same reason as Sql5)\n-- IndexScan: True\nselect id[, dist(embedding, [...])] from tbl where comment match_any 'olap' ORDER BY dist(embedding, [...]) LIMIT N;\n\n-- Sql7: TopN + Range\n-- IndexScan: True (combination of Sql1 and Sql5)\nselect id[, dist(embedding, [...])] from tbl where dist(embedding, [...]) > 10 order by dist(embedding, [...]) limit N;\n\n-- Sql8: TopN + Range + IndexFilter\n-- IndexScan: True (combination of Sql7 and Sql6)\nselect id[, dist(embedding, [...])] from tbl where comment match_any 'olap' and dist(embedding, [...]) > 10 ORDER BY dist(embedding, [...]) LIMIT N;\n\n-- Sql9: TopN + Range + CommonFilter\n-- Key points: 1) dist < 10 (not > 10); 2) common filter reads dist, not embedding\n-- Index returns dist; virtual slot for CSE ensures all reads refer to the same column\n-- In theory embedding need not materialize; in practice it still does due to residual predicates on the column\n-- IndexScan: False\nselect id[, dist(embedding, [...])] from tbl where comment match_any 'olap' and dist(embedding, [...]) < 10 AND abs(dist(embedding) + 10) > 10 ORDER BY dist(embedding, [...]) LIMIT N;\n\n-- Sql10: Variant of Sql9, dist < 10 \u2192 dist > 10\n-- Index cannot return embedding; to compute abs(dist(embedding,...)) embedding must materialize\n-- IndexScan: False\nselect id[, dist(embedding, [...])] from tbl where comment match_any 'olap' and dist(embedding, [...]) > 10 AND abs(dist(embedding) + 10) > 10 ORDER BY dist(embedding, [...]) LIMIT N;\n\n-- Sql11: Variant of Sql9, abs(dist(...)+10) > 10 \u2192 array_size(embedding) > 10\n-- array_size requires embedding materialization\n-- IndexScan: False\nselect id[, dist(embedding, [...])] from tbl where comment match_any 'olap' and dist(embedding, [...]) < 10 AND array_size(embedding) > 10 ORDER BY dist(embedding, [...]) LIMIT N;\n"})}),"\n",(0,r.jsx)(n.h3,{id:"virtual-columns-for-cse",children:"Virtual Columns for CSE"}),"\n",(0,r.jsxs)(n.p,{children:["Index Only Scan mainly eliminates IO (random reads of embedding). To further remove redundant computation, Doris introduces virtual columns that pass index-returned ",(0,r.jsx)(n.code,{children:"dist"})," into the expression engine as a column.\nDesign highlights:"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["Expression node ",(0,r.jsx)(n.code,{children:"VirtualSlotRef"})]}),"\n",(0,r.jsxs)(n.li,{children:["Column iterator ",(0,r.jsx)(n.code,{children:"VirtualColumnIterator"})]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"VirtualSlotRef"})," is a compute-time-generated column: materialized by one expression, reusable by many, computed once on first use\u2014eliminating CSE across Projection and predicates. ",(0,r.jsx)(n.code,{children:"VirtualColumnIterator"})," materializes index-returned distances into expressions, avoiding repeated distance calculations. Initially built for ANN query CSE elimination, the mechanism was generalized to Projection + Scan + Filter.\nUsing the ClickBench dataset, the query below counts the top 20 websites by Google clicks:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-sql",children:"set experimental_enable_virtual_slot_for_cse=true;\n\nSELECT counterid,\n       COUNT(*)               AS hit_count,\n       COUNT(DISTINCT userid) AS unique_users\nFROM   hits\nWHERE  ( UPPER(regexp_extract(referer, '^https?://([^/]+)', 1)) = 'GOOGLE.COM'\n         OR UPPER(regexp_extract(referer, '^https?://([^/]+)', 1)) = 'GOOGLE.RU'\n         OR UPPER(regexp_extract(referer, '^https?://([^/]+)', 1)) LIKE '%GOOGLE%' )\n       AND ( LENGTH(regexp_extract(referer, '^https?://([^/]+)', 1)) > 3\n              OR regexp_extract(referer, '^https?://([^/]+)', 1) != ''\n              OR regexp_extract(referer, '^https?://([^/]+)', 1) IS NOT NULL )\n       AND eventdate = '2013-07-15'\nGROUP  BY counterid\nHAVING hit_count > 100\nORDER  BY hit_count DESC\nLIMIT  20;\n"})}),"\n",(0,r.jsxs)(n.p,{children:["The core expression ",(0,r.jsx)(n.code,{children:"regexp_extract(referer, '^https?://([^/]+)', 1)"})," is CPU-intensive and reused across predicates. With virtual columns enabled (",(0,r.jsx)(n.code,{children:"set experimental_enable_virtual_slot_for_cse=true;"}),"):"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Enabled: 0.57 s"}),"\n",(0,r.jsx)(n.li,{children:"Disabled: 1.50 s"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"End-to-end performance improves by ~3x."}),"\n",(0,r.jsx)(n.h3,{id:"scan-parallelism-optimization",children:"Scan Parallelism Optimization"}),"\n",(0,r.jsxs)(n.p,{children:["Doris revamped Scan parallelism for Ann TopN Search. The original policy set parallelism by row count (default: 2,097,152 rows per Scan Task). Because segments are size-based, high-dimensional vector columns produce far fewer rows per segment, leading to multiple segments being scanned serially within one Scan Task. Doris switched to \u201Cone Scan Task per segment,\u201D boosting parallelism in index scanning; given Ann TopN\u2019s high filter rate (only N rows returned), the back-to-table phase can remain single-threaded without hurting performance. On SIFT 1M:\n",(0,r.jsx)(n.code,{children:"set optimize_index_scan_parallelism=true;"}),"\nTopN single-threaded query latency drops from 230 ms to 50 ms.\nAdditionally, 4.0 introduces dynamic parallelism: before each scheduling round, Doris adjusts the number of submitted Scan tasks based on thread-pool pressure\u2014reducing tasks under high load, increasing when idle\u2014to balance resource use and scheduling overhead across serial and concurrent workloads."]}),"\n",(0,r.jsx)(n.h3,{id:"global-topn-delayed-materialization",children:"Global TopN Delayed Materialization"}),"\n",(0,r.jsx)(n.p,{children:"A typical Ann TopN query executes in two stages:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Scan obtains per-segment TopN distances via the index;"}),"\n",(0,r.jsx)(n.li,{children:"Global sort merges per-segment TopN to produce the final TopN."}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["If the projection returns many columns or large types (e.g., String), stage-1 reading N rows from each segment can incur heavy IO\u2014and many rows are discarded during stage-2 global sort. Doris minimizes stage-1 IO via global TopN delayed materialization.\nFor ",(0,r.jsx)(n.code,{children:"SELECT id, l2_distance_approximate(embedding, [...]) AS dist FROM tbl ORDER BY dist LIMIT 100;"}),": stage-1 outputs only 100 ",(0,r.jsx)(n.code,{children:"dist"})," values and rowids per segment via Ann Index Only Scan + virtual columns. With M segments, stage-2 globally sorts ",(0,r.jsx)(n.code,{children:"100 * M"})," ",(0,r.jsx)(n.code,{children:"dist"})," values to obtain the final TopN and rowids, then the Materialize operator fetches the needed columns by rowid from corresponding tablet/rowset/segment."]})]})}function h(e={}){let{wrapper:n}={...(0,s.a)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},736446:function(e,n,i){i.d(n,{Z:function(){return t}});let t=i.p+"assets/images/image-1-a71923817aa9e16006c28f92a645f6c1.png"},284682:function(e,n,i){i.d(n,{Z:function(){return t}});let t=i.p+"assets/images/image-2-084767876f4603ac65a193a9e9eb1b42.png"},788513:function(e,n,i){i.d(n,{Z:function(){return t}});let t=i.p+"assets/images/image-3-4ae8ccb8374d15061bf06ea376f44969.png"},957273:function(e,n,i){i.d(n,{Z:function(){return t}});let t=i.p+"assets/images/image-4-fbf3431b2183f79f09f6605790b461a4.png"},760235:function(e,n,i){i.d(n,{Z:function(){return t}});let t=i.p+"assets/images/image-298a312d65a05b759dc5ec046a4df45a.png"},250065:function(e,n,i){i.d(n,{Z:function(){return d},a:function(){return a}});var t=i(667294);let r={},s=t.createContext(r);function a(e){let n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function d(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);