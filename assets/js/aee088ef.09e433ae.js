"use strict";(self.webpackChunkdoris_website=self.webpackChunkdoris_website||[]).push([["97421"],{514594:function(e,n,t){t.r(n),t.d(n,{default:()=>h,frontMatter:()=>o,metadata:()=>r,assets:()=>a,toc:()=>c,contentTitle:()=>l});var r=JSON.parse('{"id":"lakehouse/file-formats/text","title":"Text/CSV/JSON","description":"This document introduces the support for reading and writing text file formats in Doris.","source":"@site/versioned_docs/version-2.1/lakehouse/file-formats/text.md","sourceDirName":"lakehouse/file-formats","slug":"/lakehouse/file-formats/text","permalink":"/docs/2.1/lakehouse/file-formats/text","draft":false,"unlisted":false,"tags":[],"version":"2.1","frontMatter":{"title":"Text/CSV/JSON","language":"zh-CN"},"sidebar":"docs","previous":{"title":"ORC","permalink":"/docs/2.1/lakehouse/file-formats/orc"},"next":{"title":"Data Cache","permalink":"/docs/2.1/lakehouse/data-cache"}}'),i=t("785893"),s=t("250065");let o={title:"Text/CSV/JSON",language:"zh-CN"},l=void 0,a={},c=[{value:"Text/CSV",id:"textcsv",level:2},{value:"Supported Compression Formats",id:"supported-compression-formats",level:3},{value:"JSON",id:"json",level:2},{value:"Catalog",id:"catalog",level:3},{value:"Import",id:"import",level:3},{value:"Character Set",id:"character-set",level:2}];function d(e){let n={a:"a",code:"code",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,s.a)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.p,{children:"This document introduces the support for reading and writing text file formats in Doris."}),"\n",(0,i.jsx)(n.h2,{id:"textcsv",children:"Text/CSV"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Catalog"}),"\n",(0,i.jsxs)(n.p,{children:["Supports reading Hive tables in the ",(0,i.jsx)(n.code,{children:"org.apache.hadoop.mapred.TextInputFormat"})," format."]}),"\n",(0,i.jsx)(n.p,{children:"Support following SerDes:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe"})}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"org.apache.hadoop.hive.serde2.OpenCSVSerde"})," (Since 2.1.7)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"org.apache.hadoop.hive.serde2.MultiDelimitSerDe"})," (Since 3.1.0)"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Table Valued Function"}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Import"}),"\n",(0,i.jsx)(n.p,{children:"Import functionality supports Text/CSV formats. See the import documentation for details."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Export"}),"\n",(0,i.jsx)(n.p,{children:"Export functionality supports Text/CSV formats. See the export documentation for details."}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"supported-compression-formats",children:"Supported Compression Formats"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"uncompressed"}),"\n",(0,i.jsx)(n.li,{children:"gzip"}),"\n",(0,i.jsx)(n.li,{children:"deflate"}),"\n",(0,i.jsx)(n.li,{children:"bzip2"}),"\n",(0,i.jsx)(n.li,{children:"zstd"}),"\n",(0,i.jsx)(n.li,{children:"lz4"}),"\n",(0,i.jsx)(n.li,{children:"snappy"}),"\n",(0,i.jsx)(n.li,{children:"lzo"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"json",children:"JSON"}),"\n",(0,i.jsx)(n.h3,{id:"catalog",children:"Catalog"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"org.apache.hadoop.hive.serde2.JsonSerDe"})," (Since 3.0.4)"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"org.apache.hive.hcatalog.data.JsonSerDe"})," (Since 3.0.4)"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Supports both primitive and complex types."}),"\n",(0,i.jsxs)(n.li,{children:["Does not support the ",(0,i.jsx)(n.code,{children:"timestamp.formats"})," SERDEPROPERTIES."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["Hive table in ",(0,i.jsx)(n.a,{href:"https://github.com/rcongiu/Hive-JSON-Serde",children:(0,i.jsx)(n.code,{children:"org.openx.data.jsonserde.JsonSerDe"})})," (Since 3.0.6)"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Supports both primitive and complex types."}),"\n",(0,i.jsxs)(n.li,{children:["SERDEPROPERTIES: Only ",(0,i.jsx)(n.a,{href:"https://github.com/rcongiu/Hive-JSON-Serde?tab=readme-ov-file#importing-malformed-data",children:(0,i.jsx)(n.code,{children:"ignore.malformed.json"})})," is supported and behaves the same as in this JsonSerDe. Other SERDEPROPERTIES are not effective."]}),"\n",(0,i.jsxs)(n.li,{children:["Does not support ",(0,i.jsx)(n.a,{href:"https://github.com/rcongiu/Hive-JSON-Serde?tab=readme-ov-file#using-arrays",children:(0,i.jsx)(n.code,{children:"Using Arrays"})})," (similar to Text/CSV format, where all column data is placed into a single array)."]}),"\n",(0,i.jsxs)(n.li,{children:["Does not support ",(0,i.jsx)(n.a,{href:"https://github.com/rcongiu/Hive-JSON-Serde?tab=readme-ov-file#promoting-a-scalar-to-an-array",children:(0,i.jsx)(n.code,{children:"Promoting a Scalar to an Array"})})," (promoting a scalar to a single-element array)."]}),"\n",(0,i.jsxs)(n.li,{children:["By default, Doris can correctly recognize the table schema. However, due to the lack of support for certain parameters, automatic schema recognition might fail. In this case, you can set ",(0,i.jsx)(n.code,{children:"read_hive_json_in_one_column = true"})," to place the entire JSON row into the first column to ensure the original data is fully read. Users can then process it manually. This feature requires the first column's data type to be ",(0,i.jsx)(n.code,{children:"String"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"import",children:"Import"}),"\n",(0,i.jsx)(n.p,{children:"Import functionality supports JSON formats. See the import documentation for details."}),"\n",(0,i.jsx)(n.h2,{id:"character-set",children:"Character Set"}),"\n",(0,i.jsx)(n.p,{children:"Currently, Doris only supports the UTF-8 character set encoding. However, some data, such as the data in Hive Text-formatted tables, may contain content encoded in non-UTF-8 encoding, which will cause reading failures and result in the following error:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-text",children:"Only support csv data in utf8 codec\n"})}),"\n",(0,i.jsx)(n.p,{children:"In this case, you can set the session variable as follows:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-text",children:"SET enable_text_validate_utf8 = false\n"})}),"\n",(0,i.jsx)(n.p,{children:"This will ignore the UTF-8 encoding check, allowing you to read this content. Note that this parameter is only used to skip the check, and non-UTF-8 encoded content will still be displayed as garbled text."}),"\n",(0,i.jsx)(n.p,{children:"This parameter has been supported since version 3.0.4."})]})}function h(e={}){let{wrapper:n}={...(0,s.a)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},250065:function(e,n,t){t.d(n,{Z:function(){return l},a:function(){return o}});var r=t(667294);let i={},s=r.createContext(i);function o(e){let n=r.useContext(s);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),r.createElement(s.Provider,{value:n},e.children)}}}]);