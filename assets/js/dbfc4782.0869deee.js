"use strict";(self.webpackChunkdoris_website=self.webpackChunkdoris_website||[]).push([["281268"],{390530:function(e){e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"/data-pruning-250908","metadata":{"permalink":"/blog/data-pruning-250908","source":"@site/blog/data-pruning-250908.mdx","title":"Deep Dive: Data Pruning in Apache Doris","description":"At Apache Doris, we have implemented multiple strategies to make the system more intelligent, enabling it to skip unnecessary data processing. In this article, we will discuss all the data pruning techniques used in Apache Doris.","date":"2025-09-08T00:00:00.000Z","tags":[{"inline":true,"label":"Tech Sharing","permalink":"/blog/tags/tech-sharing"}],"hasTruncateMarker":false,"authors":[{"name":"velodb.io \xb7 VeloDB Engineering Team","key":null,"page":null}],"frontMatter":{"title":"Deep Dive: Data Pruning in Apache Doris","summary":"At Apache Doris, we have implemented multiple strategies to make the system more intelligent, enabling it to skip unnecessary data processing. In this article, we will discuss all the data pruning techniques used in Apache Doris.","description":"At Apache Doris, we have implemented multiple strategies to make the system more intelligent, enabling it to skip unnecessary data processing. In this article, we will discuss all the data pruning techniques used in Apache Doris.","date":"2025-09-08","author":"velodb.io \xb7 VeloDB Engineering Team","externalLink":"https://www.velodb.io/blog/1489","tags":["Tech Sharing"],"image":"/images/blogs/data-pruning-250905.PNG"},"unlisted":false,"nextItem":{"title":"Apache Doris Up To 40x Faster Than ClickHouse | OLAP Showdown Part 2","permalink":"/blog/coffeebench-part2-250917"}},"content":"import { BlogLink } from \'../src/components/blogs/components/blog-link\';\\nimport { SeeMore } from \'../src/components/blogs/components/see-more\';\\n\\n> <BlogLink rel=\\"noopener noreferrer\\" target=\'_blank\' href=\'https://www.velodb.io/blog/1489\'>At Apache Doris, we have implemented multiple strategies to make the system more intelligent, enabling it to skip unnecessary data processing. In this article, we will discuss all the data pruning techniques used in Apache Doris.<SeeMore /></BlogLink>"},{"id":"/coffeebench-part2-250917","metadata":{"permalink":"/blog/coffeebench-part2-250917","source":"@site/blog/coffeebench-part2-250917.mdx","title":"Apache Doris Up To 40x Faster Than ClickHouse | OLAP Showdown Part 2","description":"In every benchmark tested: CoffeeBench, TPC-H, and TPC-DS, Apache Doris consistently pulled ahead, establishing clear dominance over both ClickHouse v25.8 on-premises and ClickHouse Cloud.","date":"2025-09-07T00:00:00.000Z","tags":[{"inline":true,"label":"Tech Sharing","permalink":"/blog/tags/tech-sharing"}],"hasTruncateMarker":false,"authors":[{"name":"velodb.io \xb7 VeloDB Engineering Team","key":null,"page":null}],"frontMatter":{"title":"Apache Doris Up To 40x Faster Than ClickHouse | OLAP Showdown Part 2","summary":"In every benchmark tested: CoffeeBench, TPC-H, and TPC-DS, Apache Doris consistently pulled ahead, establishing clear dominance over both ClickHouse v25.8 on-premises and ClickHouse Cloud.","description":"In every benchmark tested: CoffeeBench, TPC-H, and TPC-DS, Apache Doris consistently pulled ahead, establishing clear dominance over both ClickHouse v25.8 on-premises and ClickHouse Cloud.","picked":"true","order":"1","date":"2025-09-07","author":"velodb.io \xb7 VeloDB Engineering Team","externalLink":"https://www.velodb.io/blog/1504","tags":["Tech Sharing"],"image":"/images/blogs/coffeebench-part2-250917.jpeg"},"unlisted":false,"prevItem":{"title":"Deep Dive: Data Pruning in Apache Doris","permalink":"/blog/data-pruning-250908"},"nextItem":{"title":"Data Traits in Apache Doris: The Secret Weapon Behind 2x Faster Performance","permalink":"/blog/data-trait-250905"}},"content":"import { BlogLink } from \'../src/components/blogs/components/blog-link\';\\nimport { SeeMore } from \'../src/components/blogs/components/see-more\';\\n\\n> <BlogLink rel=\\"noopener noreferrer\\" target=\'_blank\' href=\'https://www.velodb.io/blog/1504\'>In every benchmark tested: CoffeeBench, TPC-H, and TPC-DS, Apache Doris consistently pulled ahead, establishing clear dominance over both ClickHouse v25.8 on-premises and ClickHouse Cloud.<SeeMore /></BlogLink>"},{"id":"/data-trait-250905","metadata":{"permalink":"/blog/data-trait-250905","source":"@site/blog/data-trait-250905.mdx","title":"Data Traits in Apache Doris: The Secret Weapon Behind 2x Faster Performance","description":"At the core of database systems, the query optimizer acts as a shrewd strategist, constantly analyzing data traits to devise the optimal execution plans. Apache Doris, a high-performance MPP analytical database, employs a built-in Data Trait analysis mechanism in its optimizer. By uncovering statistical traits and semantic constraints within the data, Data Trait provides fundamental support for query optimization. Let\u2019s explore its power!","date":"2025-09-05T00:00:00.000Z","tags":[{"inline":true,"label":"Tech Sharing","permalink":"/blog/tags/tech-sharing"}],"hasTruncateMarker":false,"authors":[{"name":"velodb.io \xb7 VeloDB Engineering Team","key":null,"page":null}],"frontMatter":{"title":"Data Traits in Apache Doris: The Secret Weapon Behind 2x Faster Performance","summary":"At the core of database systems, the query optimizer acts as a shrewd strategist, constantly analyzing data traits to devise the optimal execution plans. Apache Doris, a high-performance MPP analytical database, employs a built-in Data Trait analysis mechanism in its optimizer. By uncovering statistical traits and semantic constraints within the data, Data Trait provides fundamental support for query optimization. Let\u2019s explore its power!","description":"At the core of database systems, the query optimizer acts as a shrewd strategist, constantly analyzing data traits to devise the optimal execution plans. Apache Doris, a high-performance MPP analytical database, employs a built-in Data Trait analysis mechanism in its optimizer. By uncovering statistical traits and semantic constraints within the data, Data Trait provides fundamental support for query optimization. Let\u2019s explore its power!","picked":"true","order":"4","date":"2025-09-05","author":"velodb.io \xb7 VeloDB Engineering Team","externalLink":"https://www.velodb.io/blog/1488","tags":["Tech Sharing"],"image":"/images/blogs/data-trait-250908.PNG"},"unlisted":false,"prevItem":{"title":"Apache Doris Up To 40x Faster Than ClickHouse | OLAP Showdown Part 2","permalink":"/blog/coffeebench-part2-250917"},"nextItem":{"title":"Apache Doris Tops RTABench, 6x Faster Than ClickHouse, 30x Faster Than PostgreSQL","permalink":"/blog/rtabench-250902"}},"content":"import { BlogLink } from \'../src/components/blogs/components/blog-link\';\\nimport { SeeMore } from \'../src/components/blogs/components/see-more\';\\n\\n> <BlogLink rel=\\"noopener noreferrer\\" target=\'_blank\' href=\'https://www.velodb.io/blog/1488\'>At the core of database systems, the query optimizer acts as a shrewd strategist, constantly analyzing data traits to devise the optimal execution plans. Apache Doris, a high-performance MPP analytical database, employs a built-in Data Trait analysis mechanism in its optimizer. By uncovering statistical traits and semantic constraints within the data, Data Trait provides fundamental support for query optimization. Let\u2019s explore its power!<SeeMore /></BlogLink>"},{"id":"/rtabench-250902","metadata":{"permalink":"/blog/rtabench-250902","source":"@site/blog/rtabench-250902.mdx","title":"Apache Doris Tops RTABench, 6x Faster Than ClickHouse, 30x Faster Than PostgreSQL","description":"Apache Doris, a popular real-time data warehouse, ranked first in the latest RTABench results, setting a new benchmark for real-time analytics performance. In standardized tests, Doris delivered up to 6 times the performance of ClickHouse, 30 times that of PostgreSQL, and 100 times that of MongoDB.","date":"2025-09-02T00:00:00.000Z","tags":[{"inline":true,"label":"Tech Sharing","permalink":"/blog/tags/tech-sharing"}],"hasTruncateMarker":false,"authors":[{"name":"velodb.io \xb7 VeloDB Engineering Team","key":null,"page":null}],"frontMatter":{"title":"Apache Doris Tops RTABench, 6x Faster Than ClickHouse, 30x Faster Than PostgreSQL","summary":"Apache Doris, a popular real-time data warehouse, ranked first in the latest RTABench results, setting a new benchmark for real-time analytics performance. In standardized tests, Doris delivered up to 6 times the performance of ClickHouse, 30 times that of PostgreSQL, and 100 times that of MongoDB.","description":"Apache Doris, a popular real-time data warehouse, ranked first in the latest RTABench results, setting a new benchmark for real-time analytics performance. In standardized tests, Doris delivered up to 6 times the performance of ClickHouse, 30 times that of PostgreSQL, and 100 times that of MongoDB.","picked":"true","order":"3","date":"2025-09-02","author":"velodb.io \xb7 VeloDB Engineering Team","externalLink":"https://www.velodb.io/blog/1465","tags":["Tech Sharing"],"image":"/images/blogs/rtabench-250902.JPEG"},"unlisted":false,"prevItem":{"title":"Data Traits in Apache Doris: The Secret Weapon Behind 2x Faster Performance","permalink":"/blog/data-trait-250905"},"nextItem":{"title":"The Ultimate OLAP Showdown: Apache Doris vs. ClickHouse vs. Snowflake (Part 1)","permalink":"/blog/coffeebench-olap-showdown-part1-250829"}},"content":"import { BlogLink } from \'../src/components/blogs/components/blog-link\';\\nimport { SeeMore } from \'../src/components/blogs/components/see-more\';\\n\\n> <BlogLink rel=\\"noopener noreferrer\\" target=\'_blank\' href=\'https://www.velodb.io/blog/1465\'>Apache Doris, a popular real-time data warehouse, ranked first in the latest RTABench results, setting a new benchmark for real-time analytics performance. In standardized tests, Doris delivered up to 6 times the performance of ClickHouse, 30 times that of PostgreSQL, and 100 times that of MongoDB.<SeeMore /></BlogLink>"},{"id":"/coffeebench-olap-showdown-part1-250829","metadata":{"permalink":"/blog/coffeebench-olap-showdown-part1-250829","source":"@site/blog/coffeebench-olap-showdown-part1-250829.mdx","title":"The Ultimate OLAP Showdown: Apache Doris vs. ClickHouse vs. Snowflake (Part 1)","description":"Apache Doris consistently delivers significantly faster performance in large-scale benchmarks spanning both straightforward JOINs and production-grade TPC-H/TPC-DS workloads. On top of that, Apache Doris requires just 10%-20% of the cost of Snowflake or ClickHouse for OLAP workloads.","date":"2025-08-29T00:00:00.000Z","tags":[{"inline":true,"label":"Tech Sharing","permalink":"/blog/tags/tech-sharing"}],"hasTruncateMarker":false,"authors":[{"name":"velodb.io \xb7 VeloDB Engineering Team","key":null,"page":null}],"frontMatter":{"title":"The Ultimate OLAP Showdown: Apache Doris vs. ClickHouse vs. Snowflake (Part 1)","summary":"Apache Doris consistently delivers significantly faster performance in large-scale benchmarks spanning both straightforward JOINs and production-grade TPC-H/TPC-DS workloads. On top of that, Apache Doris requires just 10%-20% of the cost of Snowflake or ClickHouse for OLAP workloads.","description":"Apache Doris consistently delivers significantly faster performance in large-scale benchmarks spanning both straightforward JOINs and production-grade TPC-H/TPC-DS workloads. On top of that, Apache Doris requires just 10%-20% of the cost of Snowflake or ClickHouse for OLAP workloads.","picked":"true","order":"2","date":"2025-08-29","author":"velodb.io \xb7 VeloDB Engineering Team","externalLink":"https://www.velodb.io/blog/1463","tags":["Tech Sharing"],"image":"/images/blogs/coffeebench-olap-showdown-part1-250829.PNG"},"unlisted":false,"prevItem":{"title":"Apache Doris Tops RTABench, 6x Faster Than ClickHouse, 30x Faster Than PostgreSQL","permalink":"/blog/rtabench-250902"},"nextItem":{"title":"Unified Lakehouse with Apache Doris and Paimon: Xiaomi Achieves 6\xd7 Faster Performance","permalink":"/blog/unified-lakehouse-with-apache-doris-and-paimon-xiaomi-achieves-6\xd7-faster-performance"}},"content":"import { BlogLink } from \'../src/components/blogs/components/blog-link\';\\nimport { SeeMore } from \'../src/components/blogs/components/see-more\';\\n\\n> <BlogLink rel=\\"noopener noreferrer\\" target=\'_blank\' href=\'https://www.velodb.io/blog/1463\'>Apache Doris consistently delivers significantly faster performance in large-scale benchmarks spanning both straightforward JOINs and production-grade TPC-H/TPC-DS workloads. On top of that, Apache Doris requires just 10%-20% of the cost of Snowflake or ClickHouse for OLAP workloads.<SeeMore /></BlogLink>"},{"id":"/unified-lakehouse-with-apache-doris-and-paimon-xiaomi-achieves-6\xd7-faster-performance","metadata":{"permalink":"/blog/unified-lakehouse-with-apache-doris-and-paimon-xiaomi-achieves-6\xd7-faster-performance","source":"@site/blog/unified-lakehouse-with-apache-doris-and-paimon-xiaomi-achieves-6\xd7-faster-performance.mdx","title":"Unified Lakehouse with Apache Doris and Paimon: Xiaomi Achieves 6\xd7 Faster Performance","description":"Xiaomi is a leading global player in consumer electronics. Best known for its smartphones and smart home devices, Xiaomi is among the top global three smartphone makers and continues to expand into new offerings like electric vehicles. With a global operation, Xiaomi requires an analytical data architecture that can support its growth and increasing demand. Their solution: Apache Doris and Apache Paimon.","date":"2025-08-28T00:00:00.000Z","tags":[{"inline":true,"label":"Best Practice","permalink":"/blog/tags/best-practice"}],"hasTruncateMarker":false,"authors":[{"name":"velodb.io \xb7 VeloDB Engineering Team","key":null,"page":null}],"frontMatter":{"title":"Unified Lakehouse with Apache Doris and Paimon: Xiaomi Achieves 6\xd7 Faster Performance","summary":"Xiaomi is a leading global player in consumer electronics. Best known for its smartphones and smart home devices, Xiaomi is among the top global three smartphone makers and continues to expand into new offerings like electric vehicles. With a global operation, Xiaomi requires an analytical data architecture that can support its growth and increasing demand. Their solution: Apache Doris and Apache Paimon.","description":"Xiaomi is a leading global player in consumer electronics. Best known for its smartphones and smart home devices, Xiaomi is among the top global three smartphone makers and continues to expand into new offerings like electric vehicles. With a global operation, Xiaomi requires an analytical data architecture that can support its growth and increasing demand. Their solution: Apache Doris and Apache Paimon.","date":"2025-08-28","author":"velodb.io \xb7 VeloDB Engineering Team","externalLink":"https://www.velodb.io/blog/1461","tags":["Best Practice"],"image":"/images/blogs/lakehouse-performance.png"},"unlisted":false,"prevItem":{"title":"The Ultimate OLAP Showdown: Apache Doris vs. ClickHouse vs. Snowflake (Part 1)","permalink":"/blog/coffeebench-olap-showdown-part1-250829"},"nextItem":{"title":"Real-time Data Analytics at Scale: Integrating Apache Flink and Doris","permalink":"/blog/real-time-data-analytics-at-scale-integrating-apache-flink-and-doris"}},"content":"import { BlogLink } from \'../src/components/blogs/components/blog-link\';\\nimport { SeeMore } from \'../src/components/blogs/components/see-more\';\\n\\n> <BlogLink rel=\\"noopener noreferrer\\" target=\'_blank\' href=\'https://www.velodb.io/blog/1461\'>Xiaomi is a leading global player in consumer electronics. Best known for its smartphones and smart home devices, Xiaomi is among the top global three smartphone makers and continues to expand into new offerings like electric vehicles. With a global operation, Xiaomi requires an analytical data architecture that can support its growth and increasing demand. Their solution: Apache Doris and Apache Paimon.<SeeMore /></BlogLink>"},{"id":"/real-time-data-analytics-at-scale-integrating-apache-flink-and-doris","metadata":{"permalink":"/blog/real-time-data-analytics-at-scale-integrating-apache-flink-and-doris","source":"@site/blog/real-time-data-analytics-at-scale-integrating-apache-flink-and-doris.mdx","title":"Real-time Data Analytics at Scale: Integrating Apache Flink and Doris","description":"In this article, we\'ll discuss the main technical use cases of Flink Doris Connector and Flink CDC, showing a complete playbook to combine Flink\'s real-time processing with Doris\'s fast analytics.","date":"2025-08-18T00:00:00.000Z","tags":[{"inline":true,"label":"Tech Sharing","permalink":"/blog/tags/tech-sharing"}],"hasTruncateMarker":false,"authors":[{"name":"velodb.io \xb7 VeloDB Engineering Team","key":null,"page":null}],"frontMatter":{"title":"Real-time Data Analytics at Scale: Integrating Apache Flink and Doris","summary":"In this article, we\'ll discuss the main technical use cases of Flink Doris Connector and Flink CDC, showing a complete playbook to combine Flink\'s real-time processing with Doris\'s fast analytics.","description":"In this article, we\'ll discuss the main technical use cases of Flink Doris Connector and Flink CDC, showing a complete playbook to combine Flink\'s real-time processing with Doris\'s fast analytics.","date":"2025-08-18","author":"velodb.io \xb7 VeloDB Engineering Team","externalLink":"https://www.velodb.io/blog/1453","tags":["Tech Sharing"],"image":"/images/blogs/real-time-data-analytics-at-scale-integrating-apache-flink-and-doris.JPEG"},"unlisted":false,"prevItem":{"title":"Unified Lakehouse with Apache Doris and Paimon: Xiaomi Achieves 6\xd7 Faster Performance","permalink":"/blog/unified-lakehouse-with-apache-doris-and-paimon-xiaomi-achieves-6\xd7-faster-performance"},"nextItem":{"title":"Apache Doris Empowers Real-time Lakehouse in Large-Scale Business Scenarios of Cainiao","permalink":"/blog/real-time-lakehouse-in-cainiao-large-scale-business-scenarios"}},"content":"import { BlogLink } from \'../src/components/blogs/components/blog-link\';\\nimport { SeeMore } from \'../src/components/blogs/components/see-more\';\\n\\n> <BlogLink rel=\\"noopener noreferrer\\" target=\'_blank\' href=\'https://www.velodb.io/blog/1453\'>In this article, we\'ll discuss the main technical use cases of Flink Doris Connector and Flink CDC, showing a complete playbook to combine Flink\'s real-time processing with Doris\'s fast analytics.<SeeMore /></BlogLink>"},{"id":"/real-time-lakehouse-in-cainiao-large-scale-business-scenarios","metadata":{"permalink":"/blog/real-time-lakehouse-in-cainiao-large-scale-business-scenarios","source":"@site/blog/real-time-lakehouse-in-cainiao-large-scale-business-scenarios.mdx","title":"Apache Doris Empowers Real-time Lakehouse in Large-Scale Business Scenarios of Cainiao","description":"Cainiao, the world of e-commerce logistics giant, chose Apache Doris to upgrade its data platform. This step-by-step migration started in 2023, including validating Doris in a mission-critical scenario, expanding Doris\'s application scenarios, and executing full-scale deployment. the cost efficiency, stability, and operational efficiency of Doris have been powerfully proven. Currently, Doris powers over 25 clusters (10,000+ CPUs) across 3 regions without any failure.","date":"2025-08-18T00:00:00.000Z","tags":[{"inline":true,"label":"Best Practice","permalink":"/blog/tags/best-practice"}],"hasTruncateMarker":false,"authors":[{"name":"velodb.io \xb7 VeloDB Engineering Team","key":null,"page":null}],"frontMatter":{"title":"Apache Doris Empowers Real-time Lakehouse in Large-Scale Business Scenarios of Cainiao","summary":"Cainiao, the world of e-commerce logistics giant, chose Apache Doris to upgrade its data platform. This step-by-step migration started in 2023, including validating Doris in a mission-critical scenario, expanding Doris\'s application scenarios, and executing full-scale deployment. the cost efficiency, stability, and operational efficiency of Doris have been powerfully proven. Currently, Doris powers over 25 clusters (10,000+ CPUs) across 3 regions without any failure.","description":"Cainiao, the world of e-commerce logistics giant, chose Apache Doris to upgrade its data platform. This step-by-step migration started in 2023, including validating Doris in a mission-critical scenario, expanding Doris\'s application scenarios, and executing full-scale deployment. the cost efficiency, stability, and operational efficiency of Doris have been powerfully proven. Currently, Doris powers over 25 clusters (10,000+ CPUs) across 3 regions without any failure.","date":"2025-08-18","author":"velodb.io \xb7 VeloDB Engineering Team","externalLink":"https://www.velodb.io/blog/1454","tags":["Best Practice"],"image":"/images/blogs/real-time-lakehouse-in-cainiao-large-scale-business-scenarios.png"},"unlisted":false,"prevItem":{"title":"Real-time Data Analytics at Scale: Integrating Apache Flink and Doris","permalink":"/blog/real-time-data-analytics-at-scale-integrating-apache-flink-and-doris"},"nextItem":{"title":"Leading Cloud Computing Service Provider Chose Apache Doris + Iceberg for Hyperscale Data Lakehouse","permalink":"/blog/apache-doris-and-iceberg-building-hyperscale-data-lakehouse"}},"content":"import { BlogLink } from \'../src/components/blogs/components/blog-link\';\\nimport { SeeMore } from \'../src/components/blogs/components/see-more\';\\n\\n> <BlogLink rel=\\"noopener noreferrer\\" target=\'_blank\' href=\'https://www.velodb.io/blog/1454\'>Cainiao, the world of e-commerce logistics giant, chose Apache Doris to upgrade its data platform. This step-by-step migration started in 2023, including validating Doris in a mission-critical scenario, expanding Doris\'s application scenarios, and executing full-scale deployment. the cost efficiency, stability, and operational efficiency of Doris have been powerfully proven. Currently, Doris powers over 25 clusters (10,000+ CPUs) across 3 regions without any failure.<SeeMore /></BlogLink>"},{"id":"/apache-doris-and-iceberg-building-hyperscale-data-lakehouse","metadata":{"permalink":"/blog/apache-doris-and-iceberg-building-hyperscale-data-lakehouse","source":"@site/blog/apache-doris-and-iceberg-building-hyperscale-data-lakehouse.mdx","title":"Leading Cloud Computing Service Provider Chose Apache Doris + Iceberg for Hyperscale Data Lakehouse","description":"The world\'s cloud computing service giant chose Apache Doris + Apache Iceberg to upgrade its data platform into a flexible, efficient data lakehouse with low costs. This solution handles reporting and BI, federated analysis, log storage and analysis, and high-concurrency analysis. With Apache Doris, this company has successfully launched 20+ projects with 50+ clusters, 3000+ nodes, and over 15 petabytes of data.","date":"2025-08-14T00:00:00.000Z","tags":[{"inline":true,"label":"Best Practice","permalink":"/blog/tags/best-practice"}],"hasTruncateMarker":false,"authors":[{"name":"velodb.io \xb7 VeloDB Engineering Team","key":null,"page":null}],"frontMatter":{"title":"Leading Cloud Computing Service Provider Chose Apache Doris + Iceberg for Hyperscale Data Lakehouse","summary":"The world\'s cloud computing service giant chose Apache Doris + Apache Iceberg to upgrade its data platform into a flexible, efficient data lakehouse with low costs. This solution handles reporting and BI, federated analysis, log storage and analysis, and high-concurrency analysis. With Apache Doris, this company has successfully launched 20+ projects with 50+ clusters, 3000+ nodes, and over 15 petabytes of data.","description":"The world\'s cloud computing service giant chose Apache Doris + Apache Iceberg to upgrade its data platform into a flexible, efficient data lakehouse with low costs. This solution handles reporting and BI, federated analysis, log storage and analysis, and high-concurrency analysis. With Apache Doris, this company has successfully launched 20+ projects with 50+ clusters, 3000+ nodes, and over 15 petabytes of data.","date":"2025-08-14","author":"velodb.io \xb7 VeloDB Engineering Team","externalLink":"https://www.velodb.io/blog/1450","tags":["Best Practice"],"image":"/images/blogs/apache-doris-and-iceberg-building-hyperscale-data-lakehouse.png"},"unlisted":false,"prevItem":{"title":"Apache Doris Empowers Real-time Lakehouse in Large-Scale Business Scenarios of Cainiao","permalink":"/blog/real-time-lakehouse-in-cainiao-large-scale-business-scenarios"},"nextItem":{"title":"Apache Doris + MCP: The Real-Time Analytical Data Platform for the Agentic AI Era","permalink":"/blog/real-time-analytical-data-platform-for-the-agentic-ai-era"}},"content":"import { BlogLink } from \'../src/components/blogs/components/blog-link\';\\nimport { SeeMore } from \'../src/components/blogs/components/see-more\';\\n\\n> <BlogLink rel=\\"noopener noreferrer\\" target=\'_blank\' href=\'https://www.velodb.io/blog/1450\'>The world\'s cloud computing service giant chose Apache Doris + Apache Iceberg to upgrade its data platform into a flexible, efficient data lakehouse with low costs. This solution handles reporting and BI, federated analysis, log storage and analysis, and high-concurrency analysis. With Apache Doris, this company has successfully launched 20+ projects with 50+ clusters, 3000+ nodes, and over 15 petabytes of data.<SeeMore /></BlogLink>"},{"id":"/real-time-analytical-data-platform-for-the-agentic-ai-era","metadata":{"permalink":"/blog/real-time-analytical-data-platform-for-the-agentic-ai-era","source":"@site/blog/real-time-analytical-data-platform-for-the-agentic-ai-era.mdx","title":"Apache Doris + MCP: The Real-Time Analytical Data Platform for the Agentic AI Era","description":"Apache Doris is built to meet the data challenges in the agentic AI era, delivering real-time analytics at scale. But to use Doris\'s power for AI agents, you need a bridge between them. That\'s where Doris MCP Server comes in, acting as a communication layer between AI agents and Doris.In this article, we\'ll explore how agentic AI is rewriting the rules for analytics, how MCP connects AI agents to data sources, and walk through two demos that bring AI agents, MCP, and Apache Doris together in action.","date":"2025-08-11T00:00:00.000Z","tags":[{"inline":true,"label":"Tech Sharing","permalink":"/blog/tags/tech-sharing"}],"hasTruncateMarker":false,"authors":[{"name":"velodb.io \xb7 VeloDB Engineering Team","key":null,"page":null}],"frontMatter":{"title":"Apache Doris + MCP: The Real-Time Analytical Data Platform for the Agentic AI Era","summary":"Apache Doris is built to meet the data challenges in the agentic AI era, delivering real-time analytics at scale. But to use Doris\'s power for AI agents, you need a bridge between them. That\'s where Doris MCP Server comes in, acting as a communication layer between AI agents and Doris.In this article, we\'ll explore how agentic AI is rewriting the rules for analytics, how MCP connects AI agents to data sources, and walk through two demos that bring AI agents, MCP, and Apache Doris together in action.","description":"Apache Doris is built to meet the data challenges in the agentic AI era, delivering real-time analytics at scale. But to use Doris\'s power for AI agents, you need a bridge between them. That\'s where Doris MCP Server comes in, acting as a communication layer between AI agents and Doris.In this article, we\'ll explore how agentic AI is rewriting the rules for analytics, how MCP connects AI agents to data sources, and walk through two demos that bring AI agents, MCP, and Apache Doris together in action.","date":"2025-08-11","author":"velodb.io \xb7 VeloDB Engineering Team","externalLink":"https://www.velodb.io/blog/1444","tags":["Tech Sharing"],"image":"/images/blogs/real-time-analytical-data-platform-for-the-agentic-ai-era.jpg"},"unlisted":false,"prevItem":{"title":"Leading Cloud Computing Service Provider Chose Apache Doris + Iceberg for Hyperscale Data Lakehouse","permalink":"/blog/apache-doris-and-iceberg-building-hyperscale-data-lakehouse"},"nextItem":{"title":"Apache Doris Empowered 5G Fully-Connected Factory with A Unified Real-time & Batch Data Platform","permalink":"/blog/apache-doris-empowered-5G-fully-connected-factory-with-a-unified-real-time-data-platform"}},"content":"import { BlogLink } from \'../src/components/blogs/components/blog-link\';\\nimport { SeeMore } from \'../src/components/blogs/components/see-more\';\\n\\n> <BlogLink rel=\\"noopener noreferrer\\" target=\'_blank\' href=\'https://www.velodb.io/blog/1444\'>Apache Doris is built to meet the data challenges in the agentic AI era, delivering real-time analytics at scale. But to use Doris\'s power for AI agents, you need a bridge between them. That\'s where Doris MCP Server comes in, acting as a communication layer between AI agents and Doris.In this article, we\'ll explore how agentic AI is rewriting the rules for analytics, how MCP connects AI agents to data sources, and walk through two demos that bring AI agents, MCP, and Apache Doris together in action.<SeeMore /></BlogLink>"},{"id":"/apache-doris-empowered-5G-fully-connected-factory-with-a-unified-real-time-data-platform","metadata":{"permalink":"/blog/apache-doris-empowered-5G-fully-connected-factory-with-a-unified-real-time-data-platform","source":"@site/blog/apache-doris-empowered-5G-fully-connected-factory-with-a-unified-real-time-data-platform.mdx","title":"Apache Doris Empowered 5G Fully-Connected Factory with A Unified Real-time & Batch Data Platform","description":"One of the world\'s biggest telecommunication companies replaced the Lambda architecture with a unified real-time & batch data platform powered by Apache Doris for its 5G Fully-Connected Factory. Leveraging Doris\'s federated query capabilities, this platform established a unified query gateway and simplified data pipelines, significantly reducing storage costs, improving data freshness, and boosting query performance and development efficiency. Currently, Doris handles 70% of real-time data ingestion and 90% of real-time queries.","date":"2025-08-08T00:00:00.000Z","tags":[{"inline":true,"label":"Tech Sharing","permalink":"/blog/tags/tech-sharing"}],"hasTruncateMarker":false,"authors":[{"name":"velodb.io \xb7 VeloDB Engineering Team","key":null,"page":null}],"frontMatter":{"title":"Apache Doris Empowered 5G Fully-Connected Factory with A Unified Real-time & Batch Data Platform","summary":"One of the world\'s biggest telecommunication companies replaced the Lambda architecture with a unified real-time & batch data platform powered by Apache Doris for its 5G Fully-Connected Factory. Leveraging Doris\'s federated query capabilities, this platform established a unified query gateway and simplified data pipelines, significantly reducing storage costs, improving data freshness, and boosting query performance and development efficiency. Currently, Doris handles 70% of real-time data ingestion and 90% of real-time queries.","description":"One of the world\'s biggest telecommunication companies replaced the Lambda architecture with a unified real-time & batch data platform powered by Apache Doris for its 5G Fully-Connected Factory. Leveraging Doris\'s federated query capabilities, this platform established a unified query gateway and simplified data pipelines, significantly reducing storage costs, improving data freshness, and boosting query performance and development efficiency. Currently, Doris handles 70% of real-time data ingestion and 90% of real-time queries.","date":"2025-08-08","author":"velodb.io \xb7 VeloDB Engineering Team","externalLink":"https://www.velodb.io/blog/1446","tags":["Tech Sharing"],"image":"/images/blogs/apache-doris-empowered-5G-fully-connected-factory-with-a-unified-real-time-data-platform.jpg"},"unlisted":false,"prevItem":{"title":"Apache Doris + MCP: The Real-Time Analytical Data Platform for the Agentic AI Era","permalink":"/blog/real-time-analytical-data-platform-for-the-agentic-ai-era"},"nextItem":{"title":"Stability at Scale: How Apache Doris Handles Pressure","permalink":"/blog/stability-at-scale-how-apache-doris-handles-pressure"}},"content":"import { BlogLink } from \'../src/components/blogs/components/blog-link\';\\nimport { SeeMore } from \'../src/components/blogs/components/see-more\';\\n\\n> <BlogLink rel=\\"noopener noreferrer\\" target=\'_blank\' href=\'https://www.velodb.io/blog/1446\'>One of the world\'s biggest telecommunication companies replaced the Lambda architecture with a unified real-time & batch data platform powered by Apache Doris for its 5G Fully-Connected Factory. Leveraging Doris\'s federated query capabilities, this platform established a unified query gateway and simplified data pipelines, significantly reducing storage costs, improving data freshness, and boosting query performance and development efficiency. Currently, Doris handles 70% of real-time data ingestion and 90% of real-time queries.<SeeMore /></BlogLink>"},{"id":"/stability-at-scale-how-apache-doris-handles-pressure","metadata":{"permalink":"/blog/stability-at-scale-how-apache-doris-handles-pressure","source":"@site/blog/stability-at-scale-how-apache-doris-handles-pressure.mdx","title":"Stability at Scale: How Apache Doris Handles Pressure","description":"This article takes you under the hood of Apache Doris, a popular real-time data warehouse, to see how it tackles stability challenges: managing massive data volumes and high concurrency, handling diverse analytical workloads, and coping with the pressure of rapid iteration. We\'ll also highlight engineering practices that any team can adapt to make their systems more resilient","date":"2025-08-08T00:00:00.000Z","tags":[{"inline":true,"label":"Tech Sharing","permalink":"/blog/tags/tech-sharing"}],"hasTruncateMarker":false,"authors":[{"name":"velodb.io \xb7 VeloDB Engineering Team","key":null,"page":null}],"frontMatter":{"title":"Stability at Scale: How Apache Doris Handles Pressure","summary":"This article takes you under the hood of Apache Doris, a popular real-time data warehouse, to see how it tackles stability challenges: managing massive data volumes and high concurrency, handling diverse analytical workloads, and coping with the pressure of rapid iteration. We\'ll also highlight engineering practices that any team can adapt to make their systems more resilient.","description":"This article takes you under the hood of Apache Doris, a popular real-time data warehouse, to see how it tackles stability challenges: managing massive data volumes and high concurrency, handling diverse analytical workloads, and coping with the pressure of rapid iteration. We\'ll also highlight engineering practices that any team can adapt to make their systems more resilient","date":"2025-08-08","author":"velodb.io \xb7 VeloDB Engineering Team","externalLink":"https://www.velodb.io/blog/1449","tags":["Tech Sharing"],"image":"/images/blogs/stability-at-scale-how-apache-doris-handles-pressure.JPEG"},"unlisted":false,"prevItem":{"title":"Apache Doris Empowered 5G Fully-Connected Factory with A Unified Real-time & Batch Data Platform","permalink":"/blog/apache-doris-empowered-5G-fully-connected-factory-with-a-unified-real-time-data-platform"},"nextItem":{"title":"Journey of telecom giant from ClickHouse to Apache Doris: 13PB in one table","permalink":"/blog/telecom-giant-journey-from-clickhouse-to-apache-doris-13pb-in-one-table"}},"content":"import { BlogLink } from \'../src/components/blogs/components/blog-link\';\\nimport { SeeMore } from \'../src/components/blogs/components/see-more\';\\n\\n> <BlogLink rel=\\"noopener noreferrer\\" target=\'_blank\' href=\'https://www.velodb.io/blog/1449\'>This article takes you under the hood of Apache Doris, a popular real-time data warehouse, to see how it tackles stability challenges: managing massive data volumes and high concurrency, handling diverse analytical workloads, and coping with the pressure of rapid iteration. We\'ll also highlight engineering practices that any team can adapt to make their systems more resilient.<SeeMore /></BlogLink>"},{"id":"/telecom-giant-journey-from-clickhouse-to-apache-doris-13pb-in-one-table","metadata":{"permalink":"/blog/telecom-giant-journey-from-clickhouse-to-apache-doris-13pb-in-one-table","source":"@site/blog/telecom-giant-journey-from-clickhouse-to-apache-doris-13pb-in-one-table.mdx","title":"Journey of telecom giant from ClickHouse to Apache Doris: 13PB in one table","description":"An enterprise big data platform of leading telecommunication company, StreamCloud, chose Apache Doris as its core database solution for ingesting and querying trillions of incremental data daily. Currently, this solution has been deployed in more than ten production scenarios. The largest cluster is deployed on 117 high-performance server nodes and has been operating stably for over six months. Its single table contains over 13 petabytes of raw data and 534 trillion records, maintaining a daily data ingestion of about 145TB and peak loads of about 158TB during holidays.","date":"2025-08-04T00:00:00.000Z","tags":[{"inline":true,"label":"Tech Sharing","permalink":"/blog/tags/tech-sharing"}],"hasTruncateMarker":false,"authors":[{"name":"velodb.io \xb7 VeloDB Engineering Team","key":null,"page":null}],"frontMatter":{"title":"Journey of telecom giant from ClickHouse to Apache Doris: 13PB in one table","summary":"An enterprise big data platform of leading telecommunication company, StreamCloud, chose Apache Doris as its core database solution for ingesting and querying trillions of incremental data daily. Currently, this solution has been deployed in more than ten production scenarios. The largest cluster is deployed on 117 high-performance server nodes and has been operating stably for over six months. Its single table contains over 13 petabytes of raw data and 534 trillion records, maintaining a daily data ingestion of about 145TB and peak loads of about 158TB during holidays.","description":"An enterprise big data platform of leading telecommunication company, StreamCloud, chose Apache Doris as its core database solution for ingesting and querying trillions of incremental data daily. Currently, this solution has been deployed in more than ten production scenarios. The largest cluster is deployed on 117 high-performance server nodes and has been operating stably for over six months. Its single table contains over 13 petabytes of raw data and 534 trillion records, maintaining a daily data ingestion of about 145TB and peak loads of about 158TB during holidays.","date":"2025-08-04","author":"velodb.io \xb7 VeloDB Engineering Team","externalLink":"https://www.velodb.io/blog/1440","tags":["Tech Sharing"],"image":"/images/blogs/telecom-giant-journey-from-clickhouse-to-apache-doris-13pb-in-one-table.jpg"},"unlisted":false,"prevItem":{"title":"Stability at Scale: How Apache Doris Handles Pressure","permalink":"/blog/stability-at-scale-how-apache-doris-handles-pressure"},"nextItem":{"title":"From Snowflake to Apache Doris: real-time analytics with 80% cost savings","permalink":"/blog/from-snowflake-to-apache-doris-real-time-analytics-with-80-percentage-cost-savings"}},"content":"import { BlogLink } from \'../src/components/blogs/components/blog-link\';\\nimport { SeeMore } from \'../src/components/blogs/components/see-more\';\\n\\n> <BlogLink rel=\\"noopener noreferrer\\" target=\'_blank\' href=\'https://www.velodb.io/blog/1440\'>An enterprise big data platform of leading telecommunication company, StreamCloud, chose Apache Doris as its core database solution for ingesting and querying trillions of incremental data daily. Currently, this solution has been deployed in more than ten production scenarios. The largest cluster is deployed on 117 high-performance server nodes and has been operating stably for over six months. Its single table contains over 13 petabytes of raw data and 534 trillion records, maintaining a daily data ingestion of about 145TB and peak loads of about 158TB during holidays.<SeeMore /></BlogLink>"},{"id":"/from-snowflake-to-apache-doris-real-time-analytics-with-80-percentage-cost-savings","metadata":{"permalink":"/blog/from-snowflake-to-apache-doris-real-time-analytics-with-80-percentage-cost-savings","source":"@site/blog/from-snowflake-to-apache-doris-real-time-analytics-with-80-percentage-cost-savings.mdx","title":"From Snowflake to Apache Doris: real-time analytics with 80% cost savings","description":"Parth Soni, a Senior Data Engineer at Planet, completed a real-world migration from Snowflake to Apache Doris. Following the migration, his team reduced their monthly costs from $25K to $5K, while gaining truly real-time data ingestion, 5x faster query performance across various scenarios, and up to 90x speed improvements for large-table analytics.","date":"2025-07-30T00:00:00.000Z","tags":[{"inline":true,"label":"Best Practice","permalink":"/blog/tags/best-practice"}],"hasTruncateMarker":false,"authors":[{"name":"velodb.io \xb7 VeloDB Engineering Team","key":null,"page":null}],"frontMatter":{"title":"From Snowflake to Apache Doris: real-time analytics with 80% cost savings","summary":"Parth Soni, a Senior Data Engineer at Planet, completed a real-world migration from Snowflake to Apache Doris. Following the migration, his team reduced their monthly costs from $25K to $5K, while gaining truly real-time data ingestion, 5x faster query performance across various scenarios, and up to 90x speed improvements for large-table analytics.","description":"Parth Soni, a Senior Data Engineer at Planet, completed a real-world migration from Snowflake to Apache Doris. Following the migration, his team reduced their monthly costs from $25K to $5K, while gaining truly real-time data ingestion, 5x faster query performance across various scenarios, and up to 90x speed improvements for large-table analytics.","date":"2025-07-30","author":"velodb.io \xb7 VeloDB Engineering Team","externalLink":"https://www.velodb.io/blog/1435","tags":["Best Practice"],"image":"/images/snowflake-to-doris.jpg"},"unlisted":false,"prevItem":{"title":"Journey of telecom giant from ClickHouse to Apache Doris: 13PB in one table","permalink":"/blog/telecom-giant-journey-from-clickhouse-to-apache-doris-13pb-in-one-table"},"nextItem":{"title":"Leading AI Company Revamped Observability: 10x Faster Queries & 83% Cost Savings with Apache Doris","permalink":"/blog/leading-ai-company-revamped-observability-with-apache-doris"}},"content":"import { BlogLink } from \'../src/components/blogs/components/blog-link\';\\nimport { SeeMore } from \'../src/components/blogs/components/see-more\';\\n\\n> <BlogLink rel=\\"noopener noreferrer\\" target=\'_blank\' href=\'https://www.velodb.io/blog/1435\'>Parth Soni, a Senior Data Engineer at Planet, completed a real-world migration from Snowflake to Apache Doris. Following the migration, his team reduced their monthly costs from $25K to $5K, while gaining truly real-time data ingestion, 5x faster query performance across various scenarios, and up to 90x speed improvements for large-table analytics.<SeeMore /></BlogLink>"},{"id":"/leading-ai-company-revamped-observability-with-apache-doris","metadata":{"permalink":"/blog/leading-ai-company-revamped-observability-with-apache-doris","source":"@site/blog/leading-ai-company-revamped-observability-with-apache-doris.mdx","title":"Leading AI Company Revamped Observability: 10x Faster Queries & 83% Cost Savings with Apache Doris","description":"A leading AI and Speech Technology company upgraded its observability platform by replacing Elasticsearch and Loki with Apache Doris. This transition addressed critical issues of high storage costs with Elasticsearch and slow query performance with Loki.","date":"2025-07-28T00:00:00.000Z","tags":[{"inline":true,"label":"Best Practice","permalink":"/blog/tags/best-practice"}],"hasTruncateMarker":false,"authors":[{"name":"velodb.io \xb7 VeloDB Engineering Team","key":null,"page":null}],"frontMatter":{"title":"Leading AI Company Revamped Observability: 10x Faster Queries & 83% Cost Savings with Apache Doris","summary":"A leading AI and Speech Technology company upgraded its observability platform by replacing Elasticsearch and Loki with Apache Doris. This transition addressed critical issues of high storage costs with Elasticsearch and slow query performance with Loki.","description":"A leading AI and Speech Technology company upgraded its observability platform by replacing Elasticsearch and Loki with Apache Doris. This transition addressed critical issues of high storage costs with Elasticsearch and slow query performance with Loki.","date":"2025-07-28","author":"velodb.io \xb7 VeloDB Engineering Team","externalLink":"https://www.velodb.io/blog/1434","tags":["Best Practice"],"image":"/images/leading-ai-company.jpg"},"unlisted":false,"prevItem":{"title":"From Snowflake to Apache Doris: real-time analytics with 80% cost savings","permalink":"/blog/from-snowflake-to-apache-doris-real-time-analytics-with-80-percentage-cost-savings"},"nextItem":{"title":"How SF Technology Replaced Presto with Apache Doris to Achieve 3x Faster Queries and 48% Cost Savings","permalink":"/blog/sf-technology-replaced-presto-with-apache-doris"}},"content":"import { BlogLink } from \'../src/components/blogs/components/blog-link\';\\nimport { SeeMore } from \'../src/components/blogs/components/see-more\';\\n\\n> <BlogLink rel=\\"noopener noreferrer\\" target=\'_blank\' href=\'https://www.velodb.io/blog/1434\'>A leading AI and Speech Technology company upgraded its observability platform by replacing Elasticsearch and Loki with Apache Doris. This transition addressed critical issues of high storage costs with Elasticsearch and slow query performance with Loki.<SeeMore /></BlogLink>"},{"id":"/sf-technology-replaced-presto-with-apache-doris","metadata":{"permalink":"/blog/sf-technology-replaced-presto-with-apache-doris","source":"@site/blog/sf-technology-replaced-presto-with-apache-doris.mdx","title":"How SF Technology Replaced Presto with Apache Doris to Achieve 3x Faster Queries and 48% Cost Savings","description":"SF Technology migrated its primary BI analytics platform from Presto to Apache Doris, supporting over 1 million daily queries. This strategic move solved critical issues with query speed, stability, and high costs.","date":"2025-07-27T00:00:00.000Z","tags":[{"inline":true,"label":"Best Practice","permalink":"/blog/tags/best-practice"}],"hasTruncateMarker":false,"authors":[{"name":"velodb.io \xb7 VeloDB Engineering Team","key":null,"page":null}],"frontMatter":{"title":"How SF Technology Replaced Presto with Apache Doris to Achieve 3x Faster Queries and 48% Cost Savings","summary":"SF Technology migrated its primary BI analytics platform from Presto to Apache Doris, supporting over 1 million daily queries. This strategic move solved critical issues with query speed, stability, and high costs.","description":"SF Technology migrated its primary BI analytics platform from Presto to Apache Doris, supporting over 1 million daily queries. This strategic move solved critical issues with query speed, stability, and high costs.","date":"2025-07-27","author":"velodb.io \xb7 VeloDB Engineering Team","externalLink":"https://www.velodb.io/blog/1433","tags":["Best Practice"],"image":"/images/sf-technology.jpg"},"unlisted":false,"prevItem":{"title":"Leading AI Company Revamped Observability: 10x Faster Queries & 83% Cost Savings with Apache Doris","permalink":"/blog/leading-ai-company-revamped-observability-with-apache-doris"},"nextItem":{"title":"Why Apache Doris excels at OLAP: a deep dive into real-time update technology","permalink":"/blog/real-time-update-deep-dive"}},"content":"import { BlogLink } from \'../src/components/blogs/components/blog-link\';\\nimport { SeeMore } from \'../src/components/blogs/components/see-more\';\\n\\n> <BlogLink rel=\\"noopener noreferrer\\" target=\'_blank\' href=\'https://www.velodb.io/blog/1433\'>SF Technology migrated its primary BI analytics platform from Presto to Apache Doris, supporting over 1 million daily queries. This strategic move solved critical issues with query speed, stability, and high costs.<SeeMore /></BlogLink>"},{"id":"/real-time-update-deep-dive","metadata":{"permalink":"/blog/real-time-update-deep-dive","source":"@site/blog/real-time-update-deep-dive.mdx","title":"Why Apache Doris excels at OLAP: a deep dive into real-time update technology","description":"Apache Doris is all about making real-time analytics faster, smoother, and more adaptable for modern data needs. For primary key tables, Apache Doris delivers seamless UPSERT semantics, leveraging primary key indexes and a mark-for-deletion mechanism to ensure top-notch write performance and low-latency queries. Plus, its user-defined conflict resolution boosts concurrency for real-time writes, while fast Schema Change functionality keeps data flows uninterrupted. Flexible column updates and options further empower a wide range of real-time use cases with ease.","date":"2025-07-25T00:00:00.000Z","tags":[{"inline":true,"label":"Tech Sharing","permalink":"/blog/tags/tech-sharing"}],"hasTruncateMarker":false,"authors":[{"name":"velodb.io \xb7 VeloDB Engineering Team","key":null,"page":null}],"frontMatter":{"title":"Why Apache Doris excels at OLAP: a deep dive into real-time update technology","summary":"Apache Doris is all about making real-time analytics faster, smoother, and more adaptable for modern data needs. For primary key tables, Apache Doris delivers seamless UPSERT semantics, leveraging primary key indexes and a mark-for-deletion mechanism to ensure top-notch write performance and low-latency queries. Plus, its user-defined conflict resolution boosts concurrency for real-time writes, while fast Schema Change functionality keeps data flows uninterrupted. Flexible column updates and options further empower a wide range of real-time use cases with ease.","description":"Apache Doris is all about making real-time analytics faster, smoother, and more adaptable for modern data needs. For primary key tables, Apache Doris delivers seamless UPSERT semantics, leveraging primary key indexes and a mark-for-deletion mechanism to ensure top-notch write performance and low-latency queries. Plus, its user-defined conflict resolution boosts concurrency for real-time writes, while fast Schema Change functionality keeps data flows uninterrupted. Flexible column updates and options further empower a wide range of real-time use cases with ease.","date":"2025-07-25","author":"velodb.io \xb7 VeloDB Engineering Team","externalLink":"https://www.velodb.io/blog/1431","tags":["Tech Sharing"],"image":"/images/real-time-update-banner.jpg"},"unlisted":false,"prevItem":{"title":"How SF Technology Replaced Presto with Apache Doris to Achieve 3x Faster Queries and 48% Cost Savings","permalink":"/blog/sf-technology-replaced-presto-with-apache-doris"},"nextItem":{"title":"Kwai Replaced ClickHouse with Apache Doris for a Smart, Unified Lakehouse Architecture","permalink":"/blog/kwai-replace-clickhouse-with-apache-doris-for-unified-lakehouse"}},"content":"import { BlogLink } from \'../src/components/blogs/components/blog-link\';\\nimport { SeeMore } from \'../src/components/blogs/components/see-more\';\\n\\n> <BlogLink rel=\\"noopener noreferrer\\" target=\'_blank\' href=\'https://www.velodb.io/blog/1431\'>Apache Doris is all about making real-time analytics faster, smoother, and more adaptable for modern data needs. For primary key tables, Apache Doris delivers seamless UPSERT semantics, leveraging primary key indexes and a mark-for-deletion mechanism to ensure top-notch write performance and low-latency queries. Plus, its user-defined conflict resolution boosts concurrency for real-time writes, while fast Schema Change functionality keeps data flows uninterrupted. Flexible column updates and options further empower a wide range of real-time use cases with ease.<SeeMore /></BlogLink>"},{"id":"/kwai-replace-clickhouse-with-apache-doris-for-unified-lakehouse","metadata":{"permalink":"/blog/kwai-replace-clickhouse-with-apache-doris-for-unified-lakehouse","source":"@site/blog/kwai-replace-clickhouse-with-apache-doris-for-unified-lakehouse.mdx","title":"Kwai Replaced ClickHouse with Apache Doris for a Smart, Unified Lakehouse Architecture","description":"Kwai, a leading social media platform, has replaced ClickHouse with Apache Doris to upgrade its OLAP system, now handling nearly 1 billion daily queries. This move shifts them from a complex lake-warehouse separation model to a unified lakehouse architecture. The new system leverages Doris\'s direct lake access and an intelligent auto-materialization service to solve critical issues of data redundancy, resource contention, and complex governance.","date":"2025-07-19T00:00:00.000Z","tags":[{"inline":true,"label":"Best Practice","permalink":"/blog/tags/best-practice"}],"hasTruncateMarker":false,"authors":[{"name":"velodb.io \xb7 VeloDB Engineering Team","key":null,"page":null}],"frontMatter":{"title":"Kwai Replaced ClickHouse with Apache Doris for a Smart, Unified Lakehouse Architecture","summary":"Kwai, a leading social media platform, has replaced ClickHouse with Apache Doris to upgrade its OLAP system, now handling nearly 1 billion daily queries. This move shifts them from a complex lake-warehouse separation model to a unified lakehouse architecture. The new system leverages Doris\'s direct lake access and an intelligent auto-materialization service to solve critical issues of data redundancy, resource contention, and complex governance.","description":"Kwai, a leading social media platform, has replaced ClickHouse with Apache Doris to upgrade its OLAP system, now handling nearly 1 billion daily queries. This move shifts them from a complex lake-warehouse separation model to a unified lakehouse architecture. The new system leverages Doris\'s direct lake access and an intelligent auto-materialization service to solve critical issues of data redundancy, resource contention, and complex governance.","date":"2025-07-19","author":"velodb.io \xb7 VeloDB Engineering Team","externalLink":"https://www.velodb.io/blog/1432","tags":["Best Practice"],"image":"/images/kwai.jpg"},"unlisted":false,"prevItem":{"title":"Why Apache Doris excels at OLAP: a deep dive into real-time update technology","permalink":"/blog/real-time-update-deep-dive"},"nextItem":{"title":"From ClickHouse to Apache Doris: Powering Trillion-Log-Scale Analytics at a Leading Music Streaming Service","permalink":"/blog/from-clickhouse-to-doris-trillion-log-scale-analytics"}},"content":"import { BlogLink } from \'../src/components/blogs/components/blog-link\';\\nimport { SeeMore } from \'../src/components/blogs/components/see-more\';\\n\\n> <BlogLink rel=\\"noopener noreferrer\\" target=\'_blank\' href=\'https://www.velodb.io/blog/1432\'>Kwai, a leading social media platform, has replaced ClickHouse with Apache Doris to upgrade its OLAP system, now handling nearly 1 billion daily queries. This move shifts them from a complex lake-warehouse separation model to a unified lakehouse architecture. The new system leverages Doris\'s direct lake access and an intelligent auto-materialization service to solve critical issues of data redundancy, resource contention, and complex governance.<SeeMore /></BlogLink>"},{"id":"/from-clickhouse-to-doris-trillion-log-scale-analytics","metadata":{"permalink":"/blog/from-clickhouse-to-doris-trillion-log-scale-analytics","source":"@site/blog/from-clickhouse-to-doris-trillion-log-scale-analytics.mdx","title":"From ClickHouse to Apache Doris: Powering Trillion-Log-Scale Analytics at a Leading Music Streaming Service","description":"A major music streaming platform successfully migrated its massive 2PB log analytics from ClickHouse to Apache Doris, achieving up to 7x faster searches, 30% lower P99 query latency, 2.5x higher concurrency, and significant operational savings.","date":"2025-07-17T00:00:00.000Z","tags":[{"inline":true,"label":"Tech Sharing","permalink":"/blog/tags/tech-sharing"}],"hasTruncateMarker":false,"authors":[{"name":"velodb.io \xb7 VeloDB Engineering Team","key":null,"page":null}],"frontMatter":{"title":"From ClickHouse to Apache Doris: Powering Trillion-Log-Scale Analytics at a Leading Music Streaming Service","summary":"A major music streaming platform successfully migrated its massive 2PB log analytics from ClickHouse to Apache Doris, achieving up to 7x faster searches, 30% lower P99 query latency, 2.5x higher concurrency, and significant operational savings.","description":"A major music streaming platform successfully migrated its massive 2PB log analytics from ClickHouse to Apache Doris, achieving up to 7x faster searches, 30% lower P99 query latency, 2.5x higher concurrency, and significant operational savings.","date":"2025-07-17","author":"velodb.io \xb7 VeloDB Engineering Team","externalLink":"https://www.velodb.io/blog/1429","tags":["Tech Sharing"],"image":"/images/blogs/from-clickhouse-to-doris-trillion-log-scale-analytics.jpg"},"unlisted":false,"prevItem":{"title":"Kwai Replaced ClickHouse with Apache Doris for a Smart, Unified Lakehouse Architecture","permalink":"/blog/kwai-replace-clickhouse-with-apache-doris-for-unified-lakehouse"},"nextItem":{"title":"Building Real-Time Lakehouse with Apache Doris: A Practical Guide","permalink":"/blog/building-real-time-lakehouse-with-apache-doris"}},"content":"import { BlogLink } from \'../src/components/blogs/components/blog-link\';\\nimport { SeeMore } from \'../src/components/blogs/components/see-more\';\\n\\n> <BlogLink rel=\\"noopener noreferrer\\" target=\'_blank\' href=\'https://www.velodb.io/blog/1429\'>A major music streaming platform successfully migrated its massive 2PB log analytics from ClickHouse to Apache Doris, achieving up to 7x faster searches, 30% lower P99 query latency, 2.5x higher concurrency, and significant operational savings.<SeeMore /></BlogLink>"},{"id":"/building-real-time-lakehouse-with-apache-doris","metadata":{"permalink":"/blog/building-real-time-lakehouse-with-apache-doris","source":"@site/blog/building-real-time-lakehouse-with-apache-doris.mdx","title":"Building Real-Time Lakehouse with Apache Doris: A Practical Guide","description":"The Lakehouse is a data management paradigm that combines the advantages of data lakes and data warehouses. Apache Doris advances this concept with its core philosophies of Boundless Data, Seamless Lakehouse. This article takes a deeper dive into its typical application scenarios to help readers better understand and apply its capabilities.","date":"2025-07-15T00:00:00.000Z","tags":[{"inline":true,"label":"Tech Sharing","permalink":"/blog/tags/tech-sharing"}],"hasTruncateMarker":false,"authors":[{"name":"velodb.io \xb7 VeloDB Engineering Team","key":null,"page":null}],"frontMatter":{"title":"Building Real-Time Lakehouse with Apache Doris: A Practical Guide","summary":"The Lakehouse is a data management paradigm that combines the advantages of data lakes and data warehouses. Apache Doris advances this concept with its core philosophies of Boundless Data, Seamless Lakehouse. This article takes a deeper dive into its typical application scenarios to help readers better understand and apply its capabilities.","description":"The Lakehouse is a data management paradigm that combines the advantages of data lakes and data warehouses. Apache Doris advances this concept with its core philosophies of Boundless Data, Seamless Lakehouse. This article takes a deeper dive into its typical application scenarios to help readers better understand and apply its capabilities.","date":"2025-07-15","author":"velodb.io \xb7 VeloDB Engineering Team","externalLink":"https://www.velodb.io/blog/1428","tags":["Tech Sharing"],"image":"/images/blogs/building-real-time-lakehouse-with-apache-doris.jpg"},"unlisted":false,"prevItem":{"title":"From ClickHouse to Apache Doris: Powering Trillion-Log-Scale Analytics at a Leading Music Streaming Service","permalink":"/blog/from-clickhouse-to-doris-trillion-log-scale-analytics"},"nextItem":{"title":"From Elasticsearch to Apache Doris: How a Leading Payment Platform Upgraded its Financial Security Analytics, Boosting Queries by 56x","permalink":"/blog/from-elasticsearch-to-doris-boosting-queries-by-56x"}},"content":"import { BlogLink } from \'../src/components/blogs/components/blog-link\';\\nimport { SeeMore } from \'../src/components/blogs/components/see-more\';\\n\\n> <BlogLink rel=\\"noopener noreferrer\\" target=\'_blank\' href=\'https://www.velodb.io/blog/1428\'>The Lakehouse is a data management paradigm that combines the advantages of data lakes and data warehouses. Apache Doris advances this concept with its core philosophies of Boundless Data, Seamless Lakehouse. This article takes a deeper dive into its typical application scenarios to help readers better understand and apply its capabilities.<SeeMore /></BlogLink>"},{"id":"/from-elasticsearch-to-doris-boosting-queries-by-56x","metadata":{"permalink":"/blog/from-elasticsearch-to-doris-boosting-queries-by-56x","source":"@site/blog/from-elasticsearch-to-doris-boosting-queries-by-56x.mdx","title":"From Elasticsearch to Apache Doris: How a Leading Payment Platform Upgraded its Financial Security Analytics, Boosting Queries by 56x","description":"A leading payment platform, handling over 600 million daily security events, replaced its Elasticsearch and Hudi stack with Apache Doris. This move achieved up to 56x faster queries, 50% lower storage costs, and 58% higher write throughput, while simplifying architecture and boosting developer efficiency.","date":"2025-07-13T00:00:00.000Z","tags":[{"inline":true,"label":"Tech Sharing","permalink":"/blog/tags/tech-sharing"}],"hasTruncateMarker":false,"authors":[{"name":"velodb.io \xb7 VeloDB Engineering Team","key":null,"page":null}],"frontMatter":{"title":"From Elasticsearch to Apache Doris: How a Leading Payment Platform Upgraded its Financial Security Analytics, Boosting Queries by 56x","summary":"A leading payment platform, handling over 600 million daily security events, replaced its Elasticsearch and Hudi stack with Apache Doris. This move achieved up to 56x faster queries, 50% lower storage costs, and 58% higher write throughput, while simplifying architecture and boosting developer efficiency.","description":"A leading payment platform, handling over 600 million daily security events, replaced its Elasticsearch and Hudi stack with Apache Doris. This move achieved up to 56x faster queries, 50% lower storage costs, and 58% higher write throughput, while simplifying architecture and boosting developer efficiency.","date":"2025-07-13","author":"velodb.io \xb7 VeloDB Engineering Team","externalLink":"https://www.velodb.io/blog/1427","tags":["Tech Sharing"],"image":"/images/blogs/from-elasticsearch-to-doris-boosting-queries-by-56x.jpg"},"unlisted":false,"prevItem":{"title":"Building Real-Time Lakehouse with Apache Doris: A Practical Guide","permalink":"/blog/building-real-time-lakehouse-with-apache-doris"},"nextItem":{"title":"1 Billion JSON Records, 1-Second Query Response","permalink":"/blog/1-billion-json-records-1-second-query-response"}},"content":"import { BlogLink } from \'../src/components/blogs/components/blog-link\';\\nimport { SeeMore } from \'../src/components/blogs/components/see-more\';\\n\\n> <BlogLink rel=\\"noopener noreferrer\\" target=\'_blank\' href=\'https://www.velodb.io/blog/1427\'>A leading payment platform, handling over 600 million daily security events, replaced its Elasticsearch and Hudi stack with Apache Doris. This move achieved up to 56x faster queries, 50% lower storage costs, and 58% higher write throughput, while simplifying architecture and boosting developer efficiency.<SeeMore /></BlogLink>"},{"id":"/1-billion-json-records-1-second-query-response","metadata":{"permalink":"/blog/1-billion-json-records-1-second-query-response","source":"@site/blog/1-billion-json-records-1-second-query-response.mdx","title":"1 Billion JSON Records, 1-Second Query Response","description":"Who reigns over JSONBench? Apache Doris ranks among the top-performers, second only to two versions of Clickhouse (the maintainer of JSONBench itself). After some simple tuning, Apache Doris outperforms ClickHouse by 39%.","date":"2025-07-02T00:00:00.000Z","tags":[{"inline":true,"label":"Tech Sharing","permalink":"/blog/tags/tech-sharing"}],"hasTruncateMarker":false,"authors":[{"name":"velodb.io \xb7 Xiaolei, Apache Doris Committer","key":null,"page":null}],"frontMatter":{"title":"1 Billion JSON Records, 1-Second Query Response","summary":"Who reigns over JSONBench? Apache Doris ranks among the top-performers, second only to two versions of Clickhouse (the maintainer of JSONBench itself). After some simple tuning, Apache Doris outperforms ClickHouse by 39%.","description":"Who reigns over JSONBench? Apache Doris ranks among the top-performers, second only to two versions of Clickhouse (the maintainer of JSONBench itself). After some simple tuning, Apache Doris outperforms ClickHouse by 39%.","date":"2025-07-02","author":"velodb.io \xb7 Xiaolei, Apache Doris Committer","externalLink":"https://www.velodb.io/blog/1422","tags":["Tech Sharing"],"image":"/images/blogs/1-billion-json-records-1-second-query-response.jpg"},"unlisted":false,"prevItem":{"title":"From Elasticsearch to Apache Doris: How a Leading Payment Platform Upgraded its Financial Security Analytics, Boosting Queries by 56x","permalink":"/blog/from-elasticsearch-to-doris-boosting-queries-by-56x"},"nextItem":{"title":"EMQX now supports real-time data ingestion into Apache Doris for efficient IoT analytics","permalink":"/blog/emqx-apache-doris-ecosystem-for-iot-analytics"}},"content":"import { BlogLink } from \'../src/components/blogs/components/blog-link\';\\nimport { SeeMore } from \'../src/components/blogs/components/see-more\';\\n\\n> <BlogLink rel=\\"noopener noreferrer\\" target=\'_blank\' href=\'https://www.velodb.io/blog/1422\'>Who reigns over JSONBench? Apache Doris ranks among the top-performers, second only to two versions of Clickhouse (the maintainer of JSONBench itself). After some simple tuning, Apache Doris outperforms ClickHouse by 39%.<SeeMore /></BlogLink>"},{"id":"/emqx-apache-doris-ecosystem-for-iot-analytics","metadata":{"permalink":"/blog/emqx-apache-doris-ecosystem-for-iot-analytics","source":"@site/blog/emqx-apache-doris-ecosystem-for-iot-analytics.mdx","title":"EMQX now supports real-time data ingestion into Apache Doris for efficient IoT analytics","description":"Apache Doris data integration is an out-of-the-box feature in EMQX, which enables complex business development through simple configuration. In a typical IoT application, EMQX, as the IoT platform, is responsible for device connection and transmitting messages. Apache Doris, as the data storage platform, is responsible for storing device status and metadata, as well as message data storage and data analysis. Using EMQX and Apache Doris, users can build high-performance, reliable, and scalable IoT applications that deliver an end-to-end solution from data acquisition through analytics to actionable intelligence.","date":"2025-07-02T00:00:00.000Z","tags":[{"inline":true,"label":"Tech Sharing","permalink":"/blog/tags/tech-sharing"}],"hasTruncateMarker":false,"authors":[{"name":"velodb.io \xb7 VeloDB Engineering Team","key":null,"page":null}],"frontMatter":{"title":"EMQX now supports real-time data ingestion into Apache Doris for efficient IoT analytics","summary":"Apache Doris data integration is an out-of-the-box feature in EMQX, which enables complex business development through simple configuration. In a typical IoT application, EMQX, as the IoT platform, is responsible for device connection and transmitting messages. Apache Doris, as the data storage platform, is responsible for storing device status and metadata, as well as message data storage and data analysis. Using EMQX and Apache Doris, users can build high-performance, reliable, and scalable IoT applications that deliver an end-to-end solution from data acquisition through analytics to actionable intelligence.","description":"Apache Doris data integration is an out-of-the-box feature in EMQX, which enables complex business development through simple configuration. In a typical IoT application, EMQX, as the IoT platform, is responsible for device connection and transmitting messages. Apache Doris, as the data storage platform, is responsible for storing device status and metadata, as well as message data storage and data analysis. Using EMQX and Apache Doris, users can build high-performance, reliable, and scalable IoT applications that deliver an end-to-end solution from data acquisition through analytics to actionable intelligence.","date":"2025-07-02","author":"velodb.io \xb7 VeloDB Engineering Team","externalLink":"https://www.velodb.io/blog/1422","tags":["Tech Sharing"],"image":"/images/blogs/emqx-apache-doris-ecosystem-for-iot-analytics.png"},"unlisted":false,"prevItem":{"title":"1 Billion JSON Records, 1-Second Query Response","permalink":"/blog/1-billion-json-records-1-second-query-response"},"nextItem":{"title":"The data lakehouse evolution: why Apache Doris is leading the way","permalink":"/blog/apache-doris-the-data-lakehouse-evolution"}},"content":"import { BlogLink } from \'../src/components/blogs/components/blog-link\';\\nimport { SeeMore } from \'../src/components/blogs/components/see-more\';\\n\\n> <BlogLink rel=\\"noopener noreferrer\\" target=\'_blank\' href=\'https://www.velodb.io/blog/1422\'>Apache Doris data integration is an out-of-the-box feature in EMQX, which enables complex business development through simple configuration. In a typical IoT application, EMQX, as the IoT platform, is responsible for device connection and transmitting messages. Apache Doris, as the data storage platform, is responsible for storing device status and metadata, as well as message data storage and data analysis. Using EMQX and Apache Doris, users can build high-performance, reliable, and scalable IoT applications that deliver an end-to-end solution from data acquisition through analytics to actionable intelligence.<SeeMore /></BlogLink>"},{"id":"/apache-doris-the-data-lakehouse-evolution","metadata":{"permalink":"/blog/apache-doris-the-data-lakehouse-evolution","source":"@site/blog/apache-doris-the-data-lakehouse-evolution.mdx","title":"The data lakehouse evolution: why Apache Doris is leading the way","description":"As enterprises push forward with building a lakehouse architecture, they often face complex challenges. Apache Doris aims to facilitate this process. It offers easy data access (from Hive, Iceberg, Hudi, Paimon), an extensible connector framework (with Kudu, BigQuery, Delta Lake, Kafka, and Redis) and convenient cross-source data processing with high analytics performance. It also allows easy deployment, a rich set of data storage and management capabilities, and strong support for open table formats and file formats.","date":"2025-06-30T00:00:00.000Z","tags":[{"inline":true,"label":"Tech Sharing","permalink":"/blog/tags/tech-sharing"}],"hasTruncateMarker":false,"authors":[{"name":"velodb.io \xb7 Rayner Chen, Apache Doris PMC Chair","key":null,"page":null}],"frontMatter":{"title":"The data lakehouse evolution: why Apache Doris is leading the way","summary":"As enterprises push forward with building a lakehouse architecture, they often face complex challenges. Apache Doris aims to facilitate this process. It offers easy data access (from Hive, Iceberg, Hudi, Paimon), an extensible connector framework (with Kudu, BigQuery, Delta Lake, Kafka, and Redis) and convenient cross-source data processing with high analytics performance. It also allows easy deployment, a rich set of data storage and management capabilities, and strong support for open table formats and file formats.","description":"As enterprises push forward with building a lakehouse architecture, they often face complex challenges. Apache Doris aims to facilitate this process. It offers easy data access (from Hive, Iceberg, Hudi, Paimon), an extensible connector framework (with Kudu, BigQuery, Delta Lake, Kafka, and Redis) and convenient cross-source data processing with high analytics performance. It also allows easy deployment, a rich set of data storage and management capabilities, and strong support for open table formats and file formats.","date":"2025-06-30","author":"velodb.io \xb7 Rayner Chen, Apache Doris PMC Chair","externalLink":"https://www.velodb.io/blog/1411","tags":["Tech Sharing"],"image":"/images/blogs/apache-doris-the-data-lakehouse-evolution.jpg"},"unlisted":false,"prevItem":{"title":"EMQX now supports real-time data ingestion into Apache Doris for efficient IoT analytics","permalink":"/blog/emqx-apache-doris-ecosystem-for-iot-analytics"},"nextItem":{"title":"Community Voice | It\u2019s 2025: How Do You Choose Between Doris and ClickHouse?","permalink":"/blog/community-voice-2025-how-do-you-choose-between-doris-and-clickhouse"}},"content":"import { BlogLink } from \'../src/components/blogs/components/blog-link\';\\nimport { SeeMore } from \'../src/components/blogs/components/see-more\';\\n\\n> <BlogLink rel=\\"noopener noreferrer\\" target=\'_blank\' href=\'https://www.velodb.io/blog/1411\'>As enterprises push forward with building a lakehouse architecture, they often face complex challenges. Apache Doris aims to facilitate this process. It offers easy data access (from Hive, Iceberg, Hudi, Paimon), an extensible connector framework (with Kudu, BigQuery, Delta Lake, Kafka, and Redis) and convenient cross-source data processing with high analytics performance. It also allows easy deployment, a rich set of data storage and management capabilities, and strong support for open table formats and file formats.<SeeMore /></BlogLink>"},{"id":"/community-voice-2025-how-do-you-choose-between-doris-and-clickhouse","metadata":{"permalink":"/blog/community-voice-2025-how-do-you-choose-between-doris-and-clickhouse","source":"@site/blog/community-voice-2025-how-do-you-choose-between-doris-and-clickhouse.mdx","title":"Community Voice | It\u2019s 2025: How Do You Choose Between Doris and ClickHouse?","description":"Those in favor of ClickHouse argued for its superior performance, while those supporting Doris emphasized its comprehensive ecosystem and high usability.Ultimately, it took us nearly two months of comprehensive testing to make a decision\u2026","date":"2025-06-30T00:00:00.000Z","tags":[{"inline":true,"label":"Tech Sharing","permalink":"/blog/tags/tech-sharing"}],"hasTruncateMarker":false,"authors":[{"name":"Medium \xb7 Zen Hua","key":null,"page":null}],"frontMatter":{"title":"Community Voice | It\u2019s 2025: How Do You Choose Between Doris and ClickHouse?","summary":"Those in favor of ClickHouse argued for its superior performance, while those supporting Doris emphasized its comprehensive ecosystem and high usability.Ultimately, it took us nearly two months of comprehensive testing to make a decision\u2026","description":"Those in favor of ClickHouse argued for its superior performance, while those supporting Doris emphasized its comprehensive ecosystem and high usability.Ultimately, it took us nearly two months of comprehensive testing to make a decision\u2026","date":"2025-06-30","author":"Medium \xb7 Zen Hua","externalLink":"https://medium.com/@ith321.vip/its-2025-how-do-you-choose-between-doris-and-clickhouse-7d98456d9199","tags":["Tech Sharing"],"image":"/images/blogs/doris-vs-ck.jpeg"},"unlisted":false,"prevItem":{"title":"The data lakehouse evolution: why Apache Doris is leading the way","permalink":"/blog/apache-doris-the-data-lakehouse-evolution"},"nextItem":{"title":"Community Voice | Cut Costs and Boost Efficiency! How Doris\u2019s Unified Lakehouse Breaks Down Data Silos","permalink":"/blog/community-voice-cut-costs-and-boost-efficiency-how-doriss-unified-lakehouse-breaks-down-data-silos"}},"content":"import { BlogLink } from \'../src/components/blogs/components/blog-link\';\\nimport { SeeMore } from \'../src/components/blogs/components/see-more\';\\n\\n> <BlogLink rel=\\"noopener noreferrer\\" target=\'_blank\' href=\'https://medium.com/@ith321.vip/its-2025-how-do-you-choose-between-doris-and-clickhouse-7d98456d9199\'>Those in favor of ClickHouse argued for its superior performance, while those supporting Doris emphasized its comprehensive ecosystem and high usability.Ultimately, it took us nearly two months of comprehensive testing to make a decision\u2026<SeeMore /></BlogLink>"},{"id":"/community-voice-cut-costs-and-boost-efficiency-how-doriss-unified-lakehouse-breaks-down-data-silos","metadata":{"permalink":"/blog/community-voice-cut-costs-and-boost-efficiency-how-doriss-unified-lakehouse-breaks-down-data-silos","source":"@site/blog/community-voice-cut-costs-and-boost-efficiency-how-doriss-unified-lakehouse-breaks-down-data-silos.mdx","title":"Community Voice | Cut Costs and Boost Efficiency! How Doris\u2019s Unified Lakehouse Breaks Down Data Silos","description":"By leveraging Multi\u2011Catalog and an extensible connector framework, Doris seamlessly connects to all major data systems and formats \u2014 Hive, Iceberg, Hudi, Paimon, LakeSoul, Elasticsearch, MySQL, Oracle, SQL Server, and more.","date":"2025-06-29T00:00:00.000Z","tags":[{"inline":true,"label":"Tech Sharing","permalink":"/blog/tags/tech-sharing"}],"hasTruncateMarker":false,"authors":[{"name":"Medium \xb7 Hayden","key":null,"page":null}],"frontMatter":{"title":"Community Voice | Cut Costs and Boost Efficiency! How Doris\u2019s Unified Lakehouse Breaks Down Data Silos","summary":"By leveraging Multi\u2011Catalog and an extensible connector framework, Doris seamlessly connects to all major data systems and formats \u2014 Hive, Iceberg, Hudi, Paimon, LakeSoul, Elasticsearch, MySQL, Oracle, SQL Server, and more.","description":"By leveraging Multi\u2011Catalog and an extensible connector framework, Doris seamlessly connects to all major data systems and formats \u2014 Hive, Iceberg, Hudi, Paimon, LakeSoul, Elasticsearch, MySQL, Oracle, SQL Server, and more.","date":"2025-06-29","author":"Medium \xb7 Hayden","externalLink":"https://medium.com/@hhj19075/cut-costs-and-boost-efficiency-how-doriss-unified-lakehouse-breaks-down-data-silos-bfb1c9cd079a","tags":["Tech Sharing"],"image":"/images/blogs/break-data-silos.jpg"},"unlisted":false,"prevItem":{"title":"Community Voice | It\u2019s 2025: How Do You Choose Between Doris and ClickHouse?","permalink":"/blog/community-voice-2025-how-do-you-choose-between-doris-and-clickhouse"},"nextItem":{"title":"Community Voice | Breaking Down the Barriers of SQL Dialects and Building a Unified Data Query Ecosystem","permalink":"/blog/community-voice-doris-breaking-down-the-barriers-of-sql-dialects-and-building-a-unified-data-query-ecosystem"}},"content":"import { BlogLink } from \'../src/components/blogs/components/blog-link\';\\nimport { SeeMore } from \'../src/components/blogs/components/see-more\';\\n\\n> <BlogLink rel=\\"noopener noreferrer\\" target=\'_blank\' href=\'https://medium.com/@hhj19075/cut-costs-and-boost-efficiency-how-doriss-unified-lakehouse-breaks-down-data-silos-bfb1c9cd079a\'>By leveraging Multi\u2011Catalog and an extensible connector framework, Doris seamlessly connects to all major data systems and formats \u2014 Hive, Iceberg, Hudi, Paimon, LakeSoul, Elasticsearch, MySQL, Oracle, SQL Server, and more.<SeeMore /></BlogLink>"},{"id":"/community-voice-doris-breaking-down-the-barriers-of-sql-dialects-and-building-a-unified-data-query-ecosystem","metadata":{"permalink":"/blog/community-voice-doris-breaking-down-the-barriers-of-sql-dialects-and-building-a-unified-data-query-ecosystem","source":"@site/blog/community-voice-doris-breaking-down-the-barriers-of-sql-dialects-and-building-a-unified-data-query-ecosystem.mdx","title":"Community Voice | Breaking Down the Barriers of SQL Dialects and Building a Unified Data Query Ecosystem","description":"Apache Doris, with its robust SQL dialect compatibility capabilities, has shattered this barrier and constructed a unified data query ecosystem for users.","date":"2025-06-28T00:00:00.000Z","tags":[{"inline":true,"label":"Tech Sharing","permalink":"/blog/tags/tech-sharing"}],"hasTruncateMarker":false,"authors":[{"name":"dev.to \xb7 Darren XU","key":null,"page":null}],"frontMatter":{"title":"Community Voice | Breaking Down the Barriers of SQL Dialects and Building a Unified Data Query Ecosystem","summary":"Apache Doris, with its robust SQL dialect compatibility capabilities, has shattered this barrier and constructed a unified data query ecosystem for users.","description":"Apache Doris, with its robust SQL dialect compatibility capabilities, has shattered this barrier and constructed a unified data query ecosystem for users.","date":"2025-06-28","author":"dev.to \xb7 Darren XU","externalLink":"https://dev.to/darren_xu/doris-breaking-down-the-barriers-of-sql-dialects-and-building-a-unified-data-query-ecosystem-37k1","tags":["Tech Sharing"],"image":"/images/blogs/doris-sql-diaclects.jpg"},"unlisted":false,"prevItem":{"title":"Community Voice | Cut Costs and Boost Efficiency! How Doris\u2019s Unified Lakehouse Breaks Down Data Silos","permalink":"/blog/community-voice-cut-costs-and-boost-efficiency-how-doriss-unified-lakehouse-breaks-down-data-silos"},"nextItem":{"title":"Community Voice | Doris Lakehouse Integration: A New Approach to Data Analysis","permalink":"/blog/community-voice-doris-lakehouse-integration"}},"content":"import { BlogLink } from \'../src/components/blogs/components/blog-link\';\\nimport { SeeMore } from \'../src/components/blogs/components/see-more\';\\n\\n> <BlogLink rel=\\"noopener noreferrer\\" target=\'_blank\' href=\'https://dev.to/darren_xu/doris-breaking-down-the-barriers-of-sql-dialects-and-building-a-unified-data-query-ecosystem-37k1\'>Apache Doris, with its robust SQL dialect compatibility capabilities, has shattered this barrier and constructed a unified data query ecosystem for users.<SeeMore /></BlogLink>"},{"id":"/community-voice-doris-lakehouse-integration","metadata":{"permalink":"/blog/community-voice-doris-lakehouse-integration","source":"@site/blog/community-voice-doris-lakehouse-integration.mdx","title":"Community Voice | Doris Lakehouse Integration: A New Approach to Data Analysis","description":"Doris Lakehouse Integration bridges data lakes and warehouses and enables seamless access, faster queries, unified management, and greater data value.","date":"2025-06-27T00:00:00.000Z","tags":[{"inline":true,"label":"Tech Sharing","permalink":"/blog/tags/tech-sharing"}],"hasTruncateMarker":false,"authors":[{"name":"DZone \xb7 Darren XU","key":null,"page":null}],"frontMatter":{"title":"Community Voice | Doris Lakehouse Integration: A New Approach to Data Analysis","summary":"Doris Lakehouse Integration bridges data lakes and warehouses and enables seamless access, faster queries, unified management, and greater data value.","description":"Doris Lakehouse Integration bridges data lakes and warehouses and enables seamless access, faster queries, unified management, and greater data value.","date":"2025-06-27","author":"DZone \xb7 Darren XU","externalLink":"https://dzone.com/articles/doris-lakehouse-integration","tags":["Tech Sharing"],"image":"/images/blogs/doris-lakehouse-integration.jpg"},"unlisted":false,"prevItem":{"title":"Community Voice | Breaking Down the Barriers of SQL Dialects and Building a Unified Data Query Ecosystem","permalink":"/blog/community-voice-doris-breaking-down-the-barriers-of-sql-dialects-and-building-a-unified-data-query-ecosystem"},"nextItem":{"title":"Community Voice | Lakehouse: Manus? MCP? Let\u2019s Talk About Lakehouse and AI","permalink":"/blog/community-voice-lakehouse-manus-mcp-lets-talk-about-lakehouse"}},"content":"import { BlogLink } from \'../src/components/blogs/components/blog-link\';\\nimport { SeeMore } from \'../src/components/blogs/components/see-more\';\\n\\n> <BlogLink rel=\\"noopener noreferrer\\" target=\'_blank\' href=\'https://dzone.com/articles/doris-lakehouse-integration\'>Doris Lakehouse Integration bridges data lakes and warehouses and enables seamless access, faster queries, unified management, and greater data value.<SeeMore /></BlogLink>"},{"id":"/community-voice-lakehouse-manus-mcp-lets-talk-about-lakehouse","metadata":{"permalink":"/blog/community-voice-lakehouse-manus-mcp-lets-talk-about-lakehouse","source":"@site/blog/community-voice-lakehouse-manus-mcp-lets-talk-about-lakehouse.mdx","title":"Community Voice | Lakehouse: Manus? MCP? Let\u2019s Talk About Lakehouse and AI","description":"The data analytics domain is no exception \u2014 Databricks, Snowflake, and Elasticsearch have all redefined themselves as AI data platforms or AI-ready data analytics and search products. Setting aside the \u201Chype\u201D, in today\u2019s article, we\u2019ll explore what relationship actually exists between Lakehouse and AI.","date":"2025-06-27T00:00:00.000Z","tags":[{"inline":true,"label":"Tech Sharing","permalink":"/blog/tags/tech-sharing"}],"hasTruncateMarker":false,"authors":[{"name":"DZone \xb7 Mingyu Chen, Apache Doris PMC Chair","key":null,"page":null}],"frontMatter":{"title":"Community Voice | Lakehouse: Manus? MCP? Let\u2019s Talk About Lakehouse and AI","summary":"The data analytics domain is no exception \u2014 Databricks, Snowflake, and Elasticsearch have all redefined themselves as AI data platforms or AI-ready data analytics and search products. Setting aside the \u201Chype\u201D, in today\u2019s article, we\u2019ll explore what relationship actually exists between Lakehouse and AI.","description":"The data analytics domain is no exception \u2014 Databricks, Snowflake, and Elasticsearch have all redefined themselves as AI data platforms or AI-ready data analytics and search products. Setting aside the \u201Chype\u201D, in today\u2019s article, we\u2019ll explore what relationship actually exists between Lakehouse and AI.","date":"2025-06-27","author":"DZone \xb7 Mingyu Chen, Apache Doris PMC Chair","externalLink":"https://dzone.com/articles/lakehouse-manus-mcp-lets-talk-about-lakehouse","tags":["Tech Sharing"],"image":"/images/blogs/doris-lakehouse-manus-mcp.jpg"},"unlisted":false,"prevItem":{"title":"Community Voice | Doris Lakehouse Integration: A New Approach to Data Analysis","permalink":"/blog/community-voice-doris-lakehouse-integration"},"nextItem":{"title":"Community Voice | Lakehouse: Starting With Apache Doris and S3 Tables","permalink":"/blog/community-voice-lakehouse-starting-with-apache-doris-s3-tables"}},"content":"import { BlogLink } from \'../src/components/blogs/components/blog-link\';\\nimport { SeeMore } from \'../src/components/blogs/components/see-more\';\\n\\n> <BlogLink rel=\\"noopener noreferrer\\" target=\'_blank\' href=\'https://dzone.com/articles/lakehouse-manus-mcp-lets-talk-about-lakehouse\'>The data analytics domain is no exception \u2014 Databricks, Snowflake, and Elasticsearch have all redefined themselves as AI data platforms or AI-ready data analytics and search products. Setting aside the \u201Chype\u201D, in today\u2019s article, we\u2019ll explore what relationship actually exists between Lakehouse and AI.<SeeMore /></BlogLink>"},{"id":"/community-voice-lakehouse-starting-with-apache-doris-s3-tables","metadata":{"permalink":"/blog/community-voice-lakehouse-starting-with-apache-doris-s3-tables","source":"@site/blog/community-voice-lakehouse-starting-with-apache-doris-s3-tables.mdx","title":"Community Voice | Lakehouse: Starting With Apache Doris and S3 Tables","description":"Based on data sharing, diverse workloads and collaboration, Lakehouse has brought us a new paradigm for data analysis. S3 Tables further simplify the setup and maintenance of underlying storage facilities on this basis, achieving a nearly out-of-the-box effect. ","date":"2025-06-26T00:00:00.000Z","tags":[{"inline":true,"label":"Tech Sharing","permalink":"/blog/tags/tech-sharing"}],"hasTruncateMarker":false,"authors":[{"name":"Medium \xb7 Mingyu Chen, Apache Doris PMC Chair","key":null,"page":null}],"frontMatter":{"title":"Community Voice | Lakehouse: Starting With Apache Doris and S3 Tables","summary":"Based on data sharing, diverse workloads and collaboration, Lakehouse has brought us a new paradigm for data analysis. S3 Tables further simplify the setup and maintenance of underlying storage facilities on this basis, achieving a nearly out-of-the-box effect. ","description":"Based on data sharing, diverse workloads and collaboration, Lakehouse has brought us a new paradigm for data analysis. S3 Tables further simplify the setup and maintenance of underlying storage facilities on this basis, achieving a nearly out-of-the-box effect. ","date":"2025-06-26","author":"Medium \xb7 Mingyu Chen, Apache Doris PMC Chair","externalLink":"https://medium.com/@morningman.cmy/lakehousewbd-1-starting-with-apache-doris-s3-tables-10c98ae39fe1","tags":["Tech Sharing"],"image":"/images/blogs/doris-s3-table.jpg"},"unlisted":false,"prevItem":{"title":"Community Voice | Lakehouse: Manus? MCP? Let\u2019s Talk About Lakehouse and AI","permalink":"/blog/community-voice-lakehouse-manus-mcp-lets-talk-about-lakehouse"},"nextItem":{"title":"Community Voice | When Doris Meets Iceberg: A Redemption of a Data Engineer","permalink":"/blog/community-voice-when-doris-meets-iceberg"}},"content":"import { BlogLink } from \'../src/components/blogs/components/blog-link\';\\nimport { SeeMore } from \'../src/components/blogs/components/see-more\';\\n\\n> <BlogLink rel=\\"noopener noreferrer\\" target=\'_blank\' href=\'https://medium.com/@morningman.cmy/lakehousewbd-1-starting-with-apache-doris-s3-tables-10c98ae39fe1\'>Based on data sharing, diverse workloads and collaboration, Lakehouse has brought us a new paradigm for data analysis. S3 Tables further simplify the setup and maintenance of underlying storage facilities on this basis, achieving a nearly out-of-the-box effect. <SeeMore /></BlogLink>"},{"id":"/community-voice-when-doris-meets-iceberg","metadata":{"permalink":"/blog/community-voice-when-doris-meets-iceberg","source":"@site/blog/community-voice-when-doris-meets-iceberg.mdx","title":"Community Voice | When Doris Meets Iceberg: A Redemption of a Data Engineer","description":"Apache Doris and Iceberg are redefining the way data lakes work. It\'s not just a simple 1 plus 1 equals to 2; it brings a qualitative leap!","date":"2025-06-25T00:00:00.000Z","tags":[{"inline":true,"label":"Tech Sharing","permalink":"/blog/tags/tech-sharing"}],"hasTruncateMarker":false,"authors":[{"name":"DZone \xb7 Zen Hua","key":null,"page":null}],"frontMatter":{"title":"Community Voice | When Doris Meets Iceberg: A Redemption of a Data Engineer","summary":"Apache Doris and Iceberg are redefining the way data lakes work. It\'s not just a simple 1 plus 1 equals to 2; it brings a qualitative leap!","description":"Apache Doris and Iceberg are redefining the way data lakes work. It\'s not just a simple 1 plus 1 equals to 2; it brings a qualitative leap!","date":"2025-06-25","author":"DZone \xb7 Zen Hua","externalLink":"https://dzone.com/articles/when-doris-meets-iceberg","tags":["Tech Sharing"],"image":"/images/blogs/doris-meets-iceberg.jpg"},"unlisted":false,"prevItem":{"title":"Community Voice | Lakehouse: Starting With Apache Doris and S3 Tables","permalink":"/blog/community-voice-lakehouse-starting-with-apache-doris-s3-tables"},"nextItem":{"title":"Top commercial bank migrated from Elasticsearch to Apache Doris for PB-scale log storage and analytics","permalink":"/blog/top-commercial-bank-migrated-from-elasticsearch-to-apache-doris-for-pb-scale-log-storage-and-analytics"}},"content":"import { BlogLink } from \'../src/components/blogs/components/blog-link\';\\nimport { SeeMore } from \'../src/components/blogs/components/see-more\';\\n\\n> <BlogLink rel=\\"noopener noreferrer\\" target=\'_blank\' href=\'https://dzone.com/articles/when-doris-meets-iceberg\'>Apache Doris and Iceberg are redefining the way data lakes work. It\'s not just a simple 1 plus 1 equals to 2; it brings a qualitative leap!<SeeMore /></BlogLink>"},{"id":"/top-commercial-bank-migrated-from-elasticsearch-to-apache-doris-for-pb-scale-log-storage-and-analytics","metadata":{"permalink":"/blog/top-commercial-bank-migrated-from-elasticsearch-to-apache-doris-for-pb-scale-log-storage-and-analytics","source":"@site/blog/top-commercial-bank-migrated-from-elasticsearch-to-apache-doris-for-pb-scale-log-storage-and-analytics.mdx","title":"Top commercial bank migrated from Elasticsearch to Apache Doris for PB-scale log storage and analytics","description":"By replacing Elasticsearch with Apache Doris, the commercial bank has saved 50% of resources while improving query speed by 2~4\xd7 and enjoying much simpler operations and maintenance.","date":"2025-06-24T00:00:00.000Z","tags":[{"inline":true,"label":"Best Practice","permalink":"/blog/tags/best-practice"}],"hasTruncateMarker":false,"authors":[{"name":"velodb.io \xb7 VeloDB Engineering Team","key":null,"page":null}],"frontMatter":{"title":"Top commercial bank migrated from Elasticsearch to Apache Doris for PB-scale log storage and analytics","summary":"By replacing Elasticsearch with Apache Doris, the commercial bank has saved 50% of resources while improving query speed by 2~4\xd7 and enjoying much simpler operations and maintenance.","description":"By replacing Elasticsearch with Apache Doris, the commercial bank has saved 50% of resources while improving query speed by 2~4\xd7 and enjoying much simpler operations and maintenance.","date":"2025-06-24","author":"velodb.io \xb7 VeloDB Engineering Team","externalLink":"https://www.velodb.io/blog/1410","tags":["Best Practice"],"image":"/images/blogs/top-commercial-bank.jpeg"},"unlisted":false,"prevItem":{"title":"Community Voice | When Doris Meets Iceberg: A Redemption of a Data Engineer","permalink":"/blog/community-voice-when-doris-meets-iceberg"},"nextItem":{"title":"Community Voice | Apache Doris and DeepSeek: Redefining Intelligent Data Analytics","permalink":"/blog/apache-doris-and-deepseek-community-voice"}},"content":"import { BlogLink } from \'../src/components/blogs/components/blog-link\';\\nimport { SeeMore } from \'../src/components/blogs/components/see-more\';\\n\\n> <BlogLink rel=\\"noopener noreferrer\\" target=\'_blank\' href=\'https://www.velodb.io/blog/1410\'>By replacing Elasticsearch with Apache Doris, the commercial bank has saved 50% of resources while improving query speed by 2~4\xd7 and enjoying much simpler operations and maintenance.<SeeMore /></BlogLink>"},{"id":"/apache-doris-and-deepseek-community-voice","metadata":{"permalink":"/blog/apache-doris-and-deepseek-community-voice","source":"@site/blog/apache-doris-and-deepseek-community-voice.mdx","title":"Community Voice | Apache Doris and DeepSeek: Redefining Intelligent Data Analytics","description":"This article will focus on the in \u2014 depth integration of Apache Doris and DeepSeek, and will analyze in detail its technical implementation, optimization strategies, application scenarios, and future trends. It is hoped that through this content, you can comprehensively understand the potential of this combination and find a practical path suitable for your business.","date":"2025-06-20T00:00:00.000Z","tags":[{"inline":true,"label":"Tech Sharing","permalink":"/blog/tags/tech-sharing"}],"hasTruncateMarker":false,"authors":[{"name":"Medium \xb7 DarrenXu","key":null,"page":null}],"frontMatter":{"title":"Community Voice | Apache Doris and DeepSeek: Redefining Intelligent Data Analytics","summary":"This article will focus on the in \u2014 depth integration of Apache Doris and DeepSeek, and will analyze in detail its technical implementation, optimization strategies, application scenarios, and future trends. It is hoped that through this content, you can comprehensively understand the potential of this combination and find a practical path suitable for your business.","description":"This article will focus on the in \u2014 depth integration of Apache Doris and DeepSeek, and will analyze in detail its technical implementation, optimization strategies, application scenarios, and future trends. It is hoped that through this content, you can comprehensively understand the potential of this combination and find a practical path suitable for your business.","date":"2025-06-20","author":"Medium \xb7 DarrenXu","externalLink":"https://medium.com/%40xudarren1023/apache-doris-and-deepseek-redefining-intelligent-data-analytics-2b6b778e1034","tags":["Tech Sharing"],"image":"/images/blogs/doris-deepseek.jpg"},"unlisted":false,"prevItem":{"title":"Top commercial bank migrated from Elasticsearch to Apache Doris for PB-scale log storage and analytics","permalink":"/blog/top-commercial-bank-migrated-from-elasticsearch-to-apache-doris-for-pb-scale-log-storage-and-analytics"},"nextItem":{"title":"Community Voice | Introduction to Apache Doris","permalink":"/blog/doris-introduction-community-voice"}},"content":"import { BlogLink } from \'../src/components/blogs/components/blog-link\';\\nimport { SeeMore } from \'../src/components/blogs/components/see-more\';\\n\\n> <BlogLink rel=\\"noopener noreferrer\\" target=\'_blank\' href=\'https://medium.com/%40xudarren1023/apache-doris-and-deepseek-redefining-intelligent-data-analytics-2b6b778e1034\'>This article will focus on the in \u2014 depth integration of Apache Doris and DeepSeek, and will analyze in detail its technical implementation, optimization strategies, application scenarios, and future trends. It is hoped that through this content, you can comprehensively understand the potential of this combination and find a practical path suitable for your business.<SeeMore /></BlogLink>"},{"id":"/doris-introduction-community-voice","metadata":{"permalink":"/blog/doris-introduction-community-voice","source":"@site/blog/doris-introduction-community-voice.mdx","title":"Community Voice | Introduction to Apache Doris","description":"With Apache Doris, we now have a comprehensive solution as a high-performance analytical database. In this tutorial, we\u2019ll explore what makes Doris different, how it works, the ecosystem and tooling, and whether it deserves a place in your data stack.","date":"2025-06-20T00:00:00.000Z","tags":[{"inline":true,"label":"Tech Sharing","permalink":"/blog/tags/tech-sharing"}],"hasTruncateMarker":false,"authors":[{"name":"baeldung.com \xb7 Hannah Igboke","key":null,"page":null}],"frontMatter":{"title":"Community Voice | Introduction to Apache Doris","summary":"With Apache Doris, we now have a comprehensive solution as a high-performance analytical database. In this tutorial, we\u2019ll explore what makes Doris different, how it works, the ecosystem and tooling, and whether it deserves a place in your data stack.","description":"With Apache Doris, we now have a comprehensive solution as a high-performance analytical database. In this tutorial, we\u2019ll explore what makes Doris different, how it works, the ecosystem and tooling, and whether it deserves a place in your data stack.","date":"2025-06-20","author":"baeldung.com \xb7 Hannah Igboke","externalLink":"https://www.baeldung.com/sql/apache-doris-tutorial","tags":["Tech Sharing"],"image":"/images/blogs/doris-introduction.jpg"},"unlisted":false,"prevItem":{"title":"Community Voice | Apache Doris and DeepSeek: Redefining Intelligent Data Analytics","permalink":"/blog/apache-doris-and-deepseek-community-voice"},"nextItem":{"title":"Elasticsearch vs ClickHouse vs Apache Doris \u2014 which powers observability better?","permalink":"/blog/which-powers-observability-better"}},"content":"import { BlogLink } from \'../src/components/blogs/components/blog-link\';\\nimport { SeeMore } from \'../src/components/blogs/components/see-more\';\\n\\n> <BlogLink rel=\\"noopener noreferrer\\" target=\'_blank\' href=\'https://www.baeldung.com/sql/apache-doris-tutorial\'>With Apache Doris, we now have a comprehensive solution as a high-performance analytical database. In this tutorial, we\u2019ll explore what makes Doris different, how it works, the ecosystem and tooling, and whether it deserves a place in your data stack.<SeeMore /></BlogLink>"},{"id":"/which-powers-observability-better","metadata":{"permalink":"/blog/which-powers-observability-better","source":"@site/blog/which-powers-observability-better.mdx","title":"Elasticsearch vs ClickHouse vs Apache Doris \u2014 which powers observability better?","description":"A side-by-side comparison of observability solutions in terms of performance, cost, usability, and ecosystem openness. This article also introduces Apache Doris as a powerful observability solution, supported by live demos and real-world user success stories.","date":"2025-06-17T00:00:00.000Z","tags":[{"inline":true,"label":"Tech Sharing","permalink":"/blog/tags/tech-sharing"}],"hasTruncateMarker":false,"authors":[{"name":"velodb.io \xb7 VeloDB Engineering Team","key":null,"page":null}],"frontMatter":{"title":"Elasticsearch vs ClickHouse vs Apache Doris \u2014 which powers observability better?","summary":"A side-by-side comparison of observability solutions in terms of performance, cost, usability, and ecosystem openness. This article also introduces Apache Doris as a powerful observability solution, supported by live demos and real-world user success stories.","description":"A side-by-side comparison of observability solutions in terms of performance, cost, usability, and ecosystem openness. This article also introduces Apache Doris as a powerful observability solution, supported by live demos and real-world user success stories.","date":"2025-06-17","author":"velodb.io \xb7 VeloDB Engineering Team","tags":["Tech Sharing"],"externalLink":"https://www.velodb.io/blog/1406","image":"/images/which-powers-observability-better.jpg"},"unlisted":false,"prevItem":{"title":"Community Voice | Introduction to Apache Doris","permalink":"/blog/doris-introduction-community-voice"},"nextItem":{"title":"Apache Doris 3.0.6 Released","permalink":"/blog/release-note-3.0.6"}},"content":"import { BlogLink } from \'../src/components/blogs/components/blog-link\';\\nimport { SeeMore } from \'../src/components/blogs/components/see-more\';\\n\\n> <BlogLink rel=\\"noopener noreferrer\\" target=\'_blank\' href=\'https://www.velodb.io/blog/1406\'>A side-by-side comparison of observability solutions in terms of performance, cost, usability, and ecosystem openness. This article also introduces Apache Doris as a powerful observability solution, supported by live demos and real-world user success stories.<SeeMore /></BlogLink>"},{"id":"/release-note-3.0.6","metadata":{"permalink":"/blog/release-note-3.0.6","source":"@site/blog/release-note-3.0.6.md","title":"Apache Doris 3.0.6 Released","description":"Dear community members, the Apache Doris 3.0.6 version was officially released on Jun 16, 2025, this version introduces continuous upgrades and enhancements in Lakehouse, Query Execution and Compute-Storage Decoupled.","date":"2025-06-16T00:00:00.000Z","tags":[{"inline":true,"label":"Release Notes","permalink":"/blog/tags/release-notes"}],"hasTruncateMarker":false,"authors":[{"name":"Apache Doris","key":null,"page":null}],"frontMatter":{"title":"Apache Doris 3.0.6 Released","summary":"Dear community members, the Apache Doris 3.0.6 version was officially released on Jun 16, 2025, this version introduces continuous upgrades and enhancements in Lakehouse, Query Execution and Compute-Storage Decoupled.","description":"Dear community members, the Apache Doris 3.0.6 version was officially released on Jun 16, 2025, this version introduces continuous upgrades and enhancements in Lakehouse, Query Execution and Compute-Storage Decoupled.","date":"2025-06-16","author":"Apache Doris","tags":["Release Notes"],"image":"/images/3.0.6.jpg"},"unlisted":false,"prevItem":{"title":"Elasticsearch vs ClickHouse vs Apache Doris \u2014 which powers observability better?","permalink":"/blog/which-powers-observability-better"},"nextItem":{"title":"Community Voice: Elasticsearch vs. Apache Doris","permalink":"/blog/elasticsearch-vs-apache-doris-community-voice"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n  http://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n\\n\\n\\nDear community members, the Apache Doris 3.0.6 version was officially released on Jun 16, 2025.\\n\\n**Quick Download:** https://doris.apache.org/download/\\n\\n**GitHub Release:** https://github.com/apache/doris/releases\\n\\n## Behavior Changes\\n\\n- **Prohibited time-series compaction for Unique tables** [#49905](https://github.com/apache/doris/pull/49905)\\n- **Adjusted Auto Bucket size to 10GB per bucket in compute-storage separation scenarios** [#50566](https://github.com/apache/doris/pull/50566)\\n\\n## New Features\\n\\n### Lakehouse\\n\\n- **Added support for accessing Iceberg table formats in AWS S3 Table Buckets** \\n\\t- For detailed information, please refer to documentation: [Iceberg on S3 Tables](https://doris.apache.org/docs/dev/lakehouse/catalogs/iceberg-catalog#iceberg-on-s3-tables)\\n\\n### Storage\\n\\n- **IAM Role authorization support for object storage access** Applies to import/export, backup/restore, and compute-storage separation scenarios [#50252](https://github.com/apache/doris/pull/50252) [#50682](https://github.com/apache/doris/pull/50682) [#49541](https://github.com/apache/doris/pull/49541) [#49565](https://github.com/apache/doris/pull/49565) [#49422](https://github.com/apache/doris/pull/49422) \\n\\t- For detailed information, please refer to [documentation](https://doris.apache.org/docs/3.0/admin-manual/auth/integrations/aws-authentication-and-authorization)\\n\\n### New Functions\\n\\n- `json_extract_no_quotes`\\n\\t- For detailed information, please refer to [documentation](https://doris.apache.org/docs/3.0/sql-manual/sql-functions/scalar-functions/json-functions/json-extract)\\n- `unhex_null`\\n\\t- For detailed information, please refer to [documentation](https://doris.apache.org/docs/3.0/sql-manual/sql-functions/scalar-functions/string-functions/unhex)\\n- `xpath_string` \\n\\t- For detailed information, please refer to [documentation](https://doris.apache.org/docs/3.0/sql-manual/sql-functions/scalar-functions/string-functions/xpath-string)\\n- `str_to_map`\\n- For detailed information, please refer to [documentation](https://doris.apache.org/docs/3.0/sql-manual/sql-functions/scalar-functions/map-functions/str-to-map)\\n- `months_between`\\n\\t- For detailed information, please refer to [documentation](https://doris.apache.org/docs/3.0/sql-manual/sql-functions/scalar-functions/date-time-functions/months-between)\\n- `next_day`\\n\\t- For detailed information, please refer to [documentation](https://doris.apache.org/docs/3.0/sql-manual/sql-functions/scalar-functions/date-time-functions/next-day)\\n- `format_round`: \\n\\t- For detailed information, please refer to [documentation](https://doris.apache.org/docs/3.0/sql-manual/sql-functions/scalar-functions/numeric-functions/format-round)\\n\\n## Improvements\\n\\n### Storage \\n\\n- Streamlined Compaction Profile and logs [#50950](https://github.com/apache/doris/pull/50950)\\n- Enhanced scheduling strategy to improve Compaction throughput [#49882](https://github.com/apache/doris/pull/49882) [#48759](https://github.com/apache/doris/pull/48759) [#51482](https://github.com/apache/doris/pull/51482) [#50672](https://github.com/apache/doris/pull/50672) [#49953](https://github.com/apache/doris/pull/49953) [#50819](https://github.com/apache/doris/pull/50819)\\n- **Reduced redundant log output** [#51093](https://github.com/apache/doris/pull/51093)\\n- **Implemented blacklist mechanism** to prevent Routine Load from distributing metadata to unavailable BEs [#50587](https://github.com/apache/doris/pull/50587)\\n- **Increased default value** of `load_task_high_priority_threshold_second` [#50478](https://github.com/apache/doris/pull/50478)\\n\\n### Storage-Compute Decoupled\\n\\n- **Startup optimization**: Accelerated File Cache initialization [#50726](https://github.com/apache/doris/pull/50726)\\n- **Query acceleration**: Improved File Cache performance [#50275](https://github.com/apache/doris/pull/50275) [#50387](https://github.com/apache/doris/pull/50387) [#50555](https://github.com/apache/doris/pull/50555)\\n- **Metadata optimization**: Resolved performance bottlenecks caused by `get_version` [#51111](https://github.com/apache/doris/pull/51111) [#50439](https://github.com/apache/doris/pull/50439)\\n- **Garbage collection acceleration**: Improved object reclamation efficiency [#50037](https://github.com/apache/doris/pull/50037) [#50766](https://github.com/apache/doris/pull/50766)\\n- **Stability enhancement**: Optimized object storage retry strategy [#50957](https://github.com/apache/doris/pull/50957)\\n- **Granular profiling**: Added tablet/segment footer dimension metrics [#49945](https://github.com/apache/doris/pull/49945) [#50564](https://github.com/apache/doris/pull/50564) [#50326](https://github.com/apache/doris/pull/50326)\\n- **Schema Change resilience**: Enabled New Tablet Compaction by default to prevent -230 errors [#51070](https://github.com/apache/doris/pull/51070)\\n\\n### Lakehouse\\n\\n#### Catalog enhancements\\n\\n- Added partition cache TTL control (`partition.cache.ttl-second`) for Hive Catalog [#50724](https://github.com/apache/doris/pull/50724) \\n\\t- For detailed information, please refer to documentation: [Metadata Cache](https://doris.apache.org/docs/dev/lakehouse/meta-cache)\\n- Supported `skip.header.line.count` property for Hive tables [#49929](https://github.com/apache/doris/pull/49929)\\n- Added compatibility for Hive tables using `org.openx.data.jsonserde.JsonSerDe` [#49958](https://github.com/apache/doris/pull/49958) \\n\\t- For detailed information, please refer to documentation: [Text Format](https://doris.apache.org/docs/dev/lakehouse/file-formats/text)\\n- Upgraded Paimon to v1.0.1\\n- Upgraded Iceberg to v1.6.1\\n\\n#### Functional extensions\\n- Added support for Alibaba Cloud OSS-HDFS Root Policy [#50678](https://github.com/apache/doris/pull/50678)\\n- Dialect compatibility: Returned query results in Hive format [#49931](https://github.com/apache/doris/pull/49931) \\n  - For detailed information, please refer to documentation: [SQL Convertor](https://doris.apache.org/docs/dev/lakehouse/sql-convertor/sql-convertor-overview)\\n\\n### Asynchronous Materialized Views\\n\\n- **Memory optimization**: Reduced memory footprint during transparent rewriting [#48887](https://github.com/apache/doris/pull/48887)\\n\\n### Query Optimizer\\n\\n- **Improved bucket pruning performance** [#49388](https://github.com/apache/doris/pull/49388)\\n- **Enhanced lambda expressions**: Supported external slot references [#44365](https://github.com/apache/doris/pull/44365)\\n\\n### Query Execution\\n\\n- **TopN query acceleration**: Optimized performance in compute-storage separation scenarios [#50803](https://github.com/apache/doris/pull/50803)\\n- **Function extension**: Added variable parameter support for `substring_index` [#50149](https://github.com/apache/doris/pull/50149)\\n- **Geospatial functions**: Added `ST_CONTAINS`, `ST_INTERSECTS`, `ST_TOUCHES`, and `ST_DISJOINT` [#49665](https://github.com/apache/doris/pull/49665)\\n\\n### Core Components\\n\\n- **Memory tracker optimization**: ~10% performance gain in high-concurrency scenarios [#50462](https://github.com/apache/doris/pull/50462)\\n- **Audit log enhancement**: Added `audit_plugin_max_insert_stmt_length` to limit INSERT statement length [#51314](https://github.com/apache/doris/pull/51314) \\n\\t- For detailed information, please refer to documentation: [Audit Plugin](https://doris.apache.org/docs/3.0/admin-manual/audit-plugin)\\n- **SQL converter control**: Introduced session variables `sql_convertor_config` and `enable_sql_convertor_features` \\n\\t- For detailed information, please refer to documentation: [SQL Dialect](https://doris.apache.org/docs/dev/lakehouse/sql-convertor/sql-convertor-overview)\\n\\n## Bug Fixes\\n\\n### Data Ingestion\\n\\n- Fixed transaction cleanup failures in BE [#50103](https://github.com/apache/doris/pull/50103)\\n- Improved error reporting accuracy for Routine Load [#51078](https://github.com/apache/doris/pull/51078)\\n- Prevented metadata task distribution to `disable_load=true` nodes [#50421](https://github.com/apache/doris/pull/50421)\\n- Fixed consumption progress rollback after FE restart [#50221](https://github.com/apache/doris/pull/50221)\\n- Resolved Group Commit and Schema Change conflict causing Core Dump [#51144](https://github.com/apache/doris/pull/51144)\\n- Fixed S3 Load errors when using HTTPS protocol [#51246](https://github.com/apache/doris/pull/51246) [#51529](https://github.com/apache/doris/pull/51529)\\n\\n### Primary Key Model\\n\\n- Fixed duplicate key issues caused by race conditions [#50019](https://github.com/apache/doris/pull/50019) [#50051](https://github.com/apache/doris/pull/50051) [#50106](https://github.com/apache/doris/pull/50106) [#50417](https://github.com/apache/doris/pull/50417) [#50847](https://github.com/apache/doris/pull/50847) [#50974](https://github.com/apache/doris/pull/50974)\\n\\n### Storage\\n\\n- Fixed CCR and disk balancing race conditions [#50663](https://github.com/apache/doris/pull/50663)\\n- Corrected missing persistence of default partition keys [#50489](https://github.com/apache/doris/pull/50489)\\n- Added Rollup table support for CCR [#50337](https://github.com/apache/doris/pull/50337)\\n- Fixed edge case when `cooldown_ttl=0` [#50830](https://github.com/apache/doris/pull/50830)\\n- Resolved data loss caused by GC and Publish contention [#50343](https://github.com/apache/doris/pull/50343)\\n- Fixed partition pruning failure in Delete Job [#50674](https://github.com/apache/doris/pull/50674)\\n\\n### Storage-Compute Decoupled\\n\\n- Fixed Schema Change blocking Compaction [#50908](https://github.com/apache/doris/pull/50908)\\n- Solved object reclamation failure when `storage_vault_prefix` is empty [#50352](https://github.com/apache/doris/pull/50352)\\n- Resolved query performance issues caused by Tablet Cache [#51193](https://github.com/apache/doris/pull/51193) [#49420](https://github.com/apache/doris/pull/49420)\\n- Eliminated performance jitter from residual Tablet Cache [#50200](https://github.com/apache/doris/pull/50200)\\n\\n### Lakehouse\\n\\n- **Export fixes** \\n  - Fixed FE memory leaks [#51171](https://github.com/apache/doris/pull/51171)\\n  - Prevented FE deadlocks [#50088](https://github.com/apache/doris/pull/50088)\\n- **Catalog fixes** \\n  - Enabled composite predicate pushdown for JDBC Catalog [#50542](https://github.com/apache/doris/pull/50542)\\n  - Fixed Deletion Vector reading for Alibaba Cloud OSS Paimon tables [#49645](https://github.com/apache/doris/pull/49645)\\n  - Supported comma-containing Hive partition values [#49382](https://github.com/apache/doris/pull/49382)\\n  - Corrected MaxCompute Timestamp column parsing [#49600](https://github.com/apache/doris/pull/49600)\\n  - Enabled `information_schema` system tables for Trino Catalog [#49912](https://github.com/apache/doris/pull/49912)\\n- **File formats** \\n  - Fixed LZO compression reading failures [#49538](https://github.com/apache/doris/pull/49538)\\n  - Added legacy ORC file compatibility [#50358](https://github.com/apache/doris/pull/50358)\\n  - Corrected complex type parsing in ORC files [#50136](https://github.com/apache/doris/pull/50136)\\n\\n### Asynchronous Materialized Views\\n\\n- Fixed refresh miss when specifying both `start time` and immediate trigger modes [#50624](https://github.com/apache/doris/pull/50624)\\n\\n### Query Optimizer\\n\\n- Fixed rewriting errors with lambda expressions [#49166](https://github.com/apache/doris/pull/49166)\\n- Resolved planning failures with constant group by keys [#49473](https://github.com/apache/doris/pull/49473)\\n- Corrected constant folding logic [#50142](https://github.com/apache/doris/pull/50142) [#50810](https://github.com/apache/doris/pull/50810)\\n- Completed system table information retrieval [#50721](https://github.com/apache/doris/pull/50721)\\n- Fixed column type handling when creating views with NULL Literal [#49881](https://github.com/apache/doris/pull/49881)\\n\\n### Query Execution\\n\\n- Fixed BE crashes caused by illegal JSON values during import [#50978](https://github.com/apache/doris/pull/50978)\\n- Corrected Intersect results with NULL constant inputs [#50951](https://github.com/apache/doris/pull/50951)\\n- Fixed predicate mis-execution with Variant types [#50934](https://github.com/apache/doris/pull/50934)\\n- Resolved `get_json_string` errors with illegal JSON Paths [#50859](https://github.com/apache/doris/pull/50859)\\n- Aligned function behaviors (JSON_REPLACE/INSERT/SET/ARRAY) with MySQL [#50308](https://github.com/apache/doris/pull/50308)\\n- Fixed `array_map` crashes with empty parameters [#50201](https://github.com/apache/doris/pull/50201)\\n- Prevented core dumps during abnormal Variant-to-JSONB conversion [#50180](https://github.com/apache/doris/pull/50180)\\n- Added missing `explode_json_array_json_outer` function [#50164](https://github.com/apache/doris/pull/50164)\\n- Aligned results between `percentile` and `percentile_array` [#49351](https://github.com/apache/doris/pull/49351)\\n- Optimized UTF8 handling for functions (url_encode/strright/append_trailing_char_if_absent) [#49127](https://github.com/apache/doris/pull/49127)\\n\\n### Others\\n\\n- Fixed audit log loss in high-concurrency scenarios [#50357](https://github.com/apache/doris/pull/50357)\\n- Prevented metadata replay failures during dynamic partition creation [#49569](https://github.com/apache/doris/pull/49569)\\n- Solved Global UDF loss after restart [#50279](https://github.com/apache/doris/pull/50279)\\n- Aligned View metadata return format with MySQL [#51058](https://github.com/apache/doris/pull/51058)"},{"id":"/elasticsearch-vs-apache-doris-community-voice","metadata":{"permalink":"/blog/elasticsearch-vs-apache-doris-community-voice","source":"@site/blog/elasticsearch-vs-apache-doris-community-voice.mdx","title":"Community Voice: Elasticsearch vs. Apache Doris","description":" Listen to what users say! Rahul Kolluri has provided a detailed technical comparison between Elasticsearch and Apache Doris based on real-world billing analytics scenarios. Based on his experience building dashboards and backend APIs around AWS usage and billing data, he saw Elasticsearch hit major limits when used for analytics.","date":"2025-06-10T00:00:00.000Z","tags":[{"inline":true,"label":"Tech Sharing","permalink":"/blog/tags/tech-sharing"}],"hasTruncateMarker":false,"authors":[{"name":"LinkedIn \xb7 Rahul Kolluri","key":null,"page":null}],"frontMatter":{"title":"Community Voice: Elasticsearch vs. Apache Doris","summary":" Listen to what users say! Rahul Kolluri has provided a detailed technical comparison between Elasticsearch and Apache Doris based on real-world billing analytics scenarios. Based on his experience building dashboards and backend APIs around AWS usage and billing data, he saw Elasticsearch hit major limits when used for analytics.","description":" Listen to what users say! Rahul Kolluri has provided a detailed technical comparison between Elasticsearch and Apache Doris based on real-world billing analytics scenarios. Based on his experience building dashboards and backend APIs around AWS usage and billing data, he saw Elasticsearch hit major limits when used for analytics.","date":"2025-06-10","author":"LinkedIn \xb7 Rahul Kolluri","tags":["Tech Sharing"],"externalLink":"https://www.linkedin.com/posts/rahul-kolluri-352447191_rethinking-elasticsearch-for-analytics-activity-7333804955700473859-2-e4?utm_source=share&utm_medium=member_desktop&rcm=ACoAACoH8OcBYW4CFSr632eidBaUEb5u1O2r30o","image":"/images/apache-doris-vs-elasticsearch.jpg"},"unlisted":false,"prevItem":{"title":"Apache Doris 3.0.6 Released","permalink":"/blog/release-note-3.0.6"},"nextItem":{"title":"Apache Doris 2.1.10 released","permalink":"/blog/release-note-2.1.10"}},"content":"import { BlogLink } from \'../src/components/blogs/components/blog-link\';\\nimport { SeeMore } from \'../src/components/blogs/components/see-more\';\\n\\n> <BlogLink rel=\\"noopener noreferrer\\" target=\'_blank\' href=\'https://www.linkedin.com/posts/rahul-kolluri-352447191_rethinking-elasticsearch-for-analytics-activity-7333804955700473859-2-e4?utm_source=share&utm_medium=member_desktop&rcm=ACoAACoH8OcBYW4CFSr632eidBaUEb5u1O2r30o\'> Listen to what users say! Rahul Kolluri has provided a detailed technical comparison between Elasticsearch and Apache Doris based on real-world billing analytics scenarios. Based on his experience building dashboards and backend APIs around AWS usage and billing data, he saw Elasticsearch hit major limits when used for analytics.<SeeMore /></BlogLink>"},{"id":"/release-note-2.1.10","metadata":{"permalink":"/blog/release-note-2.1.10","source":"@site/blog/release-note-2.1.10.md","title":"Apache Doris 2.1.10 released","description":"This version introduces continuous upgrades and enhancements in Query Execution Engine and Lakehouse. It also resolves critical bugs to improve stability and performance.","date":"2025-05-17T00:00:00.000Z","tags":[{"inline":true,"label":"Release Notes","permalink":"/blog/tags/release-notes"}],"hasTruncateMarker":false,"authors":[{"name":"Apache Doris","key":null,"page":null}],"frontMatter":{"title":"Apache Doris 2.1.10 released","summary":"This version introduces continuous upgrades and enhancements in Query Execution Engine and Lakehouse. It also resolves critical bugs to improve stability and performance.","description":"This version introduces continuous upgrades and enhancements in Query Execution Engine and Lakehouse. It also resolves critical bugs to improve stability and performance.","date":"2025-05-17","author":"Apache Doris","tags":["Release Notes"],"image":"/images/2.1.10.jpg"},"unlisted":false,"prevItem":{"title":"Community Voice: Elasticsearch vs. Apache Doris","permalink":"/blog/elasticsearch-vs-apache-doris-community-voice"},"nextItem":{"title":"Apache Doris 3.0.5 Released","permalink":"/blog/release-note-3.0.5"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n  http://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n\\nDear Community, **Apache Doris version 2.1.10 is now available**. This version introduces continuous upgrades and enhancements in Query Execution Engine and Lakehouse. It also resolves critical bugs to improve stability and performance.\\n\\n\\n- [Quick Download](https://doris.apache.org/download)\\n\\n- [GitHub Release](https://github.com/apache/doris/releases/tag/2.1.10-rc01)\\n\\n\\n\\n## Behavior Changes\\n\\n- DELETE no longer incorrectly requires the SELECT_PRIV permission on the target table. [ #49794](https://github.com/apache/doris/pull/49794)\\n- Insert Overwrite no longer restricts concurrent operations on the same table to 1. [ #48673](https://github.com/apache/doris/pull/48673)\\n- Merge on write unique tables prohibit the use of time-series compaction. [ #49905](https://github.com/apache/doris/pull/49905)\\n- Building indexes on VARIANT type columns is prohibited. [ #49159](https://github.com/apache/doris/pull/49159)\\n\\n## New Features\\n\\n### Query Execution Engine\\n\\n- Added support for more GEO type computation functions: `ST_CONTAINS`, `ST_INTERSECTS`, `ST_TOUCHES`, `GeometryFromText`, `ST_Intersects`, `ST_Disjoint`, `ST_Touches`. [ #49665](https://github.com/apache/doris/pull/49665) [ #48695](https://github.com/apache/doris/pull/48695)\\n- Added support for the `years_of_week` function. [ #48870](https://github.com/apache/doris/pull/48870)\\n\\n### Lakehouse\\n\\n- Hive Catalog now supports catalog-level partition cache control. [ #50724](https://github.com/apache/doris/pull/50724)\\n  - For more details, refer to the[ ](https://doris.apache.org/docs/dev/lakehouse/meta-cache)[documentation](https://doris.apache.org/docs/dev/lakehouse/meta-cache#disable-hive-catalog-metadata-cache).\\n\\n## Improvements\\n\\n### Lakehouse\\n\\n- Upgraded the Paimon dependency version to 1.0.1.\\n- Upgraded the Iceberg dependency version to 1.6.1.\\n- Included the memory overhead of Parquet Footer in the Memory Tracker to avoid potential OOM issues. [ #49037](https://github.com/apache/doris/pull/49037)\\n- Optimized the predicate pushdown logic for JDBC Catalog, supporting pushdown of AND/OR connected predicates. [ #50542](https://github.com/apache/doris/pull/50542)\\n- Precompiled versions now include the Jindofs extension package by default to support Alibaba Cloud oss-hdfs access.\\n\\n### Semi-Structured Data Management\\n\\n- ANY function now supports JSON type. [ #50311](https://github.com/apache/doris/pull/50311)\\n- JSON_REPLACE, JSON_INSERT, JSON_SET, JSON_ARRAY functions now support JSON data type and complex data types. [ #50308](https://github.com/apache/doris/pull/50308)\\n\\n### Query Optimizer\\n\\n- When the number of options in an IN expression exceeds `Config.max_distribution_pruner_recursion_depth`, bucket pruning is not performed to improve planning speed. [ #49387](https://github.com/apache/doris/pull/49387)\\n\\n### Storage Management\\n\\n- Reduced logging and improved some log messages. [ #47647](https://github.com/apache/doris/pull/47647) [ #48523](https://github.com/apache/doris/pull/48523)\\n\\n### Other\\n\\n- Avoided the thrift rpc END_OF_FILE exception. [ #49649](https://github.com/apache/doris/pull/49649)\\n\\n## Bug Fixes\\n\\n### Lakehouse \\n\\n- Fixed the issue where newly created tables in Hive were not immediately visible in Doris. [ #50188](https://github.com/apache/doris/pull/50188)\\n- Fixed the error \\"Storage schema reading not supported\\" when accessing certain Text format Hive tables. [ #50038](https://github.com/apache/doris/pull/50038)\\n  - Refer to the[ get_schema_from_table documentation](https://doris.apache.org/docs/dev/lakehouse/catalogs/hive-catalog?_highlight=get_schema_from_table#syntax) for details.\\n- Fixed concurrency issues with metadata submission when writing to Hive/Iceberg tables. [ #49842](https://github.com/apache/doris/pull/49842)\\n- Fixed the issue where writing to Hive tables stored on oss-hdfs failed. [ #49754](https://github.com/apache/doris/pull/49754)\\n- Fixed the issue where accessing Hive tables with partition key values containing commas failed. [ #49382](https://github.com/apache/doris/pull/49382)\\n- Fixed the issue where Split allocation for Paimon tables was uneven in certain cases. [ #50083](https://github.com/apache/doris/pull/50083)\\n- Fixed the issue where Delete files were not correctly handled when reading Paimon tables stored on oss. [ #49645](https://github.com/apache/doris/pull/49645)\\n- Fixed the issue where reading high-precision Timestamp columns in MaxCompute Catalog failed. [ #49600](https://github.com/apache/doris/pull/49600)\\n- Fixed the potential resource leakage when deleting a Catalog in certain cases. [ #49621](https://github.com/apache/doris/pull/49621)\\n- Fixed the issue where reading LZO compressed data failed in certain cases. [ #49538](https://github.com/apache/doris/pull/49538)\\n- Fixed the issue where ORC deferred materialization caused errors when reading complex types. [ #50136](https://github.com/apache/doris/pull/50136)\\n- Fixed the issue where reading ORC files generated by pyorc-0.3 version failed. [ #50358](https://github.com/apache/doris/pull/50358)\\n- Fixed the issue where EXPORT operations caused metadata deadlocks. [ #50088](https://github.com/apache/doris/pull/50088)\\n\\n### Indexing\\n\\n- Fixed errors in building inverted indexes after multiple add, delete, and rename column operations. [ #50056](https://github.com/apache/doris/pull/50056)\\n- Added validation for unique column IDs in index compaction to avoid potential data anomalies and system errors. [ #47562](https://github.com/apache/doris/pull/47562)\\n\\n### Semi-Structured Data Types\\n\\n- Fixed the issue where converting VARIANT type to JSON type returned NULL in certain cases. [ #50180](https://github.com/apache/doris/pull/50180)\\n- Fixed the crash caused by JSONB CAST in certain cases. [ #49810](https://github.com/apache/doris/pull/49810)\\n- Prohibited building indexes on VARIANT type columns. [ #49159](https://github.com/apache/doris/pull/49159)\\n- Fixed the precision correctness of decimal type in the named_struct function. [ #48964](https://github.com/apache/doris/pull/48964)\\n\\n### Query Optimizer\\n\\n- Fixed several issues in constant folding. [#49413](https://github.com/apache/doris/pull/49413) [#50425](https://github.com/apache/doris/pull/50425) [#49686](https://github.com/apache/doris/pull/49686) [#49575](https://github.com/apache/doris/pull/49575) [#50142](https://github.com/apache/doris/pull/50142)\\n- Common subexpression extraction may not work properly on lambda expressions. [#49166](https://github.com/apache/doris/pull/49166)\\n- Fixed the issue where eliminating constants in group by keys might not work properly. [#49589](https://github.com/apache/doris/pull/49589)\\n- Fixed the issue where planning failed in extreme scenarios due to incorrect statistics inference. [#49415](https://github.com/apache/doris/pull/49415)\\n- Fixed the issue where some information_schema tables depending on BE metadata could not retrieve complete data. [#50721](https://github.com/apache/doris/pull/50721)\\n\\n### Query Execution Engine\\n\\n- Fixed the issue where the explode_json_array_json_outer function was not found. [#50164](https://github.com/apache/doris/pull/50164)\\n- Fixed the issue where substring_index did not support dynamic parameters. [#50149](https://github.com/apache/doris/pull/50149)\\n- Fixed the issue where the st_contains function returned incorrect results in many cases. [#50115](https://github.com/apache/doris/pull/50115)\\n- Fixed the core dump issue that could be caused by the array_range function. [#49993](https://github.com/apache/doris/pull/49993)\\n- Fixed the issue where the date_diff function returned incorrect results. [#49429](https://github.com/apache/doris/pull/49429)\\n- Fixed a series of issues with string functions causing garbled output or incorrect results in non-ASCII encodings. [#49231](https://github.com/apache/doris/pull/49231) [#49846](https://github.com/apache/doris/pull/49846) [#49127](https://github.com/apache/doris/pull/49127) [#40710](https://github.com/apache/doris/pull/40710)\\n\\n### Storage Management\\n\\n- Fixed the issue where metadata replay for dynamic partition tables failed in certain cases. [#49569](https://github.com/apache/doris/pull/49569)\\n- Fixed the issue where streamload on ARM might lose data due to operation sequence. [#48948](https://github.com/apache/doris/pull/48948)\\n- Fixed the error in full compaction and the potential issue of mow data duplication. [#49825](https://github.com/apache/doris/pull/49825) [#48958](https://github.com/apache/doris/pull/48958)\\n- Fixed the issue where partition storage policy was not persisted. [#49721](https://github.com/apache/doris/pull/49721)\\n- Fixed the extremely rare issue where imported files did not exist. [#50343](https://github.com/apache/doris/pull/50343)\\n- Fixed the issue where ccr and disk balancing concurrency might cause files to go missing. [#50663](https://github.com/apache/doris/pull/50663)\\n- Fixed the connection reset issue that might occur during backup and restore of large snapshots. [#49649](https://github.com/apache/doris/pull/49649)\\n- Fixed the issue where FE follower lost local backup snapshots. [#49550](https://github.com/apache/doris/pull/49550)\\n\\n### Others\\n\\n- Fixed the issue where audit logs might be lost in certain scenarios. [#50357](https://github.com/apache/doris/pull/50357)\\n- Fixed the issue where the isQuery flag in audit logs might be incorrect. [#49959](https://github.com/apache/doris/pull/49959)\\n- Fixed the issue where some query sqlHash values in audit logs were incorrect. [#49984](https://github.com/apache/doris/pull/49984)"},{"id":"/release-note-3.0.5","metadata":{"permalink":"/blog/release-note-3.0.5","source":"@site/blog/release-note-3.0.5.md","title":"Apache Doris 3.0.5 Released","description":"Dear community members, the Apache Doris 3.0.5 version was officially released on April 28, 2025, this version introduces continuous upgrades and enhancements in several key areas, including Lakehouse, Asynchronous Materialized Views, Semi-structured Data Management and more.","date":"2025-04-28T00:00:00.000Z","tags":[{"inline":true,"label":"Release Notes","permalink":"/blog/tags/release-notes"}],"hasTruncateMarker":false,"authors":[{"name":"Apache Doris","key":null,"page":null}],"frontMatter":{"title":"Apache Doris 3.0.5 Released","summary":"Dear community members, the Apache Doris 3.0.5 version was officially released on April 28, 2025, this version introduces continuous upgrades and enhancements in several key areas, including Lakehouse, Asynchronous Materialized Views, Semi-structured Data Management and more.","description":"Dear community members, the Apache Doris 3.0.5 version was officially released on April 28, 2025, this version introduces continuous upgrades and enhancements in several key areas, including Lakehouse, Asynchronous Materialized Views, Semi-structured Data Management and more.","date":"2025-04-28","author":"Apache Doris","tags":["Release Notes"],"image":"/images/3.0.5.jpeg"},"unlisted":false,"prevItem":{"title":"Apache Doris 2.1.10 released","permalink":"/blog/release-note-2.1.10"},"nextItem":{"title":"How Tencent Music saved 80% in costs by migrating from Elasticsearch to Apache Doris","permalink":"/blog/tencent-music-migrate-elasticsearch-to-doris"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n  http://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n\\n\\nDear community members, the Apache Doris 3.0.5 version was officially released on April 28, 2025, this version introduces continuous upgrades and enhancements in several key areas, including Lakehouse, Asynchronous Materialized Views, Semi-structured Data Management and more.\\n\\n**Quick Download:** https://doris.apache.org/download/\\n\\n**GitHub Release:** https://github.com/apache/doris/releases\\n\\n\\n## New Features\\n\\n### Lakehouse\\n\\n- Added Catalog/Database/Table quantity monitoring metrics to FE Metrics ([#47891](https://github.com/apache/doris/pull/47891))\\n- MaxCompute Catalog now supports Timestamp type ([#48768](https://github.com/apache/doris/pull/48768))\\n\\n### Query Execution\\n\\n- Added URL processing functions: `top_level_domain`, `first_significant_subdomain`, `cut_to_first_significant_subdomain` ([#42488](https://github.com/apache/doris/pull/42488))\\n- Added `year_of_week` function with Trino-compatible implementation ([#48870](https://github.com/apache/doris/pull/48870))\\n- `percentile_array` function now supports Float and Double data types ([#48094](https://github.com/apache/doris/pull/48094))\\n\\n### Storage-Compute Separation\\n\\n- Added compute group renaming support ([#46221](https://github.com/apache/doris/pull/46221))\\n\\n## Improvements\\n\\n### Storage\\n\\n- Accelerated Compaction task generation to improve performance ([#49547](https://github.com/apache/doris/pull/49547))\\n- Stream Load now supports compressed JSON file ingestion ([#49044](https://github.com/apache/doris/pull/49044))\\n- Enhanced error messages for various ingestion scenarios ([#48436](https://github.com/apache/doris/pull/48436) [#47721](https://github.com/apache/doris/pull/47721) [#47804](https://github.com/apache/doris/pull/47804) [#48638](https://github.com/apache/doris/pull/48638) [#48344](https://github.com/apache/doris/pull/48344) [#49287](https://github.com/apache/doris/pull/49287) [#48009](https://github.com/apache/doris/pull/48009))\\n- Added multiple metrics for Routine Load ([#49045](https://github.com/apache/doris/pull/49045) [#48764](https://github.com/apache/doris/pull/48764))\\n- Optimized Routine Load scheduling algorithm to prevent single job failure from affecting overall scheduling ([#47847](https://github.com/apache/doris/pull/47847))\\n- Added Routine Load system table ([#49284](https://github.com/apache/doris/pull/49284))\\n- Improved query performance for Merge-On-Write (MOW) tables under high-frequency ingestion ([#48968](https://github.com/apache/doris/pull/48968))\\n- Enhanced Profile information display for Key Range queries ([#48191](https://github.com/apache/doris/pull/48191))\\n\\n### Compute-Storage Decoupled\\n\\n- Fixed multiple File Cache stability and performance issues ([#48786](https://github.com/apache/doris/pull/48786) [#48623](https://github.com/apache/doris/pull/48623) [#48687](https://github.com/apache/doris/pull/48687) [#49050](https://github.com/apache/doris/pull/49050) [#48318](https://github.com/apache/doris/pull/48318))\\n- Improved validation logic for Storage Vault creation ([#48073](https://github.com/apache/doris/pull/48073) [#48369](https://github.com/apache/doris/pull/48369))\\n\\n### Lakehouse\\n\\n- Optimized BE Scanner closure logic for Trino Connector Catalog to accelerate memory release ([#47857](https://github.com/apache/doris/pull/47857))\\n- ClickHouse JDBC Catalog now auto-adapts to different driver versions ([#46026](https://github.com/apache/doris/pull/46026))\\n\\n### Asynchronous Materialized Views\\n\\n- Enhanced planning performance for transparent rewrite ([#48782](https://github.com/apache/doris/pull/48782))\\n- Optimized `tvf mv_infos` performance ([#47415](https://github.com/apache/doris/pull/47415))\\n- Disabled catalog metadata refresh during external table-based MV construction to reduce memory usage ([#48767](https://github.com/apache/doris/pull/48767))\\n\\n### Query Optimizer\\n\\n- Improved statistics collection performance for key columns and partition columns ([#46534](https://github.com/apache/doris/pull/46534))\\n- Query result aliases now strictly match user input ([#47093](https://github.com/apache/doris/pull/47093))\\n- Enhanced column pruning after common subexpression extraction in aggregation operators ([#46627](https://github.com/apache/doris/pull/46627))\\n- Improved error messages for function binding failures and unsupported subqueries ([#47919](https://github.com/apache/doris/pull/47919) [#47985](https://github.com/apache/doris/pull/47985))\\n\\n### Semi-structured Data Management\\n\\n- `json_object` function now supports complex type parameters ([#47779](https://github.com/apache/doris/pull/47779))\\n- Added support for writing UInt128 to IPv6 type ([#48802](https://github.com/apache/doris/pull/48802))\\n- Enabled inverted index support for ARRAY fields in VARIANT type ([#47688](https://github.com/apache/doris/pull/47688) [#48117](https://github.com/apache/doris/pull/48117))\\n\\n### Security\\n\\n- Improved Ranger authorization performance ([#49352](https://github.com/apache/doris/pull/49352))\\n\\n### Others\\n\\n- Optimized JVM Metrics interface performance ([#49380](https://github.com/apache/doris/pull/49380))\\n\\n## Bug Fixes\\n\\n### Storage\\n\\n- Fixed data correctness issues in several edge cases ([#48056](https://github.com/apache/doris/pull/48056) [#48399](https://github.com/apache/doris/pull/48399) [#48400](https://github.com/apache/doris/pull/48400) [#48748](https://github.com/apache/doris/pull/48748) [#48775](https://github.com/apache/doris/pull/48775) [#48867](https://github.com/apache/doris/pull/48867) [#49165](https://github.com/apache/doris/pull/49165) [#49193](https://github.com/apache/doris/pull/49193) [#49350](https://github.com/apache/doris/pull/49350) [#49710](https://github.com/apache/doris/pull/49710) [#49825](https://github.com/apache/doris/pull/49825))\\n- Fixed untimely cleanup of completed transactions ([#49564](https://github.com/apache/doris/pull/49564))\\n- Changed JSONB default value to `{}` for partial column updates ([#49066](https://github.com/apache/doris/pull/49066))\\n- Fixed delete bitmap update lock release issue in Storage-Compute Separation model ([#47766](https://github.com/apache/doris/pull/47766))\\n- Fixed data loss in Stream Load on ARM architecture ([#49666](https://github.com/apache/doris/pull/49666))\\n- Fixed missing error URL return for data quality issues in Insert Into Select ([#49687](https://github.com/apache/doris/pull/49687))\\n- Fixed error URL reporting for multi-table Routine Load data quality issues ([#49130](https://github.com/apache/doris/pull/49130))\\n- Fixed incorrect results when using Insert Into Values during Schema Change ([#49338](https://github.com/apache/doris/pull/49338))\\n- Fixed core dump caused by tablet commit info reporting ([#48732](https://github.com/apache/doris/pull/48732))\\n- Added Azure China region support for S3 Load ([#48642](https://github.com/apache/doris/pull/48642))\\n- Fixed \\"get image failed\\" error in K8s environment ([#49072](https://github.com/apache/doris/pull/49072))\\n- Reduced CPU consumption in dynamic partition scheduling ([#48577](https://github.com/apache/doris/pull/48577))\\n- Fixed column exception after materialized view renaming ([#48328](https://github.com/apache/doris/pull/48328))\\n- Fixed memory and file cache leakage after failed Schema Change ([#48426](https://github.com/apache/doris/pull/48426))\\n- Fixed base compaction failure for tables with empty partitions ([#49062](https://github.com/apache/doris/pull/49062))\\n- Fixed data correctness issues in complex type modifications ([#49452](https://github.com/apache/doris/pull/49452))\\n- Fixed core dump in cold compaction ([#48329](https://github.com/apache/doris/pull/48329))\\n- Fixed cumulative point stagnation with delete operations ([#47282](https://github.com/apache/doris/pull/47282))\\n- Fixed memory insufficiency in large-scale full compaction ([#48958](https://github.com/apache/doris/pull/48958))\\n\\n### Compute-Storage Decoupled\\n\\n- Fixed file cache cleanup failure in K8s environment ([#49199](https://github.com/apache/doris/pull/49199))\\n- Fixed FE CPU spike caused by read-write locks during high-frequency ingestion ([#48564](https://github.com/apache/doris/pull/48564))\\n\\n### Lakehouse\\n\\n**Data Lakes**\\n\\n- Fixed BE core dump during concurrent writes to Hive/Iceberg tables ([#49842](https://github.com/apache/doris/pull/49842))\\n- Fixed write failures to Hive/Iceberg tables on AWS S3 ([#47162](https://github.com/apache/doris/pull/47162))\\n- Fixed incorrect Iceberg Position Deletion reads ([#47977](https://github.com/apache/doris/pull/47977))\\n- Added Tencent Cloud COS support for Iceberg table creation ([#49885](https://github.com/apache/doris/pull/49885))\\n- Fixed Kerberos authentication for Paimon data on HDFS ([#47192](https://github.com/apache/doris/pull/47192))\\n- Fixed memory leak in Hudi Jni Scanner ([#48955](https://github.com/apache/doris/pull/48955))\\n- Fixed multi-partition list reading in MaxCompute Catalog ([#48325](https://github.com/apache/doris/pull/48325))\\n\\n**JDBC**\\n\\n- Fixed NPE when fetching row count from JDBC Catalog ([#49442](https://github.com/apache/doris/pull/49442))\\n- Fixed OceanBase Oracle mode connection test ([#49442](https://github.com/apache/doris/pull/49442))\\n- Fixed column type length inconsistency in concurrent JDBC Catalog access ([#48541](https://github.com/apache/doris/pull/48541))\\n- Fixed Classloader leak in JDBC Catalog BE ([#46912](https://github.com/apache/doris/pull/46912))\\n- Fixed connection thread leak in PostgreSQL JDBC Catalog ([#49568](https://github.com/apache/doris/pull/49568))\\n\\n**Export**\\n\\n- Fixed EXPORT job stuck in EXPORTING state ([#47974](https://github.com/apache/doris/pull/47974))\\n- Disabled OUTFILE auto-retry to prevent duplicate files ([#48095](https://github.com/apache/doris/pull/48095))\\n\\n**Others**\\n\\n- Fixed NPE when executing TVF queries via FE WebUI ([#49213](https://github.com/apache/doris/pull/49213))\\n- Fixed Hadoop Libhdfs thread local null pointer exception ([#48280](https://github.com/apache/doris/pull/48280))\\n- Fixed \\"Filesystem already closed\\" error in FE Hadoop access ([#48351](https://github.com/apache/doris/pull/48351))\\n- Fixed Catalog comment persistence issue ([#46946](https://github.com/apache/doris/pull/46946))\\n- Fixed Parquet complex type reading errors ([#47734](https://github.com/apache/doris/pull/47734))\\n\\n### Asynchronous Materialized Views\\n\\n- Fixed slow MV construction in extreme scenarios ([#48074](https://github.com/apache/doris/pull/48074))\\n- Fixed nested MV transparent rewrite failure ([#48222](https://github.com/apache/doris/pull/48222))\\n\\n### Query Optimizer\\n\\n- Fixed constant folding calculation errors ([#49225](https://github.com/apache/doris/pull/49225) [#47966](https://github.com/apache/doris/pull/47966) [#49416](https://github.com/apache/doris/pull/49416) [#49087](https://github.com/apache/doris/pull/49087) [#49033](https://github.com/apache/doris/pull/49033) [#49061](https://github.com/apache/doris/pull/49061) [#48895](https://github.com/apache/doris/pull/48895) [#48957](https://github.com/apache/doris/pull/48957) [#47288](https://github.com/apache/doris/pull/47288) [#48641](https://github.com/apache/doris/pull/48641) [#49413](https://github.com/apache/doris/pull/49413) [#48783](https://github.com/apache/doris/pull/48783))\\n- Fixed unexpected errors with ORDER BY in nested window functions ([#48492](https://github.com/apache/doris/pull/48492))\\n\\n### Query Execution\\n\\n- Fixed pipeline task scheduling deadlocks/performance issues ([#49976](https://github.com/apache/doris/pull/49976) [#49007](https://github.com/apache/doris/pull/49007))\\n- Fixed memory corruption on FE connection failure ([#48370](https://github.com/apache/doris/pull/48370) [#48313](https://github.com/apache/doris/pull/48313))\\n- Fixed memory corruption with lambda and array functions ([#49140](https://github.com/apache/doris/pull/49140))\\n- Fixed BE core caused by null string-to-JSONB conversion ([#49810](https://github.com/apache/doris/pull/49810))\\n- Standardized undefined behaviors in `parse_url` ([#49149](https://github.com/apache/doris/pull/49149))\\n- Fixed `array_overlap` null handling ([#49403](https://github.com/apache/doris/pull/49403))\\n- Fixed case conversion errors for non-ASCII characters ([#49763](https://github.com/apache/doris/pull/49763))\\n- Fixed BE core in `percentile` function ([#48563](https://github.com/apache/doris/pull/48563))\\n- Fixed multiple memory corruption issues ([#48288](https://github.com/apache/doris/pull/48288) [#49737](https://github.com/apache/doris/pull/49737) [#48018](https://github.com/apache/doris/pull/48018) [#47964](https://github.com/apache/doris/pull/47964))\\n- Fixed incorrect SET operator results ([#48001](https://github.com/apache/doris/pull/48001))\\n- Reduced default Arrow Flight thread pool size to prevent FD exhaustion ([#48530](https://github.com/apache/doris/pull/48530))\\n- Fixed window function memory corruption ([#48458](https://github.com/apache/doris/pull/48458))\\n\\n### Semi-structured Data Management\\n\\n- Fixed chunked Stream Load JSON import ([#48474](https://github.com/apache/doris/pull/48474))\\n- Enhanced JSONB format validation ([#48731](https://github.com/apache/doris/pull/48731))\\n- Fixed crash with large STRUCT fields ([#49552](https://github.com/apache/doris/pull/49552))\\n- Extended VARCHAR length support in complex types ([#48025](https://github.com/apache/doris/pull/48025))\\n- Fixed `array_avg` crash with specific parameters ([#48691](https://github.com/apache/doris/pull/48691))\\n- Fixed `ColumnObject::pop_back` crash in VARIANT type ([#48935](https://github.com/apache/doris/pull/48935) [#48978](https://github.com/apache/doris/pull/48978))\\n- Disabled index building on VARIANT type ([#49844](https://github.com/apache/doris/pull/49844))\\n- Disabled inverted index v1 format for VARIANT type ([#49890](https://github.com/apache/doris/pull/49890))\\n- Fixed multi-layer CAST errors in VARIANT type ([#47954](https://github.com/apache/doris/pull/47954))\\n- Optimized inverted index metadata lookup for VARIANT with many subcolumns ([#48153](https://github.com/apache/doris/pull/48153))\\n- Reduced VARIANT schema memory consumption in Storage-Compute Separation mode ([#47629](https://github.com/apache/doris/pull/47629) [#48463](https://github.com/apache/doris/pull/48463))\\n- Fixed PreparedStatement ID overflow ([#48116](https://github.com/apache/doris/pull/48116))\\n- Fixed row storage with DELETE operations ([#49609](https://github.com/apache/doris/pull/49609))\\n\\n### Inverted Index\\n\\n- Fixed ARRAY type null bitmap handling ([#48052](https://github.com/apache/doris/pull/48052))\\n- Fixed Date/Datetimev1 Bloomfilter comparison ([#47005](https://github.com/apache/doris/pull/47005))\\n- Fixed UTF-8 4-byte character truncation ([#48792](https://github.com/apache/doris/pull/48792))\\n- Fixed index loss after immediate column addition ([#48547](https://github.com/apache/doris/pull/48547))\\n- Fixed empty data handling in ARRAY inverted index ([#48264](https://github.com/apache/doris/pull/48264))\\n- Improved FE metadata upgrade compatibility ([#49283](https://github.com/apache/doris/pull/49283))\\n- Fixed `match_phrase_prefix` cache error ([#46517](https://github.com/apache/doris/pull/46517))\\n- Fixed file cache cleanup after compaction ([#49738](https://github.com/apache/doris/pull/49738))\\n\\n### Security\\n\\n- Removed Select_Priv check for DELETE operations ([#49239](https://github.com/apache/doris/pull/49239))\\n- Prevented non-root users from modifying root privileges ([#48752](https://github.com/apache/doris/pull/48752))\\n- Fixed intermittent LDAP PartialResultException ([#47858](https://github.com/apache/doris/pull/47858))\\n\\n### Others\\n\\n- Fixed JAVA_OPTS_FOR_JDK_17 recognition ([#48170](https://github.com/apache/doris/pull/48170))\\n- Fixed BDB metadata write failure caused by InterruptException ([#47874](https://github.com/apache/doris/pull/47874))\\n- Improved SQL hash generation for multi-statement requests ([#48242](https://github.com/apache/doris/pull/48242))\\n- User attribute variables now override session variables ([#48548](https://github.com/apache/doris/pull/48548))"},{"id":"/tencent-music-migrate-elasticsearch-to-doris","metadata":{"permalink":"/blog/tencent-music-migrate-elasticsearch-to-doris","source":"@site/blog/tencent-music-migrate-elasticsearch-to-doris.md","title":"How Tencent Music saved 80% in costs by migrating from Elasticsearch to Apache Doris","description":"Handle full-text search, audience segmentation, and aggregation analysis directly within Apache Doris and slash their storage costs by 80% while boosting write performance by 4x","date":"2025-04-17T00:00:00.000Z","tags":[{"inline":true,"label":"Best Practice","permalink":"/blog/tags/best-practice"}],"hasTruncateMarker":false,"authors":[{"name":"velodb.io \xb7 VeloDB Engineering Team","key":null,"page":null}],"frontMatter":{"title":"How Tencent Music saved 80% in costs by migrating from Elasticsearch to Apache Doris","summary":"Handle full-text search, audience segmentation, and aggregation analysis directly within Apache Doris and slash their storage costs by 80% while boosting write performance by 4x","description":"Handle full-text search, audience segmentation, and aggregation analysis directly within Apache Doris and slash their storage costs by 80% while boosting write performance by 4x","date":"2025-04-17","author":"velodb.io \xb7 VeloDB Engineering Team","externalLink":"https://www.velodb.io/blog/1395","tags":["Best Practice"],"image":"/images/tencent-music-migrate-elasticsearch-to-doris.jpg"},"unlisted":false,"prevItem":{"title":"Apache Doris 3.0.5 Released","permalink":"/blog/release-note-3.0.5"},"nextItem":{"title":"Apache Doris 2.1.9 released","permalink":"/blog/release-note-2.1.9"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n  http://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\nAs a long-time user of Apache Doris, Tencent Music Entertainment (NYSE: TME) has undergone four generations of data platform evolution, with the Doris community actively supporting its transformation. From [replacing ClickHouse as the analytical engine](https://doris.apache.org/blog/Tencent-Data-Engineers-Why-We-Went-from-ClickHouse-to-Apache-Doris) to gradually offloading Elasticsearch\'s functionalities, TME has now taken a big step\u2014fully replacing Elasticsearch with Doris as its unified search engine. They can handle **full-text search**, **audience segmentation**, and **aggregation analysis** directly within Doris. By combining their self-developed SuperSonic with **AI-powered** **natural language processing**, they can perform data analytics through simple, conversational queries. The shift from Elasticsearch to Apache Doris has slashed their storage costs by 80% while boosting write performance by 4x. In this article, we dive into TME\'s journey, uncovering key insights that can serve as a blueprint for others navigating similar transitions.\\n\\n## What they do\\n\\nThe TME content library provides two types of functionality:\\n\\n- **Search**: Quickly locate artists, songs, and other textual data based on flexible query conditions.\\n- **Tag-based segmentation**: Filter data based on specific tags and criteria among billions of records and deliver sub-second query responses\\n\\n## A hybrid solution: Elasticsearch + Apache Doris\\n\\nTME previously used both Elasticsearch and Apache Doris in its content library platform to leverage the strengths from both:\\n\\n- **Elasticsearch** excelled in full-text search. It can quickly match specific keywords or phrases using inverted indexing while supporting indexing of all fields and flexible filtering conditions. However, it struggled with data aggregation, lacked support for complex queries like JOINs, and had high storage overhead. \\n- **Apache Doris** offered efficient OLAP capabilities for complex analytical queries while optimizing storage through high compression rates, but before the release of Apache Doris 2.0, it had limited search capabilities due to the absence of inverted index.\\n\\nThat\'s why TME built a hybrid architecture. In this setup, Elasticsearch handled full-text search and tag-based segmentation, while Apache Doris powered OLAP analytics. With Doris\' [Elasticsearch catalog](https://doris.apache.org/docs/lakehouse/database/es), data in Elasticsearch can be queried directly through Doris, creating a unified query interface for seamless data retrieval.\\n\\n![A hybrid solution: Elasticsearch + Apache Doris](/images/blog-tencent-alternative-es/a-hybrid-solution-Elasticsearch-and-Apache-Doris.png)\\n\\nDespite the advantages of the hybrid architecture, TME encountered several challenges during its implementation:\\n\\n- **High storage costs**: Elasticsearch continued to consume huge storage space.\\n- **Write performance bottlenecks**: As data volumes grew, the write pressure on the Elasticsearch cluster intensified. Full data writes were taking over 10 hours, nearing the business\'s operational limits.\\n- **Architectural complexity**: The multi-component architecture meant complex maintenance, extra costs due to redundant data storage, and higher risk of data inconsistency.\\n\\n## A unified solution based on Apache Doris\\n\\nIn [version 2.0](https://doris.apache.org/blog/release-note-2.0.0), Apache Doris introduced inverted index and started to support full-text search. This release drove TME to consider entrusting Doris with the full scope of full-text search, tag-based segmentation, and aggregation analysis tasks. \\n\\nWhat enables Doris to fully replace Elasticsearch?\\n\\n- In terms of **full-text search**, Doris accelerates standard equality and range queries (`=`, `!=`, `>`, `>=`, `<`, `<=`) and supports comprehensive text field searches, including tokenization for English, Chinese and Unicode, multi-keyword searches (`MATCH_ANY`, `MATCH_ALL`), phrase searches (`MATCH_PHRASE`, `MATCH_PHRASE_PREFIX`, `MATCH_PHRASE_REGEXP`), slop in phrase, and multi-field searches (`MULTI_MATCH`). It improves performance by orders of magnitude compared to traditional databases using `LIKE`-based fuzzy matching.\\n- As for **inverted index**, Doris implements it directly within the database kernel. Inverted indexing in Doris is seamlessly integrated with SQL syntax and supports any logical combinations for `AND`, `OR`, and `NOT` operations, so it allows for complex filtering and search queries. This is an example query involving five filtering conditions: full-text (`title MATCH \'love\' OR description MATCH_PHRASE \'true love\'`), date range filtering (`dt BETWEEN \'2024-09-10 00:00:00\' AND \'2024-09-10 23:59:59\'`), numeric range filtering (`rating > 4`), and equality check on strings (`country = \'Canada\'`). These conditions are combined into a single SQL query, after which results are grouped by `actor` and sorted by the highest count.\\n\\n```sql\\nSELECT actor, count() as cnt \\nFROM table1\\nWHERE dt BETWEEN \'2024-09-10 00:00:00\' AND \'2024-09-10 23:59:59\'\\n  AND (title MATCH \'love\' OR description MATCH_PHRASE \'true love\')\\n  AND rating > 4\\n  AND country = \'Canada\'\\nGROUP BY actor\\nORDER BY cnt DESC LIMIT 100;\\n```\\n\\nThis is the data platform after TME transitioned from a hybrid Elasticsearch + Doris architecture to a unified Doris solution.  \\n\\n![A unified solution based on Apache Doris](/images/blog-tencent-alternative-es/a-unified-solution-based-on-Apache-Doris.png)\\n\\nWith this upgrade, users can now experience:\\n\\n- **A big cost reduction**: Doris now handles both search and analytical workloads, leading to an **80%** reduction in operational costs. For example, a single business\' daily full data previously required 697.7 GB in Elasticsearch but now only takes 195.4 GB in Doris. \\n\\n![A big cost reduction](/images/blog-tencent-alternative-es/A-big-cost-reduction.png)\\n\\n- **Improved performance**: Data ingestion time was cut down from over 10 hours to under 3 hours, making Doris\' write performance **4x faster than Elasticsearch**. Additionally, Doris supports complex custom tag-based queries, enabling previously impractical analytics and significantly enhancing user experience.\\n- **Simplified architecture**: With a unified Doris-based architecture, TME now maintains a single technology stack and eliminates data inconsistency issues\\n\\nThe transition to a Doris-only architecture required several key design optimizations. In the following sections, we\'ll dive deeper into the technical strategies and lessons learned from this migration.\\n\\n### The game changer: Inverted Index\\n\\nTo optimize storage, TME adopts a dimension table + fact table model to efficiently handle search and analytics workloads:\\n\\n- **Dimension table**: Built using the [Primary Key model](https://doris.apache.org/docs/3.0/table-design/data-model/unique), dimension tables are can be easily updated via the partial column update feature of Doris. These tables are meant for both searching and tag-based segmentation.\\n- **Fact table**: Designed with the [Aggregate model](https://doris.apache.org/docs/3.0/table-design/data-model/aggregate), this table stores daily metric data. Given the high data volume and the independence of daily datasets, a new [partition](https://doris.apache.org/docs/3.0/table-design/data-partitioning/data-distribution#partitioning-strategy) is created every day to enhance query performance and manageability.\\n\\nTo ensure a seamless transition from Elasticsearch to Apache Doris, TME designs the table schemas and indexes based on Doris\' inverted index [docs](https://doris.apache.org/docs/3.0/table-design/index/inverted-index). The mapping follows these key principles:\\n\\n- Elasticsearch\'s `Keyword` type maps to Doris\' `Varchar`/`String` type with non-tokenized inverted indexing (`USING INVERTED`).\\n- Elasticsearch\'s `Text` type maps to Doris\' `Varchar`/`String` type with tokenized inverted indexing (`USING INVERTED PROPERTIES(\\"parser\\" = \\"english/unicode\\")`).\\n\\n```sql\\nCREATE TABLE `tag_baike_zipper_track_dim_string` (\\n  `dayno` date NOT NULL COMMENT \'date\',\\n  `id` int(11) NOT NULL COMMENT \'id\',\\n  `a4` varchar(65000) NULL COMMENT \'song_name\',\\n  `a43` varchar(65000) NULL COMMENT \'zyqk_singer_id\',\\n  INDEX idx_a4 (`a4`) USING INVERTED PROPERTIES(\\"parser\\" = \\"unicode\\", \\"support_phrase\\" = \\"true\\") COMMENT \'\',\\n  INDEX idx_a43 (`a43`) USING INVERTED PROPERTIES(\\"parser\\" = \\"english\\") COMMENT \'\'\\n) ENGINE=OLAP\\nUNIQUE KEY(`dayno`, `id`)\\nCOMMENT \'OLAP\'\\nPARTITION BY RANGE(`dayno`)\\n(PARTITION p99991230 VALUES [(\'9999-12-30\'), (\'9999-12-31\')))\\nDISTRIBUTED BY HASH(`id`) BUCKETS auto\\nPROPERTIES (\\n...\\n);\\n```\\n\\n**Before enabling inverted index in Doris:**\\n\\nTake the following complex query as an example: Without inverted indexing, it was slow and took minutes to return results.\\n\\n```sql\\n-- like (Low performance):\\nSELECT  * FROM db_tag_pro.tag_track_pro_3  WHERE \\ndayno=\'2024-08-01\' AND  ( concat(\'#\',a4,\'#\') like \'%#I\'m so busy dancing#%\' \\nor concat(\'#\',a43,\'#\') like \'%#1000#%\')  \\n\\n-- explode (Low performance and often triggers ERROR 1105 (HY000)):\\nSELECT * \\n FROM (\\n SELECT tab1.*,a4_single,a43_single FROM ( \\n SELECT * \\n FROM db_tag_pro.tag_track_pro_3\\n  WHERE dayno=\'2024-08-01\'\\n ) tab1 \\n lateral view explode_split(a4, \'#\') tmp1 as a4_single\\n lateral view explode_split(a43, \'#\') tmp2 as a43_single\\n ) tab2 \\n where a4_single=\'I\'m so busy dancing\' or a43_single=\'1000\'\\n```\\n\\n**After enabling inverted index in Doris:**\\n\\nThe query response times reduces **from minutes to just seconds**. A tip is to set `store_row_column` to enable row-based storage. This optimizes `select*` queries that reads all columns from a table.\\n\\n```sql\\n-- Retrieve the corresponding ID from the dimension table\\nSELECT  id FROM db_tag_pro.tag_baike_zipper_track_dim_string  WHERE\\n( a4 MATCH_PHRASE \'I\'m so busy dancing\' OR a43 MATCH_ALL \'1000\' )  AND  dayno =\'2024-08-01\'\\n\\n-- Fetch the detailed data from the fact table based on the ID\\nSELECT * FROM db_tag_pro.tag_baike_track_pro  WHERE  id IN ( 563559286  ) \\n```\\n\\nMoreover, Apache Doris overcomes a key limitation found in Elasticsearch\u2014**handling overly long SQL queries that previously failed due to length constraints**. Doris supports longer and more complex queries with ease. Additionally, using Doris as the unified engine means that users can leverage materialized views and BITMAP data type to further optimize intermediate query results. This eliminates the need for cross-engine synchronization.\\n\\n### Multi-service resource isolation\\n\\nTo ensure a cost-effective and seamless user experience, TME leverages Doris\' resource isolation mechanism for efficient workload management across different business scenarios.\\n\\n- **Layer 1: Physical isolation (Resource Group)** They divide the cluster into two Resource Groups to serve difference workloads: Core and Normal. The Core group is dedicated to mission-critical tasks such as content search and tag-based segmentation, while the Normal group handles general-purpose queries. This node-level physical isolation ensures that high-priority operations remain unaffected by other workloads.\\n- **Layer 2: Logical isolation (Workload Group)** Within each physically isolated Resource Group, resources are further divided into Workload Groups. For example, TME creates multiple Workload Groups within the Normal resource group, and assign a default Workload Group to each user. In this way, they prevent any single user from monopolizing cluster resources.\\n\\n![Multi-service resource isolation](/images/blog-tencent-alternative-es/Multi-service-resource-isolation.png)\\n\\nThese resource isolation mechanisms improve system stability. **In TME\'s case, the frequency of alerts has reduced from over 20 times per day to single digits per month**. The team can now focus more on system optimization and performance improvements rather than constant firefighting.\\n\\n## A seamless migration\\n\\nTME implements the migration via their self-developed [SuperSonic](https://github.com/tencentmusic/supersonic) project, which has a built-in Headless BI feature to simplify the process. All they need is to convert the queries written in Elasticsearch\'s Domain Specific Language (DSL) into SQL statements, and switch the data sources for pre-defined metrics and tags. \\n\\nThe idea of Headless BI is to decouple data modeling, management, and consumption. With it, business analysts can define metrics and tags directly on the Headless BI platform without worrying about the underlying data sources. Because Headless BI abstracts away differences between various data storage and analytics engines, users can experience a transparent, frictionless migration without disruptions.\\n\\n![A seamless migration](/images/blog-tencent-alternative-es/A-seamless-migration.png)\\n\\nThe Headless BI enables seamless data source migration and largely simplifies data management and querying. SuperSonic takes this a step further by integrating Chat BI capabilities with Headless BI, so users can perform unified data governance and data analysis using natural language. Originally developed and battle-tested in-house by TME, the SuperSonic platform is now open source: https://github.com/tencentmusic/supersonic\\n\\n## What\'s next\\n\\nThe migration from Elasticsearch to Apache Doris has yielded impressive gains. Write performance has improved 4x and storage usage has dropped by 72%, while the overall operational costs have been cut by up to 80%.\\n\\nBy replacing its Elasticsearch cluster with Doris, TME has unified its content library\'s search and analytics engines into a single, streamlined platform. The system now supports complex custom tag-based segmentation with sub-second response. The next-phase plan of TME is to explore broader use cases of Apache Doris and prepare to adopt the [compute-storage decoupled mode](https://doris.apache.org/docs/3.0/compute-storage-decoupled/overview) to drive even greater cost efficiency. \\n\\nFor direct communication, real-world insights, and best practices, join [#elasticsearch-to-doris](https://apachedoriscommunity.slack.com/archives/C08CQKX20R5) channel in the [Apache Doris community](https://join.slack.com/t/apachedoriscommunity/shared_invite/zt-2gmq5o30h-455W226d79zP3L96ZhXIoQ)."},{"id":"/release-note-2.1.9","metadata":{"permalink":"/blog/release-note-2.1.9","source":"@site/blog/release-note-2.1.9.md","title":"Apache Doris 2.1.9 released","description":"This version features improved SQL hash calculation, enhanced query result accuracy, and new metrics for better storage management. It also resolves critical bugs across multiple areas, enhancing the data management experience.","date":"2025-04-03T00:00:00.000Z","tags":[{"inline":true,"label":"Release Notes","permalink":"/blog/tags/release-notes"}],"hasTruncateMarker":false,"authors":[{"name":"Apache Doris","key":null,"page":null}],"frontMatter":{"title":"Apache Doris 2.1.9 released","summary":"This version features improved SQL hash calculation, enhanced query result accuracy, and new metrics for better storage management. It also resolves critical bugs across multiple areas, enhancing the data management experience.","description":"This version features improved SQL hash calculation, enhanced query result accuracy, and new metrics for better storage management. It also resolves critical bugs across multiple areas, enhancing the data management experience.","date":"2025-04-03","author":"Apache Doris","tags":["Release Notes"],"image":"/images/2.1.9.jpg"},"unlisted":false,"prevItem":{"title":"How Tencent Music saved 80% in costs by migrating from Elasticsearch to Apache Doris","permalink":"/blog/tencent-music-migrate-elasticsearch-to-doris"},"nextItem":{"title":"Why Apache Doris is a Better Alternative to Elasticsearch for Real-Time Analytics","permalink":"/blog/why-apache-doris-is-best-alternatives-for-real-time-analytics"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n  http://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n\\nDear Community, **Apache Doris version 2.1.9 is now available**, featuring improved SQLHash calculation, enhanced query results accuracy, and new metrics for better storage management. This update also resolves critical bugs across multiple areas for a more robust data management experience.\\n\\n\\n- [Quick Download](https://doris.apache.org/download)\\n\\n- [GitHub Release](https://github.com/apache/doris/releases/tag/2.1.9-rc02)\\n\\n\\n## Behavior Changes\\n\\n- The SQLHash in Audit Log is now accurately calculated per SQL query, resolving the issue of identical hashes in a single request. [#48242](https://github.com/apache/doris/pull/48242)\\n- Query results match ColumnLabelName exactly. [#47093](https://github.com/apache/doris/pull/47093)\\n- User property variables now take precedence over session variables. [#47185](https://github.com/apache/doris/pull/47185)\\n\\n## New Features\\n\\n### Storage Management\\n\\n- Disallow renaming partition columns. [#47596](https://github.com/apache/doris/pull/47596)\\n\\n### Others\\n\\n- Added FE monitoring metrics for Catalogs, Databases, and Tables counts. [#47891](https://github.com/apache/doris/pull/47891)\\n\\n## Improvements\\n\\n### Inverted Index\\n\\n- Support for ARRAY type in VARIANT inverted indexes. [#47688](https://github.com/apache/doris/pull/47688)\\n- Profile now shows performance metrics for each filter condition. [#47504](https://github.com/apache/doris/pull/47504)\\n\\n### Query Optimizer\\n\\n- Support for using `SELECT *` in aggregate queries with only aggregation key columns. [#48006](https://github.com/apache/doris/pull/48006)\\n\\n### Storage Management\\n\\n- Enhanced CCR for binlog recycling and small file transfer efficiency, and robustness in chaotic environments. [#47547](https://github.com/apache/doris/pull/47547) [#47313](https://github.com/apache/doris/pull/47313) [#45061](https://github.com/apache/doris/pull/45061)\\n- Enhanced import error messages to be more specific. [#47918](https://github.com/apache/doris/pull/47918) [#47470](https://github.com/apache/doris/pull/47470)\\n\\n## Bug Fixes\\n\\n### Lakehouse\\n\\n- Fixed BE krb5.conf path configuration issue. [#47679](https://github.com/apache/doris/pull/47679)\\n- Prevented `SELECT OUTFILE` statement retries to avoid duplicate data export. [#48095](https://github.com/apache/doris/pull/48095)\\n- Fixed JAVA API access to Paimon tables. [#47192](https://github.com/apache/doris/pull/47192)\\n- Resolved writing to Hive tables with `s3a://` storage location. [#47162](https://github.com/apache/doris/pull/47162)\\n- Fixed the issue of Catalog\'s Comment field not being persisted. [#46946](https://github.com/apache/doris/pull/46946)\\n- Addressed JDBC BE class loading leaks under certain conditions. [#46912](https://github.com/apache/doris/pull/46912)\\n- Resolved high version ClickHouse JDBC Driver compatibility with JDBC Catalog. [#46026](https://github.com/apache/doris/pull/46026)\\n- Fixed BE crash when reading Iceberg Position Delete. [#47977](https://github.com/apache/doris/pull/47977)\\n- Corrected reading MaxCompute table data under multi-partition columns. [#48325](https://github.com/apache/doris/pull/48325)\\n- Fixed reading Parquet complex column types errors. [#47734](https://github.com/apache/doris/pull/47734)\\n\\n### Inverted Index\\n\\n- Fixed ARRAY type inverted index null value handling. [#48231](https://github.com/apache/doris/pull/48231)\\n- Resolved `BUILD INDEX` exception for newly added columns. [#48389](https://github.com/apache/doris/pull/48389)\\n- Corrected UTF8 encoding index truncation issues leading to errors. [#48657](https://github.com/apache/doris/pull/48657)\\n\\n### Semi-structured Data Types\\n\\n- Fixed `array_agg` function crashes under special conditions. [#46927](https://github.com/apache/doris/pull/46927)\\n- Resolved JSON import crashes due to incorrect chunk parameters. [#48196](https://github.com/apache/doris/pull/48196)\\n\\n### Query Optimizer\\n\\n- Fixed constant folding issues with nested time functions like `current_date`. [#47288](https://github.com/apache/doris/pull/47288)\\n- Addressed non-deterministic function result errors. [#48321](https://github.com/apache/doris/pull/48321)\\n- Resolved `CREATE TABLE LIKE` execution issues with on update column properties. [#48007](https://github.com/apache/doris/pull/48007)\\n- Fixed unexpected planning errors for materialized views of aggregate model tables. [#47658](https://github.com/apache/doris/pull/47658)\\n- Resolved `PreparedStatement` exceptions due to internal ID overflow. [#47950](https://github.com/apache/doris/pull/47950)\\n\\n### Query Execution Engine\\n\\n- Resolved query hang or null pointer issues when querying system tables. [#48370](https://github.com/apache/doris/pull/48370)\\n- Added DOUBLE type support for LEAD/LAG functions. [#47940](https://github.com/apache/doris/pull/47940)\\n- Fixed query errors when `case when` conditions exceed 256. [#47179](https://github.com/apache/doris/pull/47179)\\n- Corrected `str_to_date` function errors with spaces. [#48920](https://github.com/apache/doris/pull/48920)\\n- Fixed `split_part` function errors during constant folding with `||`. [#48910](https://github.com/apache/doris/pull/48910)\\n- Corrected `log` function result errors. [#47228](https://github.com/apache/doris/pull/47228)\\n- Resolved core dump issues with `array` / `map` functions in lambda expressions. [#49140](https://github.com/apache/doris/pull/49140)\\n\\n### Storage Management\\n\\n- Fixed memory corruption issues during import of aggregate tables. [#47523](https://github.com/apache/doris/pull/47523)\\n- Resolved occasional core dump during MoW import under memory pressure. [#47715](https://github.com/apache/doris/pull/47715)\\n- Fixed potential duplicate key issues with MoW during BE restart and schema change. [#48056](https://github.com/apache/doris/pull/48056) [#48775](https://github.com/apache/doris/pull/48775)\\n- Corrected group commit and global column update issues with memtable promotion. [#48120](https://github.com/apache/doris/pull/48120) [#47968](https://github.com/apache/doris/pull/47968)\\n\\n### Permission Management\\n\\n- No longer throws PartialResultException when using LDAP. [#47858](https://github.com/apache/doris/pull/47858)"},{"id":"/why-apache-doris-is-best-alternatives-for-real-time-analytics","metadata":{"permalink":"/blog/why-apache-doris-is-best-alternatives-for-real-time-analytics","source":"@site/blog/why-apache-doris-is-best-alternatives-for-real-time-analytics.md","title":"Why Apache Doris is a Better Alternative to Elasticsearch for Real-Time Analytics","description":"Apache Doris is a real-time data warehouse commonly used for observability, cyber security analysis, online reports, customer profiles, data lakehouse and more. Elasticsearch is more like a search engine, but it is also widely used for data analytics, so there\'s an overlap in their use cases. The comparison in this post will focus on the real-time analytics capabilities of Apache Doris and Elasticsearch from a user-oriented perspective","date":"2025-03-25T00:00:00.000Z","tags":[{"inline":true,"label":"Tech Sharing","permalink":"/blog/tags/tech-sharing"}],"hasTruncateMarker":false,"authors":[{"name":"Medium \xb7 Kang, Apache Doris PMC Member","key":null,"page":null}],"frontMatter":{"title":"Why Apache Doris is a Better Alternative to Elasticsearch for Real-Time Analytics","summary":"Apache Doris is a real-time data warehouse commonly used for observability, cyber security analysis, online reports, customer profiles, data lakehouse and more. Elasticsearch is more like a search engine, but it is also widely used for data analytics, so there\'s an overlap in their use cases. The comparison in this post will focus on the real-time analytics capabilities of Apache Doris and Elasticsearch from a user-oriented perspective","description":"Apache Doris is a real-time data warehouse commonly used for observability, cyber security analysis, online reports, customer profiles, data lakehouse and more. Elasticsearch is more like a search engine, but it is also widely used for data analytics, so there\'s an overlap in their use cases. The comparison in this post will focus on the real-time analytics capabilities of Apache Doris and Elasticsearch from a user-oriented perspective","date":"2025-03-25","author":"Medium \xb7 Kang, Apache Doris PMC Member","externalLink":"https://medium.com/@kxiao.tiger/apache-doris-vs-elasticsearch-6f7c8232e012","tags":["Tech Sharing"],"image":"/images/es-alternatives/Alternative-to-Elasticsearch.jpg"},"unlisted":false,"prevItem":{"title":"Apache Doris 2.1.9 released","permalink":"/blog/release-note-2.1.9"},"nextItem":{"title":"Slash your cost by 90% with Apache Doris Compute-Storage Decoupled Mode","permalink":"/blog/doris-compute-storage-decoupled"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n  http://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\nThe author previously presented this topic at the VeloDB Webinar. This article expands on that presentation by providing more detailed comparative information, including test data and in-depth technical explanations: [https://www.youtube.com/embed/qnxX-FOd8Wc?si=TcEF_w-XhqgQyP4A](https://www.youtube.com/embed/qnxX-FOd8Wc?si=TcEF_w-XhqgQyP4A)\\n\\n\\nIn the past year, there\'s an increasing number of users looking to use Apache Doris as an alternative to Elasticsearch, so I\'d like to provide an in-depth comparison of the two to serve as a reference for users.\\n\\n[Apache Doris](https://doris.apache.org) is a real-time data warehouse commonly used for observability, cyber security analysis, online reports, customer profiles, data lakehouse and more. [Elasticsearch](https://www.elastic.co/elasticsearch) is more like a search engine, but it is also widely used for data analytics, so there\'s an overlap in their use cases. The comparison in this post will focus on the real-time analytics capabilities of Apache Doris and Elasticsearch from a user-oriented perspective:\\n\\n1. **Open source**\\n2. **System architecture**\\n3. **Real-time writes**\\n4. **Real-time storage**\\n5. **Real-time queries**\\n\\n## Open source\\n\\nThe license decides the level of openness of an open source product, and further decides whether users will be trapped in a vendor lock-in situation. \\n\\n**Apache Doris** is operated under the Apache Software Foundation and it is governed by [Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0.html). This is a highly liberal open source license. Anyone can freely use, modify, and distribute Apache Doris in open source or commercial projects.\\n\\n**Elasticsearch** has undergone several changes in licenses. It was subject to Apache License 2.0 at the very beginning. Then in [2021](https://www.elastic.co/blog/licensing-change), it switched to the [Elastic License](https://www.elastic.co/licensing/elastic-license) and [SSPL](https://www.mongodb.com/licensing/server-side-public-license), mostly because some cloud service providers were providing Elasticsearch as a managed service. The Elastic company tried to protect its business interests and moved to the Elastic License in order to restrict certain commercial use. And then in 2024, Elasticsearch announced that [\\"Elasticsearch Is Open Source. Again!\\"](https://www.elastic.co/blog/elasticsearch-is-open-source-again) by adding the AGPL License. This license has less restrictions on cloud providers.\\n\\nThe difference in licenses reflects the differing ways in which the two open-source projects are managed and operated. Apache Doris is operated under the Apache Software Foundation and adheres to \\"the Apache Way\\" and maintains vendor neutrality. It will always be under Apache License and maintain a high level of openness. Elasticsearch is owned and run by the Elastic company, so it is free to change its license based on the changing needs of the company.\\n\\n## System architecture\\n\\nThe system architecture of Apache Doris and Elasticsearch is relevant to how users can deploy them and the software/hardware prerequisites that must be met.\\n\\n**Apache Doris** supports various deployment models, especially after the [release of verion 3.0](https://doris.apache.org/blog/release-note-3.0.0). It can be deployed on-premise the traditional way, which means compute and storage are integrated within the same hardware. It can also be deployed with [compute and storage decoupled](https://doris.apache.org/docs/3.0/compute-storage-decoupled/overview), providing higher flexibility and elasticity. \\n\\n![System architecture.jpeg](/images/es-alternatives/System-architecture.jpeg)\\n\\nApache Doris enables the isolation of computing workloads, which makes it well-suited for multi-tenancy. In addition to decoupling compute and storage, it also provides tiered storage so you can choose different storage medium for your hot and cold data. \\n\\nFor workload isolation, Apache Doris provides the [Compute Group](https://doris.apache.org/docs/3.0/admin-manual/workload-management/compute-group) and [Workload Group](https://doris.apache.org/docs/3.0/admin-manual/workload-management/workload-group) mechanism. \\n\\n![System architecture-2.png](/images/es-alternatives/System-architecture-2.png)\\n\\nCompute Group is a mechanism for physical isolation between different workloads in a compute-storage decoupled architecture. One or more BE nodes can form a Compute Group. \\n\\n![System architecture-3.png](/images/es-alternatives/System-architecture-3.png)\\n\\nWorkload Group is an in-process mechanism for isolating workloads. It achieves resource isolation by finely partitioning or limiting resources (CPU, IO, Memory) within the BE process, which is implemented by Linux CGroup and offers strict hard isolation.\\n\\n**Elasticsearch** supports on-premise and cloud deployment. It does not support compute-storage decoupling. Elasticsearch implements workload isolation by Thread Groups, which provides limited soft resource isolation.\\n\\n## Real-time writes\\n\\nThe next step after deploying the system is to write data into it. Apache Doris and Elasticsearch are very different in data ingestion. \\n\\n### Write capabilities\\n\\nNormally there are two ways to write real-time data into a data system. One is push-based, and the other is pull-based. A push-based method means users actively push the data into the database system, such as via HTTP. A pull-based method, for example, is where database system pulls data from data source such as Kafka message queue. \\n\\n**Elasticsearch** supports push-based ingestion, but it requires Logstash to perform pull-based data ingestion, making it less convenient.\\n\\nAs for **Apache Doris**, it supports both push-based method (HTTP [Stream Load](https://doris.apache.org/docs/3.0/data-operate/import/import-way/stream-load-manual)) and pull-based method ([Routine Load](https://doris.apache.org/docs/3.0/data-operate/import/import-way/routine-load-manual) from Kafka, [Broker Load](https://doris.apache.org/docs/3.0/data-operate/import/import-way/broker-load-manual) from object storage and HDFS). In addition, output plugins for [Logstash](https://doris.apache.org/docs/ecosystem/observability/logstash) and [Beats](https://doris.apache.org/docs/ecosystem/observability/beats) are available to enable seamless data ingestion from Logstash or Beats into Doris.\\n\\nIn addition, Doris provides a special write [transaction](https://doris.apache.org/docs/3.0/data-operate/transaction) mechanism. By setting a [label](https://doris.apache.org/docs/3.0/data-operate/transaction#label-mechanism) for a batch of data through the Load API, attempting to re-load a label that has been successfully load before will result in an error, thereby achieving data deduplication. This mechanism ensures that data is written without loss or duplication without relying on the uniqueness of primary keys at the storage layer. Additionally, having a unique label for each batch of data offers better performance compared to having a unique primary key for each individual record.\\n\\n### Write performance\\n\\n**Apache Doris**, as a real-time data warehouse, supports real-time data writes and updates. One of the standout features of Doris is its incredibly high write throughput. This is because the Doris community has put a lot of effort in optimizing it for high-throughput writes such as vertorized writting and  single replica indexing. \\n\\nOn the other hand, the low write throughput of **Elasticsearch** is a well-known pain point. This is due to Elasticsearch\'s needs to generate complex inverted index on multiple replicas, causing a lot of overheads.\\n\\nWe compare the write performance of Doris and Elasticsearch using Elasticsearch\'s official benchmark [httplogs](https://github.com/elastic/rally-tracks/tree/master/http_logs), under the same hardware resources and storage schema (including field types and inverted indexes). The test environment and results are as follows:\\n\\n![write performance.png](/images/es-alternatives/write-performance.png)\\n\\nUnder the premise of creating the same inverted index, Apache Doris delivers a much higher write throughput than Elasticsearch. This is due to some key advancements in performance optimization, including:\\n\\n1. Doris is implemented in C++, which is more efficient than Elasticsearch\'s Java implementation.\\n2. The vectorized execution engine of Doris fully utilizes CPU SIMD instructions to accelerate both data writing and inverted index building. Its columnar storage also facilitates vectorized execution.\\n3. The inverted index of Doris is simplified for real-time analytics scenarios, eliminating unnecessary storage structures such as forward indexes and norms.\\n\\n## Real-time storage\\n\\n### Storage format and cost\\n\\nAs a real-time data warehouse, Doris adheres to the typical relational database model, with a storage hierarchy that includes **catalog**, **database**, and **table** levels. A table consists of one or more columns, each with its own data type, and indexes can be created on the columns.\\n\\nBy default, Doris stores tables in a columnar format, meaning that data for a single column is stored in contiguous physical files. This not only achieves a high compression ratio but also ensures high query efficiency in analysis scenarios where only certain columns are accessed, as only the required data is read and processed. Additionally, Doris supports an optional row storage format to accelerate detailed point queries.\\n\\nIn Elasticsearch, data is stored in Lucene using a document model. In this model, an Elasticsearch index is equivalent to a database table, the index mapping corresponds to the database table schema, and fields within the index mapping are akin to columns in a database, each with its own type and index type.\\n\\nBy default, Elasticsearch uses row-based storage (the _source field), where each field has an associated inverted index created for it, but it also supports optional columnar storage (docvalue). Generally, row-based storage is essential in Elasticsearch because it lays the foundation for raw detailed data queries.\\n\\nStorage space consumption, or more straightforward, storage cost, is a big concern for users. This is another pain point of Elasticsearch - it creates huge storage costs. \\n\\nApache Doris community have made a lot of optimizations and successfully reduced the storage cost significantly. We have put a lot of work to simplify [inverted index](https://doris.apache.org/docs/3.0/table-design/index/inverted-index), which is supported in Doris since version 2.0, and it now takes up much less space. **Doris** supports the ZSTD algorithm, which is much more efficient than GZIP and LZ4, and it can reach a data compression ratio of 5:1 ~ 10:1. Since the compression ratio of **Elasticsearch** is usually about 1.5:1, Doris is 3~5 times more efficient than Elasticsearch in data compression.\\n\\nAs demonstrated in the httplogs test results table above, 32GB of raw data occupies 3.2GB in Doris, whereas Elasticsearch defaults to 19.4GB. Even with the latest logsdb optimization mode enabled, Elasticsearch still consumes 11.7GB. Compared to Elasticsearch, Doris reduces storage space by 83% (73% with logsdb enabled).\\n\\n### Table model\\n\\nApache Doris provides three data models. As for Elasticsearch, I put it as \\"one and two half\\" models because it only provides limited support for certain data storage models. I will get to them one by one.\\n\\n- The first is the Duplicate model, which means it allows data duplication. This is suitable for storing logs. Both Doris and Elasticsearch support this model.\\n\\n- The second is the primary key model. It works like OLTP databases, which means the data will be deduplicated by key. The [primary key model](https://doris.apache.org/docs/table-design/data-model/unique) in **Doris** provides high performance and many user-friendly features. Just like many databases, you can define one or multiple fields as the primary and unique key. \\n  -  However, in **Elasticsearch**, you can only use a special field [_id](https://www.elastic.co/guide/en/elasticsearch/reference/8.17/mapping-id-field.html) as the unique identifier for a document (a row in database). Unlike database primary key, there are many limitations for it.\\n\\n  - The `_id` field can not be used for aggregation or sorting\\n  - The `_id` field can not be larger than 512 bytes\\n  - The `_id` field can not be multiple fields, so you will need to combine the multiple fields into one before you can use them as the primary key, but the length of the combined _id can still not exceed 512 bytes.\\n\\n- The third is the aggregation model, which means data will be aggregated or rolled up. \\n\\nIn its early stages, Elasticsearch provided the [rollup](https://www.elastic.co/guide/en/elasticsearch/reference/8.17/rollup-getting-started.html) feature through the commercial X-Pack plugin, allowing users to create rollup jobs that configured dimensions, metric fields, and aggregation intervals for rollup indexes based on a base index. However, X-Pack rollup had several limitations:\\n\\n1. **Data Asynchrony**: Rollup jobs ran in the background, meaning the data in the rollup index was not synchronized in real time with the base index.  \\n2. **Specialized Query Requirement**: Queries targeting rolled-up data required dedicated rollup queries, and users had to manually specify the rollup index in their queries.\\n\\nPerhaps these reasons explain why **Elasticsearch has deprecated the use of rollup starting in version 8.11.0** and now recommends **[downsampling](https://www.elastic.co/guide/en/elasticsearch/reference/8.17/downsampling.html)** as an alternative. Downsampling eliminates the need to specify a separate index and simplifies querying, but it also comes with its own constraints:\\n\\n1. **Time-Series Exclusive**: Downsampling is only applicable to time-series data, relying on time as the primary dimension. It cannot be used for other data types, such as business reporting data.\\n2. **Index Replacement**: A downsampled index replaces the original index, meaning aggregated data and raw data cannot coexist.\\n3. **Read-Only**: Downsampling can only be performed on the original index after it transitions to a read-only state. Data actively being written (e.g., real-time ingestion) cannot undergo downsampling.\\n\\nAs a real-time data warehouse excelling in aggregation and analytics, **Doris** has supported aggregation capabilities since its early use cases in online reporting and analytics. It offers two flexible mechanisms:  \\n\\n1. **[Aggregation Table Model](https://doris.apache.org/docs/3.0/table-design/data-model/aggregate)**:\\n    - Data is directly aggregated during ingestion, eliminating the storage of raw data.  \\n    - Only aggregated results are stored, drastically reducing storage costs.\\n2. **[Aggregated Materialized Views](https://doris.apache.org/docs/query-acceleration/materialized-view/sync-materialized-view)/ Rollup**:  \\n    - Users can create aggregated materialized views on a base table. Data is written to both the base table and the materialized view simultaneously, ensuring atomic and synchronized updates.\\n    - Queries target the base table, and the query optimizer automatically rewrites queries to leverage the materialized view for accelerated performance.\\n\\nThis design ensures real-time aggregation while maintaining query flexibility and efficiency in Doris.\\n\\n### Flexible schema\\n\\nIn most cases, especially for online business scenarios, users often need to update or modify the data schema. Elasticsearch supports flexible schema, but it only allows users to dynamically add columns.\\n\\n![Flexible schema.png](/images/es-alternatives/Flexible-schema.png)\\n\\nWhen you write JSON data into Elasticsearch, if the data contains a new field which is not in index mapping, Elasticsearch will create a new field in the schema through [dynamic mapping](https://www.elastic.co/guide/en/elasticsearch/reference/8.17/dynamic-field-mapping.html), so you don\'t need to change the  schema beforehand, like you have to do in traditional databases. However, in Elasticsearch, users cannot directly delete a field of the schema. For that, you will need a reindex operation that reads and writes the entire index in Elasticsearch. \\n\\nIn addition, Elasticsearch does not allow adding index for an already existed field. Imagine that you have 100 fields in a schema, and you have created inverted index for 10 of them, and then some days later, you want to add inverted index for another 5 fields, but no, you can\'t do that in Elasticsearch. Likewise, deleting index for a field is not allowed either.\\n\\nAs a result, to avoid such troubles, most Elasticsearch users will just create inverted index for all of their fields, but that causes much slower data writing and much larger storage usage.\\n\\nAlso, Elasticsearch does not allow you to modify your index names or your field names. To sum up, if you need to make any changes to your data other than adding a field, you will need to re-index everything. That\'s a highly resource-intensive and time-consuming operation.\\n\\nWhat about **Apache Doris**? Doris supports all these [schema changes](https://doris.apache.org/docs/3.0/table-design/schema-change). You can add or delete fields and [indexes](https://doris.apache.org/docs/3.0/table-design/index/inverted-index#managing-indexes) as you want, and you can change the name of any table or field easily. Doris can finish these changes within just a few milliseconds. Such flexibility will save you a lot of time and resource, especially when you have changing business needs.\\n\\n## Real-time queries\\n\\nWe break down the real-time querying of Elasticsearch and Apache Doris into three dimensions: usability, query capabilities, and performance.\\n\\n### Usability\\n\\nOne key aspect of usability is reflected in the user interface.\\n\\nThe Doris SQL interface is designed to maintain protocol and syntax compatibility with MySQL. It even does not have its own client or JDBC driver, allowing the use of MySQL\u2019s client and JDBC driver directly. This is a big convenience for many users familiar with MySQL. In fact, this choice was made early in the original design of Doris to make it more user-friendly. Since MySQL is the most widely used open-source OLTP database, Doris is designed to complement MySQL, creating the best practice for combining OLTP (MySQL) with OLAP (Doris). This allows engineers and data analysts to work seamlessly within a single interface and set of syntax, covering both transactional and analytical workloads.\\n\\nOn the other hand, Elasticsearch has its own Domain Specific Language (DSL), which is originally designed for searching. This DSL is part of the Elasticsearch ecosystem, it often requires engineers to invest a lot of time and effort to fully understand and use the DSL effectively.\\n\\nLet\'s take a look at one example: \\n\\n![Real-time queries.png](/images/es-alternatives/Real-time-queries.png)\\n\\nImagine we want to search for a particular keyword within a particular time range, and then group and order the data by time interval to visualize it as a trend chart. \\n\\nIn Doris, this can be done with only 7 lines of SQL. However, in Elasticsearch, the same process requires 30 lines. \\n\\nAnd it\'s not just about the code complexity. I believe many of you may find this relatable: The first attempt to learn Elasticsearch\'s DSL is quite challenging. And even after gaining familiarity with it, crafting a query often requires frequent reference to documentation and examples. Due to its inherent complexity, writing DSL queries from scratch remains a challenging task. In contrast, SQL is simple, clear, and highly structured. Most engineers can easily write a query like this without much effort.\\n\\n### Query capabilities\\n\\n**Elasticsearch** is good at searching. As its name suggests, it was built for search. But beyond search and aggregation queries, it doesn\'t support complex analytical queries, such as multi-table JOIN.\\n\\n**Apache Doris** is a general-purpose data warehouse. It provides strong support for complex analytics, including multi-table JOIN, UDF, subquery, window function, logic view, materialized view, and data lakehouse. And Doris has been improving its searching capabilities since version 2.0. We have introduced inverted index and full-text search, so it is getting increasingly competitive in this area. \\n\\n#### Search\\n\\nWhile this article focuses on real-time analytics scenarios, we have chosen not to overlook **Elasticsearch\u2019s core strength**: its search capabilities.  \\n\\nElasticsearch, renowned for its search performance, leverages the **Apache Lucene library** under the hood. It provides two primary indexing structures:  \\n\\n- **Inverted indexes** for text fields.  \\n- **BKD-Tree indexes** for numeric, date, and geospatial data.\\n\\nElasticsearch supports the following search paradigms:\\n\\n1. **Exact Matching**:  equality or range queries for numeric, date, or non-tokenized string fields (keyword), powered by **BKD-Tree indexes** (numeric/date) or **non-tokenized inverted indexes** (exact term matches).\\n2. **[Full-Text Search](https://www.elastic.co/guide/en/elasticsearch/reference/8.17/full-text-queries.html)**:  keyword, phrase, or multi-term matching. During ingestion, text is split into tokens using a configurable tokenizer. Queries match tokens, phrases, or combinations (e.g., `\\"quick brown fox\\"`).  Elasticsearch supports some advanced full-text search features, such as relevance scoring, auto-completion, spell-checking, and search suggestions.\\n3. **Vector Search based on ANN index.** \\n\\n**Doris** also supports a rich set of indexes to accelerate queries, including **prefix-sorted indexes**,  **ZoneMap indexes**,  **BloomFilter indexes and N-Gram BloomFilter indexes**.  \\n\\nStarting from version 2.0, Doris has added **inverted indexes** and BKD-Tree indexes to support **exact matching** and full-text search. **Vector search** is currently under development and is expected to be available within the next six months.\\n\\nThere are some **differences between the indexes of Apache Doris and Elasticsearch**:  \\n\\n1. **[Diverse indexing strategies](https://doris.apache.org/docs/3.0/table-design/index/index-overview)**: Doris does not rely solely on inverted indexes for query acceleration. It offers different indexes for different scenarios. **ZoneMap**, BloomFilter, and **N-Gram BloomFilter indexes** are **skip indexes** that accelerate analytical queries by skipping non-matching data blocks based on `WHERE` conditions. The **N-Gram BloomFilter index** is specifically optimized for `LIKE`-style fuzzy string matching. Inverted index and Prefix-sorted indexes are point query indexes that speed up point queries by locating which rows satisfy the WHERE conditions through the index and directly read those rows.\\n2. **Performance centric design**: Doris prioritizes **speed and efficiency** over advanced search features. To achieve faster index building and query performance (see the Query Performance section in the following), it omits certain complex functionalities of inverted indexes at present, such as relevance scoring, auto-completion, spell-checking, and search suggestions. Since there are more critical in document or web search scenarios, which are not Doris\u2019 primary focus on real-time analytics.\\n\\nFor **exact matching** and **common full-text search** requirements, such as term, range, phrase, multi-field matching, Doris is fully capable and reliable in most use cases. For **complex search needs** (e.g., relevance scoring, auto-completion, spell-checking, or search suggestions), **Elasticsearch** remains the more suitable choice.\\n\\n#### Aggregation\\n\\nBoth **Elasticsearch** and **Doris** support a wide range of aggregation operators, including:  \\n\\n- **Basic aggregations**: `MIN`, `MAX`, `COUNT`, `SUM`, `AVG`.  \\n- **Advanced aggregations**: `PERCENTILE` (quantiles), `HISTOGRAM` (bucketed distributions).  \\n- **Aggregation modes**: Global aggregation, key-based grouping (e.g., `GROUP BY`).  \\n\\nHowever, their approaches to aggregation analysis differ significantly in the following aspects:  \\n\\n**1. Query syntax and usability:** Doris uses **standard SQL syntax** (`GROUP BY` clauses + aggregation functions), making it intuitive for analysts and developers familiar with SQL. **Elasticsearch** relies on a **custom Domain-Specific Language (DSL)** with concepts like `metric agg`, `bucket agg`, and `pipeline agg`. Complexity increases with nested or multi-layered aggregations, requiring steeper learning curves.  \\n\\n**2. Result accuracy:** Elasticsearch outputs inaccurate approximate result for many aggregation types by default. Aggregations (e.g., `terms` agg) are executed per shard, with each shard returning only its top results (e.g., top 10 buckets). These partial results are merged globally, leading to potentially inaccurate final outputs. As a serious OLAP database, Doris ensures exact results by processing full datasets without approximations.\\n\\n**3. Query Performance:** Doris demonstrates **significant performance advantages over Elasticsearch** in aggregation-heavy OLAP workloads like those on ClickBench (see more in the Query performance section bellow).  \\n\\n#### JOIN\\n\\n**Elasticsearch** does not support JOIN, making it unable to execute common benchmarks like TPC-H or TPC-DS. Since JOINs are critical in data analysis, Elasticsearch provides some complex [workarounds](https://www.elastic.co/guide/en/elasticsearch/reference/8.17/joining-queries.html) with significant limitations:\\n\\n1. **[Parent-child](https://www.elastic.co/guide/en/elasticsearch/reference/8.17/parent-join.html)** relationships and has_child / has_parent queries: Simulate JOINs by establishing parent-child relationships within a single index. Child documents store the parent document\u2019s ID. `has_child` queries first find matching child documents, then retrieve their parent documents via the stored parent IDs. `has_parent` queries reverse this logic.  It\'s complex and Elasticsearch explicitly warns against equating this approach to database-style JOINs.\\n   > We don\u2019t recommend using multiple levels of relations to replicate a relational model. Each level of relation adds an overhead at query time in terms of memory and computation. For better search performance, denormalize your data instead.\\n2. **[Terms lookup](https://www.elastic.co/guide/en/elasticsearch/reference/8.17/query-dsl-terms-query.html#query-dsl-terms-lookup)**:  Similar to an IN-subquery by fetching a value list from one index and using it in a `terms` query on another.  It\'s only suitable for large-table-to-small-table JOINs (e.g., filtering a large dataset using a small reference list), but performs very poorly for large-table-to-large-table JOINs due to scalability issues.\\n\\n**Doris** provides comprehensive support for [JOIN](https://doris.apache.org/docs/3.0/query-data/join/) operations, including:\\n\\n- INNER JOIN\\n- CROSS JOIN\\n- LEFT / RIGHT / FULL OUTER JOIN\\n- LEFT / RIGHT SEMI JOIN\\n- LEFT / RIGHT ANTI JOIN\\n\\nFurther more, **Doris\u2019 query optimizer** adaptively selects optimal execution plans for JOIN operations based on data characteristics and statistics, including:  \\n\\n- **Small-large table JOIN**:  \\n  - **Broadcast JOIN**: The smaller table is broadcast to all nodes for local joins.  \\n- **Large-large table JOIN**:  \\n  - **Bucket Shuffle JOIN**: Used when the left table\u2019s bucketing distribution aligns with the JOIN key.\\n  - **Colocate JOIN**: Applied when both the left and right tables share identical bucketing distributions with the  JOIN keys.\\n  - **Runtime Filter**: Reduces data scanned in the left table by pushing down predicates from the right table using runtime-generated filters.\\n\\nThis intelligent optimization ensures efficient JOIN execution across diverse data scales and distributions, including [TPC-H](https://doris.apache.org/docs/3.0/benchmark/tpch/) and [TPC-DS](https://doris.apache.org/docs/3.0/benchmark/tpcds) benchmarks.\\n\\n#### Lakehouse\\n\\nData warehouses addresses the need for fast data analysis, while data lakes are good at data storage and management. The integration of them, known as \\"[lakehouse](https://doris.apache.org/docs/3.0/lakehouse/lakehouse-overview)\\", is to facilitate the seamless integration and free flow of data between the data lake and data warehouse. It enables users to leverage the analytic capabilities of the data warehouse while harnessing the data management power of the data lake.\\n\\n![lakehouse.png](/images/es-alternatives/lakehouse.png)\\n\\nApache Doris can work as a data lakehouse with its [Multi Catalog](https://doris.apache.org/docs/3.0/lakehouse/lakehouse-overview#multi-catalog) feature. It can access databases and data lakes including Apache Hive, Apache Iceberg, Apache Hudi, Apache Paimon, LakeSoul, Elasticsearch, MySQL, Oracle, and SQLServer. It also supports Apache Ranger for privilege management.\\n\\nElasticsearch does not support querying external data, and of course, it does not support lakehouse either.\\n\\n### Query performance\\n\\n**Apache Doris** has been extensively optimized for multiple scenarios, so it can deliver high performance in many use cases. For example, Doris can achieve tens of thousands of QPS for high-concurrency point queries, and it delivers industry-leading performance in aggregation analysis on a global scale.\\n\\n**Elasticsearch** is good at point queries (retrieving just a small amount of data). However, it might struggle with complex analytical workloads.\\n\\n[Elasticsearch httplogs](https://elasticsearch-benchmarks.elastic.co/) and the Microsoft Azure [logsbench](https://gigaom.com/report/log-data-analytics-testing/) are benchmarks for log storage and search. Both tests show that Doris is about 3 - 4 times faster than Elasticsearch in data writing, but only uses 1/6 - 1/4 of the storage space that Elasticsearch uses. Then for data queries, Doris is more than 2 times faster than Elasticsearch.\\n\\n![Query performance.png](/images/es-alternatives/Query-performance.png)\\n\\n[ClickBench](https://benchmark.clickhouse.com/#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) is a benchmark created and maintained by the ClickHouse team to evaluate the performance of analytical databases. It results show that Apache Doris delivers **out-of-the-box** query performance **21 times faster** than Elasticsearch and **6 times faster** than a fine-tuned Elasticsearch.\\n\\n![Query performance-2.png](/images/es-alternatives/Query-performance-2.png)\\n\\n## Comparison Summary\\n\\nIn summary, Doris follows Apache License 2.0, which is a highly open license to ensure that Doris remains truly open and will continue to uphold such openness in the future.\\n\\nSecondly, Doris supports both compute-storage coupled and decoupled mode, while Elasticsearch only supports the former, which enable more powerful elasticity and resource isolation.\\n\\nThirdly, Doris provides high performance in data ingestion. It is usually 3~5 times faster than Elasticsearch. \\n\\nIn terms of storage, compared to Elasticsearch, Doris can reduce storage costs by 70% or more. It is also 10 times faster than Elasticsearch in data updates. \\n\\nAs for data queries, Doris outperforms Elasticsearch by a lot. It also provides analytic capabilities that Elasticsearch does have, including multi-table JOIN, materialized views, and data lakehouse.\\n\\n![comparisons.png](/images/es-alternatives/comparisons.png)\\n![comparisons-2.png](/images/es-alternatives/comparisons-2.png)\\n![comparisons-3.png](/images/es-alternatives/comparisons-3.png)\\n\\n\\n\\n## Use cases\\n\\nMany users have replaced Elasticsearch with Apache Doris in their production environments and received exciting results. I will introduce some user stories from the fields of observability, cyber security, and real-time business analysis.\\n\\n### Observability\\n\\n**User A: a world-famous short video provider**\\n\\n- Daily incremental data: 800 billion rows (500 TB)\\n- Average write throughput: 10 million row/s (60GB/s)\\n- Peak write throughput: 30 million row/s (90GB/s)\\n\\nApache Doris supports logging and tracing data storage for this tech giant and meets the data import performance requirements for nearly all use cases within the company.\\n\\n![Observability.png](/images/es-alternatives/Observability.png)\\n\\n**User B: NetEase - one of the World\'s Highest-Yielding Game Companies**\\n\\n- Replacing Elasticsearch with Apache Doris for log analysis: reducing storage consumption by 2/3 and achieving 10X query performance\\n- Replacing InfluxDB with Apache Doris for time-series data analysis: saving 50% of server resources and reducing storage consumption by 67%\\n\\n![Observability-2.png](/images/es-alternatives/Observability-2.png)\\n\\n**User C: an observability platform provider**\\n\\nApache Doris offers a special data type, [VARIANT](https://doris.apache.org/blog/variant-in-apache-doris-2.1), to handle semi-structured data in log and tracing, reducing costs by 70% and delivering 2~3 times faster full-text search performance compared to the Elasticsearch-based solution.\\n\\n![Observability-3.png](/images/es-alternatives/Observability-3.png)\\n\\n### Cyber security\\n\\n**User A: QAX - a listed company & leading cyber security leader**\\n\\nThe Doris-based security logging solution uses 40% less storage space, delivers 2X faster write performance, and supports full-text search, aggregation analysis, and multi-table JOIN capabilities in one database system.\\n\\n![Cyber security.png](/images/es-alternatives/Cyber-security.png)\\n\\n**User B: a payment platform with nearly 600 million registered users**\\n\\nAs a unified security data storage solution, Apache Doris delivers 4X faster write speeds, 3X better query performance, and saves 50% storage space compared to the previous architecture with diverse tech stacks using Elasticsearch, Hive and Presto.\\n\\n![Cyber security-2.png](/images/es-alternatives/Cyber-security-2.png)\\n\\n**User C: a leading cyber security solution provider**\\n\\nCompared to Elasticsearch, Doris delivers 2 times faster write speeds, 4 times better query performance, and a 4 times data compression ratio.\\n\\n![Cyber security-3.png](/images/es-alternatives/Cyber-security-3.png)\\n\\n### Business analysis\\n\\n**User A: a world-leading live e-commerce company**\\n\\nThe user previously relied on Elasticsearch to handle online queries for their live stream detail pages, but they faced big challenges in cost and concurrency. After migrating to Apache Doris, they achieve:\\n\\n- 3 times faster real-time writes: 30w/s -> 100w/s\\n- 4 times higher query concurrency: 500QPS -> 2000QPS\\n\\n![Business analysis.png](/images/es-alternatives/Business-analysis.png)\\n\\n**User B: Tencent Music Entertainment**\\n\\nPreviously, TME content library used both Elasticsearch and ClickHouse to meet their needs for data searching and analysis, but managing two separate systems was complex and inefficient. With Doris, they are able to unify the two systems into one single platform to support both data searching and analysis. The new architecture delivers 4X faster write speeds, reduces storage costs by 80%, and supports complex analytical operations.\\n\\n![Business analysis-2.png](/images/es-alternatives/Business-analysis-2.png)\\n\\n**User C: a web browser provider with 600 million users**\\n\\nAfter migrating to Apache Doris for a unified solution for log storage and report analysis, the company doubled its aggregation analysis efficiency, reduced storage consumption by 60%, and cut SQL development time by half.\\n\\n![Business analysis-3.png](/images/es-alternatives/comparisons-3.png)\\n\\n## Taking Apache Doris to the next level\\n\\n![Taking Apache Doris to the next level.png](/images/es-alternatives/Taking-Apache-Doris-to-the-next-level.png)\\n\\nFor the Apache Doris community developers, the path to making Doris good enough to replace Elasticsearch wasn\'t easy. In 2022, we started adding inverted index capabilities to Doris. At that time, this decision was met with skepticism. Many viewed inverted indexes as a feature exclusive to Elasticsearch, a domain few in the industry dared to venture into. Nevertheless, we went with it, and today we can confidently say that we have succeeded. \\n\\nIn 2022, we developed this feature from the ground up, and after a year of dedicated effort, we open-sourced it. Initially, we had only one user, QAX, who was willing to test and adopt the feature. We are deeply grateful to them for their early support during this pivotal stage.\\n\\nBy 2023, the value of Doris with inverted indexes became increasingly evident, leading to broader adoption by about 10 companies.\\n\\nThe growth momentum has continued, and as of 2024, we are experiencing rapid expansion, with over 100 companies now leveraging Doris to replace Elasticsearch.\\n\\nLooking ahead, I am very much looking forward to what 2025 will bring. This progress, advancing from the ground up to such significant milestones, has been made possible by the incredible support from the Doris community users and developers. We encourage everyone to join the [Apache Doris Slack community](https://join.slack.com/t/apachedoriscommunity/shared_invite/zt-2zlwvmzon-NIb2rz50rIhcflGAFpYjDQ) and join the dedicated channel [#elasticsearch-to-doris](https://apachedoriscommunity.slack.com/archives/C08CQKX20R5), where you can receive technical assistance, stay updated with the latest news about Doris, and engage with more Doris developers and users.\\n\\nMore on Apache Doris: \\n\\n- [Apache Doris for log and time series data analysis in NetEase, why not Elasticsearch and InfluxDB?](https://doris.apache.org/blog/apache-doris-for-log-and-time-series-data-analysis-in-netease)\\n- [From Elasticsearch to Apache Doris: upgrading an observability platform](https://doris.apache.org/blog/from-elasticsearch-to-apache-doris-upgrading-an-observability-platform)\\n- [Log Analysis: How to Digest 15 Billion Logs Per Day and Keep Big Queries Within 1 Second](https://doris.apache.org/blog/Log-Analysis-How-to-Digest-15-Billion-Logs-Per-Day-and-Keep-Big-Queries-Within-1-Second)\\n- [Replacing Apache Hive, Elasticsearch and PostgreSQL with Apache Doris](https://doris.apache.org/blog/Replacing-Apache-Hive-Elasticsearch-and-PostgreSQL-with-Apache-Doris)\\n- [Log Analysis: Elasticsearch VS Apache Doris](https://doris.apache.org/blog/Building-A-Log-Analytics-Solution-10-Times-More-Cost-Effective-Than-Elasticsearch)\\n- [A Deep Dive into Inverted Index in Apache Doris](https://doris.apache.org/blog/inverted-index-accelerates-text-searches-by-40-time-apache-doris)\\n- [VARIANT in Apache Doris 2.1.0: a new data type 8 times faster than JSON for semi-structured data analysis](https://doris.apache.org/blog/variant-in-apache-doris-2.1)\\n\\nConnect with me on [Linkedin](https://www.linkedin.com/in/kang-xiao-441740316/)\\n\\nApache Doris on [GitHub](https://github.com/apache/doris)\\n\\nApache Doris [Website](https://doris.apache.org)"},{"id":"/doris-compute-storage-decoupled","metadata":{"permalink":"/blog/doris-compute-storage-decoupled","source":"@site/blog/doris-compute-storage-decoupled.md","title":"Slash your cost by 90% with Apache Doris Compute-Storage Decoupled Mode","description":"Apache Doris compute-storage decoupled mode achieves 90% cost reduction and provides elasticity and workload isolation, while maintaining high performance in data ingestion and queries.","date":"2025-03-21T00:00:00.000Z","tags":[{"inline":true,"label":"Tech Sharing","permalink":"/blog/tags/tech-sharing"}],"hasTruncateMarker":false,"authors":[{"name":"velodb.io \xb7 Thomas Yang, Apache Doris PMC Member","key":null,"page":null}],"frontMatter":{"title":"Slash your cost by 90% with Apache Doris Compute-Storage Decoupled Mode","summary":"Apache Doris compute-storage decoupled mode achieves 90% cost reduction and provides elasticity and workload isolation, while maintaining high performance in data ingestion and queries.","description":"Apache Doris compute-storage decoupled mode achieves 90% cost reduction and provides elasticity and workload isolation, while maintaining high performance in data ingestion and queries.","date":"2025-03-21","author":"velodb.io \xb7 Thomas Yang, Apache Doris PMC Member","externalLink":"https://www.velodb.io/blog/1384","tags":["Tech Sharing"],"image":"/images/compute-storage-decoupled-banner.jpg"},"unlisted":false,"prevItem":{"title":"Why Apache Doris is a Better Alternative to Elasticsearch for Real-Time Analytics","permalink":"/blog/why-apache-doris-is-best-alternatives-for-real-time-analytics"},"nextItem":{"title":"Apache Doris 3.0.4 Released","permalink":"/blog/release-note-3.0.4"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n  http://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\nIn the age of data-driven decision-making, the exponential growth of data volume and the ever-evolving demands for analytics pose great challenges. Data streams in from diverse sources (such as application logs, network interactions, and mobile devices), spanning structured, semi-structured, and unstructured formats. This diversity places pressure on storage and analytical systems. Meanwhile, the surge in demand for real-time analytics and exploratory queries requires systems to deliver millisecond-level responsiveness while achieving optimal cost efficiency and elastic scalability.\\n\\nApache Doris emerged in the era of integrated storage and computation, built on a classic **Shared Nothing** architecture. In this design, storage and computation are co-located on **Backend (BE) nodes**, leveraging an **MPP (Massively Parallel Processing)** distributed computing model. This architecture delivers key advantages, including high availability, simplified deployment, seamless horizontal scalability, and exceptional real-time analytical performance.\\n\\nFor real-time analytics and small-scale data processing, Apache Doris stands out with **predictable, stable low-latency performance**, making it an indispensable solution. However, when scaling to large-scale data processing, it encounters certain challenges, primarily in:\\n\\n- **Relatively high costs & low elasticity:** Balancing storage and compute resources remains a big challenge. Storage capacity must be sufficient to accommodate all data, while compute resources need to handle query workloads efficiently. However, dynamically scaling clusters is often time-consuming, prompting enterprises to over-provision resources to ensure stability. This approach simplifies operations but leads to resource waste and increased costs.\\n- **Limited workload isolation:** Since Apache Doris 2.0, Workload Groups provide soft isolation, while Resource Groups offer a degree of hard isolation. However, neither mechanism ensures complete physical isolation, which can impact performance in multi-tenant or resource-intensive environments.\\n- **Operational complexity:** Managing an OLAP system with built-in distributed storage requires not only overseeing compute nodes but also ensuring efficient storage administration. Storage management is inherently complex, and misconfigurations or improper operations can lead to data loss, making maintenance highly demanding.\\n\\n> Even so, in the absence of stable and large-scale storage support, a integrated storage-compute architecture remains the optimal choice.\\n\\nAs cloud infrastructure matures, enterprises increasingly seek **deeper Apache Doris integration with public clouds, private clouds, and Kubernetes (K8s) container platforms** to unlock greater elasticity and flexibility. Public clouds offer mature object storage with on-demand compute resources, eliminating the need for pre-allocated space, while private clouds leverage technologies like K8s and MinIO to build scalable resource platforms. **This evolution in cloud infrastructure has also accelerated Apache Doris\u2019 transition to a storage-compute decoupled architecture, enabling lower costs, high elasticity, and enhanced workload isolation.**\\n\\n## Apache Doris Compute-Storage Decoupled Mode\\n\\n\\n\\nSince [version 3.0](https://doris.apache.org/blog/release-note-3.0.0), Apache Doris has supported both the compute-storage decoupled mode and the compute-storage coupled mode.\\n\\n\\n\\n### 01 Compute-Storage Decoupled\\n\\nIn the compute-storage decoupled mode, Apache Doris adopts a three-tier architecture consisting of three layers: shared storage, compute groups, and meta data service:\\n\\n![compute-storage-decoupled.jpg](/images/compute-storage-decoupled.jpg)\\n\\n**Shared Storage Layer**\\n\\nData is persisted in the shared storage layer, allowing compute nodes to access and share data seamlessly. This design enhances compute node flexibility and reduces operational overhead. Leveraging mature and reliable shared storage results in ultra-low storage costs and high data reliability. Whether using public cloud object storage or enterprise-managed shared storage, this approach greatly reduces the maintenance complexity of Apache Doris.\\n\\n**Compute Groups**\\n\\nThe compute layer consists of multiple compute groups responsible for executing query plans. Each query is executed within a single compute group, ensuring isolation and scalability. Compute nodes are stateless, utilizing local disks as high-speed caches to accelerate queries while sharing the same data and metadata services. Each compute group operates independently, supporting on-demand scaling, and local caches remain isolated to ensure workload separation and performance consistency.\\n\\n**Meta Data Service**\\n\\nThe meta data layer manages system meta data, including databases, tables, schemas, rowset meta data, and transaction information, with support for horizontal scaling. Future iterations of Apache Doris\u2019 compute-storage decoupled mode will introduce stateless Frontend (FE) nodes, where memory consumption is decoupled from cluster size. This evolution will eliminate memory bottlenecks, allowing FE nodes to operate with minimal memory requirements.\\n\\n### 02 Architecture Design\\n\\nTraditional compute-storage decoupling approaches typically store both data and meta data in shared storage while centralizing transaction management on a single FE node. However, this design introduces several challenges:\\n\\n- **Write performance bottlenecks**: The two-phase commit protocol, driven by FE Master, incurs high latency and low throughput.\\n- **Small file proliferation**: Frequent meta data writes generate excessive small files, leading to system instability and inflated storage costs.\\n- **Scalability constraints**: Since FE nodes manage meta data in memory, an increasing number of Tablets amplifies memory pressure, eventually causing write bottlenecks.\\n- **Data deletion risks**: Relying on delta computation with timeout-based mechanisms for deletion introduces challenges in synchronizing writes and deletions. As a result, there is a risk of unintended data loss due to misalignment between ongoing writes and scheduled deletions.\\n\\nCompared to traditional approaches, **Apache Doris** effectively addresses these challenges through **a shared meta data service**:\\n\\n- **Real-time ingestion**: The meta data service provides a globally consistent view, enabling low-latency, high-throughput writes. Benchmarks show that the Apache Doris compute-storage decoupled mode achieves **100X higher** performance than other solutions at 50 concurrent writes and **11X higher** performance at 500 concurrent writes.\\n- **Optimized small file management**: Data is written to shared storage, while meta data is handled by the meta data service. This effectively reduces small file overheads. Tests indicate that the Apache Doris compute-storage decoupled mode generates only **1/2 the number of write files** compared to other industry solutions.\\n- **Enhanced scalability**: In future versions of Apache Doris, FE metadata will be moved to the meta data service to eliminate cluster scaling limitations and ensure seamless expansion.\\n- **Reliable data deletion**: Doris employs a forward deletion mechanism based on a globally consistent view. This ensures mutual exclusion between writes and deletions, thus eliminating the risk of accidental data loss.\\n\\n### 03 What makes it stand out\\n\\nThe Apache Doris compute-storage decoupled architecture provide values for users in three aspects: cost efficiency, elasticity, and workload isolation.\\n\\n**Firstly, it brings a 90% cost reduction compared to the compute-storage coupled mode.**\\n\\n- **Pay-as-you-go**: Unlike traditional coupled architectures, there\u2019s no need to pre-provision compute and storage resources. Storage costs scale with actual usage, while compute resources can be dynamically adjusted based on demand.\\n- **Single-replica storage**: Instead of maintaining three replicas in costly block storage, data is stored as a single replica in low-cost object storage, with hot data cached in block storage for performance. This dramatically reduces storage footprint and hardware costs. For example, S3 costs only 25% to 50% of AWS EBS.\\n- **Lower resource consumption**: In compute-storage decoupled mode, compaction operations only process a single data replica, thus largely reducing resource usage compared to multi-replica environments.\\n\\n**Secondly, with a stateless compute node design, Doris enables on-demand resource scaling to meet fluctuating workloads efficiently.**\\n\\n- **Elastic auto-scaling**: Compute resources can be dynamically scaled to accommodate traffic spikes or workload variations. When demand increases, Doris can rapidly scale out compute nodes; when demand drops, resources scale down automatically, avoiding unnecessary costs.\\n- **Fine-grained compute resource allocation**: Doris allows compute nodes to be strategically assigned to specific compute groups based on workload requirements. For example, high-performance nodes handle complex queries and high-concurrency workloads, while standard nodes manage lightweight queries and infrequent requests.\\n\\n**Thirdly, Doris provides efficient resource scheduling and workload isolation mechanisms.**\\n\\n- **Cross-business isolation**: Different business units can be assigned dedicated compute groups with physical isolation, so workloads operate on dedicated resources without interference.\\n- **Offline workload isolation**: Large-scale batch processing tasks can be segregated into dedicated compute groups, so users can leverage low-cost resources for offline data processing without impacting real-time business performance.\\n- **Read-write isolation**: Doris allows dedicated compute groups for read and write operations to ensure consistent query response times even under high write loads.\\n\\n## Benchmarking and comparison\\n\\nTo provide a clear evaluation of the compute-storage decoupled architecture of Apache Doris, we conducted a series of benchmark tests across multiple dimensions, including data ingestion, query performance, and resource cost efficiency.\\n\\n### 01 Ingestion performance\\n\\n**High-concurrency ingestion**\\n\\nWe compared Apache Doris\' coupled and decoupled modes with other mainstream solutions under the same compute resources. The tests measured real-time ingestion performance under two levels of concurrency:\\n\\n- **50 concurrent writes**: Ingesting **250 files**, each containing **20,000 rows**.\\n- **500 concurrent writes**: Ingesting **10,000 files**, each containing **500 rows**.\\n\\n**Test results:**\\n\\n- At 50 concurrent writes, Doris\' compute-storage decoupled mode performed on par with the coupled mode while achieving **100X the write performance** of other industry compute-storage decoupled solutions.\\n- At 500 concurrent writes, Doris\' decoupled mode experienced a slight performance drop compared to the coupled mode, yet still maintained an **11X advantage** over other compute-storage decoupled architectures.\\n\\n![high-concurrency-ingestion.jpg](/images/high-concurrency-ingestion.jpg)\\n\\n**Batch data ingestion**\\n\\nTo evaluate batch data ingestion efficiency, we conducted tests using **TPC-H 1TB** and **TPC-DS 1TB** datasets, comparing the compute-storage coupled and decoupled modes of Apache Doris. Data was loaded using S3 Load. Under default configurations, multiple tables were ingested sequentially, and the total ingestion time was measured for comparison.\\n\\nHardware configuration:\\n\\n- Cluster size: 4 compute instances (1 FE, 3 BE)\\n- CPU: 48 cores per instance\\n- Memory: 192GB per instance\\n- Network Bandwidth: 21 Gbps\\n- Storage: Enhanced SSD\\n\\n![batch-data-ingestion.jpg](/images/batch-data-ingestion.jpg)\\n\\nAs is shown, even when using a single replica in both architectures, the compute-storage decoupled mode outperforms the coupled mode in batch data ingestion by **20.05%** and **27.98%** in the two benchmarks, respectively. *(In real-world deployments, the coupled mode typically adopts a three-replica strategy. This further amplifies the write performance gains of the decoupled mode.)*\\n\\n### 02 Query Performance\\n\\nIn the compute-storage decoupled mode, Apache Doris leverages a multi-tier caching mechanism to accelerate queries. It improves overall query efficiency by speeding up data access and minimizing reliance on shared storage. The cache hierarchy includes:\\n\\n- **Doris Page Cache**: In-memory caching of decompressed data.\\n- **Linux Page Cache**: In-memory caching of compressed data.\\n- **Local Disk Cache**: Persistent caching of compressed data.\\n\\nHardware configuration:\\n\\n- Cluster size: 4 compute instances (1 FE, 3 BE)\\n- CPU: 48 cores per instance\\n- Memory: 192GB per instance\\n- Network Bandwidth: 21 Gbps\\n- Storage: Enhanced SSD\\n\\nWe conducted performance benchmarking under different caching scenarios in both compute-storage coupled and decoupled modes. Using the TPC-DS 1TB dataset, the test results are as follows: \\n\\n![query-performance.jpg](/images/query-performance.jpg)\\n\\n- **Full cache hit**: We execute the query twice and measure the runtime of the second execution, ensuring that all data is preloaded into the cache. Query performance in compute-storage decoupled mode matches that of the coupled architecture with no performance degradation.\\n- **Partial cache hit** (This scenario best reflects real-world usage.): Before the test begins, all caches are cleared, and we measure the runtime of the first execution while data is gradually loaded into the cache. Compared to the coupled architecture, query performance remains nearly identical, with an overall performance overhead of about 10%.\\n- **No cache hit**: All caches are cleared before each SQL execution, ensuring that every query runs without cached data. Compared to the coupled architecture, query performance sees an approximate 35% degradation.\\n\\n### 03 Resource Cost\\n\\n**Operational cost for online workloads**\\n\\nTaking a real-world enterprise workload as an example, we compare the cost differences between compute-storage coupled and decoupled modes in Apache Doris.\\n\\n- **Compute-storage coupled mode**: The dataset in Doris has a size of 100TB per replica, resulting in a total of 300TB with three replicas. To prevent frequent scaling operations from impacting business stability, disk usage is maintained at about 50%. Thus, **the monthly resource cost amounts to $36,962.7** (as detailed below).\\n\\n![operational-cost-for-online-workloads.jpg](/images/operational-cost-for-online-workloads.jpg)\\n\\n- **Compute-storage decoupled mode**: With the same data scale, adopting the compute-storage decoupled model only requires storing a single replica in object storage, while hot data is cached on local disks. As shown below, the monthly resource cost is reduced to **$22,212.65**, achieving a 40% cost savings.\\n\\n![compute-storage-decoupled-mode.jpg](/images/compute-storage-decoupled-mode.jpg)\\n\\n**Historical data cost**\\n\\nFor example, with 200TB of historical data, the resource utilization under both the compute-storage coupled and decoupled modes is shown below. The coupled model incurs a monthly cost of $48,851.10, whereas the decoupled model reduces the cost to just $4,502.40\u2014**cutting expenses by over 90%**.\\n\\n![historical-data-cost.jpg](/images/historical-data-cost.jpg)\\n\\n## What\'s next\\n\\nPowered by compute-storage decoupling, Apache Doris excels in real-time analytics, lakehouse analytics, observability and log storage & analysis. Looking ahead, Apache Doris will continue to enhance its capabilities in this mode. We will introduce new features such as snapshots, time travel, and Cross-Cluster Replication (CCR) support, and achieve stateless FE to further improve system stability and usability.\\n\\nIf you\'re interested in Apache Doris\' compute-storage decoupled mode and its future development, we invite you to join the [#compute-storage-decoupled](https://apachedoriscommunity.slack.com/archives/C08HZUZ37KJ) channel in the Apache Doris Slack community, where you can connect with core developers and users. We look forward to your thoughts and contributions!\\n\\n[Join us live on March 27](https://www.linkedin.com/events/exploringapachedoriscompute-sto7308127084457902080/comments/) for more insights into the Apache Doris compute-storage decoupled mode!"},{"id":"/release-note-3.0.4","metadata":{"permalink":"/blog/release-note-3.0.4","source":"@site/blog/release-note-3.0.4.md","title":"Apache Doris 3.0.4 Released","description":"Dear community members, the Apache Doris 3.0.4 version was officially released on February 28, 2025, this version introduces continuous upgrades and enhancements in several key areas, including Lakehouse, Storage Efficiency, Compute-Storage Separation, Query Optimizer and Asynchronous Materialized Views, and more.","date":"2025-02-28T00:00:00.000Z","tags":[{"inline":true,"label":"Release Notes","permalink":"/blog/tags/release-notes"}],"hasTruncateMarker":false,"authors":[{"name":"Apache Doris","key":null,"page":null}],"frontMatter":{"title":"Apache Doris 3.0.4 Released","summary":"Dear community members, the Apache Doris 3.0.4 version was officially released on February 28, 2025, this version introduces continuous upgrades and enhancements in several key areas, including Lakehouse, Storage Efficiency, Compute-Storage Separation, Query Optimizer and Asynchronous Materialized Views, and more.","description":"Dear community members, the Apache Doris 3.0.4 version was officially released on February 28, 2025, this version introduces continuous upgrades and enhancements in several key areas, including Lakehouse, Storage Efficiency, Compute-Storage Separation, Query Optimizer and Asynchronous Materialized Views, and more.","date":"2025-02-28","author":"Apache Doris","tags":["Release Notes"],"image":"/images/3.0.4.jpg"},"unlisted":false,"prevItem":{"title":"Slash your cost by 90% with Apache Doris Compute-Storage Decoupled Mode","permalink":"/blog/doris-compute-storage-decoupled"},"nextItem":{"title":"Apache Doris 2.1.8 just released","permalink":"/blog/release-note-2.1.8"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n  http://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\nDear community members, the Apache Doris 3.0.4 version was officially released on February 02, 2025, this version further enhances the performance and stability of the system.\\n\\n- [Quick Download](https://doris.apache.org/download/)\\n\\n- [GitHub Release](https://github.com/apache/doris/releases)\\n\\n\\n## Behavior Changes\\n\\n- In the Audit log, the `force` flag is retained for `drop table` and `drop database` statements.  [#43227](https://github.com/apache/doris/pull/43227) \\n\\n- When exporting data to Parquet/ORC formats, the `bitmap`, `quantile_state`, and `hll` types are exported in Binary format. Additionally, support has been added for exporting `jsonb` and `variant` types, which are exported as `string`. [#44041](https://github.com/apache/doris/pull/44041) \\n\\n  - For more information, please refer to documentation: [Export Overview - Apache Doris](https://doris.apache.org/docs/3.0/data-operate/export/export-overview)\\n\\n- When querying a data source with case-insensitive table names (such as Hive) through External Catalog, in previous versions, you can use any case to query the table name, but in version 3.0.4, Doris\'s own table name case sensitivity policy will be strictly followed.\\n- The Hudi JNI Scanner has been replaced from Spark API to Hadoop API to enhance compatibility. Users can switch by setting the session variable `set hudi_jni_scanner=spark/hadoop`. [#44396](https://github.com/apache/doris/pull/44396) \\n- The use of `auto bucket` in Colocate tables is prohibited.  [#44396](https://github.com/apache/doris/pull/44396) \\n- Paimon cache has been added to the Catalog, eliminating real-time data queries.  [#44911 ](https://github.com/apache/doris/pull/44911)\\n- The default value of `max_broker_concurrency` has been increased to improve performance for large-scale data imports with Broker Load. [#44929](https://github.com/apache/doris/pull/44929) \\n- The default value of the `storage medium` for Auto Partition partitions has been changed to the attribute value of the current table\'s `storage medium`, rather than using the system default value. [#45955](https://github.com/apache/doris/pull/45955) \\n- Column updates are prohibited during Schema Change execution for Key columns. [#46347](https://github.com/apache/doris/pull/46347) \\n- For Key columns containing auto-increment columns, support has been added to allow column updates without providing the auto-increment column.  [#44528](https://github.com/apache/doris/pull/44528) \\n- The FE ID generator strategy has been switched to a time-based approach, and IDs no longer start from 10000. [#44790](https://github.com/apache/doris/pull/44790) \\n- In the compute-storage separation mode, the default stale rowset recycling delay for Compaction has been reduced to 1800 seconds to decrease the recycling interval. This may cause large queries to fail in extreme scenarios, and adjustments can be made as needed. [#45460](https://github.com/apache/doris/pull/45460) \\n- The `show cache hotspot` statement has been disabled in compute-storage separation mode, and direct access to system tables is required. [#47332](https://github.com/apache/doris/pull/47332) \\n- Deleting the system-created `admin` user is prohibited. [#44751](https://github.com/apache/doris/pull/44751) \\n\\n## Improvements\\n\\n### Storage\\n\\n- Optimized the issue of Routine Load tasks frequently timing out due to a small `max_match_interval` setting. [#46292](https://github.com/apache/doris/pull/46292) \\n- Improved performance for Broker Load when importing multiple compressed files. [#43975](https://github.com/apache/doris/pull/43975) \\n- Increased the default value of `webserver_num_workers` to enhance Stream Load performance. [#46593](https://github.com/apache/doris/pull/46593) \\n- Optimized the load imbalance issue for Routine Load import tasks during BE node scaling. [#44798](https://github.com/apache/doris/pull/44798) \\n- Improved the use of Routine Load thread pools to prevent timeouts from affecting queries. [#45039](https://github.com/apache/doris/pull/45039) \\n\\n### Compute-Storage Separation\\n\\n- Enhanced the stability and observability of the Meta-service. [#44036](https://github.com/apache/doris/pull/44036), [#45617](https://github.com/apache/doris/pull/45617), [#45255](https://github.com/apache/doris/pull/45255), [#45068](https://github.com/apache/doris/pull/45068) \\n- Optimized File Cache by adding an early eviction strategy, reducing lock time, and improving query performance. [#47473](https://github.com/apache/doris/pull/47473), [#45678](https://github.com/apache/doris/pull/45678), [#47472](https://github.com/apache/doris/pull/47472) \\n- Improved initialization checks and queue transitions for File Cache to enhance stability. [#44004](https://github.com/apache/doris/pull/44004), [#44429](https://github.com/apache/doris/pull/44429), [#45057](https://github.com/apache/doris/pull/45057), [#47229](https://github.com/apache/doris/pull/47229) \\n- Increased the speed of HDFS data recycling. [#46393](https://github.com/apache/doris/pull/46393) \\n- Optimized performance issues when the FE acquires compute groups during ultra-high-frequency imports.  [#47203](https://github.com/apache/doris/pull/47203) \\n- Improved several import-related parameters for primary key tables in compute-storage separation to enhance the stability of real-time high-concurrency imports. [#47295](https://github.com/apache/doris/pull/47295), [#46750](https://github.com/apache/doris/pull/46750), [#46365](https://github.com/apache/doris/pull/46365) \\n\\n### Lakehouse\\n\\n- Supported reading Hive tables in JSON format. [#43469](https://github.com/apache/doris/pull/46393) \\n\\n  - For more information, please refer to documentation: [Text/CSV/JSON - Apache Doris](https://doris.apache.org/docs/dev/lakehouse/file-formats/text#json)\\n\\n- Introduced the session variable `enable_text_validate_utf8` to skip UTF-8 encoding checks for CSV formats.  [#45537](https://github.com/apache/doris/pull/45537) \\n\\n  - For more information, please refer to documentation: [Text/CSV/JSON - Apache Doris](https://doris.apache.org/docs/dev/lakehouse/file-formats/text#character-set)\\n\\n- Updated the Hudi version to 0.15 and optimized query planning performance for Hudi tables.\\n- Improved read performance for MaxCompute partitioned tables.  [#45148](https://github.com/apache/doris/pull/45148) \\n- Optimized performance for Parquet file delayed materialization under high filter rates.  [#46183](https://github.com/apache/doris/pull/46183) \\n- Supported delayed materialization for complex Parquet types. [#44098](https://github.com/apache/doris/pull/44098) \\n- Optimized predicate pushdown logic for ORC types, supporting more predicate conditions for index filtering. [#43255](https://github.com/apache/doris/pull/43255) \\n\\n### Asynchronous Materialized Views\\n\\n- Supported more scenarios for aggregate roll-up rewriting.  [#44412](https://github.com/apache/doris/pull/44412) \\n\\n### Query Optimizer\\n\\n- Improved partition pruning performance. [#46261](https://github.com/apache/doris/pull/46261) \\n- Added rules to eliminate `group by` keys based on data characteristics.  [#43391](https://github.com/apache/doris/pull/43391) \\n- Adaptively adjusted the wait time for Runtime Filters based on the target table size. [#42640](https://github.com/apache/doris/pull/42640) \\n- Improved the ability to push down aggregations in joins to fit more scenarios. [#43856](https://github.com/apache/doris/pull/43856), [#43380](https://github.com/apache/doris/pull/43380) \\n- Improved Limit pushdown for aggregations to fit more scenarios. [#44042](https://github.com/apache/doris/pull/44042) \\n\\n### Others\\n\\n- Optimized startup scripts for FE, BE, and MS processes to provide clearer output. [#45610](https://github.com/apache/doris/pull/45610), [#45490](https://github.com/apache/doris/pull/45490), [#45883](https://github.com/apache/doris/pull/45883) \\n- The case sensitivity of table names in `show tables` now matches MySQL behavior. [#46030](https://github.com/apache/doris/pull/46030) \\n- `show index` now supports arbitrary target table types. [#45861](https://github.com/apache/doris/pull/45861) \\n- `information_schema.columns` now supports displaying default values. [#44849](https://github.com/apache/doris/pull/44849) \\n- `information_schema.views` now supports displaying view definitions. [#45857](https://github.com/apache/doris/pull/45857) \\n- Supported the MySQL protocol `COM_RESET_CONNECTION` command. [#44747](https://github.com/apache/doris/pull/44747) \\n\\n## Bug Fixes\\n\\n### Storage\\n\\n- Fixed potential memory errors during the import process for aggregate table models.  [#46997](https://github.com/apache/doris/pull/46997) \\n- Resolved the issue of Routine Load offset loss during FE master node restarts in compute-storage separation mode. [#46566](https://github.com/apache/doris/pull/46566) \\n- Fixed memory leaks in FE Observer nodes during batch import scenarios in compute-storage mode. [#47244](https://github.com/apache/doris/pull/47244) \\n- Resolved the issue of Cumulative Point rollback during Full Compaction with Order Data Compaction. [#44359](https://github.com/apache/doris/pull/44359) \\n- Fixed the issue where Delete operations could temporarily prevent Tablet Compaction scheduling. [#43466](https://github.com/apache/doris/pull/43466) \\n- Resolved incorrect Tablet states after Schema Change in multi-compute-cluster scenarios. [#45821](https://github.com/apache/doris/pull/45821) \\n- Fixed the potential NPE error when performing Column Rename Schema Change on primary key tables with `sequence_type`. [#46906](https://github.com/apache/doris/pull/46906) \\n- **Data Correctness**: Fixed correctness issues for primary key tables when importing partial column updates containing DELETE SIGN columns. [#46194](https://github.com/apache/doris/pull/46194) \\n- Resolved potential memory leaks in FE when Publish tasks for primary key tables were continuously stuck. [#44846](https://github.com/apache/doris/pull/44846) \\n\\n### Compute-Storage Decoupled\\n\\n- Fixed the issue where File Cache size could exceed the table data size. [#46561](https://github.com/apache/doris/pull/46561), [#46390](https://github.com/apache/doris/pull/46390) \\n- Resolved upload failures at the 5MB boundary during data uploads.  [#47333](https://github.com/apache/doris/pull/47333) \\n- Enhanced robustness by adding more parameter checks for several `alter` operations in Storage Vault.  [#45155](https://github.com/apache/doris/pull/45155), [#45156](https://github.com/apache/doris/pull/45156), [#46625](https://github.com/apache/doris/pull/46625), [#47078](https://github.com/apache/doris/pull/47078), [#45685](https://github.com/apache/doris/pull/45685), [#46779](https://github.com/apache/doris/pull/46779) \\n- Resolved issues with data recycling failures or slow recycling due to improper Storage Vault configurations. [#46798](https://github.com/apache/doris/pull/46798), [#47536](https://github.com/apache/doris/pull/47536), [#47475](https://github.com/apache/doris/pull/47475), [#47324](https://github.com/apache/doris/pull/47324), [#45072](https://github.com/apache/doris/pull/45072) \\n- Fixed the issue where data recycling could stall, preventing timely recycling.  [#45760](https://github.com/apache/doris/pull/45760) \\n- Resolved incorrect retries for MTTM-230 errors in compute-storage separation mode.  [#47370](https://github.com/apache/doris/pull/47370), [#47326](https://github.com/apache/doris/pull/47326) \\n- Fixed the issue where Group Commit WAL was not fully replayed during BE decommissioning in compute-storage separation mode.  [#47187](https://github.com/apache/doris/pull/47187) \\n- Resolved the issue where Tablet Meta exceeding 2GB rendered MS unavailable.  [#44780](https://github.com/apache/doris/pull/44780) \\n- **Data Correctness**: Fixed two duplicate Key issues in primary key tables in compute-storage separation mode. [#46039](https://github.com/apache/doris/pull/46039), [#44975](https://github.com/apache/doris/pull/44975) \\n- Resolved the issue where Base Compaction could continuously fail due to large Delete Bitmaps in primary key tables during high-frequency real-time imports.  [#46969](https://github.com/apache/doris/pull/46969) \\n- Modified incorrect retry logic for Schema Change in primary key tables in compute-storage separation mode to enhance robustness. [#46748](https://github.com/apache/doris/pull/46748) \\n\\n### Lakehouse\\n\\n#### Hive\\n\\n- Fixed the issue where Hive views created by Spark could not be queried.  [#43553](https://github.com/apache/doris/pull/43553) \\n- Resolved the issue where certain Hive Transaction tables could not be read correctly. [#45753](https://github.com/apache/doris/pull/45753) \\n- Fixed the issue where partition pruning failed for Hive tables with special characters in partitions.  [#42906](https://github.com/apache/doris/pull/42906) \\n\\n#### Iceberg\\n\\n- Fixed the issue where Iceberg tables could not be created in Kerberos authentication environments.  [#43445](https://github.com/apache/doris/pull/43445) \\n- Resolved the issue where `count(*)` queries were inaccurate for Iceberg tables with dangling deletes. [#44039](https://github.com/apache/doris/pull/44039) \\n- Fixed the issue where query errors occurred due to mismatched column names in Iceberg tables. [#44470](https://github.com/apache/doris/pull/44470) \\n- Resolved the issue where Iceberg tables could not be read after partition modifications. [#45367](https://github.com/apache/doris/pull/45367) \\n\\n#### Paimon\\n\\n- Fixed the issue where Paimon Catalog could not access Alibaba Cloud OSS-HDFS.  [#42585](https://github.com/apache/doris/pull/42585) \\n\\n#### Hudi\\n\\n- Fixed the issue where partition pruning failed for Hudi tables in certain scenarios.  [#44669](https://github.com/apache/doris/pull/44669) \\n\\n#### JDBC\\n\\n- Fixed the issue where tables could not be retrieved using JDBC Catalog after enabling case-insensitive table names.\\n\\n#### MaxCompute\\n\\n- Fixed the issue where partition pruning failed for MaxCompute tables in certain scenarios. [#44508](https://github.com/apache/doris/pull/44508) \\n\\n#### Others\\n\\n- Fixed the issue where export tasks caused memory leaks in FE. [#44019](https://github.com/apache/doris/pull/44019) \\n- Resolved the issue where S3 object storage could not be accessed via HTTPS protocol. [#44242](https://github.com/apache/doris/pull/44242) \\n- Fixed the issue where Kerberos authentication tickets could not be automatically refreshed.  [#44916](https://github.com/apache/doris/pull/44916) \\n- Resolved the issue where reading Hadoop Block compressed format files failed. [#45289](https://github.com/apache/doris/pull/45289) \\n- When querying ORC format data, CHAR type predicates are no longer pushed down to avoid potential result errors. [#45484](https://github.com/apache/doris/pull/45484) \\n\\n### Asynchronous Materialized Views\\n\\n- Fixed the issue where transparent query rewriting could lead to planning or result errors in extreme scenarios.  [#44575](https://github.com/apache/doris/pull/44575), [#45744](https://github.com/apache/doris/pull/45744) \\n- Resolved the issue where multiple build tasks could be generated during asynchronous materialized view scheduling in extreme scenarios. [#46020](https://github.com/apache/doris/pull/46020), [#46280](https://github.com/apache/doris/pull/46280) \\n\\n### Query Optimizer\\n\\n- Fixed the issue where some expression rewrites could produce incorrect expressions. [#44770](https://github.com/apache/doris/pull/44770), [#44920](https://github.com/apache/doris/pull/44920), [#45922](https://github.com/apache/doris/pull/45922), [#45596](https://github.com/apache/doris/pull/45596) \\n- Resolved occasional incorrect results from SQL Cache. [#44782](https://github.com/apache/doris/pull/44782), [#44631](https://github.com/apache/doris/pull/44631), [#46443](https://github.com/apache/doris/pull/46443), [#47266](https://github.com/apache/doris/pull/47266) \\n- Fixed the issue where limit pushdown for aggregation operators could produce incorrect results in some scenarios. [#45369](https://github.com/apache/doris/pull/45369) \\n- Resolved the issue where delayed materialization optimization could produce incorrect execution plans in some scenarios. [#45693](https://github.com/apache/doris/pull/45693), [#46551](https://github.com/apache/doris/pull/46551) \\n\\n### Query Execution\\n\\n- Fixed the issue where regular expressions and `like` functions produced incorrect results with special characters. [#44547](https://github.com/apache/doris/pull/44547) \\n- Resolved the issue where SQL Cache results could be incorrect when switching databases. [#44782](https://github.com/apache/doris/pull/44782) \\n- Fixed a series of Arrow Flight-related issues. [#45023](https://github.com/apache/doris/pull/45023), [#43929](https://github.com/apache/doris/pull/43929) \\n- Resolved the issue where results were incorrect when the Hash table for HashJoin exceeded 4GB in some cases. [#46461](https://github.com/apache/doris/pull/46461) \\n- Fixed the overflow issue of the `convert_to` function with Chinese characters. [#46405](https://github.com/apache/doris/pull/46405) \\n- Resolved the issue where results could be incorrect in extreme scenarios when `group by` was used with Limit. [#47844](https://github.com/apache/doris/pull/47844) \\n- Fixed the issue where results could be incorrect when accessing certain system tables. [#47498](https://github.com/apache/doris/pull/47498) \\n- Resolved the issue where the `percentile` function could cause system crashes. [#47068](https://github.com/apache/doris/pull/47068) \\n- Fixed the performance degradation issue for single-table queries with Limit. [#46090](https://github.com/apache/doris/pull/46090) \\n- Resolved the issue where `StDistanceSphere` and `StAngleSphere` functions caused system crashes. [#45508](https://github.com/apache/doris/pull/45508) \\n- Fixed the issue where `map_agg` results were incorrect. [#40454](https://github.com/apache/doris/pull/40454) \\n\\n### Semi-structured Data Management\\n\\n#### BloomFilter Index\\n\\n- Fixed the exception caused by large parameters in BloomFilter Index. [#45780](https://github.com/apache/doris/pull/45780) \\n- Resolved the issue of high memory usage during BloomFilter Index writes. [#45833](https://github.com/apache/doris/pull/45833) \\n- Fixed the issue where BloomFilter Index was not correctly deleted when columns were dropped. [#44361](https://github.com/apache/doris/pull/44361), [#43378](https://github.com/apache/doris/pull/43378) \\n\\n#### Inverted Index\\n\\n- Fixed the occasional crash during inverted index construction. [#43246](https://github.com/apache/doris/pull/43246) \\n- Resolved the issue where words with zero occurrences occupied space during inverted index merging. [#43113](https://github.com/apache/doris/pull/43113) \\n- Prevented abnormal large values in Index Size statistics. [#46549](https://github.com/apache/doris/pull/46549) \\n- Fixed the issue with inverted indexes for VARIANT type fields. [#43375](https://github.com/apache/doris/pull/43375) \\n- Optimized local cache locality for inverted indexes to improve cache hit rates. [#46518](https://github.com/apache/doris/pull/46518) \\n- Added the metric `NumInvertedIndexRemoteIOTotal` to query profiles for remote storage reads of inverted indexes. [#45675](https://github.com/apache/doris/pull/45675), [#44863](https://github.com/apache/doris/pull/44863)\\n\\n#### Others\\n\\n- Fixed the crash issue of the `ipv6_cidr_to_range` function with special NULL data. [#44700](https://github.com/apache/doris/pull/44700) \\n\\n### Permissions\\n\\n- When granting `CREATE_PRIV`, the existence of the corresponding resource is no longer checked. [#45125](https://github.com/apache/doris/pull/45125) \\n- Fixed the issue where queries on views with permissions could fail due to missing permissions for referenced tables in extreme scenarios. [#44621](https://github.com/apache/doris/pull/44621) \\n- Resolved the issue where permission checks for `use db` did not distinguish between internal and external Catalogs. [#45720](https://github.com/apache/doris/pull/45720)"},{"id":"/release-note-2.1.8","metadata":{"permalink":"/blog/release-note-2.1.8","source":"@site/blog/release-note-2.1.8.md","title":"Apache Doris 2.1.8 just released","description":"This version introduces continuous upgrades and enhancements in several key areas, including Lakehouse, Asynchronous Materialized Views, Query Optimizer and Execution Engine, Storage Management, and more.","date":"2025-01-24T00:00:00.000Z","tags":[{"inline":true,"label":"Release Notes","permalink":"/blog/tags/release-notes"}],"hasTruncateMarker":false,"authors":[{"name":"Apache Doris","key":null,"page":null}],"frontMatter":{"title":"Apache Doris 2.1.8 just released","summary":"This version introduces continuous upgrades and enhancements in several key areas, including Lakehouse, Asynchronous Materialized Views, Query Optimizer and Execution Engine, Storage Management, and more.","description":"This version introduces continuous upgrades and enhancements in several key areas, including Lakehouse, Asynchronous Materialized Views, Query Optimizer and Execution Engine, Storage Management, and more.","date":"2025-01-24","author":"Apache Doris","tags":["Release Notes"],"image":"/images/2.1.8.jpg"},"unlisted":false,"prevItem":{"title":"Apache Doris 3.0.4 Released","permalink":"/blog/release-note-3.0.4"},"nextItem":{"title":"Apache Doris 3.0.3 just released","permalink":"/blog/release-note-3.0.3"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n  http://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n\\nDear Community, **Apache Doris version 2.1.8 was officially released on January 24, 2025.** This version introduces continuous upgrades and enhancements in several key areas, including Lakehouse, Asynchronous Materialized Views, Query Optimizer and Execution Engine, Storage Management, and more.\\n\\n- [Quick Download](https://doris.apache.org/download)\\n\\n- [GitHub Release](https://github.com/apache/doris/releases/tag/2.1.8-rc01)\\n\\n## Behavior Changes\\n\\n- When querying a data source with case-insensitive table names (such as Hive) through External Catalog, in previous versions, you can use any case to query the table name, but in version 2.1.8, Doris\'s own table name case sensitivity policy will be strictly followed.\\n- Add the environment variable `SKIP_CHECK_ULIMIT` to skip the ulimit value verification check within the BE process. This is only available to applications in the Docker quick start scenario. [#45267](https://github.com/apache/doris/pull/45267)\\n- Add the `enable_cooldown_replica_affinity` session variable to control the selection of replica affinity for queries under cold - hot separation.\\n- In FE, add the configurations `restore_job_compressed_serialization` and `backup_job_compressed_serialization` to solve the OOM problem of FE during backup and restore operations when the number of db tablets is extremely large. By default, these configurations are disabled, and once enabled, they cannot be downgraded.\\n\\n## New Features\\n\\n- The Arrow flight protocol supports accessing BE through a load - balancing device. [#43281](https://github.com/apache/doris/pull/43281)\\n- Now lambda expressions support capturing external columns (#45186).[#45186](https://github.com/apache/doris/pull/45186)\\n\\n## Improvements\\n\\n### Lakehouse\\n\\n- Update the Hudi version to 0.15. And optimize the query planning performance of Hudi tables.\\n- Optimize the read performance of MaxCompute partitioned tables. [#45148](https://github.com/apache/doris/pull/45148)\\n- Support the session variable `enable_text_validate_utf8`, which can ignore the UTF8 encoding detection in CSV format. [#45537](https://github.com/apache/doris/pull/45537)\\n- Optimize the performance of Parquet file lazy materialization under high - filtering - rate conditions. [#46183](https://github.com/apache/doris/pull/46183)\\n\\n### Asynchronous Materialized Views\\n\\n- Now it supports manually refreshing partitions that do not exist in an asynchronous materialized view [#45290](https://github.com/apache/doris/pull/45290).\\n- Optimize the performance of transparent rewrite planning [#44786](https://github.com/apache/doris/pull/44786).\\n\\n### Query Optimizer\\n\\n- Improve the adaptive ability of runtime filters [#42640](https://github.com/apache/doris/pull/42640).\\n- Add the ability to generate original column filter conditions from filter conditions on `max/min` aggregate function columns [#39252](https://github.com/apache/doris/pull/39252)\\n- Add the ability to extract single - side filter conditions from join predicates [#38479](https://github.com/apache/doris/pull/38479).\\n- Optimize the ability of predicate derivation on set operators to better generate filter predicates [#39450](https://github.com/apache/doris/pull/39450).\\n- Optimize the exception handling ability of statistic information collection and usage to avoid generating unexpected execution plans when collection exceptions occur. [#43009](https://github.com/apache/doris/pull/43009) [#43776](https://github.com/apache/doris/pull/43776) [#43865](https://github.com/apache/doris/pull/43865) [#42104](https://github.com/apache/doris/pull/42104) [#42399](https://github.com/apache/doris/pull/42399) [#41729](https://github.com/apache/doris/pull/41729)\\n\\n### Query Execution Engine\\n\\n- Optimize the execution of queries with `limit` to end faster and avoid unnecessary data scanning [#44255](https://github.com/apache/doris/pull/44255).\\n\\n### Storage Management\\n\\n- CCR supports more comprehensive operations, such as `rename table`, `rename column`, `modify comment`, `drop view`, `drop rollup`, etc.\\n- Improve the accuracy of the broker load import progress and the performance when importing multiple compressed files.\\n- Improve the routine load timeout strategy and thread - pool usage to prevent routine load timeout failures and impacts on queries.\\n\\n### Others\\n\\n- The Docker quick - start image supports starting without setting environment parameters. Add the environment variable `SKIP_CHECK_ULIMIT` to skip the `start_be.sh` script and the swap, `max_map_count`, ulimit - related verification checks within the BE process. This is only applicable to applications in the Docker quick - start scenario.  [#45269](https://github.com/apache/doris/pull/45269)\\n- Add the new LDAP configuration `ldap_group_filter` for custom group filtering. [#43292](https://github.com/apache/doris/pull/43292)\\n- Optimize performance when using ranger. [#41207](https://github.com/apache/doris/pull/41207)\\n- Fix the inaccurate statistics of `scan bytes` in the audit log. [#45167](https://github.com/apache/doris/pull/45167)\\n- Now, the default values of columns can be correctly displayed in the `COLUMNS` system table. [#44849](https://github.com/apache/doris/pull/44849)\\n- Now, the definition of views can be correctly displayed in the `VIEWS` system table. [#45857](https://github.com/apache/doris/pull/45857)\\n- Now, the `admin` user cannot be deleted. [#44751](https://github.com/apache/doris/pull/44751)\\n\\n## Bug Fixes\\n\\n### Lakehouse\\n\\n#### Hive\\n\\n- Fix the problem of being unable to query Hive views created by Spark. [#43553](https://github.com/apache/doris/pull/43553)\\n- Fix the problem of being unable to correctly read some Hive Transaction tables. [#45753](https://github.com/apache/doris/pull/45753)\\n- Fix the problem of incorrect partition pruning when Hive table partitions contain special characters. [#42906](https://github.com/apache/doris/pull/42906)\\n\\n#### Iceberg\\n\\n- Fix the problem of being unable to create Iceberg tables in a Kerberos - authenticated environment. [#43445](https://github.com/apache/doris/pull/43445)\\n- Fix the problem of inaccurate `count(*)` queries when there are dangling deletes in Iceberg tables in some cases. [#44039](https://github.com/apache/doris/pull/44039)\\n- Fix the problem of query errors due to column name mismatches in Iceberg tables in some cases . [#44470](https://github.com/apache/doris/pull/44470)\\n- Fix the problem of being unable to read Iceberg tables when their partitions are modified in some cases .[#45367](https://github.com/apache/doris/pull/45367)\\n\\n#### Paimon\\n\\n- Fix the problem that the Paimon Catalog cannot access Alibaba Cloud OSS - HDFS. [#42585](https://github.com/apache/doris/pull/42585)\\n\\n#### Hudi\\n\\n- Fix the problem of ineffective partition pruning in Hudi tables in some cases. [#44669](https://github.com/apache/doris/pull/44669)\\n\\n#### JDBC\\n\\n- Fix the problem of being unable to obtain tables using the JDBC Catalog after enabling the case insensitive table name feature in some cases.\\n\\n#### MaxCompute\\n\\n- Fix the problem of ineffective partition pruning in MaxCompute tables in some cases[#44508](https://github.com/apache/doris/pull/44508).\\n\\n#### Others\\n\\n- Fix the problem of FE memory leaks caused by EXPORT tasks in some cases.[#44019](https://github.com/apache/doris/pull/44019)\\n- Fix the problem of being unable to access S3 object storage using the https protocol in some cases [#44242](https://github.com/apache/doris/pull/44242).\\n- Fix the problem of the inability to automatically refresh Kerberos authentication tickets in some cases  [#44916](https://github.com/apache/doris/pull/44916)\\n- Fix the problem of errors when reading Hadoop Block compressed format files in some cases. [#45289](https://github.com/apache/doris/pull/45289)\\n- When querying ORC - formatted data, no longer push down CHAR - type predicates to avoid possible result errors. [#45484](https://github.com/apache/doris/pull/45484)\\n\\n### Asynchronous Materialized Views\\n\\n- Fix the problem that when there is a CTE in the materialized view definition, it cannot be refreshed [#44857](https://github.com/apache/doris/pull/44857).\\n- Fix the problem that when columns are added to the base table, the asynchronous materialized view cannot hit the transparent rewrite. [#44867](https://github.com/apache/doris/pull/44867)\\n- Fix the problem that when the same filter predicate is included in different positions in a query, the transparent rewrite fails. [#44575](https://github.com/apache/doris/pull/44575)\\n- Fix the problem that when column aliases are used in filter predicates or join predicates, the transparent rewrite cannot be performed. [#44779](https://github.com/apache/doris/pull/44779)\\n\\n### Inverted Index\\n\\n- Fix the problem of abnormal handling of inverted index compaction. [#45773](https://github.com/apache/doris/pull/45773)\\n- Fix the problem that inverted index construction fails due to lock - waiting timeout. [#43589](https://github.com/apache/doris/pull/43589)\\n- Fix the problem of inverted index write crashes in abnormal situations. [#46075](https://github.com/apache/doris/pull/46075)\\n- Fix the null - pointer problem of the `match` function with special parameters. [#45774](https://github.com/apache/doris/pull/45774)\\n- Fix problems related to the variant inverted index and disable the use of the index v1 format for variants [#43971](https://github.com/apache/doris/pull/43971) [#45179](https://github.com/apache/doris/pull/45179/) \\n- Fix the problem of crashes when setting `gram_size = 65535` for the ngram bloomfilter index [#43654](https://github.com/apache/doris/pull/43654)\\n- Fix the problem of incorrect calculation of DATE and DATETIME for the bloomfilter index [#43622](https://github.com/apache/doris/pull/43622)\\n- Fix the problem that dropping a column does not automatically drop the bloomfilter index [#44478](https://github.com/apache/doris/pull/44478)\\n- Reduce the memory footprint when writing the bloomfilter index [#46047](https://github.com/apache/doris/pull/46047)\\n\\n### Semi-Structure Data \\n\\n- Optimize memory usage and reduce the memory consumption of the `variant` data type [#43349](https://github.com/apache/doris/pull/43349) [#44585](https://github.com/apache/doris/pull/44585) [#45734](https://github.com/apache/doris/pull/45734)\\n- Optimize the performance of `variant` schema copy. [#45731](https://github.com/apache/doris/pull/45731)\\n- Do not use `variant` as a key when automatically inferring tablet keys. [#44736](https://github.com/apache/doris/pull/44736)\\n- Fix the problem of changing `variant` from `NOT NULL` to `NULL` [#45734](https://github.com/apache/doris/pull/45734)\\n- Fix the problem of incorrect type inference of lambda functions. [#45798](https://github.com/apache/doris/pull/45798)\\n- Fix the coredump problem at the boundary conditions of the `ipv6_cidr_to_range` function [#46252](https://github.com/apache/doris/pull/46252)\\n\\n### Query Optimizer\\n\\n- Fix the potential deadlock problem caused by mutual exclusion of table read locks and optimize the lock - using logic [#45045](https://github.com/apache/doris/pull/45045) [#43376](https://github.com/apache/doris/pull/43376) [#44164](https://github.com/apache/doris/pull/44164) [#44967](https://github.com/apache/doris/pull/44967) [#45995](https://github.com/apache/doris/pull/45995).\\n- Fix the problem that the SQL Cache function incorrectly uses constant folding, resulting in incorrect results when using functions containing time formats . [#44631](https://github.com/apache/doris/pull/44631)\\n- Fix the problem of incorrect optimization of comparison expressions in edge cases, which may lead to incorrect results. [#44054](https://github.com/apache/doris/pull/44054) [#44725](https://github.com/apache/doris/pull/44725) [#44922](https://github.com/apache/doris/pull/44922) [#45735](https://github.com/apache/doris/pull/45735) [#45868](https://github.com/apache/doris/pull/45868)\\n- Fix the problem of incorrect audit logs for high - concurrent point queries [ #43345 ](https://github.com/apache/doris/pull/43345)[#44588](https://github.com/apache/doris/pull/44588)\\n- Fix the problem of continuous error reporting after an exception occurs in high - concurrent point queries [#44582](https://github.com/apache/doris/pull/44582)\\n- Fix the problem of incorrectly prepared statements for some fields.[#45732 ](https://github.com/apache/doris/pull/45732)\\n\\n### Query Execution Engine\\n\\n- Fix the problem of incorrect results of regular expressions and `like` functions for special characters. [#44547](https://github.com/apache/doris/pull/44547)\\n- Fix the problem that the SQL Cache may have incorrect results when switching databases. [#44782](https://github.com/apache/doris/pull/44782)\\n- Fix the problem of incorrect results of the `cut_ipv6` function. [#43921](https://github.com/apache/doris/pull/43921)\\n- Fix the problem of casting from numeric types to bool types. [#46275](https://github.com/apache/doris/pull/46275)\\n- Fix a series of problems related to arrow flight. [#45661](https://github.com/apache/doris/pull/45661) [#45023](https://github.com/apache/doris/pull/45023) [#43960](https://github.com/apache/doris/pull/43960) [#43929](https://github.com/apache/doris/pull/43929) \\n- Fix the problem of incorrect results in some cases when the hash table of hash join exceeds 4G. [#46461](https://github.com/apache/doris/pull/46461/files)\\n- Fix the overflow problem of the `convert_to` function for Chinese characters. [#46505](https://github.com/apache/doris/pull/46405)\\n\\n### Storage Management\\n\\n- Fix the problem that high - concurrent DDL may cause FE startup failure.\\n- Fix the problem that auto - increment columns may have duplicate values.\\n- Fix the problem that routine load cannot use the newly expanded BE during expansion.\\n\\n### Permission Management\\n\\n- Fix the problem of frequent access to the Ranger service when using Ranger as the authentication plugin [#45645](https://github.com/apache/doris/pull/45645).\\n\\n### Others\\n\\n- Fix the potential memory leak problem when `enable_jvm_monitor=true` is enabled on the BE side [#44311](https://github.com/apache/doris/pull/44311)."},{"id":"/release-note-3.0.3","metadata":{"permalink":"/blog/release-note-3.0.3","source":"@site/blog/release-note-3.0.3.md","title":"Apache Doris 3.0.3 just released","description":"Dear community members, the Apache Doris 3.0.3 version was officially released on December 02, 2024, this version further enhances the performance and stability of the system.","date":"2024-12-02T00:00:00.000Z","tags":[{"inline":true,"label":"Release Notes","permalink":"/blog/tags/release-notes"}],"hasTruncateMarker":false,"authors":[{"name":"Apache Doris","key":null,"page":null}],"frontMatter":{"title":"Apache Doris 3.0.3 just released","summary":"Dear community members, the Apache Doris 3.0.3 version was officially released on December 02, 2024, this version further enhances the performance and stability of the system.","description":"Dear community members, the Apache Doris 3.0.3 version was officially released on December 02, 2024, this version further enhances the performance and stability of the system.","date":"2024-12-02","author":"Apache Doris","tags":["Release Notes"],"image":"/images/3.0.3.jpg"},"unlisted":false,"prevItem":{"title":"Apache Doris 2.1.8 just released","permalink":"/blog/release-note-2.1.8"},"nextItem":{"title":"Scaling Bitcoin data to billions of records with Apache Doris: our journey to auto-partitioning","permalink":"/blog/ortege-studio-1-scaling-bitcoin-data-to-billions-of-records"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n  http://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n\\nDear community members, the Apache Doris 3.0.3 version was officially released on December 02, 2024, this version further enhances the performance and stability of the system.\\n\\n**Quick Download:** https://doris.apache.org/download/\\n\\n**GitHub Release:** https://github.com/apache/doris/releases\\n\\n## Behavioral Changes\\n\\n- Prohibited column updates on MOW tables with synchronous materialized views. [#40190](https://github.com/apache/doris/pull/40190)\\n- Adjusted the default parameters of RoutineLoad to improve import efficiency. [#42968](https://github.com/apache/doris/pull/42968)\\n- When StreamLoad fails, the return value of LoadedRows is adjusted to 0. [#41946](https://github.com/apache/doris/pull/41946) [#42291](https://github.com/apache/doris/pull/42291)\\n- Adjusted the default memory limit of Segment cache to 5%. [#42308](https://github.com/apache/doris/pull/42308) [#42436](https://github.com/apache/doris/pull/42436)\\n\\n## New Features\\n\\n- Introduced the session variable `enable_cooldown_replica_affinity` to control the affinity of cold and hot tiered replicas. [#42677](https://github.com/apache/doris/pull/42677)\\n\\n- Added `table$partition` syntax for querying partition information of Hive tables. [#40774](https://github.com/apache/doris/pull/40774)\\n  \\n  - [View Documentation](https://doris.apache.org/docs/3.0/lakehouse/datalake-analytics/hive)\\n\\n- Supported creation of Hive tables in Text format. [#41860](https://github.com/apache/doris/pull/41860) [#42175](https://github.com/apache/doris/pull/42175)\\n\\n  - [View Documentation](https://doris.apache.org/zh-CN/docs/3.0/lakehouse/datalake-building/hive-build#table)\\n\\n### Asynchronous Materialized Views\\n\\n- Introduced new materialized view attribute `use_for_rewrite`. When `use_for_rewrite` is set to false, the materialized view does not participate in transparent rewriting. [#40332](https://github.com/apache/doris/pull/40332)\\n\\n### Query Optimizer\\n\\n- Supported correlated non-aggregate subqueries. [#42236](https://github.com/apache/doris/pull/42236)\\n\\n### Query Execution\\n\\n- Added functions `ngram_search`, `normal_cdf`, `to_iso8601`, `from_iso8601_date`, `SESSION_USER()`, `last_query_id`. [#38226](https://github.com/apache/doris/pull/38226) [#40695](https://github.com/apache/doris/pull/40695) [#41075](https://github.com/apache/doris/pull/41075) [#41600](https://github.com/apache/doris/pull/41600) [#39575](https://github.com/apache/doris/pull/39575) [#40739](https://github.com/apache/doris/pull/40739)\\n- The `aes_encrypt` and `aes_decrypt` functions support GCM mode. [#40004](https://github.com/apache/doris/pull/40004)\\n- Profile outputs the changed session variable values. [#41016](https://github.com/apache/doris/pull/41016) [#41318](https://github.com/apache/doris/pull/41318)\\n\\n### Semi-structured Data Management\\n\\n- Added array functions `array_match_all` and `array_match_any`. [#40605](https://github.com/apache/doris/pull/40605) [#43514](https://github.com/apache/doris/pull/43514)\\n- The array function `array_agg` supports nesting ARRAY/MAP/STRUCT within ARRAY. [#42009](https://github.com/apache/doris/pull/42009)\\n- Added approximate aggregate statistical functions `approx_top_k` and `approx_top_sum`. [#44082](https://github.com/apache/doris/pull/44082)\\n\\n## Improvements\\n\\n### Storage\\n\\n- Supported `bitmap_empty` as the default value. [#40364](https://github.com/apache/doris/pull/40364)\\n- Introduced the session variable `insert_timeout` to control the timeout of DELETE statements. [#41063](https://github.com/apache/doris/pull/41063)\\n- Improved some error message prompts. [#41048](https://github.com/apache/doris/pull/41048) [#39631](https://github.com/apache/doris/pull/39631)\\n- Improved the priority scheduling of replica repair. [#41076](https://github.com/apache/doris/pull/41076)\\n- Enhanced the robustness of timezone handling when creating tables. [#41926](https://github.com/apache/doris/pull/41926) [#42389](https://github.com/apache/doris/pull/42389)\\n- Checked the validity of partition expressions when creating tables. [#40158](https://github.com/apache/doris/pull/40158)\\n- Supported Unicode-encoded column names in DELETE operations. [#39381](https://github.com/apache/doris/pull/39381)\\n\\n### Compute-Storage Decoupled\\n\\n- Supported ARM architecture deployment in storage and compute separation mode. [#42467](https://github.com/apache/doris/pull/42467) [#43377](https://github.com/apache/doris/pull/43377)\\n- Optimized the eviction strategy and lock competition of file cache, improving hit rate and high concurrency point query performance. [#42451](https://github.com/apache/doris/pull/42451) [#43201](https://github.com/apache/doris/pull/43201) [#41818](https://github.com/apache/doris/pull/41818) [#43401](https://github.com/apache/doris/pull/43401)\\n- S3 storage vault supported `use_path_style`, solving the problem of using custom domain names for object storage. [#43060](https://github.com/apache/doris/pull/43060) [#43343](https://github.com/apache/doris/pull/43343) [#43330](https://github.com/apache/doris/pull/43330)\\n- Optimized storage and compute separation configuration and deployment, preventing misoperations in different modes. [#43381](https://github.com/apache/doris/pull/43381) [#43522](https://github.com/apache/doris/pull/43522) [#43434](https://github.com/apache/doris/pull/43434) [#40764](https://github.com/apache/doris/pull/40764) [#43891](https://github.com/apache/doris/pull/43891)\\n- Optimized observability and provided an interface for deleting specified segment file cache. [#38489](https://github.com/apache/doris/pull/38489) [#42896](https://github.com/apache/doris/pull/42896) [#41037](https://github.com/apache/doris/pull/41037) [#43412](https://github.com/apache/doris/pull/43412)\\n- Optimized Meta-service operation and maintenance interface: RPC rate limiting and tablet metadata correction. [#42413](https://github.com/apache/doris/pull/42413) [#43884](https://github.com/apache/doris/pull/43884) [#41782](https://github.com/apache/doris/pull/41782) [#43460](https://github.com/apache/doris/pull/43460)\\n\\n### Lakehouse\\n\\n- Paimon Catalog supported Alibaba Cloud DLF and OSS-HDFS storage. [#41247](https://github.com/apache/doris/pull/41247) [#42585](https://github.com/apache/doris/pull/42585)\\n  \\n  - View [Documentation](https://doris.apache.org/docs/3.0/lakehouse/datalake-analytics/paimon)\\n\\n- Supported reading of Hive tables in OpenCSV format. [#42257](https://github.com/apache/doris/pull/42257) [#42942](https://github.com/apache/doris/pull/42942)\\n- Optimized the performance of accessing the `information_schema.columns` table in External Catalog. [#41659](https://github.com/apache/doris/pull/41659) [#41962](https://github.com/apache/doris/pull/41962)\\n- Used the new MaxCompute open storage API to access MaxCompute data sources. [#41614](https://github.com/apache/doris/pull/41614)\\n- Optimized the scheduling policy of the JNI part of Paimon tables, making scan tasks more balanced. [#43310](https://github.com/apache/doris/pull/43310)\\n- Optimized the read performance of small ORC files. [#42004](https://github.com/apache/doris/pull/42004) [#43467](https://github.com/apache/doris/pull/43467)\\n- Supported reading of parquet files in brotli compressed format. [#42177](https://github.com/apache/doris/pull/42177)\\n- Added `file_cache_statistics` table under the `information_schema` library to view metadata cache statistics. [#42160](https://github.com/apache/doris/pull/42160)\\n\\n### Query Optimizer\\n\\n- Optimization: When queries only differ in comments, the same SQL Cache can be reused. [#40049](https://github.com/apache/doris/pull/40049)\\n- Optimization: Improved the stability of statistical information when data is frequently updated. [#43865](https://github.com/apache/doris/pull/43865) [#39788](https://github.com/apache/doris/pull/39788) [#43009](https://github.com/apache/doris/pull/43009) [#40457](https://github.com/apache/doris/pull/40457) [#42409](https://github.com/apache/doris/pull/42409) [#41894](https://github.com/apache/doris/pull/41894)\\n- Optimization: Enhanced the stability of constant folding. [#42910](https://github.com/apache/doris/pull/42910) [#41164](https://github.com/apache/doris/pull/41164) [#39723](https://github.com/apache/doris/pull/39723) [#41394](https://github.com/apache/doris/pull/41394) [#42256](https://github.com/apache/doris/pull/42256) [#40441](https://github.com/apache/doris/pull/40441)\\n- Optimization: Column pruning can generate better execution plans. [#41719](https://github.com/apache/doris/pull/41719) [#41548](https://github.com/apache/doris/pull/41548)\\n\\n### Query Execution\\n\\n- Optimized the memory usage of the sort operator. [#39306](https://github.com/apache/doris/pull/39306)\\n- Optimized the performance of computations on ARM. [#38888](https://github.com/apache/doris/pull/38888) [#38759](https://github.com/apache/doris/pull/38759)\\n- Optimized the computational performance of a series of functions. [#40366](https://github.com/apache/doris/pull/40366) [#40821](https://github.com/apache/doris/pull/40821) [#40670](https://github.com/apache/doris/pull/40670) [#41206](https://github.com/apache/doris/pull/41206) [#40162](https://github.com/apache/doris/pull/40162)\\n- Used SSE instructions to optimize the performance of the `match_ipv6_subnet` function. [#38755](https://github.com/apache/doris/pull/38755)\\n- Supported automatic creation of new partitions during insert overwrite. [#38628](https://github.com/apache/doris/pull/38628) [#42645](https://github.com/apache/doris/pull/42645)\\n- Added the status of each PipelineTask in Profile. [#42981](https://github.com/apache/doris/pull/42981)\\n- IP type supported runtime filter. [#39985](https://github.com/apache/doris/pull/39985)\\n\\n### Semi-structured Data Management\\n\\n- Output the real SQL of prepared statements in audit logs. [#43321](https://github.com/apache/doris/pull/43321)\\n- The filebeat doris output plugin supports fault tolerance and progress reporting. [#36355](https://github.com/apache/doris/pull/36355)\\n- Optimized the performance of inverted index queries. [#41547](https://github.com/apache/doris/pull/41547) [#41585](https://github.com/apache/doris/pull/41585) [#41567](https://github.com/apache/doris/pull/41567) [#41577](https://github.com/apache/doris/pull/41577) [#42060](https://github.com/apache/doris/pull/42060) [#42372](https://github.com/apache/doris/pull/42372)\\n- The array function `array overlaps` supports acceleration using inverted indexes. [#41571](https://github.com/apache/doris/pull/41571)\\n- The IP function `is_ip_address_in_range` supports acceleration using inverted indexes. [#41571](https://github.com/apache/doris/pull/41571)\\n- Optimized the CAST performance of the VARIANT data type. [#41775](https://github.com/apache/doris/pull/41775) [#42438](https://github.com/apache/doris/pull/42438) [#43320](https://github.com/apache/doris/pull/43320)\\n- Optimized the CPU resource consumption of the Variant data type. [#42856](https://github.com/apache/doris/pull/42856) [#43062](https://github.com/apache/doris/pull/43062) [#43634](https://github.com/apache/doris/pull/43634)\\n- Optimized the metadata and execution memory resource consumption of the Variant data type. [#42448](https://github.com/apache/doris/pull/42448) [#43326](https://github.com/apache/doris/pull/43326) [#41482](https://github.com/apache/doris/pull/41482) [#43093](https://github.com/apache/doris/pull/43093) [#43567](https://github.com/apache/doris/pull/43567) [#43620](https://github.com/apache/doris/pull/43620)\\n\\n### Permissions\\n\\n- Added a new configuration item `ldap_group_filter` in LDAP for custom group filtering. [#43292](https://github.com/apache/doris/pull/43292)\\n\\n### Other\\n\\n- Supported displaying connection count information by user in FE monitoring items. [#39200](https://github.com/apache/doris/pull/39200)\\n\\n## Bug Fixes\\n\\n### Storage\\n\\n- Fixed the issue with using IPv6 hostnames. [#40074](https://github.com/apache/doris/pull/40074)\\n- Fixed the inaccurate display of broker/s3 load progress. [#43535](https://github.com/apache/doris/pull/43535)\\n- Fixed the issue where queries might hang from FE. [#41303](https://github.com/apache/doris/pull/41303) [#42382](https://github.com/apache/doris/pull/42382)\\n- Fixed the issue of duplicate auto-increment IDs under exceptional circumstances. [#43774](https://github.com/apache/doris/pull/43774)  [#43983](https://github.com/apache/doris/pull/43983)\\n- Fixed occasional NPE issues with groupcommit. [#43635](https://github.com/apache/doris/pull/43635)\\n- Fixed the inaccurate calculation of auto bucket. [#41675](https://github.com/apache/doris/pull/41675) [#41835](https://github.com/apache/doris/pull/41835)\\n- Fixed the issue where FE might not correctly plan multi-table flows after restart. [#41677](https://github.com/apache/doris/pull/41677) [#42290](https://github.com/apache/doris/pull/42290)\\n\\n### Compute-Storage Decoupled\\n\\n- Fixed the issue that MOW primary key tables with large delete bitmaps might cause coredump. [#43088](https://github.com/apache/doris/pull/43088) [#43457](https://github.com/apache/doris/pull/43457) [#43479](https://github.com/apache/doris/pull/43479) [#43407](https://github.com/apache/doris/pull/43407) [#43297](https://github.com/apache/doris/pull/43297) [#43613](https://github.com/apache/doris/pull/43613) [#43615](https://github.com/apache/doris/pull/43615) [#43854](https://github.com/apache/doris/pull/43854) [#43968](https://github.com/apache/doris/pull/43968) [#44074](https://github.com/apache/doris/pull/44074) [#41793](https://github.com/apache/doris/pull/41793) [#42142](https://github.com/apache/doris/pull/42142)\\n- Fixed the issue that segment files, when being a multiple of 5MB, would fail to upload objects. [#43254](https://github.com/apache/doris/pull/43254)\\n- Fixed the issue that the default retry policy of aws sdk did not take effect. [#43575](https://github.com/apache/doris/pull/43575) [#43648](https://github.com/apache/doris/pull/43648)\\n- Fixed the issue that altering storage vault could continue execution even when the wrong type was specified. [#43489](https://github.com/apache/doris/pull/43489) [#43352](https://github.com/apache/doris/pull/43352) [#43495](https://github.com/apache/doris/pull/43495)\\n- Fixed the issue that tablet_id might be 0 during the delayed commit process of large transactions. [#42043](https://github.com/apache/doris/pull/42043) [#42905](https://github.com/apache/doris/pull/42905)\\n- Fixed the issue that constant folding RCP and FE forwarding SQL might not be executed in the expected computation group. [#43110](https://github.com/apache/doris/pull/43110) [#41819](https://github.com/apache/doris/pull/41819) [#41846](https://github.com/apache/doris/pull/41846)\\n- Fixed the issue that meta-service did not strictly check instance_id upon receiving RPC. [#43253](https://github.com/apache/doris/pull/43253) [#43832](https://github.com/apache/doris/pull/43832)\\n- Fixed the issue that FE follower information_schema version did not update in time. [#43496](https://github.com/apache/doris/pull/43496)\\n- Fixed the issue of atomicity in file cache rename and inaccurate metrics. [#42869](https://github.com/apache/doris/pull/42869) [#43504](https://github.com/apache/doris/pull/43504) [#43220](https://github.com/apache/doris/pull/43220)\\n\\n### Lakehouse\\n\\n- Prohibited implicit conversion predicates from being pushed down to JDBC data sources to avoid inconsistent query results. [#42102](https://github.com/apache/doris/pull/42102)\\n- Fixed some read issues with high-version Hive transactional tables. [#42226](https://github.com/apache/doris/pull/42226)\\n- Fixed the issue that the Export command might cause deadlocks. [#43083](https://github.com/apache/doris/pull/43083) [#43402](https://github.com/apache/doris/pull/43402)\\n- Fixed the issue of being unable to query Hive views created by Spark. [#43552](https://github.com/apache/doris/pull/43552)\\n- Fixed the issue that Hive partition paths containing special characters led to incorrect partition pruning. [#42906](https://github.com/apache/doris/pull/42906)\\n- Fixed the issue that Iceberg Catalog could not use AWS Glue. [#41084](https://github.com/apache/doris/pull/41084)\\n\\n### Asynchronous Materialized Views\\n\\n- Fixed the issue that asynchronous materialized views might not refresh after the base table is rebuilt. [#41762](https://github.com/apache/doris/pull/41762)\\n\\n### Query Optimizer\\n\\n- Fixed the issue that partition pruning results might be incorrect when using multi-column range partitioning. [#43332](https://github.com/apache/doris/pull/43332)\\n- Fixed the issue of incorrect calculation results in some limit offset scenarios. [#42576](https://github.com/apache/doris/pull/42576)\\n\\n### Query Execution\\n\\n- Fixed the issue that hash join with array types larger than 4G could cause BE Core. [#43861](https://github.com/apache/doris/pull/43861)\\n- Fixed the issue that is null predicate operations might yield incorrect results in some scenarios. [#43619](https://github.com/apache/doris/pull/43619)\\n- Fixed the issue that bitmap types might produce incorrect output results in hash join. [#43718](https://github.com/apache/doris/pull/43718)\\n- Fixed some issues where function results were calculated incorrectly. [#40710](https://github.com/apache/doris/pull/40710) [#39358](https://github.com/apache/doris/pull/39358) [#40929](https://github.com/apache/doris/pull/40929) [#40869](https://github.com/apache/doris/pull/40869) [#40285](https://github.com/apache/doris/pull/40285) [#39891](https://github.com/apache/doris/pull/39891) [#40530](https://github.com/apache/doris/pull/40530) [#41948](https://github.com/apache/doris/pull/41948) [#43588](https://github.com/apache/doris/pull/43588)\\n- Fixed some issues with JSON type parsing. [#39937](https://github.com/apache/doris/pull/39937)\\n- Fixed issues with varchar and char types in runtime filter operations. [#43758](https://github.com/apache/doris/pull/43758) [#43919](https://github.com/apache/doris/pull/43919)\\n- Fixed some issues with the use of decimal256 in scalar and aggregate functions. [#42136](https://github.com/apache/doris/pull/42136) [#42356](https://github.com/apache/doris/pull/42356)\\n- Fixed the issue that arrow flight reported `Reach limit of connections` errors upon connection. [#39127](https://github.com/apache/doris/pull/39127)\\n- Fixed the issue of incorrect memory usage statistics for BE in k8s environments. [#41123](https://github.com/apache/doris/pull/41123)\\n\\n### Semi-structured Data Management\\n\\n- Adjusted the default values of `segment_cache_fd_percentage` and `inverted_index_fd_number_limit_percent`. [#42224](https://github.com/apache/doris/pull/42224)\\n- logstash now supports group_commit. [#40450](https://github.com/apache/doris/pull/40450)\\n- Fixed the issue of coredump when building index. [#43246](https://github.com/apache/doris/pull/43246) [#43298](https://github.com/apache/doris/pull/43298)\\n- Fixed issues with variant index. [#43375](https://github.com/apache/doris/pull/43375) [#43773](https://github.com/apache/doris/pull/43773)\\n- Fixed potential fd and memory leaks under abnormal compaction circumstances. [#42374](https://github.com/apache/doris/pull/42374)\\n- Inverted index match null now correctly returns null instead of false. [#41786](https://github.com/apache/doris/pull/41786)\\n- Fixed the issue of coredump when ngram bloomfilter index bf_size is set to 65536. [#43645](https://github.com/apache/doris/pull/43645)\\n- Fixed the issue of potential coredump during complex data type JOINs. [#40398](https://github.com/apache/doris/pull/40398)\\n- Fixed the issue of coredump with TVF JSON data. [#43187](https://github.com/apache/doris/pull/43187)\\n- Fixed the precision issue of bloom filter calculations for dates and times. [#43612](https://github.com/apache/doris/pull/43612)\\n- Fixed the issue of coredump with IPv6 type storage. [#43251](https://github.com/apache/doris/pull/43251)\\n- Fixed the issue of coredump when using VARIANT type with light_schema_change disabled. [#40908](https://github.com/apache/doris/pull/40908)\\n- Improved cache performance for high-concurrency point queries. [#44077](https://github.com/apache/doris/pull/44077)\\n- Fixed the issue that bloom filter indexes were not synchronized when columns were deleted. [#43378](https://github.com/apache/doris/pull/43378)\\n- Fixed instability issues with es catalog under special circumstances such as mixed array and scalar data. [#40314](https://github.com/apache/doris/pull/40314) [#40385](https://github.com/apache/doris/pull/40385) [#43399](https://github.com/apache/doris/pull/43399) [#40614](https://github.com/apache/doris/pull/40614)\\n- Fixed coredump issues caused by abnormal regular pattern matching. [#43394](https://github.com/apache/doris/pull/43394)\\n\\n### Permissions\\n\\n- Fixed several issues where permissions were not properly restricted after authorization. [#43193](https://github.com/apache/doris/pull/43193) [#41723](https://github.com/apache/doris/pull/41723) [#42107](https://github.com/apache/doris/pull/42107) [#43306](https://github.com/apache/doris/pull/43306)\\n- Enhanced several permission checks. [#40688](https://github.com/apache/doris/pull/40688) [#40533](https://github.com/apache/doris/pull/40533) [#41791](https://github.com/apache/doris/pull/41791) [#42106](https://github.com/apache/doris/pull/42106)\\n\\n### Other\\n\\n- Supplemented missing audit log fields in audit log tables and files. [#43303](https://github.com/apache/doris/pull/43303)\\n  \\n  - [View Documentation](https://doris.apache.org/docs/3.0/admin-manual/system-tables/internal_schema/audit_log)"},{"id":"/ortege-studio-1-scaling-bitcoin-data-to-billions-of-records","metadata":{"permalink":"/blog/ortege-studio-1-scaling-bitcoin-data-to-billions-of-records","source":"@site/blog/ortege-studio-1-scaling-bitcoin-data-to-billions-of-records.md","title":"Scaling Bitcoin data to billions of records with Apache Doris: our journey to auto-partitioning","description":"To power lightning-fast queries and ensure the Deep Dive dashboards deliver real-time insights, Ortege relies on Apache Doris. A crucial feature they embrace is Auto Partition.","date":"2024-11-20T00:00:00.000Z","tags":[{"inline":true,"label":"Best Practice","permalink":"/blog/tags/best-practice"}],"hasTruncateMarker":false,"authors":[{"name":"Justin Trollip","key":null,"page":null}],"frontMatter":{"title":"Scaling Bitcoin data to billions of records with Apache Doris: our journey to auto-partitioning","description":"To power lightning-fast queries and ensure the Deep Dive dashboards deliver real-time insights, Ortege relies on Apache Doris. A crucial feature they embrace is Auto Partition.","summary":"To power lightning-fast queries and ensure the Deep Dive dashboards deliver real-time insights, Ortege relies on Apache Doris. A crucial feature they embrace is Auto Partition.","date":"2024-11-20","author":"Justin Trollip","tags":["Best Practice"],"image":"/images/ortege-1.jpg"},"unlisted":false,"prevItem":{"title":"Apache Doris 3.0.3 just released","permalink":"/blog/release-note-3.0.3"},"nextItem":{"title":"Fine-tuning Apache Doris for maximum performance and resilience: a deep dive into fe.conf","permalink":"/blog/ortege-studio-2-fine-tuning-apache-doris-for-maximum-performance-and-resilience"}},"content":"\x3c!-- \\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n:::info Special Thanks\\n\\n The Apache Doris community shares our deep appreciation for [Justin Trollip](https://www.linkedin.com/in/justintrollip/), Founder of [Ortege AI](https://www.ortege.ai/), for providing his hands-on experience in applying Auto Partition of Apache Doris to manage massive blockchain data. Justin is a passionate and visionary technologist and entrepreneur, and we are glad to have him join the Apache Doris user community and share his insights with us.\\n\\n:::\\n\\nAt Ortege, we\'re building the ultimate blockchain data platform, providing comprehensive and accessible insights to empower the crypto community. One of our biggest challenges is managing the sheer volume of data, especially for Bitcoin. Our bitcoin.outputs table alone contains over **3 billion records** and it\'s growing every day!\\n\\nTo power our lightning-fast queries and ensure Ortege Studio\'s Deep Dive dashboards deliver real-time insights, we rely on Apache Doris, a high-performance analytical database. A crucial feature we\'ve embraced is **Doris\' auto-partitioning**.\\n\\n**From manual to automated:**\\n\\nInitially, we manually defined partitions for our Bitcoin tables, meticulously crafting ranges based on block height. While this worked well for Bitcoin\'s relatively predictable growth, it became cumbersome for newer chains like Stacks, where data patterns are more dynamic.\\n\\n**Enter Doris auto-partitioning:**\\n\\nDoris\' auto-partitioning changed the game. It automatically creates partitions based on the data being ingested, eliminating the need for manual intervention. We can now define partition rules (e.g., partition by month based on the block timestamp) and let Doris handle the rest. This is a game-changer for:\\n\\n- **Scalability:** Effortlessly accommodate massive data growth without manual schema updates.\\n- **Performance:** Optimize query speed by ensuring data is distributed efficiently across partitions.\\n- **Flexibility:** Easily adapt to changing data patterns, especially for newer chains with less predictable growth.\\n\\n**A tailored approach:**\\n\\nWhile auto-partitioning is perfect for many scenarios, we\'ve found that a hybrid approach works best for Bitcoin. We use auto-partitioning for recent data while manually managing partitions for historical data, balancing automation with fine-grained control.\\n\\n**Lessons learned:**\\n\\nOur experience with Doris auto-partitioning has taught us that:\\n\\n- **Flexibility is key:** Choose a data platform that can adapt to your specific needs and data characteristics.\\n- **Automation is powerful:** Leverage automation to streamline data management and focus on extracting insights.\\n- **Performance matters:** A high-performance database like Apache Doris is essential for handling the scale of blockchain data.\\n\\n**We\'re committed to providing the most comprehensive and accessible blockchain data platform, and Apache Doris\' auto-partitioning is a crucial part of that journey. Stay tuned as we continue to innovate and empower the crypto community with data-driven insights.**\\n\\nThis article was written by Justin Trollip and originally posted on [Linkedin](https://www.linkedin.com/pulse/scaling-bitcoin-data-billions-records-apache-doris-our-journey-diknc/?trackingId=pSs7z3aeSguCbWyxTcew6w%3D%3D)."},{"id":"/ortege-studio-2-fine-tuning-apache-doris-for-maximum-performance-and-resilience","metadata":{"permalink":"/blog/ortege-studio-2-fine-tuning-apache-doris-for-maximum-performance-and-resilience","source":"@site/blog/ortege-studio-2-fine-tuning-apache-doris-for-maximum-performance-and-resilience.md","title":"Fine-tuning Apache Doris for maximum performance and resilience: a deep dive into fe.conf","description":"Ortege handles massive volumes of blockchain data to power its analytics platform, Ortege Studio. Apache Doris forms the backbone of its Lakehouse v2, enabling it to process billions of records and deliver real-time insights.","date":"2024-11-20T00:00:00.000Z","tags":[{"inline":true,"label":"Best Practice","permalink":"/blog/tags/best-practice"}],"hasTruncateMarker":false,"authors":[{"name":"Justin Trollip","key":null,"page":null}],"frontMatter":{"title":"Fine-tuning Apache Doris for maximum performance and resilience: a deep dive into fe.conf","description":"Ortege handles massive volumes of blockchain data to power its analytics platform, Ortege Studio. Apache Doris forms the backbone of its Lakehouse v2, enabling it to process billions of records and deliver real-time insights.","summary":"Ortege handles massive volumes of blockchain data to power its analytics platform, Ortege Studio. Apache Doris forms the backbone of its Lakehouse v2, enabling it to process billions of records and deliver real-time insights.","date":"2024-11-20","author":"Justin Trollip","tags":["Best Practice"],"image":"/images/ortege-2.jpg"},"unlisted":false,"prevItem":{"title":"Scaling Bitcoin data to billions of records with Apache Doris: our journey to auto-partitioning","permalink":"/blog/ortege-studio-1-scaling-bitcoin-data-to-billions-of-records"},"nextItem":{"title":"Apache Doris 2.1.7 just released","permalink":"/blog/release-note-2.1.7"}},"content":"\x3c!-- \\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n:::info Special Thanks\\n\\nThis is the second contribution from Justin Trollip. He introduces the fe.conf setting optimization for higher performance and resilience in his case. He uses Apache Doris as the backbone of Ortege\'s lakehouse, which handles massive volumes of blockchain data. Previously, he also shared his experience in using Auto Partition of Doris.\\n\\n:::\\n\\n**[Apache Doris](https://www.linkedin.com/company/doris-apache/)** is renowned for its speed and scalability as an MPP analytical database. But to unlock its full potential, especially in demanding blockchain data environments, careful configuration is key. Today, we\'ll explore the critical settings in Doris\' fe.conf file that directly impact performance and resilience, using our own configuration at Ortege as a real-world case study.\\n\\n## Ortege\'s Lakehouse v2: Powered by Doris\\n\\nAt Ortege, we handle massive volumes of blockchain data to power our analytics platform, **[Ortege Studio](https://app.ortege.ai/dashboard/list)**. Apache Doris forms the backbone of our **[Lakehouse v2](https://docs.ortege.ai/ortege-documentation/ortege-guides/ortege-lakehouse)**, enabling us to process **billions** of records and deliver real-time insights. Our journey has involved continuous optimization and fine-tuning of Doris, and fe.conf plays a central role.\\n\\n## Key Areas for Performance and Resilience:\\n\\nHere are the crucial fe.conf settings we\'ve prioritized, along with areas where we\'re actively experimenting for further improvement:\\n\\n### Query Engine Optimization\\n\\n- `default_max_query_instances`: This setting controls the maximum number of query instances allowed per user. We currently have it set to the default (-1 for unlimited), but we\'re considering setting a reasonable limit to prevent resource exhaustion from runaway queries, especially in multi-tenant scenarios.\\n- `max_query_retry_time` (Default: 3): While the default retry attempts provide some fault tolerance, increasing this value could improve query resilience in unstable network environments, but we need to carefully consider the potential impact on overall query latency.\\n- `enable_local_replica_selection` **(set to `true`):**  We have now enabled this feature to optimize query performance. In our deployment, FE and BE nodes are co-located, so this should reduce network overhead by preferentially using local replicas for queries.\\n- `rewrite_count_distinct_to_bitmap_hll` **(Default: true):** We\'ve kept this enabled to leverage bitmap and HLL optimizations for COUNT(DISTINCT) queries, which are common in our analytical workloads. \\n\\n### Load and Export Management\\n\\n- `enable_pipeline_load` **(Default: true),** `enable_vectorized_load` **(Default: true),** `enable_new_load_scan_node` **(Default: true):** We\'ve enabled these settings to leverage the latest performance enhancements in Doris\' load process. Pipeline execution, vectorized loading, and the new scan node contribute to faster data ingestion.\\n- `max_running_txn_num_per_db` **(Default: 1000):** While the default concurrency limit is sufficient for our current needs, we\'re monitoring this closely as our data volume and user base grow. We might need to increase this limit to accommodate more concurrent load jobs without causing contention.\\n- `max_stream_load_timeout_second` **(Default: 3 days):** The default timeout is very generous. We\'re evaluating if a shorter timeout, balanced with potential retry mechanisms, could improve resource utilization and prevent long-running, potentially problematic stream loads.\\n\\n### Metadata and Cluster Management\\n\\n- `meta_delay_toleration_second` **(Default: 5 minutes):** This setting controls how long non-master FE nodes tolerate metadata delays from the master. We might need to adjust this based on our network latency and the frequency of metadata updates. \\n- `disable_colocate_balance` **(Default: false):** We haven\'t disabled colocation balance, as it\'s crucial for maintaining optimal data distribution and query performance. However, we\'re exploring the implications of temporarily disabling it during specific maintenance operations or upgrades. \\n\\n### Storage Optimization\\n\\n- `storage_flood_stage_usage_percent` **(Default: 95%)**, storage_flood_stage_left_capacity_bytes **(Default: 1GB):** These settings control the thresholds at which Doris rejects write operations to prevent storage exhaustion. We\'re continuously monitoring disk space usage and adjusting these thresholds as needed to avoid disruptions.\\n- `storage_high_watermark_usage_percent` **(Default: 85%)**, storage_min_left_capacity_bytes **(Default: 2GB):** These parameters govern how Doris selects storage paths for data balancing. We\'re actively experimenting with these thresholds to find the optimal balance between data distribution and avoiding disk space issues. \\n\\n### Query Caching\\n\\n- `cache_enable_sql_mode` **(set to** `true`**):** We\'ve enabled query caching to significantly speed up query responses, especially for our frequently accessed Deep Dive dashboards. \\n- `cache_result_max_row_count` **(set to** `20000`**):** We increased the maximum number of rows that can be cached to accommodate the typical result set sizes of our dashboard queries. \\n\\n## **Continuous Improvement**\\n\\nFine-tuning Doris is an ongoing process, and we\'re constantly exploring new configuration options and optimization strategies. By sharing our fe.conf learnings and areas for improvement, we hope to help others get the most out of this powerful analytical database.\\n\\nThis article was written by Justin Trollip and originally posted on [Linkedin](https://www.linkedin.com/pulse/fine-tuning-apache-doris-maximum-performance-resilience-deep-dive-jiwac/?trackingId=jJ%2FO3s%2FHRGee3mxQ9LTnxQ%3D%3D)."},{"id":"/release-note-2.1.7","metadata":{"permalink":"/blog/release-note-2.1.7","source":"@site/blog/release-note-2.1.7.md","title":"Apache Doris 2.1.7 just released","description":"This version brings continuous upgrades and improvements. Additionally, several fixes have been implemented in areas such as the  to the Lakehouse, Async Materialized Views, and Semi-Structured Data Management, Query Optimizer and Permission Management.","date":"2024-11-10T00:00:00.000Z","tags":[{"inline":true,"label":"Release Notes","permalink":"/blog/tags/release-notes"}],"hasTruncateMarker":false,"authors":[{"name":"Apache Doris","key":null,"page":null}],"frontMatter":{"title":"Apache Doris 2.1.7 just released","summary":"This version brings continuous upgrades and improvements. Additionally, several fixes have been implemented in areas such as the  to the Lakehouse, Async Materialized Views, and Semi-Structured Data Management, Query Optimizer and Permission Management.","description":"This version brings continuous upgrades and improvements. Additionally, several fixes have been implemented in areas such as the  to the Lakehouse, Async Materialized Views, and Semi-Structured Data Management, Query Optimizer and Permission Management.","date":"2024-11-10","author":"Apache Doris","tags":["Release Notes"],"image":"/images/2.1.7.jpg"},"unlisted":false,"prevItem":{"title":"Fine-tuning Apache Doris for maximum performance and resilience: a deep dive into fe.conf","permalink":"/blog/ortege-studio-2-fine-tuning-apache-doris-for-maximum-performance-and-resilience"},"nextItem":{"title":"New milestone: Apache Doris 3.0 has been released","permalink":"/blog/release-note-3.0.0"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n  http://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n\\nDear community, **Apache Doris version 2.1.7 was officially released on November 10, 2024.** This version brings continuous upgrades and improvements. Additionally, several fixes have been implemented in areas such as the  to the Lakehouse, Async Materialized Views, and Semi-Structured Data Management, Query Optimizer and Permission Management. \\n\\n**Quick Download:** https://doris.apache.org/download/\\n\\n**GitHub Release:** https://github.com/apache/doris/releases\\n\\n## Behavior changes\\n\\n- The following global variables will be forcibly set to the following default values:\\n  - enable_nereids_dml: true\\n  - enable_nereids_dml_with_pipeline: true\\n  - enable_nereids_planner: true\\n  - enable_fallback_to_original_planner: true\\n  - enable_pipeline_x_engine: true\\n- New columns have been added to the audit log. [#42262](https://github.com/apache/doris/pull/42262)\\n  - For more information, please  refer to [docs](https://doris.apache.org/docs/admin-manual/audit-plugin/)\\n\\n## New features\\n\\n### Async Materialized View\\n\\n- An asynchronous materialized view has added a property called `use_for_rewrite` to control whether it participates in transparent rewriting. [#40332](https://github.com/apache/doris/pull/40332)\\n\\n### Query Execution\\n\\n- The list of changed session variables is now output in the Profile. [#41016](https://github.com/apache/doris/pull/41016)\\n- Support for `trim_in`, `ltrim_in`, and `rtrim_in` functions has been added. [#42641](https://github.com/apache/doris/pull/42641) (Note: This is a duplicate mention, but I\'m including it as per your original list.)\\n- Support for several URL functions (top_level_domain, first_significant_subdomain, cut_to_first_significant_subdomain) has been added. [#42916](https://github.com/apache/doris/pull/42916)\\n- The `bit_set` function has been added. [#42916](https://github.com/apache/doris/pull/42099)\\n- The `count_substrings` function has been added. [#42055](https://github.com/apache/doris/pull/42055)\\n- The `translate` and `url_encode` functions have been added. [#41051](https://github.com/apache/doris/pull/41051)\\n- The `normal_cdf`, `to_iso8601`, and `from_iso8601_date` functions have been added. [#40695](https://github.com/apache/doris/pull/40695)\\n\\n### Storage Management\\n\\n- The `information_schema.table_options` and `table_properties` system tables have been added, supporting the querying of attributes set during table creation. [#34384](https://github.com/apache/doris/pull/34384)\\n- Support for `bitmap_empty` as a default value has been implemented. [#40364](https://github.com/apache/doris/pull/40364)\\n- A new session variable `require_sequence_in_insert` has been introduced to control whether a sequence column must be provided when performing `INSERT INTO SELECT` writes to a unique key table. [#41655](https://github.com/apache/doris/pull/41655)\\n\\n### Others\\n\\n- Allow for generating flame graphs on the BE WebUI page.[#41044](https://github.com/apache/doris/pull/41044)\\n\\n## Improvements\\n\\n### Lakehouse\\n\\n- Support for writing data to Hive text format tables. [#40537](https://github.com/apache/doris/pull/40537)\\n  - For more information, please  refer to [docs](https://doris.apache.org/docs/lakehouse/datalake-building/hive-build)\\n- Access MaxCompute data using MaxCompute Open Storage API. [#41610](https://github.com/apache/doris/pull/41610)\\n  - For more information, please  refer to [docs](https://doris.apache.org/docs/lakehouse/database/max-compute)\\n- Support for Paimon DLF Catalog. [#41694](https://github.com/apache/doris/pull/41694)\\n  - For more information, please  refer to [docs](https://doris.apache.org/docs/lakehouse/datalake-analytics/paimon)\\n- Added `table$partitions` syntax to directly query Hive partition information.[#41230](https://github.com/apache/doris/pull/41230)\\n  - For more information, please  refer to [docs](https://doris.apache.org/docs/lakehouse/datalake-analytics/hive)\\n- Support for reading Parquet files in brotli compression format.[#42162](https://github.com/apache/doris/pull/42162)\\n- Support for reading DECIMAL 256 types in Parquet files. [#42241](https://github.com/apache/doris/pull/42241)\\n- Support for reading Hive tables in OpenCsvSerde format.[#42939](https://github.com/apache/doris/pull/42939)\\n\\n### Async Materialized View\\n\\n- Refined the granularity of lock holding during the build process for asynchronous materialized views. [#40402](https://github.com/apache/doris/pull/40402) [#41010](https://github.com/apache/doris/pull/41010).\\n\\n### Query optimizer\\n\\n- Improved the accuracy of statistic information collection and usage in extreme cases to enhance planning stability. [#40457](https://github.com/apache/doris/pull/40457)\\n- Runtime filters can now be generated in more scenarios to improve query performance. [#40815](https://github.com/apache/doris/pull/40815)\\n- Enhanced constant folding capabilities for numerical, date, and string functions to boost query performance.  [#40820](https://github.com/apache/doris/pull/40820)\\n- Optimized the column pruning algorithm to enhance query performance. [#41548](https://github.com/apache/doris/pull/41548)\\n\\n### Query Execution\\n\\n- Supported parallel preparation to reduce the time consumed by short queries. [#40270](https://github.com/apache/doris/pull/40270)\\n- Corrected the names of some counters in the profile to match the audit logs. [#41993](https://github.com/apache/doris/pull/41993)\\n- Added new local shuffle rules to speed up certain queries. [#40637](https://github.com/apache/doris/pull/40637)\\n\\n### Storage Management\\n\\n- The `SHOW PARTITIONS` command now supports displaying the commit version. [#28274](https://github.com/apache/doris/pull/28274)\\n- Checked for unreasonable partition expressions when creating tables. [#40158](https://github.com/apache/doris/pull/40158)\\n- Optimized the scheduling logic when encountering EOF in Routine Load. [#40509](https://github.com/apache/doris/pull/40509)\\n- Made Routine Load aware of schema changes. [#40508](https://github.com/apache/doris/pull/40508)\\n- Improved the timeout logic for Routine Load tasks. [#41135](https://github.com/apache/doris/pull/41135)\\n\\n### Others\\n\\n- Allowed closing the built-in service port of BRPC via BE configuration. [#41047](https://github.com/apache/doris/pull/41047)\\n- Fixed issues with missing fields and duplicate records in audit logs. [#43015](https://github.com/apache/doris/pull/43015)\\n\\n## Bug fixes\\n\\n### Lakehouse\\n\\n- Fixed the inconsistency in the behavior of INSERT OVERWRITE with Hive. [#39840](https://github.com/apache/doris/pull/39840)\\n- Cleaned up temporarily created folders to address the issue of too many empty folders on HDFS.  [#40424](https://github.com/apache/doris/pull/40424)\\n- Resolved memory leaks in FE caused by using the JDBC Catalog in some cases. [#40923](https://github.com/apache/doris/pull/40923)\\n- Resolved memory leaks in BE caused by using the JDBC Catalog in some cases. [#41266](https://github.com/apache/doris/pull/41266)\\n- Fixed errors in reading Snappy compressed formats in certain scenarios. [#40862](https://github.com/apache/doris/pull/40862)\\n- Addressed potential FileSystem leaks on the FE side in certain scenarios. [#41108](https://github.com/apache/doris/pull/41108)\\n- Resolved issues where using EXPLAIN VERBOSE to view external table execution plans could cause null pointer exceptions in some cases. [#41231] (https://github.com/apache/doris/pull/41231)\\n- Fixed the inability to read tables in Paimon parquet format. [#41487](https://github.com/apache/doris/pull/41487)\\n- Addressed performance issues introduced by compatibility changes in the JDBC Oracle Catalog.  [#41407](https://github.com/apache/doris/pull/41407)\\n- Disabled predicate pushing down after implicit conversion to resolve incorrect query results in some cases with JDBC Catalog. [#42242](https://github.com/apache/doris/pull/42242)\\n- Fixed issues with case-sensitive access to table names in the External Catalog. [#42261](https://github.com/apache/doris/pull/42261)\\n\\n### Async Materialized View\\n\\n- Fixed the issue where user-specified start times were not effective. [#39573](https://github.com/apache/doris/pull/39573)\\n- Resolved the issue of nested materialized views not refreshing. [#40433](https://github.com/apache/doris/pull/40433)\\n- Fixed the issue where materialized views might not refresh after the base table was deleted and recreated.  [#41762](https://github.com/apache/doris/pull/41762)\\n- Addressed issues where partition compensation rewrites could lead to incorrect results. [#40803](https://github.com/apache/doris/pull/40803)\\n- Fixed potential errors in rewrite results when `sql_select_limit` was set. [#40106](https://github.com/apache/doris/pull/40106)\\n\\n### Semi-Structured Data Management\\n\\n- Fixed the issue of index file handle leaks. [#41915](https://github.com/apache/doris/pull/41915)\\n- Addressed inaccuracies in the `count()` function of inverted indexes in special cases. (#41127)[https://github.com/apache/doris/pull/41127]\\n- Fixed exceptions with variant when light schema change was not enabled. [#40908](https://github.com/apache/doris/pull/40908)\\n- Resolved memory leaks when variant returns arrays. [#41339](https://github.com/apache/doris/pull/41339)\\n\\n### Query optimizer\\n\\n- Corrected potential errors in nullable calculations for filter conditions during external table queries, leading to execution exceptions. [#41014](https://github.com/apache/doris/pull/41014)\\n- Fixed potential errors in optimizing range comparison expressions. [#41356](https://github.com/apache/doris/pull/41356)\\n\\n### Query Execution\\n\\n- The match_regexp function could not correctly handle empty strings. [#39503](https://github.com/apache/doris/pull/39503)\\n- Resolved issues where the scanner thread pool could become stuck in high-concurrency scenarios. [#40495](https://github.com/apache/doris/pull/40495)\\n- Fixed errors in the results of the `data_floor` function. [#41948](https://github.com/apache/doris/pull/41948)\\n- Addressed incorrect cancel messages in some scenarios. [#41798](https://github.com/apache/doris/pull/41798)\\n- Fixed issues with excessive warning logs printed by arrow flight. [#41770](https://github.com/apache/doris/pull/41770)\\n- Resolved issues where runtime filters failed to send in some scenarios. [#41698](https://github.com/apache/doris/pull/41698)\\n- Fixed problems where some system table queries could not end normally or became stuck. [#41592](https://github.com/apache/doris/pull/41592)\\n- Addressed incorrect results from window functions. ][#40761](https://github.com/apache/doris/pull/40761)\\n- Fixed issues where the encrypt and decrypt functions caused BE cores. [#40726](https://github.com/apache/doris/pull/40726)\\n- Resolved errors in the results of the conv function. [#40530](https://github.com/apache/doris/pull/40530)\\n\\n### Storage Management\\n\\n- Fixed import failures when Memtable migration was used in multi-replica scenarios with machine crashes.  [#38003](https://github.com/apache/doris/pull/38003)\\n- Addressed inaccurate memory statistics during the Memtable flush phase during imports. [#39536](https://github.com/apache/doris/pull/39536)\\n- Fixed fault tolerance issues with Memtable migration in multi-replica scenarios. [#40477](https://github.com/apache/doris/pull/40477)\\n- Resolved inaccurate bvar statistics with Memtable migration. [#40985](https://github.com/apache/doris/pull/40985)\\n- Fixed inaccurate progress reporting for S3 loads. [#40987](https://github.com/apache/doris/pull/40987)\\n\\n### Permissions\\n\\n- Fixed permission issues related to show columns, show sync, and show data from db.table. [#39726](https://github.com/apache/doris/pull/39726)\\n\\n### Others\\n\\n- Fixed the issue where the audit log plugin for version 2.0 could not be used in version 2.1. [#41400](https://github.com/apache/doris/pull/41400)"},{"id":"/release-note-3.0.0","metadata":{"permalink":"/blog/release-note-3.0.0","source":"@site/blog/release-note-3.0.0.md","title":"New milestone: Apache Doris 3.0 has been released","description":"Starting from version 3.X, Apache Doris supports a compute-storage decoupled mode inaddition to the compute-storage mode for cluster deployment. ","date":"2024-10-15T00:00:00.000Z","tags":[{"inline":true,"label":"Release Notes","permalink":"/blog/tags/release-notes"}],"hasTruncateMarker":false,"authors":[{"name":"Apache Doris","key":null,"page":null}],"frontMatter":{"title":"New milestone: Apache Doris 3.0 has been released","summary":"Starting from version 3.X, Apache Doris supports a compute-storage decoupled mode inaddition to the compute-storage mode for cluster deployment.","description":"Starting from version 3.X, Apache Doris supports a compute-storage decoupled mode inaddition to the compute-storage mode for cluster deployment. ","date":"2024-10-15","author":"Apache Doris","tags":["Release Notes"],"image":"/images/3.0.0.jpeg"},"unlisted":false,"prevItem":{"title":"Apache Doris 2.1.7 just released","permalink":"/blog/release-note-2.1.7"},"nextItem":{"title":"Apache Doris 3.0.2 just released","permalink":"/blog/release-note-3.0.2"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n  http://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n\\nWe are excited to announce the release of Apache Doris 3.0! \\n\\n**Starting from version 3.X, Apache Doris supports a compute-storage decoupled mode in addition to the compute-storage coupled mode for cluster deployment. With the cloud-native architecture that decouples the computation and storage layers, users can achieve physical isolation between query loads across multiple compute clusters, as well as isolation between read and write loads. Additionally, users can take advantage of low-cost shared storage systems such as object storage or HDFS to significantly reduce storage costs.**\\n\\nVersion 3.0 marks a milestone in the evolution of Apache Doris towards a unified data lake and data warehouse architecture. This version introduces the ability to write data back to data lakes, allowing users to perform data analysis, sharing, processing, and storage operations across multiple data sources within Apache Doris. With capabilities such as asynchronous materialized views, Apache Doris can serve as a unified data processing engine for enterprises, helping users better manage data across lakes, warehouses, and databases. Also, Apache Doris 3.0 introduces the Trino Connector. It allows users to quickly connect or adapt to more data sources, and leverage the high-performance compute engine of Doris to deliver faster query results than Trino.\\n\\nVersion 3.0 also enhances support for ETL batch processing scenarios, adding explicit transaction support for operations like `insert into select`, `delete` and `update`. The observability of query execution has also been improved.\\n\\nIn terms of performance, we have improved the framework capabilities, infrastructure, and rules of the query optimizer in version 3.0. This provides optimized performance, which has been proven by blind testing in more complex and diverse business scenarios.\\n\\nThe adaptive Runtime Filter computation method now accurately estimates filters based on data size during execution, delivering better performance under large data volumes and high loads. Additionally, asynchronous materialized view has been more stable and user-friendly in query acceleration and data modeling. \\n\\n**During the development of version 3.0, over 200 contributors submitted nearly 5,000 optimizations** and fixes to Apache Doris. Contributors from companies such as VeloDB, Baidu, Meituan, ByteDance, Tencent, Alibaba, Kwai, Huawei, and Tianyi Cloud actively collaborated with the community, contributing test cases from real-world use cases to help us improve Apache Doris. We extend our heartfelt thanks to all the contributors involved in the development, testing, and feedback process for this release.\\n\\n- **GitHub**: https://github.com/apache/doris/releases\\n\\n- **Website**: https://doris.apache.org/download\\n\\n## 1. Compute-storage decoupled mode\\n\\nSince V3.0, Apache Doris supports the compute-storage decoupled mode. Users can choose between it and the compute-storage coupled mode during cluster deployment.\\n\\nIn the compute-storage decoupled mode, the BE nodes no longer store the data, but instead, a shared storage layer (HDFS and object storage) is introduced as the shared data storage layer. The computing and storage resources can be scaled independently, bringing multiple benefits to users:\\n\\n- **Workload isolation**: Multiple compute clusters can share the same data, allowing users to isolate different business workloads or offline loads using separate compute clusters.\\n\\n- **Reduced storage costs**: The full dataset is stored in the more cost-effective and highly reliable shared storage, with only hot data cached locally. Compared to the compute-storage coupled mode with three data replicas, the storage cost can be reduced by up to 90%.\\n\\n- **Elastic computing resources**: Since no data is stored on the BE nodes, the computing resources can be scaled flexibly based on the load requirements. Users can scale in or out an individual compute cluster or increase/decrease the number of compute clusters. This also leads to cost savings.\\n\\n- **Improved system robustness**: By storing the data in shared storage, Doris no longer needs to handle the complex logic of multi-replica consistency, thus simplifying distributed storage complexity and improving the overall system robustness.\\n\\n- **Flexible data sharing and cloning**: The flexibility of the compute-storage decoupled mode extends beyond a single Doris cluster. Tables from one Doris cluster can be easily cloned to another Doris cluster, with just metadata replication.\\n\\n### 1-1. From coupled to decoupled\\n\\nIn the compute-storage coupled mode, the Apache Doris architecture consists of two main process types: Frontend (FE) and Backend (BE). The FE is primarily responsible for user request access, query parsing and planning, metadata management, and node management. The BE is responsible for data storage and query plan execution. \\n\\nThe BE nodes employ an MPP (Massively Parallel Processing) distributed computing architecture, leveraging a multi-replica consistency protocol to ensure high service availability and high data reliability.\\n\\n![From coupled to decoupled](/images/storage-compute-decoupled.PNG)\\n\\n\\nThe maturation of emerging cloud computing infrastructure, including public clouds, private clouds, and Kubernetes-based container platforms, has driven the need for cloud-native capabilities. Increasingly, users are seeking deeper integration between Apache Doris and cloud computing infrastructure to provide more elasticity.\\n\\n**To address this need, the VeloDB team has designed and implemented a cloud-native version of Apache Doris that decouples compute and storage, known as VeloDB Cloud.  After extensive production testing and refinement across hundreds of enterprises over a long time, this cloud-native solution has now been contributed to the Apache Doris community, manifesting as the Apache Doris 3.0 in the compute-storage decoupled mode.**\\n\\nIn the compute-storage decoupled mode, the Apache Doris architecture consists of three layers:\\n\\n- **Meta data layer**: A new Meta Service module has been introduced to provide meta data services, such as processing database and table information, schemas, rowset meta, and transactions. The Meta Service is stateless and horizontally scalable. In V3.0, all of the BE\'s meta data and parts of the FE\'s meta data have been migrated to the Meta Service. We will finish the migration of the remains in future versions.\\n- **Computation layer**: The stateless BE nodes execute query plans and cache a portion of the data and tablet meta data locally to improve query performance. Multiple stateless BE nodes can be organized into a computing resource pool (i.e., compute cluster), and multiple compute clusters can share the same data and metadata service. The compute clusters can be elastically scaled by adding or removing nodes as needed.\\n- **Shared storage layer**: Data is persisted to the shared storage layer, which currently supports HDFS as well as various cloud-based object storage systems that are compatible with the S3 protocol, such as S3, OSS, GCS, Azure Blob, COS, BOS, and MinIO.\\n\\n![From coupled to decoupled-2](/images/storage-compute-decoupled-2.JPEG)\\n\\n### 1-2 Design highlight\\n\\nThe design of the compute-storage decoupled mode of Apache Doris highlights the transformation of the FE\'s in-memory metadata model into a shared metadata service. This approach offers a globally consistent state view, allowing any node to directly submit writes without needing to go through the FE for publishing. During write operations, data is stored in shared storage, while metadata is managed by the metadata service. **This effectively controls the number of small files in shared storage. Meanwhile, the real-time write performance for individual tables is nearly on par with that in the compute-storage coupled mode. The system\'s overall write capacity is no longer limited by the processing power of a single FE node.**\\n\\n![Design highlight](/images/design-hightlight.PNG)\\n\\nBased on the globally consistent state view, for data garbage collection, we have adopted a design approach for data deletion that is easier to prove correct and more efficient.\\n\\nSpecifically, data in the shared storage is incorporated into the globally consistent view offered by the shared meta data service. Whenever data is generated, we bind it to a separate, independent transaction. Similarly, for a meta data deletion operation, we also bind it to a separate, independent transaction. The purpose of this approach is to ensure that deletion and write operations cannot succeed together. The view records which data needs to be deleted, and the asynchronous deletion process can simply perform a forward deletion of the data based on the transaction records, without the need for reverse garbage collection.\\n\\nAs the tablet-related meta data in the FE is gradually migrated to the shared meta data service, the scalability of the Doris cluster will no longer be constrained by the memory capacity of a single FE node. Building upon the shared meta data service and the forward data deletion technique, we can conveniently expand functionality such as data sharing and lightweight cloning.\\n\\n### 1-3 Comparison with alternative solutions\\n\\nAnother design of decoupling compute and storage in the industry is to store the data and BE node meta data in a shared object storage or HDFS. However, this approach brings the following problems:\\n\\n- **Inability to support real-time writes**: During data writes, the data is mapped to tablets based on the partitioning and bucketing rules, generating segment files and rowset meta data. During the write process, a two-phase commit (Publish) is performed through the FE. When a BE node receives the Publish request, it then sets the rowset as visible. The Publish operation must not fail. If the rowset meta data is stored in the shared storage, the total small file data during the real-time write process would triple the size of the actual data files - one replica of data files, one for rowset meta data, and another for rowset meta data changes during Publish. The Publish operation is driven by a single FE node, so the write capacity of a single table or even the entire system is limited by the FE node\'s capabilities.\\n  \\n  ![Comparison with alternative solutions](/images/comparison-with-alternative-solutions.png)\\n\\n  We compared the real-time data write performance of Apache Doris 3.0 with the above-described solution. We simulated 500 concurrent tasks writing 10,000 data files with 500 rows each, and 50 concurrent tasks writing 250 data files with 20,000 rows each, using the same computational resources. \\n\\n  **The results showed that at 50 concurrent tasks, the micro-batch write performances of Apache Doris in both compute-storage coupled and decoupled modes were almost identical, while the industry solution lagged behind Apache Doris by a factor of 100.** \\n\\n  At 500 concurrent tasks, the performance of Apache Doris in the compute-storage decoupled mode showed slight degradation, but it still maintained an 11X advantage over the industry solution. To ensure a fair test, Apache Doris did not enable the Group Commit feature (which the industry solution lacks). Enabling Group Commit would further enhance real-time write performance.\\n\\n  ![Comparison with alternative solutions](/images/real-time-write-performance..png)\\n\\n  Additionally, the industry solution also faces stability and cost issues in terms of real-time data ingestion:\\n\\n  - Stability concerns: A large number of small files can put pressure on the shared storage, especially HDFS, and introduce stability risks.\\n\\n  - High object storage request costs: Some public cloud object storage services charge 10 times more for Put and Delete operations compared to Get operations. A large number of small files can lead to a significant increase in object storage request costs, which can even exceed the storage costs.\\n\\n- **Limited scalability**: Use cases of the compute-storage decoupled model often handles larger data storage sizes, since the FE (Frontend) meta data is entirely in-memory, when the number of tablets reaches a certain high level (e.g. tens of millions), the FE\'s memory pressure can become a bottleneck that limits the overall write throughput of the system.\\n\\n- **Potential data deletion logic issues**: In the compute-storage decoupled architecture, data is stored with one single replica. Therefore, the data deletion logic is critical for the system\'s reliability. The conventional approach of cross-system data deletion by comparing the differences can be challenging. During the write process, there is no way to completely avoid deletion and write from succeeding together, which can lead to data loss. Additionally, when the storage system experiences anomalies, the input used for difference calculation may be incorrect, which potentially leads to unintended data deletion.\\n\\n- **Data sharing and lightweight cloning**: The flexibility of the decoupled storage-compute architecture can enable future data sharing and lightweight data cloning, reducing the burden of enterprise data management. However, if each cluster has a separate FE, after cloning data across clusters, it becomes difficult to accurately determine which data is no longer referenced and can be safely deleted, as calculating cross-cluster references can easily lead to unintended data deletion.\\n\\nBy evolving the FE\'s full in-memory meta data model into a shared meta data service, Apache Doris 3.0 avoids all the aforementioned issues.\\n\\n### 1-4 Query performance comparison\\n\\nIn the compute-storage decoupled mode, data needs to be read from the remote shared storage system, the main bottleneck has become the network bandwidth instead of the disk I/O in the compute-storage coupled mode. \\n\\nTo accelerate data access, Apache Doris has implemented a high-speed caching mechanism based on local disks, and provides two cache management policies: LRU (Least Recently Used) and TTL (Time-To-Live). The newly imported data is asynchronously written to the cache to accelerate the first-time access to the latest data. If the data required by a query is not in the cache, the system will read the data from the remote storage into memory and synchronously write it to the cache for subsequent queries.\\n\\nIn use cases involving multiple compute clusters, Apache Doris provides a cache preheating function. When a new compute cluster is established, users can choose to preheat specific data (such as tables or partitions) to further improve query efficiency.\\n\\nIn this context, we have conducted performance tests with different caching strategies in both the compute-storage coupled and decoupled modes, using the TPC-DS 1TB test dataset. The results are concluded as follows:\\n\\n- When the cache is fully hit (i.e., all the data required for the query is loaded into the cache), **the query performance of the compute-storage decoupled mode is on par with that of the compute-storage coupled mode**.\\n\\n- When the cache is partially hit (i.e., the cache is cleared before the test, and data is gradually loaded into the cache during the test, with performance continuously improving), the query performance of the compute-storage decoupled mode is about 10% lower than that of the compute-storage coupled mode. This test scenario is the most similar to the real-life use cases.\\n\\n- When the cache is completely missed (i.e., the cache is cleared before every SQL execution, simulating an extreme case), the performance loss is around 35%. **Even so, Apache Doris in the compute-storage decoupled mode delivers much higher performance than its alternative solutions.**\\n\\n![Query performance comparison](/images/query-performance-comparison.png)\\n\\n### 1-5 Write speed comparison\\n\\nIn terms of write performance, we have simulated two test cases under the same computing resources: batch import and high-concurrency real-time import. The comparison of write performance between the compute-storage coupled mode and the compute-storage decoupled mode is as follows:\\n\\n- **Batch import**: When importing the 1TB TPC-H and 1TB TPC-DS test datasets, **the write performance of the compute-storage decoupled mode is 20.05% and 27.98% higher than the compute-storage coupled mode**, respectively, under the single-replica configuration. During batch import, the segment file size is generally in the range of tens to hundreds of MB. In the compute-storage decoupled mode, the segment files are split into smaller files and concurrently uploaded to the object storage, which can result in higher throughput compared to writing to local disks. In real-life deployments, the compute-storage coupled mode typically uses three replicas, which means the write speed advantage of the compute-storage decoupled mode will be even more pronounced.\\n\\n- **High-concurrency real-time import**: as described in the \\"Comparison with alternative solutions\\" section.\\n\\n![Write speed comparison](/images/write-speed-comparison.png)\\n\\n### 1-6 Tips for production environment\\n\\n- **Performance**: For real-time data analysis, users can achieve query performance comparable to the compute-storage coupled mode by specifying a TTL (Time-To-Live) for the cache and writing newly ingested data into the cache. To prevent query jitter, users can cache the data generated by background tasks such as compaction and schema changes based on how frequently used the data is.\\n\\n- **Workload isolation**: Users can achieve physical resource isolation for different business using multiple compute clusters. For workload isolation within a single compute cluster, users can utilize the Workload Group mechanism to limit and isolate resources for different queries.\\n\\n### 1-7 Notes\\n\\n- Apache Doris 3.0 does not support the co-existence of the compute-storage coupled mode and the compute-storage decoupled mode. Users need to specify one of them during cluster deployment.\\n\\n- If users need the compute-storage coupled mode, following the [documentation](https://doris.apache.org/docs/3.0/install/source-install/compilation-with-docker/) for its deployment and upgrade. We recommend using Doris Manager for quick deployment and cluster upgrades. However, the compute-storage decoupled mode does not yet support Doris Manager deployment and upgrade. We will continue iteration for better support in future versions.\\n\\n- Currently Apache Doris does not support in-place upgrade from V2.1 to the compute-storage decoupled mode of V3.0. For such purpose, users need to perform data migration after deploying the compute-storage decoupled clusters. In the future, we will support migration without service interruption through the CCR (Change Data Capture) capability.\\n\\n:::info\\nSee doc:\\nhttps://doris.apache.org/docs/3.0/compute-storage-decoupled/overview/\\n:::\\n\\n## 2. Data lakehouse\\n\\nApache Doris is positioned as a real-time data warehouse, but it is much more than that. In previous versions, we have consistently pushed beyond the boundaries of traditional data warehouse capabilities, advancing towards a unified data lakehouse. Version 3.0 marks a milestone in this journey, with its capabilities in the lakehouse architecture becoming fully mature. We believe that a unified lakehouse is identified by **boundaryless data** and **lakehouse fusion**:\\n\\n**Boundaryless data: Apache Doris serves as a unified query processing engine, breaking down data barriers across different systems. It provides a consistent and ultra-fast analysis experience across all data sources, including data warehouses, data lakes, data streams, and local data files.**\\n\\n- **Lakehouse query acceleration**: Without the need to migrate data to Apache Doris, users can leverage Doris\u2019 efficient query engine to directly query data stored in data lakes such as Iceberg, Hudi, Paimon, and offline data warehouses like Hive, thereby accelerating query analysis.\\n\\n- **Federated analysis**: By extending its catalog and storage plugins, Apache Doris enhances its federated analysis capabilities, allowing users to perform unified analysis across multiple heterogeneous data sources without physically centralizing the data in a single storage system. This enables external table queries and federated joins between internal and external tables, breaking down data silos and providing globally consistent data insights.\\n\\n- **Data lake construction**: Apache Doris introduces write-back functionality for Hive and Iceberg, allowing users to directly create Hive and Iceberg tables through Doris and write data into them. This allows users to write internal table data back to the offline lakehouse or process offline lakehouse data using Doris and save the results back into the lakehouse, simplifying and streamlining the data lake construction process.\\n\\n**Lakehouse fusion: As data lake architectures become increasingly complex, the costs of technology selection and maintenance rise for users. Achieving consistent fine-grained access control across multiple systems also becomes challenging, and real-time performance suffers. To address this, Apache Doris integrates core features of the data lake, transforming itself into a lightweight, efficient, native real-time lakehouse.**\\n\\n- **Real-time data updates**: Starting with version 1.2, Apache Doris enhanced the primary key model by introducing Merge-on-Write, supporting real-time updates. This feature allows high-frequency, real-time data updates based on primary key changes from upstream data sources.\\n\\n- **Data science and** **AI** **computation support**: From version 2.1, Apache Doris, using the efficient Arrow Flight protocol, increased the openness of its storage system and its support for various compute loads, enabling data science and AI computations.\\n\\n- **Enhancements for semi-structured and unstructured Data**: Apache Doris has introduced support for data types like Array, Map, Struct, JSON, and Variant, with plans to support vector indexing in the future.\\n\\n- **Improved resource efficiency by decoupling storage and compute**: With version 3.0, Apache Doris supports a decoupled storage and compute mode, further improving resource efficiency and scalability.\\n\\n### 2-1 Faster queries in the data lakehouse\\n\\nTPC-H and TPC-DS benchmarking proves that Apache Doris achieves average query performance that is 3 to 5 times faster than Trino/Presto.\\n\\nIn V3.0, we have focused on optimizing query performance for production environments, including:\\n\\n- **More granular task splitting strategy**: By adjusting the consistent hashing algorithm and introducing a task sharding weighting mechanism, we ensure balanced query loads across all nodes.\\n\\n- **Scheduling optimizations for use cases with numerous partitions and files**: For cases with a large number of files (over 1 million), we have largely reduced query latency (from 100 seconds to 10 seconds) and alleviated memory pressure on the Frontend (FE) by asynchronously and batch-fetching file shards.\\n\\nWe will continue to specifically enhance query acceleration performance in real-world business scenarios, improve the actual user experience, and build an industry-leading lakehouse query acceleration engine.\\n\\n### 2-2 Federated analysis: more data connectors\\n\\nPrevious versions of Apache Doris support connectors for over 10 mainstream data lakehouses, warehouses, and relational databases. In V3.0, we have introduced the Trino Connector compatibility framework, which expands the range of data sources that Apache Doris can connect to. With this framework, users can easily adapt their existing setups to access corresponding data sources using Doris and leverage its high-speed computing engine for data analysis.\\n\\nCurrently, Doris has completed adaptations for Delta Lake, Kudu, BigQuery, Kafka, TPCH, and TPCDS. We also encourage contributions from developers to prolong this list.\\n\\n:::info Note\\n\\nSee doc:\\n\\n- Trino Connector: https://doris.apache.org/community/how-to-contribute/trino-connector-developer-guide/\\n\\n- TPC-H: https://doris.apache.org/docs/3.0/lakehouse/datalake-analytics/tpch/\\n\\n- TPC-DS: https://doris.apache.org/docs/3.0/lakehouse/datalake-analytics/tpcds/\\n\\n- Delta Lake: https://doris.apache.org/docs/3.0/lakehouse/datalake-analytics/deltalake/\\n\\n- Kudu: https://doris.apache.org/docs/3.0/lakehouse/datalake-analytics/kudu/\\n\\n- BigQuery: https://doris.apache.org/docs/3.0/lakehouse/datalake-analytics/bigquery/\\n:::\\n\\n\\n### 2-3 Data lake building\\n\\nIn V3.0, we have introduced data writeback functionality for Hive and Iceberg. This allows users to create Hive and Iceberg tables directly through Doris and write data into these tables, and enables users to perform data analysis, sharing, processing, and storage operations across multiple data sources within Doris.\\n\\nIn future iterations, Apache Doris will further enhance support for data lake table formats and improve the openness of storage APIs.\\n\\n:::info Note\\nSee doc: https://doris.apache.org/docs/3.0/lakehouse/datalake-building/hive-build/\\n:::\\n\\n## 3. Upgraded semi-structured data analysis capabilities\\n\\nIn versions 2.0 and 2.1, Apache Doris introduced some well-embraced features such as inverted index, NGram Bloom Filter, and Variant data type to support high-performance full-text search and multi-dimensional analysis. With them, the storage and processing of complex semi-structured data have been more flexible and efficient.\\n\\nIn V3.0, we have further enhanced the capabilities in this scenario.\\n\\nAfter extensive testing in production environments, the Variant data type has gained sufficient stability and become the preferred choice for JSON data storage and analysis. In V3.0, we have made multiple optimizations to it:\\n\\n- Support for indexing of the Variant data type to accelerate queries, including inverted index, Bloom Filter index, and the built-in ZoneMap index.\\n\\n- Support for flexible partial column updates for Unique Key tables containing the Variant data type.\\n\\n- Support for the use of the Variant data type in the compute-storage decoupled mode, with optimizations of its metadata storage.\\n\\n- Support for exporting the Variant data type to formats such as Parquet and CSV.\\n\\nThe inverted index, introduced since V2.0, has reached a high level of maturity after more than a year of refinement and is now running in production environments of hundreds of enterprises. In V3.0, we have made multiple optimizations to the inverted index:\\n\\n- After performance optimizations, including lock concurrency, Apache Doris outperforms Elasticsearch in key metrics such as query latency and concurrency in real-time reporting analysis.\\n\\n- Optimized index file in the compute-storage decoupled mode to reduce remote storage calls and decrease index query latency.\\n\\n- Support for the Array data type to accelerate the `array_contains` queries.\\n\\n- Enhanced the `match_phrase_*` functionality, including support for slop and phrase prefix matching `match_phrase_prefix`.\\n\\n## 4. Enhanced ETL capabilities\\n\\n### 4-1. Transaction improvements\\n\\nData processing in data warehouses often involves multiple data changes that need to be handled as a single transaction. V3.0 provides explicit transaction support for `insert into select`, `delete`, and `update` operations. Example cases include:\\n\\n- **Transactional requirements**: For example, when updating data within a time range, the typical approach is to first delete the data in that time range, and then insert the new data. Considering that the data might already be in service, there is a need to ensure that queries visit either the old data or the new data. Thus, it can be achieved by executing the `delete` and `insert into select` operations in a transaction.\\n\\n  ```Java\\n  BEGIN;\\n  DELETE FROM table WHERE date >= \\"2024-07-01\\" AND date <= \\"2024-07-31\\";\\n  INSERT INTO table SELECT * FROM stage_table;\\n  COMMIT;\\n  ```\\n\\n- **Simplified the processing of failed tasks**: For example, when two `insert into select` operations are executed within a single transaction, if any of the operations fail, it can be retried directly.\\n  \\n  ```Java\\n  BEGIN WITH LABEL label_etl_1;\\n  INTO table1 SELECT * FROM stage_table1;\\n  INSERT INTO table SELECT * FROM stage_table;\\n  COMMIT;\\n  ```\\n\\n:::info Note\\nSee doc: https://doris.apache.org/docs/3.0/data-operate/transaction/\\nCurrently, explicit transaction synchronization is not supported in Cross-Cluster Replication (CCR).\\n:::\\n\\n### 4-2. Improved observability\\n\\n- **Real-time profile retrieval**: In previous versions, due to issues with the execution plan or the data, some complex queries might have high computational requirements, so developers can only access the query profile for performance analysis after the completion of the query. This makes it hard to promptly identify issues in query execution to guarantee stability of the production environment. Now, with the ability to retrieve real-time profiles, V3.0 allows users to monitor query execution as the query is running. It also allows them to better monitor the progress of each ETL job.\\n\\n- **`backend_active_tasks` system table**: The `backend_active_tasks` system table provides real-time resource consumption information for each query on each BE node. Users can analyze this system table using SQL to obtain the resource usage of each query, which helps identify large queries or abnormal workloads.\\n\\n## 5. Asynchronous materialized view\\n\\nIn V3.0, asynchronous materialized view is faster and more stable. It is also more user-friendly for query acceleration and data modeling scenarios. We have restructured the logic for transparent rewrite and expanded its capabilities, making it 2X faster.\\n\\n### 5-1 Refresh\\n\\n- Support for incremental update of materialized views by partitions and partition roll-ups on materialized views to allow refreshes at different granularities.\\n\\n- Support for nested materialized views, which is useful in data modeling scenarios.\\n\\n- Support for index creation and sort key specification in asynchronous materialized views, which will improve query performance after the materialized view is hit.\\n\\n- Higher usability of materialized view DDL with support for atomically replacing materialized views, allowing modifications to the materialized view definition SQL while keeping the materialized view available. \\n\\n- Support for non-deterministic functions in materialized views to better serve daily materialized view creation.\\n\\n- Support for trigger-based materialized view refresh, which ensures data consistency in data modeling with nested materialized views.\\n\\n- Support for a broader range of SQL patterns for building partitioned materialized views, making the incremental update capability available to more use cases.\\n\\n### 5-2 Refresh stability\\n\\n- V3.0 supports specifying a Workload Group for building materialized views. This is to limit the resources used by the materialized view build process and ensure that sufficient resources remain available for ongoing queries.\\n\\n### 5-3 Transparent rewrite\\n\\n- Support for transparent rewrite of more Join types, including derived Joins. Even when there is a mismatch of Join types between the query and materialized view, transparent rewrite can still be performed by compensating with additional predicates, as long as the materialized view can provide all the data needed for the query.\\n\\n- Support for more aggregate functions for roll-up as well as rewrite of multi-dimensional aggregations like GROUPING SETS, ROLLUP, and CUBE; support rewriting queries with aggregations when the materialized view does not contain aggregations, simplifying Join operations and expression computation.\\n\\n- Support for transparent rewrite of nested materialized views, enabling higher performance for complex queries.\\n\\n- For partially invalid partitioned materialized views, V3.0 supports `Union All` the base tables for data completion, expanding the applicability of partitioned materialized views.\\n\\n### 5-4 Transparent rewrite performance\\n\\n- Continuous optimization has been done to improve the transparent rewrite performance, achieving 2X the speed compared to version 2.1.0.\\n\\n:::info Note\\n\\nSee doc:\\n\\nhttps://doris.apache.org/docs/3.0/query/view-materialized-view/query-async-materialized-view\\n\\nhttps://doris.apache.org/docs/3.0/query/view-materialized-view/async-materialized-view/\\n\\n:::\\n\\n## 6. Performance improvement\\n\\n### 6-1 Smarter optimizer\\n\\nIn V3.0, the query optimizer has been enhanced in terms of framework capabilities, distributed plan support, optimizer infrastructure, and rule expansion. It provides better optimization capabilities for more complex and diverse business scenarios, with higher blind test performance for complex SQL:\\n\\n- **Improved plan enumeration capability**: The key structure Memo for plan enumeration has been restructured and normalized. This improves the efficiency of the Cascades framework in plan enumeration and the possibility of producing better plans. Additionally, it fixes incomplete column pruning during the Join Reorder process in older versions, which led to unnecessary overhead of the Join operator, thus improving the execution performance in the relevant scenarios.\\n\\n- **Improved distributed plan support**: The distributed query plan has been enhanced to allow aggregation, join, and window function operations to more intelligently identify the data characteristics of intermediate computation results, avoiding ineffective data redistribution operations. Meanwhile, we have optimized the execution under the multi-replica continuous execution mode, making it more data cache-friendly.\\n\\n- **Improved optimizer infrastructure**: V3 has fixed several issues in cost model and statistics information estimation. The fixes to the cost model are more adaptable to the evolution of the execution engine, making the execution plan more stable compared to previous versions.\\n\\n- **Enhanced Runtime Filter plan support**: On the basis of Join Runtime Filter, V3.0 has expanded the capability of the TopN Runtime Filter to achieve better performance in use cases that involve a TopN operator.\\n\\n- **Enriched optimization rule library**: Based on user feedback and internal testing results, we have introduced optimization rules such as Intersect Reorder to enrich the rule set of the optimizer.\\n\\n### 6-2 Self-adaptive Runtime Filter\\n\\nIn previous versions, the generation of Runtime Filter relies on manual setting by users based on statistical information. However, inaccurate settings in certain cases could lead to performance instability.\\n\\nIn V3.0, Doris implements a self-adaptive Runtime Filter calculation approach. It can estimate the Runtime Filter at runtime based on the data size with high accuracy, enabling better performance in use cases with large data volumes and high workloads. \\n\\n### 6-3 Function performance optimization\\n\\n- V3.0 has improved the vectorized implementation of dozens of functions, enabling a performance improvement of over 50% for some commonly used functions.\\n- V3.0 has also made extensive optimizations to the aggregation of nullable data types, enabling a 30% performance improvement.\\n\\n### 6-4 Blind test performance improvement\\n\\nOur blind tests on V3.0 and V2.1 show that the new version is 7.3% and 6.2% faster in TPC-DS and TPC-H benchmark tests, respectively.\\n\\n![Blind test performance improvement](/images/blind-test-performance-improvement.png)\\n\\n## 7. New features\\n\\n### 7-1 Java UDTF\\n\\nVersion 3.0 has added support for Java UDTFs. The key operations are as follows:\\n\\n- Implementing a UDTF: Similar to a UDF, a UDTF requires the user to implement an `evaluate` method. Note that the return value of a UDTF function must be of the `Array` data type.\\n\\n  ```sql\\n  public class UDTFStringTest {\\n      public ArrayList<String> evaluate(String value, String separator) {\\n          if (value == null || separator == null) {\\n              return null;\\n          } else {\\n              return new ArrayList<>(Arrays.asList(value.split(separator)));\\n          }\\n      }\\n  }\\n  ```\\n\\n- Creating a UDTF: By default, two corresponding functions will be created - `java-utdf`and `java-utdf_outer`. The `_outer` suffix adds a single row of `NULL` data when the table function generates 0 rows of output.\\n\\n  ```sql\\n  CREATE TABLES FUNCTION java-utdf(string, string) RETURNS array<string> PROPERTIES (\\n      \\"file\\"=\\"file:///pathTo/java-udaf.jar\\",\\n      \\"symbol\\"=\\"org.apache.doris.udf.demo.UDTFStringTest\\",\\n      \\"always_nullable\\"=\\"true\\",\\n      \\"type\\"=\\"JAVA_UDF\\"\\n  );\\n  ```\\n\\n:::info\\n\\nSee doc: https://doris.apache.org/docs/3.0/query/udf/java-user-defined-function/#udtf-1\\n\\n:::\\n\\n### 7-2 Generated column\\n\\nA generated column is a special column whose value is calculated from the values of other columns rather than directly inserted or updated by the user. It supports pre-computing the results of expressions and storing them in the database, which is suitable for scenarios that require frequent queries or complex calculations.\\n\\nResults can be automatically calculated based on predefined expressions when data is imported or updated, and then stored persistently. In this way, during subsequent queries, the system can directly access these calculated results without performing complex calculations, thereby improving query performance.\\n\\nGenerated columns are supported since V3.0. When creating a table, you can specify a column as generated column. A generated column automatically calculates values based on the defined expression when data is written. Generated columns allow for more complex expressions to be defined, but the value cannot be explicitly written or set.\\n\\n:::info\\n\\nSee doc: https://doris.apache.org/docs/3.0/sql-manual/sql-statements/Data-Definition-Statements/Create/CREATE-TABLE-AND-GENERATED-COLUMN/\\n\\n:::\\n\\n## 8. Functional improvements\\n\\n### 8-1. Materialized view\\n\\nWe have refactored the selection logic for materialized views and migrated it from the rule-based optimizer (RBO) to the cost-based optimizer (CBO). This aligns the selection logic with that of asynchronous materialized views. This functionality is enabled by default. If any issues are encountered, you can revert to the RBO mode using `set global enable_sync_mv_cost_based_rewrite = false`.\\n\\n### 8-2. Routine Load\\n\\nIn previous versions, the Routine Load functionality faced some usability challenges, such as uneven task scheduling across BE nodes, untimely task scheduling, complex configuration requirements (the need to change multiple FE and BE settings for optimization), insufficient overall stability (where restarts or upgrades could frequently pause Routine Load jobs, requiring manual user intervention to resume).\\n\\nTo address these issues, we have made extensive optimizations to the Routine Load feature:\\n\\n- **Resource scheduling**: We have improved the scheduling balance to make sure that tasks are more evenly distributed across BE nodes. Jobs that encounter unrepairable errors will be promptly paused to avoid wasting resources on futile scheduling attempts. Additionally, we have improved the timeliness of the scheduling process, which has enhanced the import performance of Routine Load.\\n\\n- **Parameter configuration**: Users in most environments no longer need to modify FE and BE configurations for optimization. An automatic adjustment mechanism with timeout parameter has been introduced to prevent tasks from constantly retrying when cluster pressure increases.\\n\\n- **Stability**: We have enhanced the robustness of Doris in various exceptional scenarios, such as FE failovers, BE rolling upgrades, and Kafka cluster anomalies, ensuring continuous stable operation. We have also optimized the Auto Resume mechanism, allowing Routine Load to automatically resume operation after faults are repaired, reducing the need for manual user intervention.\\n\\n## 9. Behavior changed\\n\\n- `cpu_resource_limit` will no longer be supported, and all types of resource isolation will be implemented through Workload Groups.\\n\\n- Please use JDK 17 for Apache Doris 3.0 and later versions. The recommended version being `jdk-17.0.10_linux-x64_bin.tar.gz`.\\n\\n## Try Apache Doris 3.0 now!\\n\\nBefore the official release of version 3.0, the compute-storage decoupled mode of Apache Doris has undergone nearly two years of extensive testing and optimization in the production environments of hundreds of enterprises. Contributors from many tech giants have collaborated with the community to provide a significant number of test cases based on their real-world business needs. This has rigorously validated the usability and stability of version 3.0.\\n\\nWe highly recommend users with compute-storage decoupling needs to download version 3.0 and experience it firsthand.\\n\\nGoing forward, we will accelerate our release iteration cycle to deliver a more stable version experience for all users. Feel free to join us in the [Apache Doris community](https://join.slack.com/t/apachedoriscommunity/shared_invite/zt-2gmq5o30h-455W226d79zP3L96ZhXIoQ) and engage directly with the core developers.\\n\\n## Credits\\n\\nSpecial thanks to the following contributors who participated in the development, testing, and provided feedback for this version:\\n\\n@133tosakarin\u3001@390008457\u3001@924060929\u3001@AcKing-Sam\u3001@AshinGau\u3001@BePPPower\u3001@BiteTheDDDDt\u3001@ByteYue\u3001@CSTGluigi\u3001@CalvinKirs\u3001@Ceng23333\u3001@DarvenDuan\u3001@DongLiang-0\u3001@Doris-Extras\u3001@Dragonliu2018\u3001@Emor-nj\u3001@FreeOnePlus\u3001@Gabriel39\u3001@GoGoWen\u3001@HappenLee\u3001@HowardQin\u3001@Hyman-zhao\u3001@INNOCENT-BOY\u3001@JNSimba\u3001@JackDrogon\u3001@Jibing-Li\u3001@KassieZ\u3001@Lchangliang\u3001@LemonLiTree\u3001@LiBinfeng-01\u3001@LompleZ\u3001@M1saka2003\u3001@Mryange\u3001@Nitin-Kashyap\u3001@On-Work-Song\u3001@SWJTU-ZhangLei\u3001@StarryVerse\u3001@TangSiyang2001\u3001@Tech-Circle-48\u3001@Thearas\u3001@Vallishp\u3001@WinkerDu\u3001@XieJiann\u3001@XuJianxu\u3001@XuPengfei-1020\u3001@Yukang-Lian\u3001@Yulei-Yang\u3001@Z-SWEI\u3001@ZhongJinHacker\u3001@adonis0147\u3001@airborne12\u3001@allenhooo\u3001@amorynan\u3001@bingquanzhao\u3001@biohazard4321\u3001@bobhan1\u3001@caiconghui\u3001@cambyzju\u3001@caoliang-web\u3001@catpineapple\u3001@cjj2010\u3001@csun5285\u3001@dataroaring\u3001@deardeng\u3001@dongsilun\u3001@dutyu\u3001@echo-hhj\u3001@eldenmoon\u3001@elvestar\u3001@englefly\u3001@feelshana\u3001@feifeifeimoon\u3001@feiniaofeiafei\u3001@felixwluo\u3001@freemandealer\u3001@gavinchou\u3001@ghkang98\u3001@gnehil\u3001@hechao-ustc\u3001@hello-stephen\u3001@httpshirley\u3001@hubgeter\u3001@hust-hhb\u3001@iszhangpch\u3001@iwanttobepowerful\u3001@ixzc\u3001@jacktengg\u3001@jackwener\u3001@jeffreys-cat\u3001@kaijchen\u3001@kaka11chen\u3001@kindred77\u3001@koarz\u3001@kobe6th\u3001@kylinmac\u3001@larshelge\u3001@liaoxin01\u3001@lide-reed\u3001@liugddx\u3001@liujiwen-up\u3001@liutang123\u3001@lsy3993\u3001@luwei16\u3001@luzhijing\u3001@lxliyou001\u3001@mongo360\u3001@morningman\u3001@morrySnow\u3001@mrhhsg\u3001@my-vegetable-has-exploded\u3001@mymeiyi\u3001@nanfeng1999\u3001@nextdreamblue\u3001@pingchunzhang\u3001@platoneko\u3001@py023\u3001@qidaye\u3001@qzsee\u3001@raboof\u3001@rohitrs1983\u3001@rotkang\u3001@ryanzryu\u3001@seawinde\u3001@shoothzj\u3001@shuke987\u3001@sjyango\u3001@smallhibiscus\u3001@sollhui\u3001@sollhui\u3001@spaces-X\u3001@stalary\u3001@starocean999\u3001@superdiaodiao\u3001@suxiaogang223\u3001@taptao\u3001@vhwzx\u3001@vinlee19\u3001@w41ter\u3001@wangbo\u3001@wangshuo128\u3001@whutpencil\u3001@wsjz\u3001@wuwenchi\u3001@wyxxxcat\u3001@xiaokang\u3001@xiedeyantu\u3001@xiedeyantu\u3001@xingyingone\u3001@xinyiZzz\u3001@xy720\u3001@xzj7019\u3001@yagagagaga\u3001@yiguolei\u3001@yongjinhou\u3001@ytwp\u3001@yuanyuan8983\u3001@yujun777\u3001@yuxuan-luo\u3001@zclllyybb\u3001@zddr\u3001@zfr9527\u3001@zgxme\u3001@zhangbutao\u3001@zhangstar333\u3001@zhannngchen\u3001@zhiqiang-hhhh\u3001@ziyanTOP\u3001@zxealous\u3001@zy-kkk\u3001@zzzxl1993\u3001@zzzzzzzs"},{"id":"/release-note-3.0.2","metadata":{"permalink":"/blog/release-note-3.0.2","source":"@site/blog/release-note-3.0.2.md","title":"Apache Doris 3.0.2 just released","description":"In this version, Apache Doris has improvements in compute-storage decoupling, data storage, lakehouse, query optimizer, query execution and more.","date":"2024-10-15T00:00:00.000Z","tags":[{"inline":true,"label":"Release Notes","permalink":"/blog/tags/release-notes"}],"hasTruncateMarker":false,"authors":[{"name":"Apache Doris","key":null,"page":null}],"frontMatter":{"title":"Apache Doris 3.0.2 just released","summary":"In this version, Apache Doris has improvements in compute-storage decoupling, data storage, lakehouse, query optimizer, query execution and more.","description":"In this version, Apache Doris has improvements in compute-storage decoupling, data storage, lakehouse, query optimizer, query execution and more.","date":"2024-10-15","author":"Apache Doris","tags":["Release Notes"],"image":"/images/3.0.2.jpg"},"unlisted":false,"prevItem":{"title":"New milestone: Apache Doris 3.0 has been released","permalink":"/blog/release-note-3.0.0"},"nextItem":{"title":"Apache Doris version 2.0.15 has been released","permalink":"/blog/release-note-2.0.15"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n  http://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n\\n\\nDear community members, the Apache Doris 3.0.2 version was officially released on October 15, 2024, featuring updates and improvements in compute-storage decoupling, data storage, lakehouse, query optimizer, query execution and more.\\n\\n**Quick Download:** https://doris.apache.org/download/\\n\\n**GitHub Release:** https://github.com/apache/doris/releases\\n\\n## Behavioral Changes\\n\\n### Storage\\n\\n- Limited the number of tablets in a single backup task to prevent FE memory overflow. [#40518](https://github.com/apache/doris/pull/40518)  \\n- The `SHOW PARTITIONS` command now displays the `CommittedVersion` of partitions. [#28274](https://github.com/apache/doris/pull/28274)  \\n\\n### Other\\n\\n- The default printing mode (asynchronous) of `fe.log` now includes file line number information. If performance issues are encountered due to line number output, please switch to BRIEF mode. [#39419](https://github.com/apache/doris/pull/39419)  \\n- The default value of the session variable `ENABLE_PREPARED_STMT_AUDIT_LOG` has been changed from `true` to `false`, and the audit log of prepare statements will no longer be printed. [#38865](https://github.com/apache/doris/pull/38865)  \\n- The default value of the session variable `max_allowed_packet` has been adjusted from 1MB to 16MB to align with MySQL 8.4. [#38697](https://github.com/apache/doris/pull/38697)  \\n- The JVM of FE and BE defaults to using the UTF-8 character set. [#39521](https://github.com/apache/doris/pull/39521)  \\n\\n## New Features\\n\\n### Storage\\n\\n- Backup and recovery now support clearing tables or partitions that are not in the backup. [#39028](https://github.com/apache/doris/pull/39028)  \\n\\n### Compute-Storage Decoupled\\n\\n- Support for parallel recycling of expired data on multiple tablets. [#37630](https://github.com/apache/doris/pull/37630)  \\n- Support for changing storage vaults through `ALTER` statements. [#38685](https://github.com/apache/doris/pull/38685)  [#37606](https://github.com/apache/doris/pull/37606)  \\n- Support for importing a large number of tablets (5000+) in a single transaction (experimental feature). [#38243](https://github.com/apache/doris/pull/38243)  \\n- Support for automatically aborting pending transactions caused by reasons such as node restarts, solving the issue of pending transactions blocking decommission or schema change. [#37669](https://github.com/apache/doris/pull/37669)  \\n- A new session variable `enable_segment_cache` has been added to control whether to use segment cache during queries (default is `true`). [#37141](https://github.com/apache/doris/pull/37141)  \\n- Resolved the issue of not being able to import a large amount of data during schema changes in compute-storage decoupled mode. [#39558](https://github.com/apache/doris/pull/39558)  \\n- Support for adding multiple follower roles of FE in compute-storage decoupled mode. [#38388](https://github.com/apache/doris/pull/38388)  \\n- Support for using memory as file cache to accelerate queries in environments with no disks or low-performance HDDs. [#38811](https://github.com/apache/doris/pull/38811)  \\n\\n### Lakehouse\\n\\n- New Lakesoul Catalog has been added. [Apache Doris Docs](https://doris.apache.org/zh-CN/docs/dev/lakehouse/datalake-analytics/lakesoul)  \\n- A new system table `catalog_meta_cache_statistics` has been added to view the usage of various metadata caches in external catalog. [#40155](https://github.com/apache/doris/pull/40155)  \\n\\n### Query Optimizer\\n\\n- Support for `is [not] true/false` expressions. [#38623](https://github.com/apache/doris/pull/38623)  \\n\\n### Query Execution\\n\\n- A new CRC32 function has been added. [#38204](https://github.com/apache/doris/pull/38204)  \\n- New aggregate functions skew and kurt have been added. [#41277](https://github.com/apache/doris/pull/41277)  \\n- Profiles are now persisted to the FE\'s disk to retain more profiles. [#33690](https://github.com/apache/doris/pull/33690)  \\n- A new system table `workload_group_privileges` has been added to view permission information related to workload groups. [#38436](https://github.com/apache/doris/pull/38436)  \\n- A new system table `workload_group_resource_usage` has been added to monitor resource statistics of workload groups. [#39177](https://github.com/apache/doris/pull/39177)  \\n- Workload groups now support limiting reads of local IO and remote IO. [#39012](https://github.com/apache/doris/pull/39012)  \\n- Workload groups now support cgroupv2 to limit CPU usage. [#39374](https://github.com/apache/doris/pull/39374)  \\n- A new system table `information_schema.partitions` has been added to view some table creation attributes. [#40636](https://github.com/apache/doris/pull/40636)  \\n\\n### Other\\n\\n- Support for using the `SHOW` statement to display BE\'s configuration information, such as `SHOW BACKEND CONFIG LIKE ${pattern}`. [#36525](https://github.com/apache/doris/pull/36525)  \\n\\n## Improvements\\n\\n### Load\\n\\n- Improved the import efficiency of routine load when encountering frequent EOFs from Kafka. [#39975](https://github.com/apache/doris/pull/39975)  \\n- The stream load result now includes the time taken to read HTTP data, `ReceiveDataTimeMs`, which can quickly determine slow stream load issues caused by network reasons. [#40735](https://github.com/apache/doris/pull/40735)  \\n- Optimized the routine load timeout logic to avoid frequent timeouts during inverted index and mow writes. [#40818](https://github.com/apache/doris/pull/40818)  \\n\\n### Storage\\n\\n- Support for batch addition of partitions. [#37114](https://github.com/apache/doris/pull/37114)  \\n\\n### Compute-Storage Decoupled\\n\\n- Added the meta-service HTTP interface `/MetaService/http/show_meta_ranges` to facilitate the statistics of KV distribution in FDB. [#39208](https://github.com/apache/doris/pull/39208)  \\n- The meta-service/recycler stop script ensures that the process fully exits before returning. [#40218](https://github.com/apache/doris/pull/40218)  \\n- Support for using the session variable `version_comment` (Cloud Mode) to display the current deployment mode as compute-storage decoupled. [#38269](https://github.com/apache/doris/pull/38269)  \\n- Fixed the detailed message returned when transaction submission fails. [#40584](https://github.com/apache/doris/pull/40584)  \\n- Support for using one meta-service process to provide both metadata services and data recycling services. [#40223](https://github.com/apache/doris/pull/40223)  \\n- Optimized the default configuration of file_cache to avoid potential issues when not set. [#41421](https://github.com/apache/doris/pull/41421)  [#41507](https://github.com/apache/doris/pull/41507)  \\n- Improved query performance by batch retrieving the version of multiple partitions. [#38949](https://github.com/apache/doris/pull/38949)  \\n- Delayed the redistribution of tablets to avoid query performance issues caused by temporary network fluctuations. [#40371](https://github.com/apache/doris/pull/40371)  \\n- Optimized the read-write lock logic in the balance. [#40633](https://github.com/apache/doris/pull/40633)  \\n- Enhanced the robustness of file cache in handling TTL filenames during restarts/crashes. [#40226](https://github.com/apache/doris/pull/40226)  \\n- Added the BE HTTP interface `/api/file_cache?op=hash` to facilitate the calculation of the hash file names of segment files on disk. [#40831](https://github.com/apache/doris/pull/40831)  \\n- Optimized the unified naming to be compatible with using compute group to represent BE groups (original cloud cluster). [#40767](https://github.com/apache/doris/pull/40767)  \\n- Optimized the waiting time for obtaining locks when calculating delete bitmaps in primary key tables. [#40341](https://github.com/apache/doris/pull/40341) \\n- When there are many delete bitmaps in primary key tables, optimized the high CPU consumption during queries by pre-merging multiple delete bitmaps. [#40204](https://github.com/apache/doris/pull/40204)  \\n- Support for managing FE/BE nodes in compute-storage decoupled mode through SQL statements, hiding the logic of direct interaction with meta-service when deploying in compute-storage decoupled mode. [#40264](https://github.com/apache/doris/pull/40264)  \\n- Added a script for rapid deployment of FDB. [#39803](https://github.com/apache/doris/pull/39803)  \\n- Optimized the output of `SHOW CACHE HOTSPOT` to unify the column name style with other `SHOW` statements. [#41322](https://github.com/apache/doris/pull/41322)  \\n- When using a storage vault as the storage backend, disallowed the use of `latest_fs()` to avoid binding different storage backends to the same table. [#40516](https://github.com/apache/doris/pull/40516)  \\n- Optimized the timeout strategy for calculating delete bitmaps when importing mow tables. [#40562](https://github.com/apache/doris/pull/40562)  [#40333](https://github.com/apache/doris/pull/40333)  \\n- The enable_file_cache in be.conf is now enabled by default in compute-storage decoupled mode. [#41502](https://github.com/apache/doris/pull/41502)  \\n\\n### Lakehouse\\n\\n- When reading tables in CSV format, support for the session `keep_carriage_return` setting to control the reading behavior of the `\\\\r` symbol. [#39980](https://github.com/apache/doris/pull/39980)  \\n- The default maximum memory of BE\'s JVM has been adjusted to 2GB (affecting only new deployments). [#41403](https://github.com/apache/doris/pull/41403)  \\n- Hive Catalog has added `hive.recursive_directories_table` and `hive.ignore_absent_partitions` properties to specify whether to recursively traverse data directories and whether to ignore missing partitions. [#39494](https://github.com/apache/doris/pull/39494)  \\n- Optimized the Catalog refresh logic to avoid generating a large number of connections during refresh. [#39205](https://github.com/apache/doris/pull/39205)  \\n- `SHOW CREATE DATABASE` and `SHOW CREATE TABLE` for external data sources now display location information. [#39179](https://github.com/apache/doris/pull/39179)  \\n- The new optimizer supports inserting data into JDBC external tables using the `INSERT INTO` statement. [#41511](https://github.com/apache/doris/pull/41511)  \\n- MaxCompute Catalog now supports complex data types. [#39259](https://github.com/apache/doris/pull/39259)  \\n- Optimized the logic for reading and merging data shards of external tables. [#38311](https://github.com/apache/doris/pull/38311)  \\n- Optimized some refresh strategies for metadata caches of external tables. [#38506](https://github.com/apache/doris/pull/38506)  \\n- Paimon tables now support pushing down `IN/NOT IN` predicates. [#38390](https://github.com/apache/doris/pull/38390)  \\n- Compatible with tables created in Parquet format by Paimon version 0.9. [#41020](https://github.com/apache/doris/pull/41020)  \\n\\n### Asynchronous Materialized Views\\n\\n- Building asynchronous materialized views now supports the use of both immediate and starttime. [#39573](https://github.com/apache/doris/pull/39573)  \\n- Asynchronous materialized views based on external tables will refresh the metadata cache of the external tables before refreshing the materialized views, ensuring construction based on the latest external table data. [#38212](https://github.com/apache/doris/pull/38212)  \\n- Partition incremental construction now supports rolling up according to weekly and quarterly granularities. [#39286](https://github.com/apache/doris/pull/39286)  \\n\\n### Query Optimizer\\n\\n- The aggregate function `GROUP_CONCAT` now supports the use of both `DISTINCT` and `ORDER BY`. [#38080](https://github.com/apache/doris/pull/38080)  \\n- Optimized the collection and use of statistical information, as well as the logic for estimating row counts and cost calculations, to generate more efficient and stable execution plans.\\n- Window function partition data pre-filtering now supports cases containing multiple window functions. [#38393](https://github.com/apache/doris/pull/38393)  \\n\\n### Query Execution\\n\\n- Reduced query latency by running prepare pipeline tasks in parallel. [#40874](https://github.com/apache/doris/pull/40874)  \\n- Display Catalog information in Profile. [#38283](https://github.com/apache/doris/pull/38283)  \\n- Optimized the computational performance of `IN` filtering conditions. [#40917](https://github.com/apache/doris/pull/40917)  \\n- Supported cgroupv2 in K8S to limit Doris\'s memory usage. [#39256](https://github.com/apache/doris/pull/39256)  \\n- Optimized the performance of converting strings to datetime types. [#38385](https://github.com/apache/doris/pull/38385)  \\n- When a `string` is a decimal number, support casting it to an `int`, which will be more compatible with certain behaviors of MySQL. [#38847](https://github.com/apache/doris/pull/38847)  \\n\\n### Semi-Structured Data Management\\n\\n- Optimized the performance of inverted index matching. [#41122](https://github.com/apache/doris/pull/41122)  \\n- Temporarily prohibited the creation of inverted indexes with tokenization on arrays. [#39062](https://github.com/apache/doris/pull/39062)  \\n- `explode_json_array` now supports binary JSON types. [#37278](https://github.com/apache/doris/pull/37278)  \\n- IP data types now support bloomfilter indexes. [#39253](https://github.com/apache/doris/pull/39253)  \\n- IP data types now support row storage. [#39258](https://github.com/apache/doris/pull/39258)  \\n- Nested data types such as ARRAY, MAP, and STRUCT now support schema changes. [#39210](https://github.com/apache/doris/pull/39210)  \\n- When creating MTMV, automatically truncate KEYs encountered in VARIANT data types. [#39988](https://github.com/apache/doris/pull/39988)  \\n- Lazy loading of inverted indexes during queries to improve performance. [#38979](https://github.com/apache/doris/pull/38979)  \\n- `add inverted index file size for open file`. [#37482](https://github.com/apache/doris/pull/37482)  \\n- Reduced access to object storage interfaces during compaction to improve performance. [#41079](https://github.com/apache/doris/pull/41079)  \\n- Added three new query profile metrics related to inverted indexes. [#36696](https://github.com/apache/doris/pull/36696)  \\n- Reduced cache overhead for non-PreparedStatement SQL to improve performance. [#40910](https://github.com/apache/doris/pull/40910)  \\n- Pre-warming cache now supports inverted indexes. [#38986](https://github.com/apache/doris/pull/38986)  \\n- Inverted indexes are now cached immediately after writing. [#39076](https://github.com/apache/doris/pull/39076)  \\n\\n### Compatibility\\n\\n- Fixed the issue of Thrift ID incompatibility on the master with branch-2.1. [#41057](https://github.com/apache/doris/pull/41057)  \\n\\n### Other\\n\\n- BE HTTP API now supports authentication; set config::enable_all_http_auth to true (default is false) when authentication is required. [#39577](https://github.com/apache/doris/pull/39577)  \\n- Optimized the user permissions required for the REFRESH operation. Permissions have been relaxed from ALTER to SHOW. [#39008](https://github.com/apache/doris/pull/39008)  \\n- Reduced the range of nextId when calling advanceNextId(). [#40160](https://github.com/apache/doris/pull/40160)  \\n- Optimized the caching mechanism for Java UDFs. [#40404](https://github.com/apache/doris/pull/40404)  \\n\\n## Bug Fixes\\n\\n### Load\\n\\n- Fixed the issue where `abortTransaction` did not handle return codes. [#41275](https://github.com/apache/doris/pull/41275)  \\n- Fixed the issue where transactions failed to commit or abort in compute-storage decoupled mode without calling `afterCommit/afterAbort`. [#41267](https://github.com/apache/doris/pull/41267)  \\n- Fixed the issue where Routine Load could not work properly when modifying consumer offsets in compute-storage decoupled mode. [#39159](https://github.com/apache/doris/pull/39159)  \\n- Fixed the issue of repeatedly closing file handles when obtaining error log file paths. [#41320](https://github.com/apache/doris/pull/41320)  \\n- Fixed the issue of incorrect job progress caching for Routine Load in compute-storage decoupled mode. [#39313](https://github.com/apache/doris/pull/39313)  \\n- Fixed the issue where Routine Load could get stuck when failing to commit transactions in compute-storage decoupled mode. [#40539](https://github.com/apache/doris/pull/40539)  \\n- Fixed the issue where Routine Load kept reporting data quality check errors in compute-storage decoupled mode. [#39790](https://github.com/apache/doris/pull/39790)  \\n- Fixed the issue where Routine Load did not check transactions before committing in compute-storage decoupled mode. [#39775](https://github.com/apache/doris/pull/39775)  \\n- Fixed the issue where Routine Load did not check transactions before aborting in compute-storage decoupled mode. [#40463](https://github.com/apache/doris/pull/40463)  \\n- Fixed the issue where cluster keys did not support certain data types. [#38966](https://github.com/apache/doris/pull/38966)  \\n- Fixed the issue of transactions being repeatedly committed. [#39786](https://github.com/apache/doris/pull/39786)  \\n- Fixed the issue of use after free with WAL when BE exits. [#33131](https://github.com/apache/doris/pull/33131)  \\n- Fixed the issue where WAL playback did not skip completed import transactions in compute-storage decoupled mode. [#41262](https://github.com/apache/doris/pull/41262)  \\n- Fixed the logic for selecting BE in group commit in compute-storage decoupled mode. [#39986](https://github.com/apache/doris/pull/39986)  [#38644](https://github.com/apache/doris/pull/38644)  \\n- Fixed the issue where BE might crash when group commit was enabled for insert into. [#39339](https://github.com/apache/doris/pull/39339)  \\n- Fixed the issue where insert into with group commit enabled might get stuck. [#39391](https://github.com/apache/doris/pull/39391)  \\n- Fixed the issue where not enabling the group commit option during import might result in a table not found error. [#39731](https://github.com/apache/doris/pull/39731)  \\n- Fixed the issue of transaction submission timeouts due to too many tablets. [#40031](https://github.com/apache/doris/pull/40031)  \\n- Fixed the issue of concurrent opens with Auto Partition. [#38605](https://github.com/apache/doris/pull/38605)  \\n- Fixed the issue of import lock granularity being too large. [#40134](https://github.com/apache/doris/pull/40134)  \\n- Fixed the issue of coredumps caused by zero-length varchars. [#40940](https://github.com/apache/doris/pull/40940)  \\n- Fixed the issue of incorrect index Id values in log prints. [#38790](https://github.com/apache/doris/pull/38790)  \\n- Fixed the issue of memtable shifting not closing BRPC streaming. [#40105](https://github.com/apache/doris/pull/40105)  \\n- Fixed the issue of inaccurate bvar statistics during memtable shifting. [#39075](https://github.com/apache/doris/pull/39075)  \\n- Fixed the issue of multi-replication fault tolerance during memtable shifting. [#38003](https://github.com/apache/doris/pull/38003)  \\n- Fixed the issue of incorrect message length calculations for Routine Load with multiple tables in one stream. [#40367](https://github.com/apache/doris/pull/40367)  \\n- Fixed the issue of inaccurate progress reporting for Broker Load. [#40325](https://github.com/apache/doris/pull/40325)  \\n- Fixed the issue of inaccurate data scan volume reporting for Broker Load. [#40694](https://github.com/apache/doris/pull/40694)  \\n- Fixed the issue of concurrency with Routine Load in compute-storage decoupled mode. [#39242](https://github.com/apache/doris/pull/39242)  \\n- Fixed the issue of Routine Load jobs being canceled in compute-storage decoupled mode. [#39514](https://github.com/apache/doris/pull/39514)  \\n- Fixed the issue of progress not being reset when deleting Kafka topics. [#38474](https://github.com/apache/doris/pull/38474)  \\n- Fixed the issue of updating progress during transaction state transitions in Routine Load. [#39311](https://github.com/apache/doris/pull/39311)  \\n- Fixed the issue of Routine Load switching from a paused state to a paused state. [#40728](https://github.com/apache/doris/pull/40728)  \\n- Fixed the issue of Stream Load records being missed due to database deletion. [#39360](https://github.com/apache/doris/pull/39360)  \\n\\n### Storage\\n\\n- Fixed the issue of missing storage policies. [#38700](https://github.com/apache/doris/pull/38700)  \\n- Fixed the issue of errors during cross-version backup and recovery. [#38370](https://github.com/apache/doris/pull/38370)  \\n- Fixed the NPE issue with ccr binlog. [#39909](https://github.com/apache/doris/pull/39909)  \\n- Fixed potential issues with duplicate keys in mow. [#41309](https://github.com/apache/doris/pull/41309)  [#39791](https://github.com/apache/doris/pull/39791)  [#39958](https://github.com/apache/doris/pull/39958)  [#38369](https://github.com/apache/doris/pull/38369)  [#38331](https://github.com/apache/doris/pull/38331)  \\n- Fixed the issue of not being able to write after backup and recovery in high-frequency write scenarios. [#40118](https://github.com/apache/doris/pull/40118)  [#38321](https://github.com/apache/doris/pull/38321)  \\n- Fixed the issue of data errors potentially triggered by deleting empty strings and schema changes. [#41064](https://github.com/apache/doris/pull/41064)  \\n- Fixed the issue of incorrect statistics due to column updates. [#40880](https://github.com/apache/doris/pull/40880)  \\n- Limited the size of tablet meta pb to prevent BE crashes due to oversized meta. [#39455](https://github.com/apache/doris/pull/39455)  \\n- Fixed the potential column misalignment issue with the new optimizer in `begin; insert into values; commit`. [#39295](https://github.com/apache/doris/pull/39295)  \\n\\n### Compute-Storage Decoupled\\n\\n- Fixed the issue where the tablet distribution might be inconsistent across multiple FEs in compute-storage decoupled mode. [#41458](https://github.com/apache/doris/pull/41458)  \\n- Fixed the issue where TVF might not work in multi-computing group environments. [#39249](https://github.com/apache/doris/pull/39249)  \\n- Fixed the issue where compaction used resources that had already been released when BE exited in compute-storage decoupled mode. [#39302](https://github.com/apache/doris/pull/39302)  \\n- Fixed the issue where automatic start-stop might cause FE replay to get stuck. [#40027](https://github.com/apache/doris/pull/40027)  \\n- Fixed the issue where the BE status and the stored status in meta-service were inconsistent. [#40799](https://github.com/apache/doris/pull/40799)  \\n- Fixed the issue where the FE->meta-service connection pool could not automatically expire and reconnect. [#41202](https://github.com/apache/doris/pull/41202)  [#40661](https://github.com/apache/doris/pull/40661)  \\n- Fixed the issue where some tablets might repeatedly undergo unexpected balance processes during rebalance. [#39792](https://github.com/apache/doris/pull/39792)  \\n- Fixed the issue where storage vault permissions were lost after FE restarted. [#40260](https://github.com/apache/doris/pull/40260)  \\n- Fixed the issue where tablet row counts and other statistical information might be incomplete due to FDB scan range pagination. [#40494](https://github.com/apache/doris/pull/40494)  \\n- Fixed the performance issue caused by a large number of aborted transactions associated with the same label. [#40606](https://github.com/apache/doris/pull/40606)  \\n- Fixed the issue where `commit_txn` did not automatically re-enter, maintaining consistent behavior between compute-storage decoupled and integrated modes. [#39615](https://github.com/apache/doris/pull/39615)  \\n- Fixed the issue where the number of projected columns increased when dropping columns. [#40187](https://github.com/apache/doris/pull/40187)  \\n- Fixed the issue where delete statements did not correctly handle return values, causing data to still be visible after deletion. [#39428](https://github.com/apache/doris/pull/39428)  \\n- Fixed the coredump issue caused by rowset metadata competition during file cache preheating. [#39361](https://github.com/apache/doris/pull/39361)  \\n- Fixed the issue where the entire cache space would be used up when TTL cache enabled LRU eviction. [#39814](https://github.com/apache/doris/pull/39814)  \\n- Fixed the issue where temporary files could not be recycled when importing commit rowset failed with HDFS storage backend. [#40215](https://github.com/apache/doris/pull/40215)  \\n\\n### Lakehouse\\n\\n- Fixed some issues with predicate pushdown in JDBC Catalog. [#39064](https://github.com/apache/doris/pull/39064)  \\n- Fixed the issue of not being able to read when `S``TRUCT` type columns are missing in Parquet format. [#38718](https://github.com/apache/doris/pull/38718)  \\n- Fixed the issue of FileSystem leaks on the FE side in some cases. [#38610](https://github.com/apache/doris/pull/38610)  \\n- Fixed the issue of metadata cache information being inconsistent when Hive/Iceberg tables write back in some cases. [#40729](https://github.com/apache/doris/pull/40729)  \\n- Fixed the issue of unstable partition ID generation for external tables in some cases. [#39325](https://github.com/apache/doris/pull/39325)  \\n- Fixed the issue of external table queries selecting BE nodes in the blacklist in some cases. [#39451](https://github.com/apache/doris/pull/39451)  \\n- Optimized the timeout time for batch retrieval of external table partition information to avoid long-term thread occupation. [#39346](https://github.com/apache/doris/pull/39346)  \\n- Fixed the issue of memory leaks when querying Hudi tables in some cases. [#41256](https://github.com/apache/doris/pull/41256)  \\n- Fixed the issue of connection pool connection leaks in JDBC Catalog in some cases. [#39582](https://github.com/apache/doris/pull/39582)  \\n- Fixed the issue of BE memory leaks in JDBC Catalog in some cases. [#41041](https://github.com/apache/doris/pull/41041)  \\n- Fixed the issue of not being able to query Hudi data on Alibaba Cloud OSS. [#41316](https://github.com/apache/doris/pull/41316)  \\n- Fixed the issue of not being able to read empty partitions in MaxCompute. [#40046](https://github.com/apache/doris/pull/40046)  \\n- Fixed the issue of poor performance when querying Oracle through JDBC Catalog. [#41513](https://github.com/apache/doris/pull/41513)  \\n- Fixed the issue of BE crashes when querying deletion vector of Paimon tables after enabling file cache features. [#39877](https://github.com/apache/doris/pull/39877)  \\n- Fixed the issue of not being able to access Paimon tables on HDFS clusters with HA enabled. [#39806](https://github.com/apache/doris/pull/39806)  \\n- Temporarily disabled the page index filtering feature of Parquet to avoid potential issues. [#38691](https://github.com/apache/doris/pull/38691)  \\n- Fixed the issue of not being able to read unsigned types in Parquet files. [#39926](https://github.com/apache/doris/pull/39926)  \\n- Fixed the issue of potential infinite loops when reading Parquet files in some cases. [#39523](https://github.com/apache/doris/pull/39523)  \\n\\n### Asynchronous Materialized Views\\n\\n- Fixed the issue where partition construction might select the wrong table to track partitions if both sides have the same column names. [#40810](https://github.com/apache/doris/pull/40810)  \\n- Fixed the issue where transparent rewrite partition compensation might result in incorrect results. [#40803](https://github.com/apache/doris/pull/40803)  \\n- Fixed the issue where transparent rewrite did not take effect on external tables. [#38909](https://github.com/apache/doris/pull/38909)  \\n- Fixed the issue where nested materialized views might not refresh properly. [#40433](https://github.com/apache/doris/pull/40433)  \\n\\n### Synchronous Materialized Views\\n\\n- Fixed the issue where creating synchronous materialized views on MOW tables might result in incorrect query results. [#39171](https://github.com/apache/doris/pull/39171)  \\n\\n### Query Optimizer\\n\\n- Fixed the issue where existing synchronous materialized views might not be usable after upgrading. [#41283](https://github.com/apache/doris/pull/41283)  \\n- Fixed the issue of not correctly handling milliseconds when comparing datetime literals. [#40121](https://github.com/apache/doris/pull/40121)  \\n- Fixed the issue of potential errors in conditional function partition pruning. [#39298](https://github.com/apache/doris/pull/39298)  \\n- Fixed the issue where MOW tables with synchronous materialized views could not perform delete operations. [#39578](https://github.com/apache/doris/pull/39578)  \\n- Fixed the issue where the nullable of slots in JDBC external table query predicates might be incorrectly planned, causing query errors. [#41014](https://github.com/apache/doris/pull/41014)  \\n\\n### Query Execution\\n\\n- Fixed the memory leak issue caused by the use of runtime filters. [#39155](https://github.com/apache/doris/pull/39155)  \\n- Fixed the issue of excessive memory usage by window functions. [#39581](https://github.com/apache/doris/pull/39581)  \\n- Fixed a series of function compatibility issues during rolling upgrades. [#41023](https://github.com/apache/doris/pull/41023)  [#40438](https://github.com/apache/doris/pull/40438)  [#39648](https://github.com/apache/doris/pull/39648)  \\n- Fixed the issue of incorrect results with `encryption_function` when used with constants. [#40201](https://github.com/apache/doris/pull/40201)  \\n- Fixed the issue of errors when importing single-table materialized views. [#39061](https://github.com/apache/doris/pull/39061)  \\n- Fixed the issue of incorrect partition result calculations for window functions. [#39100](https://github.com/apache/doris/pull/39100)  [#40761](https://github.com/apache/doris/pull/40761)  \\n- Fixed the issue of incorrect calculations for topn when null values are present. [#39497](https://github.com/apache/doris/pull/39497)  \\n- Fixed the issue of incorrect results with the `map_agg` function. [#39743](https://github.com/apache/doris/pull/39743)  \\n- Fixed the issue of incorrect messages returned by cancel. [#38982](https://github.com/apache/doris/pull/38982)  \\n- Fixed the issue of BE core dumps caused by encrypt and decrypt functions. [#40726](https://github.com/apache/doris/pull/40726)  \\n- Fixed the issue of queries getting stuck due to too many scanners in high-concurrency scenarios. [#40495](https://github.com/apache/doris/pull/40495)  \\n- Supported time types in runtime filters. [#38258](https://github.com/apache/doris/pull/38258)  \\n- Fixed the issue of incorrect results with window funnel functions. [#40960](https://github.com/apache/doris/pull/40960)  \\n\\n### Semi-Structured Data Management\\n\\n- Fixed the issue of match function errors when no indexes were present. [#38989](https://github.com/apache/doris/pull/38989)  \\n- Fixed the issue of crashes when ARRAY data types were used as parameters for array_min/array_max functions. [#39492](https://github.com/apache/doris/pull/39492)  \\n- Fixed the issue of nullable with the `array_enumerate_uniq` function. [#38384](https://github.com/apache/doris/pull/38384)  \\n- Fixed the issue of bloomfilter indexes not being updated when adding or deleting columns. [#38431](https://github.com/apache/doris/pull/38431)  \\n- Fixed the issue of es-catalog parsing exceptions with array data. [#39104](https://github.com/apache/doris/pull/39104)  \\n- Fixed the issue of improper predicate push-down in es-catalog. [#40111](https://github.com/apache/doris/pull/40111)  \\n- Fixed the issue of exceptions caused by modifying input data with`map()` and `struct()` functions. [#39699](https://github.com/apache/doris/pull/39699)  \\n- Fixed the issue of index compaction crashes in special cases. [#40294](https://github.com/apache/doris/pull/40294)  \\n- Fixed the issue of ARRAY type inverted indexes missing nullbitmaps. [#38907](https://github.com/apache/doris/pull/38907)  \\n- Fixed the issue of incorrect results with the `count()` function on inverted indexes. [#41152](https://github.com/apache/doris/pull/41152)  \\n- Fixed the issue of correct results with the `explode_map` function when using aliases. [#39757](https://github.com/apache/doris/pull/39757)  \\n- Fixed the issue of VARIANT type not being able to use row storage for exceptional JSON data. [#39394](https://github.com/apache/doris/pull/39394)  \\n- Fixed the issue of memory leaks when returning ARRAY results with VARIANT type. [#41358](https://github.com/apache/doris/pull/41358)  \\n- Fixed the issue of changing column names with VARIANT type. [#40320](https://github.com/apache/doris/pull/40320)  \\n- Fixed the issue of potential precision loss when converting VARIANT type to DECIMAL type. [#39650](https://github.com/apache/doris/pull/39650)  \\n- Fixed the issue of nullable handling with VARIANT type. [#39732](https://github.com/apache/doris/pull/39732)  \\n- Fixed the issue of sparse column reading with VARIANT type. [#40295](https://github.com/apache/doris/pull/40295)  \\n\\n### Other\\n\\n- Fixed the compatibility issue between new and old audit log plugins. [#41401](https://github.com/apache/doris/pull/41401)  \\n- Fixed the issue where users could see processes of others in certain cases. [#39747](https://github.com/apache/doris/pull/39747)  \\n- Fixed the issue where users with permissions could not export. [#38365](https://github.com/apache/doris/pull/38365)  \\n- Fixed the issue where create table like required create permissions for the existing table. [#37879](https://github.com/apache/doris/pull/37879)  \\n- Fixed the issue where some features did not verify permissions. [#39726](https://github.com/apache/doris/pull/39726)  \\n- Fixed the issue of not correctly closing connections when using SSL. [#38587](https://github.com/apache/doris/pull/38587)  \\n- Fixed the issue where executing ALTER VIEW operations in some cases caused FE to fail to start. [#40872](https://github.com/apache/doris/pull/40872)"},{"id":"/release-note-2.0.15","metadata":{"permalink":"/blog/release-note-2.0.15","source":"@site/blog/release-note-2.0.15.md","title":"Apache Doris version 2.0.15 has been released","description":"Thanks to our community users and developers, about 157 improvements and bug fixes have been made in Doris 2.0.15 version","date":"2024-09-30T00:00:00.000Z","tags":[{"inline":true,"label":"Release Notes","permalink":"/blog/tags/release-notes"}],"hasTruncateMarker":false,"authors":[{"name":"Apache Doris","key":null,"page":null}],"frontMatter":{"title":"Apache Doris version 2.0.15 has been released","summary":"Thanks to our community users and developers, about 157 improvements and bug fixes have been made in Doris 2.0.15 version","description":"Thanks to our community users and developers, about 157 improvements and bug fixes have been made in Doris 2.0.15 version","date":"2024-09-30","author":"Apache Doris","tags":["Release Notes"],"image":"/images/2.0.15.jpg"},"unlisted":false,"prevItem":{"title":"Apache Doris 3.0.2 just released","permalink":"/blog/release-note-3.0.2"},"nextItem":{"title":"Apache Doris Flink Connector 24.0.0  just released!","permalink":"/blog/release-flink-doris-connector-24.0.0"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n  http://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\nThanks to our community users and developers, about 157 improvements and bug fixes have been made in Doris 2.0.15 version\\n\\n- Quick Download: https://doris.apache.org/download\\n\\n- GitHub: https://github.com/apache/doris/releases/tag/2.0.15 \\n\\n## 1 Behavior Change\\n\\nNA\\n\\n## 2 New Features\\n\\n- Restore now supports deleting redundant tablets and partition options. [#39028](https://github.com/apache/doris/pull/39028)\\n\\n- Support JSON function `json_search`.[#40948](https://github.com/apache/doris/pull/40948)\\n\\n## 3 Improvement and Optimizations\\n\\n### Stability\\n\\n- Add a FE configuration `abort_txn_after_lost_heartbeat_time_second` for transaction abort time. [#28662](https://github.com/apache/doris/pull/28662)\\n\\n- Abort transactions after a BE loses heartbeat for over 1 minute instead of 5 seconds, to avoid overly sensitive transaction aborts. [#22781](https://github.com/apache/doris/pull/22781)\\n\\n- Delay scheduling EOF tasks of routine load to avoid an excessive number of small transactions. [#39975](https://github.com/apache/doris/pull/39975)\\n\\n- Prefer querying from online disk services to be more robust. [#39467](https://github.com/apache/doris/pull/39467)\\n\\n- Skip checking newly inserted rows in non-strict mode partial updates if the row\'s delete sign is marked. [#40322](https://github.com/apache/doris/pull/40322)\\n\\n- To prevent FE OOM, limit the number of tablets in backup tasks, with a default value of 300,000. [#39987](https://github.com/apache/doris/pull/39987)\\n\\n### Performance\\n\\n- Optimize slow column updates caused by concurrent column updates and compactions. [#38487](https://github.com/apache/doris/pull/38487)\\n\\n- When a NullLiteral exists in a filter condition, it can now be folded into False and further converted to an EmptySet to reduce unnecessary data scanning and computation. [#38135](https://github.com/apache/doris/pull/38135)\\n\\n- Improve performance of `ORDER BY` permutation. [#38985](https://github.com/apache/doris/pull/38985)\\n\\n- Improve the performance of string processing in inverted indexes. [#37395](https://github.com/apache/doris/pull/37395)\\n\\n### Optimizer and Statistics\\n\\n- Added support for statements beginning with a semicolon. [#39399](https://github.com/apache/doris/pull/39399)\\n\\n- Polish aggregate function signature matching. [#39352](https://github.com/apache/doris/pull/39352)\\n\\n- Drop column statistics and trigger auto analysis after schema change. [#39101](https://github.com/apache/doris/pull/39101)\\n\\n- Support dropping cached stats using `DROP CACHED STATS table_name`. [#39367](https://github.com/apache/doris/pull/39367)\\n\\n### Multi Catalog and Others\\n\\n- Optimize JDBC Catalog refresh to reduce the frequency of client creation. [#40261](https://github.com/apache/doris/pull/40261)\\n\\n- Fix thread leaks in JDBC Catalog under certain conditions. [#39423](https://github.com/apache/doris/pull/39423)\\n\\n- ARRAY MAP STRUCT types now support `REPLACE_IF_NOT_NULL`. [#38304](https://github.com/apache/doris/pull/38304)\\n\\n- Retry delete jobs for failures that are not `DELETE_INVALID_XXX`. [#37834](https://github.com/apache/doris/pull/37834)\\n\\n**Credits**\\n\\n@924060929, @BePPPower, @BiteTheDDDDt, @CalvinKirs, @GoGoWen, @HappenLee, @Jibing-Li, @Johnnyssc, @LiBinfeng-01, @Mryange, @SWJTU-ZhangLei, @TangSiyang2001, @Toms1999, @Vallishp, @Yukang-Lian, @airborne12, @amorynan, @bobhan1, @cambyzju, @csun5285, @dataroaring, @eldenmoon, @englefly, @feiniaofeiafei, @hello-stephen, @htyoung, @hubgeter, @justfortaste, @liaoxin01, @liugddx, @liutang123, @luwei16, @mongo360, @morrySnow, @qidaye, @smallx, @sollhui, @starocean999, @w41ter, @xiaokang, @xzj7019, @yujun777, @zclllyybb, @zddr, @zhangstar333, @zhannngchen, @zy-kkk, @zzzxl1993"},{"id":"/release-flink-doris-connector-24.0.0","metadata":{"permalink":"/blog/release-flink-doris-connector-24.0.0","source":"@site/blog/release-flink-doris-connector-24.0.0.md","title":"Apache Doris Flink Connector 24.0.0  just released!","description":"Dear community, We are excited to announce the official release of Apache Doris Flink Connector version 24.0.0 on September 5th, 2024.","date":"2024-09-25T00:00:00.000Z","tags":[{"inline":true,"label":"Release Notes","permalink":"/blog/tags/release-notes"}],"hasTruncateMarker":false,"authors":[{"name":"Apache Doris","key":null,"page":null}],"frontMatter":{"title":"Apache Doris Flink Connector 24.0.0  just released!","description":"Dear community, We are excited to announce the official release of Apache Doris Flink Connector version 24.0.0 on September 5th, 2024.","summary":"Dear community, We are excited to announce the official release of Apache Doris Flink Connector version 24.0.0 on September 5th, 2024.","date":"2024-09-25","author":"Apache Doris","tags":["Release Notes"],"image":"/images/release-flink-doris-connector-24.0.jpg"},"unlisted":false,"prevItem":{"title":"Apache Doris version 2.0.15 has been released","permalink":"/blog/release-note-2.0.15"},"nextItem":{"title":"Apache Doris 2.1.6 just released","permalink":"/blog/release-note-2.1.6"}},"content":"\x3c!-- \\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\nDear community, We are excited to announce the official release of Apache Doris Flink Connector version 24.0.0 on September 5th, 2024. This release brings several enhancements and new capabilities, including support for Flink 1.20 and high-speed data retrieval from Doris via Arrow Flight SQL. Additionally, the FlinkCDC version required for full database synchronization has been upgraded to 3.1.x for optimal performance.\\n\\n- Download Link: https://github.com/apache/doris-flink-connector/releases/tag/24.0.0\\n\\n## Behavioral Changes\\n\\n- FlinkCDC Upgrade: To leverage the full potential of this release, the FlinkCDC version used for full database synchronization must be upgraded to 3.1.x. Due to the incompatibility between FlinkCDC 3.1.x and earlier versions (e.g., 2.4), running full database synchronization jobs will require a stateless restart after upgrading FlinkCDC. Please refer to the [Apache Flink CDC 3.1.0 Release Announcement](https://mp.weixin.qq.com/s/qYW5Bw0IqUHUc8bnfWOIog) for details on compatibility.\\n\\n- Version Renaming: To maintain consistency with other Connectors (e.g., Spark and Kafka) and account for the aforementioned incompatibilities, the Connector version has been renamed to the 24.x series. See the discussion thread [DISCUSS\\\\] About the next version change of Connector](https://lists.apache.org/thread/8tp215yk0tkgtdfkjdl4svvbljnmxzst) for more information.\\n\\n## New Features\\n\\n- Supported Flink v1.20.\\n\\n- DB2 Database synchronization is supported.\\n\\n- CDC Schema Change enhancement supported the use of the JSQLParser framework for DDL.\\n\\n- Supported Stream Load with GZ compression.\\n\\n- Enabled Arrow Flight SQL integration for high-speed data retrieval from Doris.\\n\\n## Improvements\\n\\n- Upgraded FlinkCDC  to 3.1.1.\\n\\n- JDBC parameter configuration for DB2/Postgres/SQLServer synchronization.\\n\\n- Optimized batch writing mode.\\n\\n- Refined CDC synchronization logic.\\n\\n- Supported MySQL full database synchronization with `INTEGER` type.\\n\\n## Bug Fixes\\n\\n- Resolved serialization issues with `MAP` subtypes of `DATE`/ `DATETIME`\\n\\n- Fixed FlinkSQL projection pushdown bugs\\n\\n- Resolved `DECIMAL` type sync issues with MongoDB\\n\\n- Compatibility update for Doris arrow-based timestamp reading\\n\\n- Fixed non-effective delete events in CDC full database synchronization\\n\\n- Corrected schema change logic when default values are null\\n\\n## Credits\\n\\n@bingquanzhao\u3001@DongLiang-0\u3001@JasonLeeCoding\u3001@JNSimba@MaoMiMao\u3001@qg-lin@tmc9031\u3001@vinlee19"},{"id":"/release-note-2.1.6","metadata":{"permalink":"/blog/release-note-2.1.6","source":"@site/blog/release-note-2.1.6.md","title":"Apache Doris 2.1.6 just released","description":"This version brings continuous upgrades and improvements to the Lakehouse, Async Materialized Views, and Semi-Structured Data Management.","date":"2024-09-10T00:00:00.000Z","tags":[{"inline":true,"label":"Release Notes","permalink":"/blog/tags/release-notes"}],"hasTruncateMarker":false,"authors":[{"name":"Apache Doris","key":null,"page":null}],"frontMatter":{"title":"Apache Doris 2.1.6 just released","summary":"This version brings continuous upgrades and improvements to the Lakehouse, Async Materialized Views, and Semi-Structured Data Management.","description":"This version brings continuous upgrades and improvements to the Lakehouse, Async Materialized Views, and Semi-Structured Data Management.","date":"2024-09-10","author":"Apache Doris","tags":["Release Notes"],"image":"/images/2.1.6.jpg"},"unlisted":false,"prevItem":{"title":"Apache Doris Flink Connector 24.0.0  just released!","permalink":"/blog/release-flink-doris-connector-24.0.0"},"nextItem":{"title":"Creator of Talkie migrated from Loki and built a PB-scale logging system with Apache Doris","permalink":"/blog/ai-unicorn-minimax-from-loki-and-built-a-pb-scale-logging-system-with-doris"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n  http://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n\\nDear community, **Apache Doris version 2.1.6 was officially released on September 10, 2024.** This version brings continuous upgrades and improvements to the Lakehouse, Async Materialized Views, and Semi-Structured Data Management. Additionally, several fixes have been implemented in areas such as the query optimizer, execution engine, storage management, permission management. \\n\\n**Quick Download:** https://doris.apache.org/download/\\n\\n**GitHub Release:** https://github.com/apache/doris/releases\\n\\n## Behavior changes\\n\\n- Removed the `delete_if_exists` option from create repository. [#38192](https://github.com/apache/doris/pull/38192)\\n\\n- Added the `enable_prepared_stmt_audit_log` session variable to control whether JDBC prepared statements record audit logs, with the default being no recording. [#38624](https://github.com/apache/doris/pull/38624) [#39009](https://github.com/apache/doris/pull/39009)\\n\\n- Implemented fd limit and memory constraints for segment cache. [#39689](https://github.com/apache/doris/pull/39689)\\n\\n- When the FE configuration item `sys_log_mode` is set to BRIEF, file location information is added to the logs. [#39571](https://github.com/apache/doris/pull/39571)\\n\\n- Changed the default value of the session variable `max_allowed_packet` to 16MB. [#38697](https://github.com/apache/doris/pull/38697)\\n\\n- When a single request contains multiple statements, semicolons must be used to separate them. [#38670](https://github.com/apache/doris/pull/38670)\\n\\n- Added support for statements to begin with a semicolon. [#39399](https://github.com/apache/doris/pull/39399)\\n\\n- Aligned type formatting with MySQL in statements such as `show create table`. [#38012](https://github.com/apache/doris/pull/38012)\\n\\n- When the new optimizer planning times out, it no longer falls back to prevent the old optimizer from using longer planning times. [#39499](https://github.com/apache/doris/pull/39499)\\n\\n## New features\\n\\n### Lakehouse\\n\\n- Supported writeback for Iceberg tables. \\n\\n  - For more information, please refer to the [documentation](https://doris.apache.org/docs/lakehouse/datalake-building/iceberg-build). \\n\\n- SQL interception rules now support external tables. \\n\\n  - For more information, please refer to the [documentation](https://doris.apache.org/docs/admin-manual/query-admin/sql-interception).\\n\\n- Added the system table `file_cache_statistics` to view BE data cache metrics.\\n\\n  - For more information, please refer to the [documentation](https://doris.apache.org/docs/admin-manual/system-tables/file_cache_statistics).\\n\\n### Async Materialized View\\n\\n- Supported transparent rewriting during inserts. [#38115](https://github.com/apache/doris/pull/38115)\\n\\n- Supported transparent rewriting when variant types exist in queries.[ #37929](https://github.com/apache/doris/pull/37929)\\n\\n### Semi-Structured Data Management\\n\\n- Supported casting ARRAY MAP to JSON type.[ #36548](https://github.com/apache/doris/pull/36548)\\n\\n- Supported the `json_keys` function.[ #36411](https://github.com/apache/doris/pull/36411)\\n\\n- Supported specifying the JSON path $. when importing JSON. [#38213](https://github.com/apache/doris/pull/38213)\\n\\n- ARRAY, MAP, STRUCT types now support `replace_if_not_null`[#38304](https://github.com/apache/doris/pull/38304)\\n\\n- ARRAY, MAP, STRUCT types now support adjusting column order.[#39210](https://github.com/apache/doris/pull/39210)\\n\\n- Added the `multi_match` function to match keywords across multiple fields, with support for inverted index acceleration. [#37722](https://github.com/apache/doris/pull/37722)\\n\\n### Query Optimizer\\n\\n- Filled in the original database name, table name, column name, and alias for returned columns in the MySQL protocol. [ #38126](https://github.com/apache/doris/pull/38126)\\n\\n- Supported the aggregation function `group_concat` with both order by and distinct simultaneously. [#38080](https://github.com/apache/doris/pull/38080)\\n\\n- SQL cache now supports reusing cached results for queries with different comments. [#40049](https://github.com/apache/doris/pull/40049)\\n\\n- In partition pruning, supported including `date_trunc` and date functions in filter conditions. [#38025](https://github.com/apache/doris/pull/38025) [#38743](https://github.com/apache/doris/pull/38743)\\n\\n- Allowed using the database name where the table resides as a qualifier prefix for table aliases. [#38640](https://github.com/apache/doris/pull/38640)\\n\\n- Supported hint-style comments.[#39113](https://github.com/apache/doris/pull/39113)\\n\\n### Others\\n\\n- Added the system table `table_properties` for viewing table properties.\\n\\n  - For more information, please refer to the [documentation](https://doris.apache.org/docs/admin-manual/system-tables/information_schema/table_properties). \\n\\n- Introduced deadlock and slow lock detection in FE. \\n\\n  - For more information, please refer to the [documentation](https://doris.apache.org/docs/admin-manual/maint-monitor/frontend-lock-manager). \\n\\n## Improvements\\n\\n### Lakehouse\\n\\n- Reimplemented the external table metadata caching mechanism. \\n\\n  - For details, refer to the [documentation](https://doris.apache.org/docs/lakehouse/metacache). \\n\\n- Added the session variable `keep_carriage_return` with a default value of false. By default, reading Hive Text format tables treats both `\\\\r\\\\n` and `\\\\n` as newline characters. [#38099](https://github.com/apache/doris/pull/38099)\\n\\n- Optimized memory statistics for Parquet/ORC file read/write operations.[#37257](https://github.com/apache/doris/pull/37257)\\n\\n- Supported pushing down IN/NOT IN predicates for Paimon tables. [#38390](https://github.com/apache/doris/pull/38390)\\n\\n- Enhanced the optimizer to support Time Travel syntax for Hudi tables. [#38591](https://github.com/apache/doris/pull/38591)\\n\\n- Optimized Kerberos authentication-related processes. [ #37301](https://github.com/apache/doris/pull/37301)\\n\\n- Enabled reading Hive tables after renaming column operations. [#38809](https://github.com/apache/doris/pull/38809)\\n\\n- Optimized the reading performance of partition columns for external tables. [#38810](https://github.com/apache/doris/pull/38810)\\n\\n- Improved the data shard merging strategy during external table query planning to avoid performance degradation caused by a large number of small shards.[#38964](https://github.com/apache/doris/pull/38964)\\n\\n- Added attributes such as location to `SHOW CREATE DATABASE/TABLE`. [#39644](https://github.com/apache/doris/pull/39644)\\n\\n- Supported complex types in MaxCompute Catalog. [#39822](https://github.com/apache/doris/pull/39822)\\n\\n- Optimized the file cache loading strategy by using asynchronous loading to avoid long BE startup times. [#39036](https://github.com/apache/doris/pull/39036)\\n\\n- Improved the file cache eviction strategy, such as evicting locks held for extended periods. [#39721](https://github.com/apache/doris/pull/39721)\\n\\n### Async Materialized View\\n\\n- Supported hourly, weekly, and quarterly partition roll-up construction. [#37678](https://github.com/apache/doris/pull/37678)\\n\\n- For materialized views based on Hive external tables, the metadata cache is now updated before refresh to ensure the latest data is obtained during each refresh. [#38212](https://github.com/apache/doris/pull/38212)\\n\\n- Improved the performance of transparent rewrite planning in storage-compute decoupled mode by batch fetching metadata. [#39301](https://github.com/apache/doris/pull/39301)\\n\\n- Enhanced the performance of transparent rewrite planning by prohibiting duplicate enumerations. [#39541](https://github.com/apache/doris/pull/39541)\\n\\n- Improved the performance of transparent rewrite for refreshing materialized views based on Hive external table partitions.[#38525](https://github.com/apache/doris/pull/38525)\\n\\n### Semi-Structured Data Management\\n\\n- Optimized memory allocation for TOPN queries to improve performance. [#37429](https://github.com/apache/doris/pull/37429)\\n\\n- Enhanced the performance of string processing in inverted indexes.[#37395](https://github.com/apache/doris/pull/37395)\\n\\n- Optimized the performance of inverted indexes in MOW tables. [#37428](https://github.com/apache/doris/pull/37428)\\n\\n- Supported specifying the row-store `page_size` during table creation to control compression effectiveness. [#37145](https://github.com/apache/doris/pull/37145)\\n\\n### Query Optimizer\\n\\n- Adjusted the row count estimation algorithm for mark joins, resulting in more accurate cardinality estimates for mark joins. [#38270](https://github.com/apache/doris/pull/38270)\\n\\n- Optimized the cost estimation algorithm for semi/anti joins, enabling more accurate selection of semi/anti join orders. [#37951](https://github.com/apache/doris/pull/37951)\\n\\n- Adjusted the filter estimation algorithm for cases where some columns have no statistical information, leading to more accurate cardinality estimates. [#39592](https://github.com/apache/doris/pull/39592)\\n\\n- Modified the instance calculation logic for set operation operators to prevent insufficient parallelism in extreme cases. [#39999](https://github.com/apache/doris/pull/39999)\\n\\n- Adjusted the usage strategy of bucket shuffle, achieving better performance when data is not sufficiently shuffled. [#36784](https://github.com/apache/doris/pull/36784)\\n\\n- Enabled early filtering of window function data, supporting multiple window functions in a single projection. [#38393](https://github.com/apache/doris/pull/38393)\\n\\n- When a `NullLiteral` exists in a filter condition, it can now be folded into false, further converted to an `EmptySet` to reduce unnecessary data scanning and computation. [#38135](https://github.com/apache/doris/pull/38135)\\n\\n- Expanded the scope of predicate derivation, reducing data scanning in queries with specific patterns. [#37314](https://github.com/apache/doris/pull/37314)\\n\\n- Supported partial short-circuit evaluation logic in partition pruning to improve partition pruning performance, achieving over 100% improvement in specific scenarios. [#38191](https://github.com/apache/doris/pull/38191)\\n\\n- Enabled the computation of arbitrary scalar functions within user variables. [#39144](https://github.com/apache/doris/pull/39144)\\n\\n- Maintained error messages consistent with MySQL when alias conflicts exist in queries. [#38104](https://github.com/apache/doris/pull/38104)\\n\\n### Query Execution\\n\\n- Adapted AggState for compatibility from 2.1 to 3.x and fixed coredump issues. [#37104](https://github.com/apache/doris/pull/37104)\\n\\n- Refactored the strategy selection for local shuffle when no joins are involved. [#37282](https://github.com/apache/doris/pull/37282)\\n\\n- Modified the scanner for internal table queries to an asynchronous approach to prevent blocking during internal table queries. [#38403](https://github.com/apache/doris/pull/38403)\\n\\n- Optimized the block merge process when building hash tables in Join operators. [#37471](https://github.com/apache/doris/pull/37471)\\n\\n- Reduced the lock holding time for MultiCast operations. [37462](https://github.com/apache/doris/pull/37462)\\n\\n- Optimized gRPC\'s keepAliveTime and added a connection monitoring mechanism, reducing the probability of query failures due to RPC errors during query execution. [#37304](https://github.com/apache/doris/pull/37304)\\n\\n- Cleaned up all dirty pages in jemalloc when memory limits are exceeded. [#37164](https://github.com/apache/doris/pull/37164)\\n\\n- Improved the performance of `aes_encrypt`/`decrypt` functions when handling constant types. [#37194](https://github.com/apache/doris/pull/37194)\\n\\n- Optimized the performance of `json_extract` functions when processing constant data. [#36927](https://github.com/apache/doris/pull/36927)\\n\\n- Optimized the performance of ParseURL functions when processing constant data. [#36882](https://github.com/apache/doris/pull/36882)\\n\\n### Backup Recovery / CCR\\n\\n- Restore now supports deleting redundant tablets and partition options. [#39363](https://github.com/apache/doris/pull/39363)\\n\\n- Check storage connectivity when creating a repository. [#39538](https://github.com/apache/doris/pull/39538)\\n\\n- Enables binlog to support `DROP TABLE`, allowing CCR to incrementally synchronize `DROP TABLE` operations. [#38541](https://github.com/apache/doris/pull/38541)\\n\\n### Compaction\\n\\n- Improves the issue where high-priority compaction tasks were not subject to task concurrency control limits. [#38189](https://github.com/apache/doris/pull/38189)\\n\\n- Automatically reduces compaction memory consumption based on data characteristics. [#37486](https://github.com/apache/doris/pull/37486)\\n\\n- Fixes an issue where the sequential data optimization strategy could lead to incorrect data in aggregate tables or MOR UNIQUE tables. [ #38299](https://github.com/apache/doris/pull/38299)\\n\\n- Optimizes the rowset selection strategy during compaction during replica replenishment to avoid triggering -235 errors. [#39262](https://github.com/apache/doris/pull/39262)\\n\\n### MOW (Merge-On-Write)\\n\\n- Optimizes slow column updates caused by concurrent column updates and compactions. [#38682](https://github.com/apache/doris/pull/38682)\\n\\n- Fixes an issue where segcompaction during bulk data imports could lead to incorrect MOW data. [#38992](https://github.com/apache/doris/pull/38992) [#39707](https://github.com/apache/doris/pull/39707)\\n\\n- Fixes data loss in column updates that may occur after BE restarts. [#39035](https://github.com/apache/doris/pull/39035)\\n\\n### Storage Management\\n\\n- Adds FE configuration to control whether queries under hot-cold tiering prefer local data replicas. [#38322](https://github.com/apache/doris/pull/38322)\\n\\n- Optimizes expired BE report messages to include newly created tablets. [#38839](https://github.com/apache/doris/pull/38839) [#39605](https://github.com/apache/doris/pull/39605)\\n\\n- Optimizes replica scheduling priority strategy to prioritize replicas with missing data. [#38884](https://github.com/apache/doris/pull/38884)\\n\\n- Prevents tablets with unfinished ALTER jobs from being balanced. [#39202](https://github.com/apache/doris/pull/39202)\\n\\n- Enables modifying the number of buckets for tables with list partitioning. [#39688](https://github.com/apache/doris/pull/39688)\\n\\n- Prefers querying from online disk services. [#39654](https://github.com/apache/doris/pull/39654)\\n\\n- Improves error messages for materialized view base tables that do not support deletion during synchronization. [#39857](https://github.com/apache/doris/pull/39857)\\n\\n- Improves error messages for single columns exceeding 4GB. [#39897](https://github.com/apache/doris/pull/39897)\\n\\n- Fixes an issue where aborted transactions were omitted when plan errors occurred during `INSERT` statements.[#38260](https://github.com/apache/doris/pull/38260)\\n\\n- Fixes exceptions during SSL connection closure.[#38677](https://github.com/apache/doris/pull/38677)\\n\\n- Fixes an issue where table locks were not held when aborting transactions using labels. [#38842](https://github.com/apache/doris/pull/38842)\\n\\n- Fixes `gson pretty` causing large image issues. [#39135](https://github.com/apache/doris/pull/39135)\\n\\n- Fixes an issue where the new optimizer did not check for bucket values of 0 in `CREATE TABLE` statements.[#38999](https://github.com/apache/doris/pull/38999)\\n\\n- Fixes errors when Chinese column names are included in `DELETE` condition predicates. [#39500](https://github.com/apache/doris/pull/39500)\\n\\n- Fixes frequent tablet balancing issues in partition balancing mode. [#39606](https://github.com/apache/doris/pull/39606)\\n\\n- Fixes an issue where partition storage policy attributes were lost. [#39677](https://github.com/apache/doris/pull/39677)\\n\\n- Fixes incorrect statistics when importing multiple tables within a transaction. [#39548](https://github.com/apache/doris/pull/39548)\\n\\n- Fixes errors when deleting random bucket tables. [#39830](https://github.com/apache/doris/pull/39830)\\n\\n- Fixes issues where FE fails to start due to non-existent UDFs. [#39868](https://github.com/apache/doris/pull/39868)\\n\\n- Fixes inconsistencies in the last failed version between FE master and slave. [#39947](https://github.com/apache/doris/pull/39947)\\n\\n- Fixes an issue where related tablets may still be in schema change state when schema change jobs are canceled. [ #39327](https://github.com/apache/doris/pull/39327)\\n\\n- Fixes errors when modifying type and column order in a single statement schema change (SC). [#39107](https://github.com/apache/doris/pull/39107)\\n\\n### Data Loading\\n\\n- Improves error messages for -238 errors during imports. [#39182](https://github.com/apache/doris/pull/39182)\\n\\n- Allows importing to other partitions while restoring a partition. [#39915](https://github.com/apache/doris/pull/39915)\\n\\n- Optimizes the strategy for FE to select BEs during group commit. [#37830](https://github.com/apache/doris/pull/37830) [#39010](https://github.com/apache/doris/pull/39010)\\n\\n- Avoids printing stack traces for some common streamload error messages. [#38418](https://github.com/apache/doris/pull/38418)\\n\\n- Improves handling of issues where offline BEs may affect import errors. [#38256](https://github.com/apache/doris/pull/38256)\\n\\n### Permissions\\n\\n- Optimizes access performance after enabling the Ranger authentication plugin. [#38575](https://github.com/apache/doris/pull/38575)\\n- Optimizes permission strategies for Refresh Catalog/Database/Table operations, allowing users to perform these operations with only SHOW permissions. [#39008](https://github.com/apache/doris/pull/39008)\\n\\n## Bug fixes\\n\\n### Lakehouse\\n\\n- Fixes the issue where switching catalogs may result in an error of not finding the database. [#38114](https://github.com/apache/doris/pull/38114)\\n\\n- Addresses exceptions caused by attempting to read non-existent data on S3. [#38253](https://github.com/apache/doris/pull/38253)\\n\\n- Resolves the issue where specifying an abnormal path during export operations may lead to incorrect export locations. [#38602](https://github.com/apache/doris/pull/38602)\\n\\n- Fixes the timezone issue for time columns in Paimon tables. [#37716](https://github.com/apache/doris/pull/37716)\\n\\n- Temporarily disables the Parquet PageIndex feature to avoid certain erroneous behaviors.\\n\\n- Corrects the selection of Backend nodes in the blacklist during external table queries. [#38984](https://github.com/apache/doris/pull/38984)\\n\\n- Resolves errors caused by missing subcolumns in Parquet Struct column types.[#39192](https://github.com/apache/doris/pull/39192)\\n\\n- Addresses several issues with predicate pushdown in JDBC Catalog. [#39082](https://github.com/apache/doris/pull/39082)\\n\\n- Fixes issues where some historical Parquet formats led to incorrect query results. [#39375](https://github.com/apache/doris/pull/39375)\\n\\n- Improves compatibility with ojdbc6 drivers for Oracle JDBC Catalog. [#39408](https://github.com/apache/doris/pull/39408)\\n\\n- Resolves potential FE memory leaks caused by Refresh Catalog/Database/Table operations. [#39186](https://github.com/apache/doris/pull/39186) [#39871](https://github.com/apache/doris/pull/39871)\\n\\n- Fixes thread leaks in JDBC Catalog under certain conditions. [#39666](https://github.com/apache/doris/pull/39666) [#39582](https://github.com/apache/doris/pull/39582)\\n\\n- Addresses potential event processing failures after enabling Hive Metastore event subscription. [#39239](https://github.com/apache/doris/pull/39239)\\n\\n- Disables reading Hive Text format tables with custom escape characters and null formats to prevent data errors. [#39869](https://github.com/apache/doris/pull/39869)\\n\\n- Resolves issues accessing Iceberg tables created via the Iceberg API under certain conditions. [#39203](https://github.com/apache/doris/pull/39203)\\n\\n- Fixes the inability to read Paimon tables stored on HDFS clusters with high availability enabled. [#39876](https://github.com/apache/doris/pull/39876)\\n\\n- Addresses errors that may occur when reading Paimon table deletion vectors after enabling file caching. [#39875](https://github.com/apache/doris/pull/39875)\\n\\n- Resolves potential deadlocks when reading Parquet files under certain conditions. [#39945](https://github.com/apache/doris/pull/39945)\\n\\n### Async Materialized View\\n\\n- Fixes the inability to use `SHOW CREATE MATERIALIZED VIEW` on follower FEs. [#38794](https://github.com/apache/doris/pull/38794)\\n\\n- Unifies the object type of asynchronous materialized views in metadata as tables to enable proper display in data tools. [#38797](https://github.com/apache/doris/pull/38797)\\n\\n- Resolves the issue where nested asynchronous materialized views always perform full refreshes. [#38698](https://github.com/apache/doris/pull/38698)\\n\\n- Fixes the issue where canceled tasks may show as running after restarting FEs. [ #39424](https://github.com/apache/doris/pull/39424)\\n\\n- Addresses incorrect use of contexts, which may lead to unexpected failures of materialized view refresh tasks. [#39690](https://github.com/apache/doris/pull/39690)\\n\\n- Resolves issues that may cause varchar type write failures due to unreasonable lengths when creating asynchronous materialized views based on external tables.[#37668](https://github.com/apache/doris/pull/37668)\\n\\n- Fixes the potential invalidation of asynchronous materialized views based on external tables after FE restarts or catalog rebuilds. [#39355](https://github.com/apache/doris/pull/39355)\\n\\n- Prohibits the use of partition rollup for materialized views with list partitions to prevent the generation of incorrect data. [#38124](https://github.com/apache/doris/pull/38124)\\n\\n- Fixes incorrect results when literals exist in the select list during transparent rewriting for aggregation rollup. [#38958](https://github.com/apache/doris/pull/38958)\\n\\n- Addresses potential errors during transparent rewriting when queries contain filters like `a = a`. [#39629](https://github.com/apache/doris/pull/39629)\\n\\n- Fixes issues where transparent rewriting for direct external table queries fails. [#39041](https://github.com/apache/doris/pull/39041)\\n\\n### Semi-Structured Data Management\\n\\n- Removes support for prepared statements in the old optimizer. [#39465](https://github.com/apache/doris/pull/39465)\\n\\n- Fixes issues with JSON escape character handling. [#37251](https://github.com/apache/doris/pull/37251)\\n\\n- Resolves issues with duplicate processing of JSON fields. [#38490](https://github.com/apache/doris/pull/38490)\\n\\n- Fixes issues with some ARRAY and MAP functions. [#39307](https://github.com/apache/doris/pull/39307) [#39699](https://github.com/apache/doris/pull/39699) [#39757](https://github.com/apache/doris/pull/39757)\\n\\n- Resolves complex combinations of inverted index queries and LIKE queries. [#36687](https://github.com/apache/doris/pull/36687)\\n\\n### Query Optimizer\\n\\n- Fixed the potential partition pruning error issue when the \'OR\' condition exists in partition filter conditions. [#38897](https://github.com/apache/doris/pull/38897)\\n\\n- Fixed the potential partition pruning error issue when complex expressions are involved. [#39298](https://github.com/apache/doris/pull/39298)\\n\\n- Fixed the issue where nullable in `agg_state` subtypes might be planned incorrectly, leading to execution errors. [#37489](https://github.com/apache/doris/pull/37489)\\n\\n- Fixed the issue where nullable in set operation operators might be planned incorrectly, leading to execution errors. [#39109](https://github.com/apache/doris/pull/39109)\\n\\n- Fixed the incorrect execution priority issue of intersect operator. [#39095](https://github.com/apache/doris/pull/39095)\\n\\n- Fixed the NPE issue that may occur when the maximum valid date literal exists in the query. [#39482](https://github.com/apache/doris/pull/39482)\\n\\n- Fixed the occasional planning error that results in an illegal slot error during execution. [#39640](https://github.com/apache/doris/pull/39640)\\n\\n- Fixed the issue where repeatedly referencing columns in cte may lead to missing data in some columns in the result. [#39850](https://github.com/apache/doris/pull/39850)\\n\\n- Fixed the occasional planning error issue when \'case when\' exists in the query. [#38491](https://github.com/apache/doris/pull/38491)\\n\\n- Fixed the issue where IP types cannot be implicitly converted to string types. [#39318](https://github.com/apache/doris/pull/39318)\\n\\n- Fixed the potential planning error issue when using multi-dimensional aggregation and the same column and its alias exist in the select list. [ #38166](https://github.com/apache/doris/pull/38166)\\n\\n- Fixed the issue where boolean types might be handled incorrectly when using BE constant folding. [#39019](https://github.com/apache/doris/pull/39019)\\n\\n- Fixed the planning error issue caused by `default_cluster`: as a prefix for the database name in expressions. [#39114](https://github.com/apache/doris/pull/39114)\\n\\n- Fixed the potential deadlock issue caused by` insert into`. [#38660](https://github.com/apache/doris/pull/38660)\\n\\n- Fixed the potential planning error issue caused by not holding table locks throughout the planning process. [#38950](https://github.com/apache/doris/pull/38950)\\n\\n- Fixed the issue where CHAR(0), VARCHAR(0) are not handled correctly when creating tables. [#38427](https://github.com/apache/doris/pull/38427)\\n\\n- Fixed the issue where `show create table` may incorrectly display hidden columns. [#38796](https://github.com/apache/doris/pull/38796)\\n\\n- Fixed the issue where columns with the same name as hidden columns are not prohibited when creating tables. [#38796](https://github.com/apache/doris/pull/38796)\\n\\n- Fixed the occasional planning error issue when executing `insert into as select` with CTEs. [#38526](https://github.com/apache/doris/pull/38526)\\n\\n- Fixed the issue where `insert into values` cannot automatically fill null default values. **[[fix](Nereids) fix insert into table with null literal default value #39122](https://github.com/apache/doris/pull/39122)**\\n\\n- Fixed the NPE issue caused by using cte in delete without using it. [#39379](https://github.com/apache/doris/pull/39379)\\n\\n- Fixed the issue where deleting from a randomly distributed aggregation model table fails. [#37985](https://github.com/apache/doris/pull/37985)\\n\\n### Query Execution\\n\\n- Fixed the issue where the pipeline execution engine gets stuck in multiple scenarios, causing queries not to end. [#38657](https://github.com/apache/doris/pull/38657) [#38206](https://github.com/apache/doris/pull/38206) [#38885](https://github.com/apache/doris/pull/38885)\\n\\n- Fixed the coredump issue caused by null and non-null columns in set difference calculations.[#38737](https://github.com/apache/doris/pull/38737)\\n\\n- Fixed the incorrect result issue of the `width_bucket` function. [#37892](https://github.com/apache/doris/pull/37892)\\n\\n- Fixed the query error issue when a single row of data is large and the result set is also large (exceeding 2GB). [#37990](https://github.com/apache/doris/pull/37990)\\n\\n- Fixed the incorrect result issue of `stddev` with DecimalV2 type. [#38731](https://github.com/apache/doris/pull/38731)\\n\\n- Fixed the coredump issue caused by the `MULTI_MATCH_ANY` function. [#37959](https://github.com/apache/doris/pull/37959)\\n\\n- Fixed the issue where `insert overwrite auto partition` causes transaction rollback. [#38103](https://github.com/apache/doris/pull/38103)\\n\\n- Fixed the incorrect result issue of the `convert_tz` function. [#37358](https://github.com/apache/doris/pull/37358) [#38764](https://github.com/apache/doris/pull/38764)\\n\\n- Fixed the coredump issue when using the `collect_set` function with window functions. [#38234](https://github.com/apache/doris/pull/38234)\\n\\n- Fixed the coredump issue caused by the mod function with abnormal input. [#37999](https://github.com/apache/doris/pull/37999)\\n\\n- Fixed the issue where executing the same expression in multiple threads may lead to incorrect Java UDF results. [#38612](https://github.com/apache/doris/pull/38612)\\n\\n- Fixed the overflow issue caused by the incorrect return type of the `conv` function. [#38001](https://github.com/apache/doris/pull/38001)\\n\\n- Fixed the unstable result issue of the histogram function. [#38608](https://github.com/apache/doris/pull/38608)\\n\\n### Backup & Recovery / CCR\\n\\n- Fixed the issue where the data version after backup and recovery may be incorrect, leading to unreadability. [#38343](https://github.com/apache/doris/pull/38343)\\n\\n- Fixed the issue of using restore version across versions. [#38396](https://github.com/apache/doris/pull/38396)\\n\\n- Fixed the issue where the job is not canceled when backup fails. [#38993](https://github.com/apache/doris/pull/38993)\\n\\n- Fixed the NPE issue in ccr during the upgrade from 2.1.4 to 2.1.5, causing the FE to fail to start. [#39910](https://github.com/apache/doris/pull/39910)\\n\\n- Fixed the issue where views and materialized views cannot be used after restoration. [#38072](https://github.com/apache/doris/pull/38072) [#39848](https://github.com/apache/doris/pull/39848)\\n\\n### Storage Management\\n\\n- Fixed possible memory leaks in routine load when loading multiple tables from a single stream. [#38824](https://github.com/apache/doris/pull/38824)\\n\\n- Fixed the issue where delimiters and escape characters in routine load were not effective. [#38825](https://github.com/apache/doris/pull/38825)\\n\\n- Fixed incorrectly show routine load results when the routine load task name contained uppercase letters. [#38826](https://github.com/apache/doris/pull/38826)\\n\\n- Fixed the issue where the offset cache was not reset when changing the routineload topic. [#38474](https://github.com/apache/doris/pull/38474)\\n\\n- Fixed the potential exception triggered by show routineload under concurrent scenarios. [#39525](https://github.com/apache/doris/pull/39525)\\n\\n- Fixed the issue where routine load might import data repeatedly. [#39526](https://github.com/apache/doris/pull/39526)\\n\\n- Fixed the data error caused by `setNull` when enabling group commit via JDBC. [#38276](https://github.com/apache/doris/pull/38276)\\n\\n- Fixed the potential NPE issue when enabling group commit insert to a non-master FE. [#38345](https://github.com/apache/doris/pull/38345)\\n\\n- Fixed incorrect error handling during internal data writing in group commit. [#38997](https://github.com/apache/doris/pull/38997)\\n\\n- Fixed the coredump that might be triggered when the group commit execution plan failed. [#39396](https://github.com/apache/doris/pull/39396)\\n\\n- Fixed the issue where concurrent imports into auto partition tables might report non-existent tablets. [#38793](https://github.com/apache/doris/pull/38793)\\n\\n- Fixed potential load stream leakage issues. [#39039](https://github.com/apache/doris/pull/39039)\\n\\n- Fixed the issue where transactions were opened for `insert into select` with no data. [#39108](https://github.com/apache/doris/pull/39108)\\n\\n- Ignored the single-replica import configuration when using memtable prefetching. [#39154](https://github.com/apache/doris/pull/39154)\\n\\n- Fixed the issue where background imports of stream load records might be abnormally aborted upon encountering db deletion. [#39527](https://github.com/apache/doris/pull/39527)\\n\\n- Fixed inaccurate error messages when data errors occurred in strict mode. [#39587](https://github.com/apache/doris/pull/39587)\\n\\n- Fixed the issue where streamload did not return an error URL upon encountering erroneous data. [#38417](https://github.com/apache/doris/pull/38417)\\n\\n- Fixed the issue with the combined use of insert overwrite and auto partition. [#38442](https://github.com/apache/doris/pull/38442)\\n\\n- Fixed parsing errors when CSV encountered data where the line delimiter was enclosed by the enclosing character. [#38445](https://github.com/apache/doris/pull/38445)\\n\\n### Data Exporting\\n\\n- Fixed the issue where enabling the delete_existing_files property during export operations might result in duplicate deletion of exported data. [#39304](https://github.com/apache/doris/pull/39304))\\n\\n### Permissions\\n\\n- Fixed the incorrect requirement of ALTER TABLE permission when creating a materialized view. [#38011](https://github.com/apache/doris/pull/38011)\\n\\n- Fixed the issue where the db was explicitly displayed as empty when showing routine load. [#38365](https://github.com/apache/doris/pull/38365)\\n\\n- Fixed the incorrect requirement of CREATE permission on the original table when using CREATE TABLE LIKE. [#37879](https://github.com/apache/doris/pull/37879)\\n\\n- Fixed the issue where grant operations did not check if the object existed. [#39597](https://github.com/apache/doris/pull/39597)\\n\\n## Upgrade suggestions\\n\\nWhen upgrading Doris, please follow the principle of not skipping two minor versions and upgrade sequentially.\\n\\nFor example, if you are upgrading from version 0.15.x to 2.0.x, it is recommended to first upgrade to the latest version of 1.1, then upgrade to the latest version of 1.2, and finally upgrade to the latest version of 2.0.\\n\\nFor more upgrade information, see the documentation: [Cluster Upgrade](../../admin-manual/cluster-management/upgrade)"},{"id":"/ai-unicorn-minimax-from-loki-and-built-a-pb-scale-logging-system-with-doris","metadata":{"permalink":"/blog/ai-unicorn-minimax-from-loki-and-built-a-pb-scale-logging-system-with-doris","source":"@site/blog/ai-unicorn-minimax-from-loki-and-built-a-pb-scale-logging-system-with-doris.md","title":"Creator of Talkie migrated from Loki and built a PB-scale logging system with Apache Doris","description":"Serving a PB-scale data size with over 99.9% availability, Apache Doris is the vital signs monitor of MiniMax, the maker of Talkie-the soulful AI that is sweeping the world.","date":"2024-08-29T00:00:00.000Z","tags":[{"inline":true,"label":"Best Practice","permalink":"/blog/tags/best-practice"}],"hasTruncateMarker":false,"authors":[{"name":"velodb.io \xb7 VeloDB Engineering Team","key":null,"page":null}],"frontMatter":{"title":"Creator of Talkie migrated from Loki and built a PB-scale logging system with Apache Doris","summary":"Serving a PB-scale data size with over 99.9% availability, Apache Doris is the vital signs monitor of MiniMax, the maker of Talkie-the soulful AI that is sweeping the world.","description":"Serving a PB-scale data size with over 99.9% availability, Apache Doris is the vital signs monitor of MiniMax, the maker of Talkie-the soulful AI that is sweeping the world.","date":"2024-08-29","externalLink":"https://www.velodb.io/blog/883","author":"velodb.io \xb7 VeloDB Engineering Team","tags":["Best Practice"],"image":"/images/minimax-creator-of-talkie.jpeg"},"unlisted":false,"prevItem":{"title":"Apache Doris 2.1.6 just released","permalink":"/blog/release-note-2.1.6"},"nextItem":{"title":"Apache Doris 3.0.1 just released","permalink":"/blog/release-note-3.0.1"}},"content":"\x3c!-- \\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\nYou might not have heard of [MiniMax](https://www.minimaxi.com/en), but you\'ve probably heard of [Talkie](https://www.talkie-ai.com), the AI chatbot taking the world by storm. In the summer of 2024, Talkie ranked fifth among the most-downloaded free entertainment apps in the US, bringing its creator, MiniMax, into the global spotlight. As the maker of this market-stunning product, the AI unicorn maintains a low profile, but its history of smart decision-making dates back a long way.\\n\\nBefore MoE (Mixture of Experts) became an industry consensus, MiniMax has been investing most of its efforts in it. In April 2024, MiniMax launched its first commercially deployed MoE-based LLM, **MiniMax-abab 6.5**, which contains over a trillion parameters and delivers performances comparable to GPT-4, Claude-3, and Gemini-1.5. \\n\\nAs their LLM is getting more complex and called upon more frequently, it generates an exploding amount of logs from model training and inference. These logs provide the basis for performance monitoring, optimization, and troubleshooting. The existing Grafana Loki-based logging system of MiniMax faced performance and stability issues, so they planned for an upgrade. After looking at the common industry solutions, they came to Apache Doris.\\n\\n**Now, all of MiniMax\'s business lines have been integrated with the Apache Doris-based logging system, which serves a PB-scale data size with over 99.9% availability. The query latency on 100 million logs is within seconds.**\\n\\n## The old Grafana Loki-based logging system\\n\\nThe design of Loki, an open-source log aggregation system, was inspired by Prometheus and developed by the Grafana Labs team. It does not have an indexing structure, but instead builds indexes only on log labels and metadata. \\n\\nThe major components of a Loki-based system typically include:\\n\\n- **Loki**: the main server responsible for log storage and querying.\\n\\n- **Promtail**: the agent layer for collecting logs and sending them to Loki.\\n\\n- **Grafana**: for user interface visualization.\\n\\nTo deploy Grafana Loki, each cluster should be deployed with a complete set of log collectors and Loki log storage/query services. \\n\\nLoki uses an Index + Chunk design for log storage, where during ingestion, the different log streams are dispersed across various Ingesters based on a hash of the log labels, and the Ingesters are responsible for writing the log data to object storage. During querying, the Querier retrieves the relevant Chunks from the object storage based on the Index, and then performs the log matching.\\n\\n![The old Grafana Loki-based logging system](/images/the-old-grafana-Loki-based-logging-system.png)\\n\\nAlthough Grafana Loki is positioned as a lightweight, horizontally scalable, and highly available log management system, it still faces some challenges in practical business use:\\n\\n- **Excessive query resource consumption**: Loki does not create indexes based on the log, but instead, it only performs preliminary filtering of logs at the label granularity. Thus, for searches on the logs, it applies the query mechanism to perform full-text regular expression matching on the entire log data set. This operation can lead to spikes in resource consumption, including CPU, memory, and network bandwidth. As the volume of data being queried and the query per second (QPS) increases, Loki shows increasingly intolerable resource consumption and instability.\\n\\n- **Complex architecture**: In addition to the modules shown in the above diagram, Loki also includes components like the Index Gateway, Memcache, and Compactor. The large number of architectural components makes the system challenging to operate and manage, and complex to configure.\\n\\n- **High maintenance cost and difficulty**: MiniMax has a large number of deployed clusters, and each cluster has differences in its system, resources, storage, and network environments. The need to deploy an independent Loki architecture in each cluster adds to the maintenance difficulty.\\n\\n## Why Apache Doris\\n\\nAs one of the most data-intensive industries, AI use cases are characterized by long processing pipelines, abundant contextual data, and large per-request data volumes. Thus, the log size the MiniMax generates far exceeds those of non-AI software products of the same user base. The gigantic log size of MiniMax requires their logging system to be:\\n\\n- **High-performance**: They need the system to return query results on 100 million log entries within seconds.\\n\\n- **Flexible**: The system should support log alerting and log metric queries, such as generating statistical trend lines for key terms.\\n\\n- **Low-cost**: The petabyte-scale raw log data continues to grow, so it\'s a make-or-break factor to keep the storage and computational costs within reasonable bounds.\\n\\nAfter an evaluation of mature logging system architectures in the industry, MiniMax identified the following key components typically found in leading log management solutions: \\n\\n- **Collection agent**: collecting logs from service standard outputs and pushing the data into a central message queue.\\n\\n- **Message queue**: decoupling upstream and downstream components, absorbing spikes, and ensuring system stability even when downstream components are unavailable.\\n\\n- **Storage and query middleware**: storing and querying the log data. In a logging system, this middleware should be capable of inverted indexing to support efficient log searches.\\n\\nMiniMax decided to use iLogtail for the collection agents, Kafka for the message queue, and Apache Doris as the storage and query middleware. In selecting the storage middleware, MiniMax compared the representative technologies of Apache Doris and Elasticsearch.\\n\\nBased on such reference architecture, MiniMax decided to use iLogtail as the collection agent, Apache Kafka for the message queue, and **[Apache Doris](https://doris.apache.org) as the storage and query middleware**. The middleware decision was made after comparing Apache Doris and Elasticsearch.\\n\\n![Why Apache Doris](/images/why-Apache-Doris.png)\\n\\nApache Doris shows competitiveness in cost and performance. It stands out particularly in storage efficiency, write throughput, and aggregation. Additionally, its compatibility with the MySQL syntax makes it more user-friendly.\\n\\n## Apache Doris-based logging system\\n\\n![Apache Doris-based logging system](/images/apache-doris-based-logging-system.png)\\n\\nThe new logging system of MiniMax, called Mlogs, is more streamlined, with a single architecture serving all clusters. The upper layer acts as the control plane for the logging system, which consists of the encapsulation of log query interfaces and the module for automatic configuration generation and distribution. The lower layer represents the data plane of the logging system, containing the log collection agent, message queue, log writer, and the **Apache Doris** database.\\n\\nLogs generated by the cluster services are collected by iLogtail and pushed to Kafka. Part of these logs is pulled from Kafka by the Mlogs Ingester and written to the Doris cluster via the Stream Load method of Apache Doris. The rest is directly subscribed to in real-time by Doris via Routine Load, pulling the message stream from Kafka. **Ultimately, Apache Doris handles the storage and querying of all log data, eliminating the need for separate deployments for each cluster.**\\n\\n## Hands-on experience from MiniMax\\n\\n**Log ingestion**\\n\\nThe new architecture utilizes both the Routine Load and Stream Load methods of Apache Doris. Routine Load is ready to use out of the box and can directly handle JSON logs without the need for additional parsing. For more complex logs that require filtering and processing, MiniMax has introduced a log writer called Mlogs Ingester between Kafka and Doris. The Mlogs Ingester parses and processes the logs before writing them to Doris via Stream Load.\\n\\n**Log search**\\n\\nFor log searches, MiniMax utilizes the inverted indexes and full-text regular expression query capabilities of Apache Doris.\\n\\n- The inverted index of Apache Doris fits into a wide range of use cases and delivers high query performance. It\'s mainly used in `MATCH` and `MATCH_PHRASE` queries.\\n\\n- Full-text regular expression query (`REGEXP`) provides higher precision but lower performance than token-based queries. It is suitable for smaller-scale queries where precision is critical.\\n\\n**Performance improvement**\\n\\nMiniMax implements **query truncation** to further accelerate queries. Log data is arranged linearly in chronological order. If a query requests data of a large range, it can consume excessive computation, storage, and network resources and potentially lead to query timeouts or even system unavailability. So they set and truncate the time range of the queries to prevent overly broad queries, and pre-calculate the data volume for all tables every 15 minutes to dynamically estimate the maximum queryable time range across different tables.\\n\\n**Cost control**\\n\\nTo cut down storage costs, MiniMax utilizes the **[tiered storage](https://doris.apache.org/docs/table-design/cold-hot-separation/)** capabilities of Apache Doris. They define data within the last 7 days as hot data and data older than 7 days as cold data. Data will be moved to object storage as soon as it turns cold. Furthermore, they archive object storage data that is over 30 days old and only restore the archived data when necessary.\\n\\n## Value to MiniMax\\n\\nNow, the Apache Doris-based logging system has been supporting all business line log data within MiniMax, serving a **PB-scale data size** with over **99.9% availability**. It has also brought the following values to MiniMax:\\n\\n- **Simplified architecture**: The new system is easier to deploy and allows a single framework to serve all clusters. This reduces maintenance and management complexity, thus saving operational manpower and costs.\\n\\n- **Fast query response**: The new system can respond to keyword searches and aggregation queries from 1 billion log records within 2 seconds. Most log queries can return results within seconds, too.\\n\\n- **High write performance**: With the current hardware setups, the system can deliver a log write throughput of 10 GB/s, while maintaining data latency within seconds.\\n\\n- **Low storage costs**: The data compression ratio reaches 5:1 and tiered storage further reduces storage costs by 70%.\\n\\n## What\'s next\\n\\nAfter a successful initial experience with Apache Doris, MiniMax proceeds with the next phase of its upgrade plan, which includes the following efforts:\\n\\n- **Log pre-processing**: introduce log sampling and structuring to improve data usability and storage efficiency.\\n\\n- **Tracing**: integrate the logging system with other observability systems (monitoring, alerting, tracing, etc.) to provide comprehensive operational insights.\\n\\n- **Lakehousing**: expand the use of Apache Doris include big data processing and analysis within MiniMax, laying the foundation for a data lakehouse.\\n\\nIf you have any questions or require assistance regarding Apache Doris, join the [community](https://join.slack.com/t/apachedoriscommunity/shared_invite/zt-2gmq5o30h-455W226d79zP3L96ZhXIoQ)."},{"id":"/release-note-3.0.1","metadata":{"permalink":"/blog/release-note-3.0.1","source":"@site/blog/release-note-3.0.1.md","title":"Apache Doris 3.0.1 just released","description":"In this version, Apache Doris has improvements in compute-storage decoupling, lakehouse, semi-structured data analysis and more.","date":"2024-08-23T00:00:00.000Z","tags":[{"inline":true,"label":"Release Notes","permalink":"/blog/tags/release-notes"}],"hasTruncateMarker":false,"authors":[{"name":"Apache Doris","key":null,"page":null}],"frontMatter":{"title":"Apache Doris 3.0.1 just released","summary":"In this version, Apache Doris has improvements in compute-storage decoupling, lakehouse, semi-structured data analysis and more.","description":"In this version, Apache Doris has improvements in compute-storage decoupling, lakehouse, semi-structured data analysis and more.","date":"2024-08-23","author":"Apache Doris","tags":["Release Notes"],"image":"/images/3.0.1.jpg"},"unlisted":false,"prevItem":{"title":"Creator of Talkie migrated from Loki and built a PB-scale logging system with Apache Doris","permalink":"/blog/ai-unicorn-minimax-from-loki-and-built-a-pb-scale-logging-system-with-doris"},"nextItem":{"title":"Automatic and flexible data sharding: Auto Partition in Apache Doris","permalink":"/blog/auto-partition-in-apache-doris"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n  http://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n\\nDear community members, the Apache Doris 3.0.1 version was officially released on August 23, 2024, featuring updates and improvements in compute-storage decoupling, lakehouse, semi-structured data analysis, asynchronous materialized views, and more.\\n\\n**Quick Download:** https://doris.apache.org/download/\\n\\n**GitHub Release:** https://github.com/apache/doris/releases\\n\\n## Behavior Changes\\n\\n### Query Optimizer\\n\\n- Added the variable `use_max_length_of_varchar_in_ctas` to control the length behavior of VARCHAR type when executing `CREATE TABLE AS SELECT` (CTAS) operations.  [#37069](https://github.com/apache/doris/pull/37069)\\n  \\n  - This variable is set to true by default. \\n  \\n  - When set to true, if the VARCHAR type column originates from a table, the derived length is used; otherwise, the maximum length is used. \\n  \\n  - When set to false, the VARCHAR type will always use the derived length.\\n\\n- All data types will now be displayed in lowercase to maintain compatibility with MySQL format. [#38012](https://github.com/apache/doris/pull/38012)\\n\\n- Multiple query statements in the same query request must now be separated by semicolons. [#38670](https://github.com/apache/doris/pull/38670)\\n\\n### Query Execution\\n\\n- The default number of parallel tasks after shuffle operations in the cluster is set to 100, which will improve query stability and concurrent processing capability in large clusters. [#38196](https://github.com/apache/doris/pull/38196)\\n\\n### Storage\\n\\n- The default value of `trash_file_expire_time_sec` has been changed from 86400 seconds to 0 seconds, which means that if files are deleted by mistake and the FE trash is cleared, the data cannot be recovered.\\n\\n- The table attribute `enable_mow_delete_on_delete_predicate` (introduced in version 3.0.0) has been renamed to `enable_mow_light_delete`.\\n\\n- Explicit transactions are now prohibited from performing delete operations on tables with written data.\\n\\n- Heavy schema change operations are prohibited on tables with auto-increment fields.\\n\\n\\n\\n## New Features\\n\\n### Job Scheduling\\n\\n- Optimized the execution logic of internal scheduling jobs, decoupling the strong association between start time and immediate execution parameters. Now, tasks can be created with a specified start time or selected for immediate execution, without conflict, enhancing scheduling flexibility. [#36805](https://github.com/apache/doris/pull/36805)\\n\\n### Compute-Storage Decoupled\\n\\n- Supports dynamic modification of the upper limit for file cache usage. [#37484](https://github.com/apache/doris/pull/37484)\\n\\n- Recycler now supports object storage rate limiting and server-side rate limiting retry functionality. [#37663](https://github.com/apache/doris/pull/37663) [#37680](https://github.com/apache/doris/pull/37680)\\n\\n### Lakehouse\\n\\n- Added the session variable `serde_dialect` to set the output format for complex types. [#37039](https://github.com/apache/doris/pull/37039)\\n\\n- SQL interception now supports external tables.\\n\\n  - For more information, refer to the documentation on  [SQL Interception](https://doris.apache.org/docs/admin-manual/query-admin/sql-interception).\\n\\n- Insert overwrite now supports Iceberg tables. [#37191](https://github.com/apache/doris/pull/37191)\\n\\n### Asynchronous Materialized Views\\n\\n- Supports partition roll-up and build at the hourly level. [#37678](https://github.com/apache/doris/pull/37678)\\n\\n- Supports atomic replacement of asynchronous materialized view definition statements. [#36749](https://github.com/apache/doris/pull/36749)\\n\\n- Transparent rewriting now supports Insert statements. [#38115](https://github.com/apache/doris/pull/38115)\\n\\n- Transparent rewriting now supports the VARIANT type. [#37929](https://github.com/apache/doris/pull/37929)\\n\\n### Query Execution\\n\\n- The group concat function now supports DISTINCT and ORDER BY options. [#38744](https://github.com/apache/doris/pull/38744)\\n\\n### Semi-Structured Data Management\\n\\n- The ES Catalog now maps `nested` or `object` types in Elasticsearch to the JSON type in Doris. [#37101](https://github.com/apache/doris/pull/37101)\\n\\n- Added the `MULTI_MATCH` function, which supports matching keywords across multiple fields and can leverage inverted indexes to accelerate searches. [#37722](https://github.com/apache/doris/pull/37722)\\n\\n- Added the `explode_json_object` function, which can unfold objects in JSON data into multiple rows. [#36887](https://github.com/apache/doris/pull/36887)\\n\\n- Inverted indexes now support memtable advancement, requiring index construction only once during multi-replica writes, reducing CPU consumption and improving performance. [#35891](https://github.com/apache/doris/pull/35891)\\n\\n- Added `MATCH_PHRASE` support for positive slop, e.g., `msg MATCH_PHRASE \'a b 2+\'` can match instances containing words a and b with a slop of no more than two, and a preceding b; regular slop without the final `+` does not guarantee this order. [#36356](https://github.com/apache/doris/pull/36356)\\n\\n### Other\\n\\n- Added the FE parameter `skip_audit_user_list`, where user operations specified in this configuration will not be recorded in the audit log. [#38310](https://github.com/apache/doris/pull/38310)\\n\\n  - For more information, refer to the documentation on [Audit Plugin](https://doris.apache.org/docs/admin-manual/audit-plugin/).\\n\\n\\n\\n## Improvements\\n\\n### Storage\\n\\n- Reduced the likelihood of write failures caused by disk balancing within a single BE. [#38000](https://github.com/apache/doris/pull/38000)\\n\\n- Decreased memory consumption by the memtable limiter. [#37511](https://github.com/apache/doris/pull/37511)\\n\\n- Moved old partitions to the FE trash during partition replacement operations. [#36361](https://github.com/apache/doris/pull/36361)\\n\\n- Optimized memory consumption during compaction. [#37099](https://github.com/apache/doris/pull/37099)\\n\\n- Added a session variable to control audit logs for JDBC PreparedStatement, with default setting to not print. [#38419](https://github.com/apache/doris/pull/38419)\\n\\n- Optimized the logic for selecting BEs for group commits. [#35558](https://github.com/apache/doris/pull/35558)\\n\\n- Improved the performance of column updates. [#38487](https://github.com/apache/doris/pull/38487)\\n\\n- Optimized the use of `delete bitmap cache`. [#38761](https://github.com/apache/doris/pull/38761)\\n\\n- Added a configuration to control query affinity during hot and cold tiering. [#37492](https://github.com/apache/doris/pull/37492)\\n\\n### Compute-Storage Decoupled\\n\\n- Implemented automatic retries when encountering object storage server rate limiting. [#37199](https://github.com/apache/doris/pull/37199)\\n\\n- Adapted the number of threads for memtable flush in the compute-storage decoupled mode. [#38789](https://github.com/apache/doris/pull/38789)\\n\\n- Added Azure as a compile option to support compilation in environments without Azure support.\\n\\n- Optimized the observability of object storage access rate limiting. [#38294](https://github.com/apache/doris/pull/38294)\\n\\n- Allowed the file cache TTL queue to perform LRU eviction, enhancing TTL queue usability. [#37312](https://github.com/apache/doris/pull/37312)\\n\\n- Optimized the number of balance writeeditlog IO operations in the storage and compute separation mode. [#37787](https://github.com/apache/doris/pull/37787)\\n\\n- Improved table creation speed in the storage and compute separation mode by sending tablet creation requests in batches. [#36786](https://github.com/apache/doris/pull/36786)\\n\\n- Optimized read failures caused by potential inconsistencies in the local file cache through backoff retries. [#38645](https://github.com/apache/doris/pull/38645)\\n\\n### Lakehouse\\n\\n- Optimized memory statistics for Parquet/ORC format read and write operations. [#37234](https://github.com/apache/doris/pull/37234)\\n\\n- Trino Connector Catalog now supports predicate pushdown. [#37874](https://github.com/apache/doris/pull/37874)\\n\\n- Added a session variable `enable_count_push_down_for_external_table` to control whether to enable `count(*)` pushdown optimization for external tables. [#37046](https://github.com/apache/doris/pull/37046)\\n\\n- Optimized the read logic for Hudi snapshot reads, returning an empty set when the snapshot is empty, consistent with Spark behavior. [#37702](https://github.com/apache/doris/pull/37702)\\n\\n- Improved the read performance of partition columns for Hive tables. [#37377](https://github.com/apache/doris/pull/37377)\\n\\n### Asynchronous Materialized Views\\n\\n- Improved transparent rewrite plan speed by 20%. [#37197](https://github.com/apache/doris/pull/37197)\\n\\n- Eliminated roll-up during transparent rewrite if the group key satisfies data uniqueness for better nested matching. [#38387](https://github.com/apache/doris/pull/38387)\\n\\n- Transparent rewrite now performs better aggregation elimination to improve the matching success rate of nested materialized views. [#36888](https://github.com/apache/doris/pull/36888)\\n\\n### MySQL Compatibility\\n\\n- Now correctly populates the database name, table name, and original name in the MySQL protocol result columns. [#38126](https://github.com/apache/doris/pull/38126)\\n\\n- Supported the hint format `/*+ func(value) */`. [#37720](https://github.com/apache/doris/pull/37720)\\n\\n### Query Optimizer\\n\\n- Significantly improved the plan speed for complex queries. [#38317](https://github.com/apache/doris/pull/38317)\\n\\n- Adaptively chose whether to perform bucket shuffle based on the number of data buckets to avoid performance degradation in extreme cases. [#36784](https://github.com/apache/doris/pull/36784)\\n\\n- Optimized the cost estimation logic for SEMI / ANTI JOIN. [#37951](https://github.com/apache/doris/pull/37951) [#37060](https://github.com/apache/doris/pull/37060)\\n\\n- Supported pushing Limit down to the first stage of aggregation to improve performance. [#34853](https://github.com/apache/doris/pull/34853)\\n\\n- Partition pruning now supports filter conditions containing the `date_trunc` or `date` function. [#38025](https://github.com/apache/doris/pull/38025) [#38743](https://github.com/apache/doris/pull/38743)\\n\\n- SQL cache now supports query scenarios that include user variables. [#37915](https://github.com/apache/doris/pull/37915)\\n\\n- Optimized error messages for invalid aggregation semantics. [#38122](https://github.com/apache/doris/pull/38122)\\n\\n### Query Execution\\n\\n- Adapted AggState compatibility from 2.1 to 3.x and fixed Coredump issues. [#37104](https://github.com/apache/doris/pull/37104)\\n\\n- Refactored the strategy selection for local shuffle without Join. [#37282](https://github.com/apache/doris/pull/37282)\\n\\n- Modified the scanner for internal table queries to be asynchronous to prevent stalling during such queries. [#38403](https://github.com/apache/doris/pull/38403)\\n\\n- Optimized the block merge process during Hash table construction for Join operators. [#37471](https://github.com/apache/doris/pull/37471)\\n\\n- Optimized the duration of lock holding for MultiCast. [#37462](https://github.com/apache/doris/pull/37462)\\n\\n- Optimized gRPC keepAliveTime and added link monitoring to reduce the probability of query failure due to RPC errors. [#37304](https://github.com/apache/doris/pull/37304)\\n\\n- Cleaned up all dirty pages in jemalloc when memory limits were exceeded. [#37164](https://github.com/apache/doris/pull/37164)\\n\\n- Optimized the processing performance of `aes_encrypt`/`decrypt` functions for constant types. [#37194](https://github.com/apache/doris/pull/37194)\\n\\n- Optimized the processing performance of the `json_extract` function for constant data. [#36927](https://github.com/apache/doris/pull/36927)\\n\\n- Optimized the processing performance of the `ParseUrl` function for constant data. [#36882](https://github.com/apache/doris/pull/36882)\\n\\n### Semi-Structured Data Management\\n\\n- Bitmap indexes now default to using inverted indexes, with `enable_create_bitmap_index_as_inverted_index` set to true by default. [#36692](https://github.com/apache/doris/pull/36692)\\n\\n- In the compute-storage decoupled mode, DESC can now view sub-columns of VARIANT type. [#38143](https://github.com/apache/doris/pull/38143)\\n\\n- Removed the step of checking file existence during inverted index queries to reduce access latency to remote storage. [#36945](https://github.com/apache/doris/pull/36945)\\n\\n- Complex types ARRAY / MAP / STRUCT now support `replace_if_not_null` for AGG tables. [#38304](https://github.com/apache/doris/pull/38304)\\n\\n- Escape characters for JSON data are now supported. [#37176](https://github.com/apache/doris/pull/37176) [#37251](https://github.com/apache/doris/pull/37251)\\n\\n- Inverted index queries now behave consistently on MOW tables and DUP tables. [#37428](https://github.com/apache/doris/pull/37428)\\n\\n- Optimized the performance of inverted index acceleration for IN queries. [#37395](https://github.com/apache/doris/pull/37395)\\n\\n- Reduced unnecessary memory allocation during TOPN queries to improve performance. [#37429](https://github.com/apache/doris/pull/37429)\\n\\n- When creating an inverted index with tokenization, the `support_phrase` option is now automatically enabled to accelerate `match_phrase` series phrase queries. [#37949](https://github.com/apache/doris/pull/37949)\\n\\n### Other\\n\\n- Audit log now can record SQL types. [#37790](https://github.com/apache/doris/pull/37790)\\n\\n- Added support for `information_schema.processlist` to show all FE. [#38701](https://github.com/apache/doris/pull/38701)\\n\\n- Cached ranger\'s `atamask` and `rowpolicy` to accelerate query efficiency. [#37723](https://github.com/apache/doris/pull/37723)\\n\\n- Optimized metadata management in job manager to release locks immediately after modifying metadata, reducing lock holding time. [#38162](https://github.com/apache/doris/pull/38162)\\n\\n\\n\\n## Bug Fixes\\n\\n### Upgrade\\n\\n- Fix the issue where `mtmv load` fails during upgrade from version 2.1. [#38799](https://github.com/apache/doris/pull/38799)\\n\\n- Resolve the issue where `null_type` cannot be found during the upgrade to version 2.1. [#39373](https://github.com/apache/doris/pull/39373)\\n\\n- Address the compatibility issue with permission persistence during the upgrade from version 2.1 to 3.0. [#39288](https://github.com/apache/doris/pull/39288)\\n\\n### Load\\n\\n- Fix the issue where parsing fails when the newline character is surrounded by delimiters in CSV format parsing. [#38347](https://github.com/apache/doris/pull/38347)\\n- Resolve potential exception issues when FE forwards group commit. [#38228](https://github.com/apache/doris/pull/38228) [#38265](https://github.com/apache/doris/pull/38265)\\n\\n- Group commit now supports the new optimizer. [#37002](https://github.com/apache/doris/pull/37002)\\n\\n- Fix the issue where group commit reports data errors when JDBC setNull is used. [#38262](https://github.com/apache/doris/pull/38262)\\n\\n- Optimize the retry logic for group commit when encountering `delete bitmap lock` errors. [#37600](https://github.com/apache/doris/pull/37600)\\n\\n- Resolve the issue where routine load cannot use CSV delimiters and escape characters. [#38402](https://github.com/apache/doris/pull/38402)\\n\\n- Fix the issue where routine load job names with mixed case cannot be displayed. [#38523](https://github.com/apache/doris/pull/38523)\\n\\n- Optimize the logic for actively recovering routine load during FE master-slave switching. [#37876](https://github.com/apache/doris/pull/37876)\\n\\n- Resolve the issue where routine load pauses when all data in Kafka is expired. [#37288](https://github.com/apache/doris/pull/37288)\\n\\n- Fix the issue where `show routine load` returns empty results. [#38199](https://github.com/apache/doris/pull/38199)\\n\\n- Resolve the memory leak issue during multi-table stream import in routine load. [#38255](https://github.com/apache/doris/pull/38255)\\n\\n- Fix the issue where stream load does not return the error URL. [#38325](https://github.com/apache/doris/pull/38325)\\n\\n- Resolve potential load channel leak issues. [#38031](https://github.com/apache/doris/pull/38031) [#37500](https://github.com/apache/doris/pull/37500)\\n\\n- Fix the issue where no error may be reported when importing fewer segments than expected. [#36753](https://github.com/apache/doris/pull/36753)\\n\\n- Resolve the load stream leak issue. [#38912](https://github.com/apache/doris/pull/38912)\\n\\n- Optimize the impact of offline nodes on import operations. [#38198](https://github.com/apache/doris/pull/38198)\\n\\n- Fix the issue where transactions do not end when inserting into empty data. [#38991](https://github.com/apache/doris/pull/38991)\\n\\n### Storage\\n\\n**01 Backup and Restoration**\\n\\n- Fix the issue where tables cannot be written after backup and restoration. [#37089](https://github.com/apache/doris/pull/37089)\\n\\n- Resolve the issue where view database names are incorrect after backup and restoration. [#37412](https://github.com/apache/doris/pull/37412)\\n\\n**02 Compaction**\\n\\n- Fix the issue where cumu compaction handles delete errors incorrectly during ordered data compression. [#38742](https://github.com/apache/doris/pull/38742)\\n\\n- Resolve the issue of duplicate keys in aggregate tables caused by sequential compression optimization. [#38224](https://github.com/apache/doris/pull/38224)\\n\\n- Fix the issue where compression operations cause coredump in large wide tables. [#37960](https://github.com/apache/doris/pull/37960)\\n\\n- Resolve the compression starvation issue caused by inaccurate concurrent statistics of compression tasks. [#37318](https://github.com/apache/doris/pull/37318)\\n\\n**03 MOW Unique Key**\\n\\n- Resolve the issue of inconsistent data between replicas caused by cumulative compression deletion of delete sign. [#37950](https://github.com/apache/doris/pull/37950)\\n\\n- MOW delete now uses partial column updates with the new optimizer. [#38751](https://github.com/apache/doris/pull/38751)\\n\\n- Fix the potential duplicate key issue in MOW tables under compute-storage decoupled. [#39018](https://github.com/apache/doris/pull/39018)\\n\\n- Resolve the issue where MOW unique and duplicate tables cannot modify column order. [#37067](https://github.com/apache/doris/pull/37067)\\n\\n- Fix the potential data correctness issue caused by segcompaction. [#37760](https://github.com/apache/doris/pull/37760)\\n\\n- Resolve the potential memory leak issue during column updates. [#37706](https://github.com/apache/doris/pull/37706)\\n\\n**04 Other**\\n\\n- Fix the small probability of exceptions in TOPN queries. [#39119](https://github.com/apache/doris/pull/39119) [#39199](https://github.com/apache/doris/pull/39199)\\n\\n- Resolve the issue where auto-increment IDs may duplicate during FE restart. [#37306](https://github.com/apache/doris/pull/37306)\\n\\n- Fix the potential queuing issue in the delete operation priority queue. [#37169](https://github.com/apache/doris/pull/37169)\\n\\n- Optimize the delete retry logic. [#37363](https://github.com/apache/doris/pull/37363)\\n\\n- Resolve the issue with `bucket = 0` in table creation statements under the new optimizer. [#38971](https://github.com/apache/doris/pull/38971)\\n\\n- Fix the issue where FE reports success incorrectly when image generation fails. [#37508](https://github.com/apache/doris/pull/37508)\\n\\n- Resolve the issue where using the wrong nodename during FE offline nodes may cause inconsistent FE members. [#37987](https://github.com/apache/doris/pull/37987)\\n\\n- Fix the issue where CCR partition addition may fail. [#37295](https://github.com/apache/doris/pull/37295)\\n\\n- Resolve the `int32` overflow issue in inverted index files. [#38891](https://github.com/apache/doris/pull/38891)\\n\\n- Fix the issue where TRUNCATE TABLE failure may cause BE to fail to go offline. [#37334](https://github.com/apache/doris/pull/37334)\\n\\n- Resolve the issue where publish cannot continue due to null pointers. [#37724](https://github.com/apache/doris/pull/37724) [#37531](https://github.com/apache/doris/pull/37531)\\n\\n- Fix the potential coredump issue when manually triggering disk migration. [#37712](https://github.com/apache/doris/pull/37712)\\n\\n### Compute-Storage Decoupled\\n\\n- Fixed the issue where `show create table` might display the `file_cache_ttl_seconds` attribute twice. [#38052](https://github.com/apache/doris/pull/38052)\\n\\n- Fixed the issue where segment Footer TTL was not set correctly after setting file cache TTL. [#37485](https://github.com/apache/doris/pull/37485)\\n\\n- Fixed the issue where file cache might cause coredump due to massive conversion of cache types. [#38518](https://github.com/apache/doris/pull/38518)\\n\\n- Fixed the potential file descriptor (fd) leak in file cache. [#38051](https://github.com/apache/doris/pull/38051)\\n\\n- Fixed the issue where schema change Job overwriting compaction Job prevented base tablet compaction from completing normally. [#38210](https://github.com/apache/doris/pull/38210)\\n\\n- Fixed the potential inaccuracy of base compaction score due to data race. [#38006](https://github.com/apache/doris/pull/38006)\\n\\n- Fixed the issue where error messages from imports might not be uploaded correctly to object storage. [#38359](https://github.com/apache/doris/pull/38359)\\n\\n- Fixed the inconsistency in return information between compute-storage decoupled mode and storage and compute integration mode for 2PC imports. [#38076](https://github.com/apache/doris/pull/38076)\\n\\n- Fix the issue where incorrect file size setting during file cache warm-up leads to coredump. [#38939](https://github.com/apache/doris/pull/38939)\\n\\n- Fixed the issue where partial column updates did not correctly dequeue delete operations. [#37151](https://github.com/apache/doris/pull/37151)\\n\\n- Fixed compatibility issues with permission persistence in compute-storage decoupled mode. [#38136](https://github.com/apache/doris/pull/38136) [#37708](https://github.com/apache/doris/pull/37708)\\n\\n- Fixed the issue where observer did not retry correctly when encountering a `-230` error. [#37625](https://github.com/apache/doris/pull/37625)\\n\\n- Fixed the issue where `show load` with conditions did not perform correct analysis. [#37656](https://github.com/apache/doris/pull/37656)\\n\\n- Fixed the issue where `show streamload` in compute-storage decoupled mode caused BE coredump. [#37903](https://github.com/apache/doris/pull/37903)\\n\\n- Fixed the issue where `copy into` did not correctly verify column names in strict mode. [#37650](https://github.com/apache/doris/pull/37650)\\n\\n- Fixed the issue where multi-stream imports into a single table lacked permissions. [#38878](https://github.com/apache/doris/pull/38878)\\n\\n- Fixed the potential overflow issue in `getVersionUpdateTimeMs`. [#38074](https://github.com/apache/doris/pull/38074)\\n\\n- Fixed the issue where FE azure blob list was not implemented correctly. [#37986](https://github.com/apache/doris/pull/37986)\\n\\n- Fixed the issue where inaccurate azure blob recycling time calculation prevented recycling. [#37535](https://github.com/apache/doris/pull/37535)\\n\\n- Fixed the issue where inverted index files were not deleted in compute-storage decoupled mode. [#38306](https://github.com/apache/doris/pull/38306)\\n\\n### Lakehouse\\n\\n- Fixed the issue with reading binary data from Oracle Catalog. [#37078](https://github.com/apache/doris/pull/37078)\\n\\n- Fixed the potential deadlock issue when acquiring external table metadata in multi-FE scenarios. [#37756](https://github.com/apache/doris/pull/37756)\\n\\n- Fixed the issue where JNI scanner failure caused BE nodes to crash. [#37697](https://github.com/apache/doris/pull/37697)\\n\\n- Fixed the issue with slow reading of date types from Trino Connector Catalog. [#37266](https://github.com/apache/doris/pull/37266)\\n\\n- Optimized kerberos authentication logic for Hive Catalog. [#37301](https://github.com/apache/doris/pull/37301)\\n\\n- Fixed the issue where region attributes might be parsed incorrectly when parsing MinIO properties. [#37249](https://github.com/apache/doris/pull/37249)\\n\\n- Fixed the issue where creating too many FileSystems by FE caused memory leaks. [#36954](https://github.com/apache/doris/pull/36954)\\n\\n- Fixed the issue with reading incorrect time zone information from Paimon. [#37716](https://github.com/apache/doris/pull/37716)\\n\\n- Fixed the potential thread leak issue caused by Hive write-back operations. [#36990](https://github.com/apache/doris/pull/36990)\\n\\n- Fixed the null pointer issue caused by enabling Hive metastore event synchronization. [#38421](https://github.com/apache/doris/pull/38421)\\n\\n- Fixed the issue where error messages were unclear or caused stalling when creating catalogs. [#37551](https://github.com/apache/doris/pull/37551)\\n\\n- Fixed the issue where reading Hive text format tables behaved differently from Hive. [#37638](https://github.com/apache/doris/pull/37638)\\n\\n- Fixed the logic error when switching between catalogs and databases. [#37828](https://github.com/apache/doris/pull/37828)\\n\\n### MySQL Compatibility\\n\\n- Fixed the issue where certain flags in the MySQL protocol were set incorrectly when SSL was enabled. [#38086](https://github.com/apache/doris/pull/38086)\\n\\n### Asynchronous Materialized Views\\n\\n- Fixed the issue where construction might fail when the base table had a very large number of partitions. [#37589](https://github.com/apache/doris/pull/37589)\\n\\n- Fixed the issue where nested materialized views incorrectly performed full table refreshes even when partition refreshes were possible. [#38698](https://github.com/apache/doris/pull/38698)\\n\\n- Fixed the issue where partition refresh could not handle the simultaneous existence of valid and invalid dependencies when analyzing partition dependencies. [#38367](https://github.com/apache/doris/pull/38367)\\n\\n- Fixed the issue where the final result containing NULL type might cause asynchronous materialized views to fail. [#37019](https://github.com/apache/doris/pull/37019)\\n\\n- Fixed the planning error that might occur during transparent rewriting when both synchronous and asynchronous materialized views with the same name were present. [#37311](https://github.com/apache/doris/pull/37311)\\n\\n### Synchronous Materialized Views\\n\\n- The rewritten synchronous materialized views now can correctly perform partition pruning. [#38527](https://github.com/apache/doris/pull/38527)\\n\\n- When rewriting synchronous materialized views, those with unready data are no longer selected. [#38148](https://github.com/apache/doris/pull/38148)\\n\\n### Query Optimizer\\n\\n- Fixed the deadlock issue that might occur when queries and delete operations are performed simultaneously. [#38660](https://github.com/apache/doris/pull/38660)\\n\\n- Fixed the issue where bucket pruning might incorrectly prune on decimal column buckets. [#37889](https://github.com/apache/doris/pull/37889)\\n\\n- Fixed the issue where planning might be incorrect when mark join participates in join reorder. [#39152](https://github.com/apache/doris/pull/39152)\\n\\n- Fixed the issue where the result is incorrect when the correlation condition of a correlated subquery is not a simple column. [#37644](https://github.com/apache/doris/pull/37644)\\n\\n- Fixed the issue where partition pruning cannot correctly handle or expressions. [#38897](https://github.com/apache/doris/pull/38897)\\n\\n- Fixed the planning error that might occur when optimizing the execution order of JOIN and AGG. [#37343](https://github.com/apache/doris/pull/37343)\\n\\n- Fixed the issue where `str_to_date` performs incorrect constant folding calculations on datev1 types. [#37360](https://github.com/apache/doris/pull/37360)\\n\\n- Fixed the issue where the ACOS function\'s constant folding returns non-NaN values. [#37932](https://github.com/apache/doris/pull/37932)\\n\\n- Fixed the occasional planning error: \\"The children format needs to be [WhenClause+, DefaultValue?]\\". [#38491](https://github.com/apache/doris/pull/38491)\\n\\n- Fixed the issue where planning might be incorrect when the projection includes window functions and there is both the original column and its alias. [#38166](https://github.com/apache/doris/pull/38166)\\n\\n- Fixed the issue where planning might report an error when the aggregation parameter contains a lambda expression. [#37109](https://github.com/apache/doris/pull/37109)\\n\\n- Fixed the insert error that might occur in extreme cases: \\"MultiCastDataSink cannot be cast to DataStreamSink\\". [#38526](https://github.com/apache/doris/pull/38526)\\n\\n- Fixed the issue where the new optimizer does not correctly handle `char(0)/varchar(0)` when creating a table. [#38427](https://github.com/apache/doris/pull/38427)\\n\\n- Fixed the incorrect behavior of `char(255) toSql`. [#37340](https://github.com/apache/doris/pull/37340)\\n\\n- Fixed the issue where the nullable attribute within the `agg_state` type might lead to planning errors. [#37489](https://github.com/apache/doris/pull/37489)\\n- Fixed the issue where row count statistics are inaccurate during mark Join. [#38270](https://github.com/apache/doris/pull/38270)\\n\\n### Query Execution\\n\\n- Fixed issues where the Pipeline execution engine was stuck, causing queries to not end, in multiple scenarios. [#38657](https://github.com/apache/doris/pull/38657), [#38206](https://github.com/apache/doris/pull/38206), [#38885](https://github.com/apache/doris/pull/38885), [#38151](https://github.com/apache/doris/pull/38151), [#37297](https://github.com/apache/doris/pull/37297)\\n\\n- Fixed the coredump issue caused by NULL and non-NULL columns during set difference calculations. [#38750](https://github.com/apache/doris/pull/38750)\\n\\n- Fixed the error when using the DECIMAL type with pure decimals in delete statements. [#37801](https://github.com/apache/doris/pull/37801)\\n\\n- Fixed the issue where the `width_bucket` function returned incorrect results. [#37892](https://github.com/apache/doris/pull/37892)\\n\\n- Fixed the query error when a single row of data was very large and the result set was also large (exceeding 2GB). [#37990](https://github.com/apache/doris/pull/37990)\\n\\n- Fixed the coredump issue caused by incorrect release of rpc connections during single-replica imports. [#38087](https://github.com/apache/doris/pull/38087)\\n\\n- Fixed the coredump issue caused by processing NULL values with the `foreach` function. [#37349](https://github.com/apache/doris/pull/37349)\\n\\n- Fixed the issue where stddev returned incorrect results for DECIMALV2 types. [#38731](https://github.com/apache/doris/pull/38731)\\n\\n- Fixed the slow performance of `bitmap union` calculations. [#37816](https://github.com/apache/doris/pull/37816)\\n\\n- Fixed the issue where RowsProduced for aggregation operators was not set in the profile. [#38271](https://github.com/apache/doris/pull/38271)\\n\\n- Fixed the overflow issue when calculating the number of buckets for the hash table under hash join. [#37193](https://github.com/apache/doris/pull/37193), [#37493](https://github.com/apache/doris/pull/37493)\\n\\n- Fixed the inaccurate recording of the `jemalloc cache memory tracker`. [#37464](https://github.com/apache/doris/pull/37464)\\n\\n- Added the `enable_stacktrace` configuration option, allowing users to control whether exception stacks are output in BE logs. [#37713](https://github.com/apache/doris/pull/37713)\\n\\n- Fixed the issue where Arrow Flight SQL did not work correctly when `enable_parallel_result_sink` was set to false. [#37779](https://github.com/apache/doris/pull/37779)\\n\\n- Fixed the incorrect use of colocate Join. [#37361](https://github.com/apache/doris/pull/37361), [#37729](https://github.com/apache/doris/pull/37729)\\n\\n- Fixed the calculation overflow issue of the `round` function on DECIMAL128 types. [#37733](https://github.com/apache/doris/pull/37733), [#38106](https://github.com/apache/doris/pull/38106)\\n\\n- Fixed the coredump issue when passing a const string to the `sleep` function. [#37681](https://github.com/apache/doris/pull/37681)\\n\\n- Increased the queue length for audit logs, solving the issue where audit logs could not be recorded normally under high concurrency scenarios with thousands of concurrent connections. [#37786](https://github.com/apache/doris/pull/37786)\\n\\n- Fixed the issue where creating a workload group caused too many threads, leading to BE coredump. [#38096](https://github.com/apache/doris/pull/38096)\\n\\n- Fixed the coredump issue caused by the `MULTI_MATCH_ANY` function. [#37959](https://github.com/apache/doris/pull/37959)\\n\\n- Fixed the transaction rollback issue caused by `insert overwrite auto partition`. [#38103](https://github.com/apache/doris/pull/38103)\\n\\n- Fixed the issue where the TimeUtils formatter did not use the correct time zone. [#37465](https://github.com/apache/doris/pull/37465)\\n\\n- Fixed the issue where results were incorrect under constant folding scenarios for week/yearweek. [#37376](https://github.com/apache/doris/pull/37376)\\n\\n- Fixed the issue where the `convert_tz` function returned incorrect results. [#37358](https://github.com/apache/doris/pull/37358), [#38764](https://github.com/apache/doris/pull/38764)\\n\\n- Fixed the coredump issue when using the `collect_set` function with window functions. [#38234](https://github.com/apache/doris/pull/38234)\\n\\n- Fixed the coredump issue caused by `percentile_approx` during rolling upgrades. [#39321](https://github.com/apache/doris/pull/39321)\\n\\n- Fixed the coredump issue caused by the `mod` function when encountering abnormal input. [#37999](https://github.com/apache/doris/pull/37999)\\n\\n- Fixed the issue where the hash table was not fully built when the broadcast join probe started running. [#37643](https://github.com/apache/doris/pull/37643)\\n\\n- Fixed the issue where executing the same expression in multithreaded environments might lead to incorrect results for Java UDFs. [#38612](https://github.com/apache/doris/pull/38612)\\n\\n- Fixed the overflow issue caused by incorrect return types of the `conv` function. [#38001](https://github.com/apache/doris/pull/38001)\\n\\n- Fixed the issue where the `json_replace` function returned incorrect types. [#3701](https://github.com/apache/doris/pull/37014)\\n\\n- Fixed the issue where the nullable attribute setting was unreasonable for the `percentile` aggregation function. [#37330](https://github.com/apache/doris/pull/37330)\\n\\n- Fixed the issue where the results of the `histogram` function were unstable. [#38608](https://github.com/apache/doris/pull/38608)\\n\\n- Fixed the issue where task state was displayed incorrectly in the profile. [#38082](https://github.com/apache/doris/pull/38082)\\n\\n- Fixed the issue where some queries were incorrectly canceled when the system just started. [#37662](https://github.com/apache/doris/pull/37662)\\n\\n### Semi-Structured Data Management\\n\\n- Fix some issues with time series compression. [#39170](https://github.com/apache/doris/pull/39170) [#39176](https://github.com/apache/doris/pull/39176)\\n\\n- Fix the issue of incorrect index size statistics during compression. [#37232](https://github.com/apache/doris/pull/37232)\\n\\n- Fix the potential incorrect matching of ultra-long strings without tokenization in inverted indexes. [#37679](https://github.com/apache/doris/pull/37679) [#38218](https://github.com/apache/doris/pull/38218)\\n\\n- Fix the high memory usage issue of `array_range` and `array_with_const` functions when dealing with large data volumes. [#38284](https://github.com/apache/doris/pull/38284) [#37495](https://github.com/apache/doris/pull/37495)\\n\\n- Fix the potential coredump issue when selecting columns of ARRAY / MAP / STRUCT types. [#37936](https://github.com/apache/doris/pull/37936) \\n\\n- Fix the import failure issue caused by simdjson parsing errors when specifying jsonpath in Stream Load. [#38490](https://github.com/apache/doris/pull/38490)\\n\\n- Fix the exception handling issue when there are duplicate keys in JSON data. [#38146](https://github.com/apache/doris/pull/38146)\\n\\n- Fix the potential query error after DROP INDEX. [#37646](https://github.com/apache/doris/pull/37646)\\n\\n- Fix the error return issue in row merging checks during index compression. [#38732](https://github.com/apache/doris/pull/38732)\\n\\n- Inverted index v2 format now supports renaming columns. [#38079](https://github.com/apache/doris/pull/38079)\\n\\n- Fix the coredump issue when the `MATCH` function matches an empty string without an index. [#37947](https://github.com/apache/doris/pull/37947)\\n\\n- Fix the handling of NULL values in inverted indexes. [#37921](https://github.com/apache/doris/pull/37921) [#37842](https://github.com/apache/doris/pull/37842) [#38741](https://github.com/apache/doris/pull/38741)\\n\\n- Fix the incorrect `row_store_page_size` after FE restart. [#38240](https://github.com/apache/doris/pull/38240)\\n\\n### Other\\n\\n- Fix the timezone configuration issue. The default timezone is no longer fixed at UTC+8 and is now obtained from system configuration. [#37294](https://github.com/apache/doris/pull/37294)\\n\\n- Fix the class conflict issue when using ranger due to multiple JSR specification implementations. [#37575](https://github.com/apache/doris/pull/37575)\\n\\n- Fix the potential uninitialized field issue in some BE code. [#37403](https://github.com/apache/doris/pull/37403)\\n\\n- Fix the error in delete statements for random distributed tables. [#37985](https://github.com/apache/doris/pull/37985)\\n\\n- Fix the incorrect requirement for `alter_priv` permission on the base table when creating a synchronized materialized view. [#38011](https://github.com/apache/doris/pull/38011)\\n\\n- Fix the issue of not authenticating resources when used in TVF. [#36928](https://github.com/apache/doris/pull/36928)\\n\\n\\n## Credits\\n\\nThanks all who contribute to this release: \\n\\n@133tosakarin, @924060929, @AshinGau, @Baymine, @BePPPower, @BiteTheDDDDt, @ByteYue, @CalvinKirs, @Ceng23333, @DarvenDuan, @FreeOnePlus, @Gabriel39, @HappenLee, @JNSimba, @Jibing-Li, @KassieZ, @Lchangliang, @LiBinfeng-01, @Mryange, @SWJTU-ZhangLei, @TangSiyang2001, @Tech-Circle-48, @Vallishp, @Yukang-Lian, @Yulei-Yang, @airborne12, @amorynan, @bobhan1, @cambyzju, @cjj2010, @csun5285, @dataroaring, @deardeng, @eldenmoon, @englefly, @feiniaofeiafei, @felixwluo, @freemandealer, @gavinchou, @ghkang98, @hello-stephen, @hubgeter, @hust-hhb, @jacktengg, @kaijchen, @kaka11chen, @keanji-x, @liaoxin01, @liutang123, @luwei16, @luzhijing, @lxr599, @morningman, @morrySnow, @mrhhsg, @mymeiyi, @platoneko, @qidaye, @qzsee, @seawinde, @shuke987, @sollhui, @starocean999, @suxiaogang223, @w41ter, @wangbo, @wangshuo128, @whutpencil, @wsjz, @wuwenchi, @wyxxxcat, @xiaokang, @xiedeyantu, @xinyiZzz, @xy720, @xzj7019, @yagagagaga, @yiguolei, @yujun777, @z404289981, @zclllyybb, @zddr, @zfr9527, @zhangbutao, @zhangstar333, @zhannngchen, @zhiqiang-hhhh, @zjj, @zy-kkk, @zzzxl1993"},{"id":"/auto-partition-in-apache-doris","metadata":{"permalink":"/blog/auto-partition-in-apache-doris","source":"@site/blog/auto-partition-in-apache-doris.md","title":"Automatic and flexible data sharding: Auto Partition in Apache Doris","description":"Apache Doris 2.1.0 introduces Auto Partition. It supports partitioning data by RANGE or by LIST and further enhances flexibility on top of automatic partitioning.","date":"2024-08-14T00:00:00.000Z","tags":[{"inline":true,"label":"Tech Sharing","permalink":"/blog/tags/tech-sharing"}],"hasTruncateMarker":false,"authors":[{"name":"velodb.io \xb7 VeloDB Engineering Team","key":null,"page":null}],"frontMatter":{"title":"Automatic and flexible data sharding: Auto Partition in Apache Doris","summary":"Apache Doris 2.1.0 introduces Auto Partition. It supports partitioning data by RANGE or by LIST and further enhances flexibility on top of automatic partitioning.","description":"Apache Doris 2.1.0 introduces Auto Partition. It supports partitioning data by RANGE or by LIST and further enhances flexibility on top of automatic partitioning.","date":"2024-08-14","author":"velodb.io \xb7 VeloDB Engineering Team","externalLink":"https://www.velodb.io/blog/882","tags":["Tech Sharing"],"image":"/images/auto-partition-in-apache-doris.jpg"},"unlisted":false,"prevItem":{"title":"Apache Doris 3.0.1 just released","permalink":"/blog/release-note-3.0.1"},"nextItem":{"title":"Apache Doris version 2.0.14 has been released","permalink":"/blog/release-note-2.0.14"}},"content":"\x3c!-- \\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\nTo handle large datasets, distributed databases introduce strategies like partitioning and bucketing. Data is divided into smaller units based on specific rules and distributed across different nodes, so databases can perform parallel processing for higher performance and data management flexibility.\\n\\nLike in many databases, [Apache Doris](https://doris.apache.org) shards data into partitions, and then a partition is further divided into buckets. **Partitions** are typically defined by time or other continuous values. This allows query engines to quickly locate the target data during queries by pruning irrelevant data ranges.\\n\\n**Bucketing**, on the other hand, distributes data based on the hash values of one or more columns, which prevents data skew.\\n\\nPrior to version [2.1.0](https://doris.apache.org/blog/release-note-2.1.0), there are two way you can create data partitions in Apache Doris:\\n\\n- **[Manual Partition](https://doris.apache.org/docs/table-design/data-partitioning/manual-partitioning)**: Users specify the partitions in the table creation statement, or modify them through DDL statements afterwards.\\n\\n- **[Dynamic Partition](https://doris.apache.org/docs/table-design/data-partitioning/dynamic-partitioning)**: The system automatically maintains partitions within a pre-defined range based on the data ingestion time.\\n\\nIn Apache Doris 2.1.0, we have introduced **[Auto Partition](https://doris.apache.org/docs/table-design/data-partitioning/auto-partitioning)**. It supports partitioning data by RANGE or by LIST and further enhances flexibility on top of automatic partitioning.\\n\\n## Evolution of partitioning strategies in Doris\\n\\nIn the design of data distribution, we focus more on partition planning, because the choice of partition columns and partition intervals heavily depends on the actual data distribution patterns, and a good partition design can largely improve the query and storage efficiency of the table.\\n\\nIn Doris, the data table is divided into partitions and then buckets in a hierarchical manner. The data within the same bucket then forms a data **tablet**, which is the minimum physical storage unit in Doris for data replication, inter-cluster data scheduling, and load balancing.\\n\\n![Evolution of partitioning strategies in Doris](/images/evolution-of-partitioning-strategies-in-Doris.png)\\n\\n\\n### Manual Partition\\n\\nDoris allows users to manually create data partitions by RANGE and by LIST. \\n\\nFor time-stamped data like logs and transaction records, users typically create partitions based on the time dimension. Here\'s an example of the CREATE TABLE statement:\\n\\n```sql\\nCREATE TABLE IF NOT EXISTS example_range_tbl\\n(\\n    `user_id` LARGEINT NOT NULL COMMENT \\"User ID\\",\\n    `date` DATE NOT NULL COMMENT \\"Data import date\\",\\n    `timestamp` DATETIME NOT NULL COMMENT \\"Data import timestamp\\",\\n    `city` VARCHAR(20) COMMENT \\"Location of user\\",\\n    `age` SMALLINT COMMENT \\"Age of user\\",\\n    `sex` TINYINT COMMENT \\"Sex of user\\",\\n    `last_visit_date` DATETIME REPLACE DEFAULT \\"1970-01-01 00:00:00\\" COMMENT \\"Last visit date of user\\",\\n    `cost` BIGINT SUM DEFAULT \\"0\\" COMMENT \\"User consumption\\",\\n    `max_dwell_time` INT MAX DEFAULT \\"0\\" COMMENT \\"Maximum dwell time of user\\",\\n    `min_dwell_time` INT MIN DEFAULT \\"99999\\" COMMENT \\"Minimum dwell time of user\\"\\n)\\nENGINE=OLAP\\nAGGREGATE KEY(`user_id`, `date`, `timestamp`, `city`, `age`, `sex`)\\nPARTITION BY RANGE(`date`)\\n(\\n    PARTITION `p201701` VALUES LESS THAN (\\"2017-02-01\\"),\\n    PARTITION `p201702` VALUES LESS THAN (\\"2017-03-01\\"),\\n    PARTITION `p201703` VALUES LESS THAN (\\"2017-04-01\\"),\\n    PARTITION `p2018` VALUES [(\\"2018-01-01\\"), (\\"2019-01-01\\"))\\n)\\nDISTRIBUTED BY HASH(`user_id`) BUCKETS 16\\nPROPERTIES\\n(\\n    \\"replication_num\\" = \\"1\\"\\n);\\n```\\n\\nThe table is partitioned by the data import date `date`, and 4 partitions have been pre-created. Within each partition, the data is further divided into 16 buckets based on the hash value of the `user_id`.\\n\\nWith this partitioning and bucketing design, when querying data from 2018 onwards, the system only need to scan the `p2018` partition. This is what the query SQL looks like:\\n\\n```sql\\nmysql> desc select count() from example_range_tbl where date >= \'20180101\';\\n+--------------------------------------------------------------------------------------+\\n| Explain String(Nereids Planner)                                                      |\\n+--------------------------------------------------------------------------------------+\\n| PLAN FRAGMENT 0                                                                      |\\n|   OUTPUT EXPRS:                                                                      |\\n|     count(*)[#11]                                                                    |\\n|   PARTITION: UNPARTITIONED                                                           |\\n|                                                                                      |\\n|    ......                                                                            |\\n|                                                                                      |\\n|   0:VOlapScanNode(193)                                                               |\\n|      TABLE: test.example_range_tbl(example_range_tbl), PREAGGREGATION: OFF.          |\\n|      PREDICATES: (date[#1] >= \'2018-01-01\')                                          |\\n|      partitions=1/4 (p2018), tablets=16/16, tabletList=561490,561492,561494 ...      |\\n|      cardinality=0, avgRowSize=0.0, numNodes=1                                       |\\n|      pushAggOp=NONE                                                                  |\\n|                                                                                      |\\n+--------------------------------------------------------------------------------------+\\n```\\n\\nIf the data is distributed unevenly across partitions, the hash-based bucketing mechanism can further divide the data based on the `user_id`. This helps to avoid load imbalance on some machines during querying and storage.\\n\\nHowever, in real-world business scenarios, one cluster may have tens of thousands of tables, which means it is impossible to manage them manually.\\n\\n```sql\\nCREATE TABLE `DAILY_TRADE_VALUE`\\n(\\n    `TRADE_DATE`              datev2 NOT NULL COMMENT \'Trade date\',\\n    `TRADE_ID`                varchar(40) NOT NULL COMMENT \'Trade ID\',\\n    ......\\n)\\nUNIQUE KEY(`TRADE_DATE`, `TRADE_ID`)\\nPARTITION BY RANGE(`TRADE_DATE`)\\n(\\n    PARTITION p_200001 VALUES [(\'2000-01-01\'), (\'2000-02-01\')),\\n    PARTITION p_200002 VALUES [(\'2000-02-01\'), (\'2000-03-01\')),\\n    PARTITION p_200003 VALUES [(\'2000-03-01\'), (\'2000-04-01\')),\\n    PARTITION p_200004 VALUES [(\'2000-04-01\'), (\'2000-05-01\')),\\n    PARTITION p_200005 VALUES [(\'2000-05-01\'), (\'2000-06-01\')),\\n    PARTITION p_200006 VALUES [(\'2000-06-01\'), (\'2000-07-01\')),\\n    PARTITION p_200007 VALUES [(\'2000-07-01\'), (\'2000-08-01\')),\\n    PARTITION p_200008 VALUES [(\'2000-08-01\'), (\'2000-09-01\')),\\n    PARTITION p_200009 VALUES [(\'2000-09-01\'), (\'2000-10-01\')),\\n    PARTITION p_200010 VALUES [(\'2000-10-01\'), (\'2000-11-01\')),\\n    PARTITION p_200011 VALUES [(\'2000-11-01\'), (\'2000-12-01\')),\\n    PARTITION p_200012 VALUES [(\'2000-12-01\'), (\'2001-01-01\')),\\n    PARTITION p_200101 VALUES [(\'2001-01-01\'), (\'2001-02-01\')),\\n    ......\\n)\\nDISTRIBUTED BY HASH(`TRADE_DATE`) BUCKETS 10\\nPROPERTIES (\\n  ......\\n);\\n```\\n\\nIn the above example, data is partitioned on a monthly basis. This requires the database administrator (DBA) to manually add a new partition each month and maintain table schema regularly. Imagine the case of real-time data processing, where you might need to create partitions daily or even hourly, manually doing this is no long a choice. That\'s why we introduced Dynamic Partition.\\n\\n### Dynamic Partition\\n\\nBy Dynamic Partition, Doris automatically creates and reclaims data partitions as long as the user specifies the partition unit, the number of historical partitions, and the number of future partitions. This functionality relies on a fixed thread on the Doris Frontend. It continuously polls and checks for new partitions to be created or old partitions to be reclaimed, and updates the partition schema of the table.\\n\\nThis is an example CREATE TABLE statement for a table which is partitioned by day. The `start` and `end` parameters are set to `-7` and `3`, respectively, meaning that data partitions for the next 3 days will be pre-created and the historical partitions that are older than 7 days will be reclaimed.\\n\\n```sql\\nCREATE TABLE `DAILY_TRADE_VALUE`\\n(\\n    `TRADE_DATE`              datev2 NOT NULL COMMENT \'Trade date\',\\n    `TRADE_ID`                varchar(40) NOT NULL COMMENT \'Trade ID\',\\n    ......\\n)\\nUNIQUE KEY(`TRADE_DATE`, `TRADE_ID`)\\nPARTITION BY RANGE(`TRADE_DATE`) ()\\nDISTRIBUTED BY HASH(`TRADE_DATE`) BUCKETS 10\\nPROPERTIES (\\n    \\"dynamic_partition.enable\\" = \\"true\\",\\n    \\"dynamic_partition.time_unit\\" = \\"DAY\\",\\n    \\"dynamic_partition.start\\" = \\"-7\\",\\n    \\"dynamic_partition.end\\" = \\"3\\",\\n    \\"dynamic_partition.prefix\\" = \\"p\\",\\n    \\"dynamic_partition.buckets\\" = \\"10\\"\\n);\\n```\\n\\nOver time, the table will always maintain partitions within the range of `[current date - 7, current date + 3]`. Dynamic Partition is particularly useful for real-time data ingestion scenarios, such as when the ODS (Operational Data Store) layer directly receives data from external sources like Kafka.\\n\\nThe `start` and `end` parameters define a fixed range for the partitions, allowing the user to manage the partitions only within this range. However, if the user needs to include more historical data, they would have to dial up the `start` value, and that could lead to unnecessary metadata overhead in the cluster.\\n\\nTherefore, when applying Dynamic Partition, there is a trade-off between the convenience and efficiency of metadata management.\\n\\n## Developers\' words\\n\\nAs the complexity of business adds up, Dynamic Partition becomes inadequate because:\\n\\n- It only supports partitioning by RANGE but not by LIST.\\n\\n- It can only be applied to the current real-world timestamps.\\n\\n- It only supports a single continuous partition range, and cannot accommodate partitions outside of that range.\\n\\nGiven these functional limitations, we started to plan a new partitioning mechanism that can both automate partition management and simplify data table maintenance.\\n\\nWe figured out that the ideal partitioning implementation should:\\n\\n- Save the need for manually creating partitions after table creation; \\n\\n- Be able to accommodate all ingested data in corresponding partitions.\\n\\n**The former stands for automation and the latter for flexibility. The essence of realizing them both is associating partition creation with the actual data.**\\n\\nThen we started to think about: What if we hold off the creation of partitions until the data is ingested, rather than doing it during table creation or through regular polling. Instead of pre-constructing the partition distribution, we can define the \\"data-to-partition\\" mapping rules, so the partitions are created after data arrives.\\n\\nCompared to Manual Partition, this whole process would be fully automated, eliminating the need for human maintenance. Compared to Dynamic Partition, it avoids having partitions that are not used, or partitions that are needed but not present.\\n\\n## Auto Partition\\n\\nWith [Apache Doris 2.1.0](https://doris.apache.org/blog/release-note-2.1.0), we bring the above plan into fruition. During data ingestion, Doris creates data partitions based on the configured rules. The Doris Backend nodes that are responsible for data processing and distribution will attempt to find the appropriate partition for each row of data in the DataSink operator of the execution plan. It no longer filters out data that does not fit into any existing partition or reports an error for such a situation, but automatically generates partitions for all ingested data.\\n\\n### Auto Partition by RANGE\\n\\nAuto Partition by RANGE provides an optimized partitioning solution based on the time dimension. It is more flexible than Dynamic Partition in terms of parameter configuration. The syntax for it is as follows:\\n\\n```sql\\nAUTO PARTITION BY RANGE (FUNC_CALL_EXPR)\\n()\\nFUNC_CALL_EXPR ::= DATE_TRUNC ( <partition_column>, \'<interval>\' )\\n```\\n\\nThe `<partition_column>` above is the partition column (i.e., the column that the partitioning is based on). `<interval>`specifies the partition unit, which is the desired width of each partition. \\n\\nFor example, if the partition column is `k0` and you want to partition by month, the partition statement would be `AUTO PARTITION BY RANGE (DATE_TRUNC(k0, \'month\'))`. For all the imported data, the system will call `DATE_TRUNC(k0, \'month\')` to calculate the left endpoint of the partition, and then the right endpoint by adding one `interval`.\\n\\nNow, we can apply Auto Partition to the `DAILY_TRADE_VALUE` table introduced in the previous section on Dynamic Partition.\\n\\n```sql\\nCREATE TABLE DAILY_TRADE_VALUE\\n(\\n    `TRADE_DATE`    DATEV2 NOT NULL COMMENT \'Trade Date\',\\n    `TRADE_ID`      VARCHAR(40) NOT NULL COMMENT \'Trade ID\',\\n    ......\\n)\\nAUTO PARTITION BY RANGE (DATE_TRUNC(`TRADE_DATE`, \'month\'))\\n()\\nDISTRIBUTED BY HASH(`TRADE_DATE`) BUCKETS 10\\nPROPERTIES\\n(\\n    ......\\n);\\n```\\n\\nAfter importing some data, these are the partitions we get:\\n\\n```sql\\nmysql> show partitions from DAILY_TRADE_VALUE;\\nEmpty set (0.10 sec)\\n\\nmysql> insert into DAILY_TRADE_VALUE values (\'2015-01-01\', 1), (\'2020-01-01\', 2), (\'2024-03-05\', 10000), (\'2024-03-06\', 10001);\\nQuery OK, 4 rows affected (0.24 sec)\\n{\'label\':\'label_2a7353a3f991400e_ae731988fa2bc568\', \'status\':\'VISIBLE\', \'txnId\':\'85097\'}\\n\\nmysql> show partitions from DAILY_TRADE_VALUE;\\n+-------------+-----------------+----------------+---------------------+--------+--------------+--------------------------------------------------------------------------------+-----------------+---------+----------------+---------------+---------------------+---------------------+--------------------------+----------+------------+-------------------------+-----------+--------------------+--------------+\\n| PartitionId | PartitionName   | VisibleVersion | VisibleVersionTime  | State  | PartitionKey | Range                                                                          | DistributionKey | Buckets | ReplicationNum | StorageMedium | CooldownTime        | RemoteStoragePolicy | LastConsistencyCheckTime | DataSize | IsInMemory | ReplicaAllocation       | IsMutable | SyncWithBaseTables | UnsyncTables |\\n+-------------+-----------------+----------------+---------------------+--------+--------------+--------------------------------------------------------------------------------+-----------------+---------+----------------+---------------+---------------------+---------------------+--------------------------+----------+------------+-------------------------+-----------+--------------------+--------------+\\n| 588395      | p20150101000000 | 2              | 2024-06-01 19:02:40 | NORMAL | TRADE_DATE   | [types: [DATEV2]; keys: [2015-01-01]; ..types: [DATEV2]; keys: [2015-02-01]; ) | TRADE_DATE      | 10      | 1              | HDD           | 9999-12-31 23:59:59 |                     | NULL                     | 0.000    | false      | tag.location.default: 1 | true      | true               | NULL         |\\n| 588437      | p20200101000000 | 2              | 2024-06-01 19:02:40 | NORMAL | TRADE_DATE   | [types: [DATEV2]; keys: [2020-01-01]; ..types: [DATEV2]; keys: [2020-02-01]; ) | TRADE_DATE      | 10      | 1              | HDD           | 9999-12-31 23:59:59 |                     | NULL                     | 0.000    | false      | tag.location.default: 1 | true      | true               | NULL         |\\n| 588416      | p20240301000000 | 2              | 2024-06-01 19:02:40 | NORMAL | TRADE_DATE   | [types: [DATEV2]; keys: [2024-03-01]; ..types: [DATEV2]; keys: [2024-04-01]; ) | TRADE_DATE      | 10      | 1              | HDD           | 9999-12-31 23:59:59 |                     | NULL                     | 0.000    | false      | tag.location.default: 1 | true      | true               | NULL         |\\n+-------------+-----------------+----------------+---------------------+--------+--------------+--------------------------------------------------------------------------------+-----------------+---------+----------------+---------------+---------------------+---------------------+--------------------------+----------+------------+-------------------------+-----------+--------------------+--------------+\\n3 rows in set (0.09 sec)\\n```\\n\\nAs is shown, partitions are automatically created for the imported data, and it doesn\'t create partitions that are beyond the range of the existing data.\\n\\n### Auto Partition by LIST\\n\\nAuto Partition by LIST is to shard data based on non-time-based dimensions, such as `region` and `department`. It fills that gap for Dynamic Partition, which does not support data partitioning by LIST. \\n\\nAuto Partition by RANGE provides an optimized partitioning solution based on the time dimension. It is more flexible than Dynamic Partition in terms of parameter configuration. The syntax for it is as follows:\\n\\n```sql\\nAUTO PARTITION BY LIST (`partition_col`)\\n()\\n```\\n\\nThis is an example of Auto Partition by LIST using `city` as the partition column:\\n\\n```SQL\\nmysql> CREATE TABLE `str_table` (\\n    ->     `city` VARCHAR NOT NULL,\\n    ->     ......\\n    -> )\\n    -> DUPLICATE KEY(`city`)\\n    -> AUTO PARTITION BY LIST (`city`)\\n    -> ()\\n    -> DISTRIBUTED BY HASH(`city`) BUCKETS 10\\n    -> PROPERTIES (\\n    ->     ......\\n    -> );\\nQuery OK, 0 rows affected (0.09 sec)\\n\\nmysql> insert into str_table values (\\"Denver\\"), (\\"Boston\\"), (\\"Los_Angeles\\");\\nQuery OK, 3 rows affected (0.25 sec)\\n\\nmysql> show partitions from str_table;\\n+-------------+-----------------+----------------+---------------------+--------+--------------+-------------------------------------------+-----------------+---------+----------------+---------------+---------------------+---------------------+--------------------------+----------+------------+-------------------------+-----------+--------------------+--------------+\\n| PartitionId | PartitionName   | VisibleVersion | VisibleVersionTime  | State  | PartitionKey | Range                                     | DistributionKey | Buckets | ReplicationNum | StorageMedium | CooldownTime        | RemoteStoragePolicy | LastConsistencyCheckTime | DataSize | IsInMemory | ReplicaAllocation       | IsMutable | SyncWithBaseTables | UnsyncTables |\\n+-------------+-----------------+----------------+---------------------+--------+--------------+-------------------------------------------+-----------------+---------+----------------+---------------+---------------------+---------------------+--------------------------+----------+------------+-------------------------+-----------+--------------------+--------------+\\n| 589685      | pDenver7        | 2              | 2024-06-01 20:12:37 | NORMAL | city         | [types: [VARCHAR]; keys: [Denver]; ]      | city            | 10      | 1              | HDD           | 9999-12-31 23:59:59 |                     | NULL                     | 0.000    | false      | tag.location.default: 1 | true      | true               | NULL         |\\n| 589643      | pLos5fAngeles11 | 2              | 2024-06-01 20:12:37 | NORMAL | city         | [types: [VARCHAR]; keys: [Los_Angeles]; ] | city            | 10      | 1              | HDD           | 9999-12-31 23:59:59 |                     | NULL                     | 0.000    | false      | tag.location.default: 1 | true      | true               | NULL         |\\n| 589664      | pBoston8        | 2              | 2024-06-01 20:12:37 | NORMAL | city         | [types: [VARCHAR]; keys: [Boston]; ]      | city            | 10      | 1              | HDD           | 9999-12-31 23:59:59 |                     | NULL                     | 0.000    | false      | tag.location.default: 1 | true      | true               | NULL         |\\n+-------------+-----------------+----------------+---------------------+--------+--------------+-------------------------------------------+-----------------+---------+----------------+---------------+---------------------+---------------------+--------------------------+----------+------------+-------------------------+-----------+--------------------+--------------+\\n3 rows in set (0.10 sec)\\n```\\n\\nAfter inserting data for the cities of Denver, Boston, and Los Angeles, the system automatically created corresponding partitions based on the city names. Previously, this type of custom partitioning could only be achieved through manual DDL statements. This is how Auto Partition by LIST simplifies database maintenance.\\n\\n### Tips & notes\\n\\n**Manually adjust historical partitions**\\n\\nFor tables that receive both real-time data and occasional historical updates, since Auto Partition does not automatically reclaim historical partitions, we recommend two options:\\n\\n- Use Auto Partition, which will automatically create partitions for the occasional historical data updates. \\n\\n- Use Auto Partition and manually create a `LESS THAN` partition to accommodate the historical updates. This allows for a clearer separation of historical and real-time data, and makes data management easier.\\n\\n```sql\\nmysql> CREATE TABLE DAILY_TRADE_VALUE\\n    -> (\\n    ->     `TRADE_DATE`    DATEV2 NOT NULL COMMENT \'Trade Date\',\\n    ->     `TRADE_ID`      VARCHAR(40) NOT NULL COMMENT \'Trade ID\'\\n    -> )\\n    -> AUTO PARTITION BY RANGE (DATE_TRUNC(`TRADE_DATE`, \'DAY\'))\\n    -> (\\n    ->     PARTITION `pHistory` VALUES LESS THAN (\\"2024-01-01\\")\\n    -> )\\n    -> DISTRIBUTED BY HASH(`TRADE_DATE`) BUCKETS 10\\n    -> PROPERTIES\\n    -> (\\n    ->     \\"replication_num\\" = \\"1\\"\\n    -> );\\nQuery OK, 0 rows affected (0.11 sec)\\n\\nmysql> insert into DAILY_TRADE_VALUE values (\'2015-01-01\', 1), (\'2020-01-01\', 2), (\'2024-03-05\', 10000), (\'2024-03-06\', 10001);\\nQuery OK, 4 rows affected (0.25 sec)\\n{\'label\':\'label_96dc3d20c6974f4a_946bc1a674d24733\', \'status\':\'VISIBLE\', \'txnId\':\'85092\'}\\n\\nmysql> show partitions from DAILY_TRADE_VALUE;\\n+-------------+-----------------+----------------+---------------------+--------+--------------+--------------------------------------------------------------------------------+-----------------+---------+----------------+---------------+---------------------+---------------------+--------------------------+----------+------------+-------------------------+-----------+--------------------+--------------+\\n| PartitionId | PartitionName   | VisibleVersion | VisibleVersionTime  | State  | PartitionKey | Range                                                                          | DistributionKey | Buckets | ReplicationNum | StorageMedium | CooldownTime        | RemoteStoragePolicy | LastConsistencyCheckTime | DataSize | IsInMemory | ReplicaAllocation       | IsMutable | SyncWithBaseTables | UnsyncTables |\\n+-------------+-----------------+----------------+---------------------+--------+--------------+--------------------------------------------------------------------------------+-----------------+---------+----------------+---------------+---------------------+---------------------+--------------------------+----------+------------+-------------------------+-----------+--------------------+--------------+\\n| 577871      | pHistory        | 2              | 2024-06-01 08:53:49 | NORMAL | TRADE_DATE   | [types: [DATEV2]; keys: [0000-01-01]; ..types: [DATEV2]; keys: [2024-01-01]; ) | TRADE_DATE      | 10      | 1              | HDD           | 9999-12-31 23:59:59 |                     | NULL                     | 0.000    | false      | tag.location.default: 1 | true      | true               | NULL         |\\n| 577940      | p20240305000000 | 2              | 2024-06-01 08:53:49 | NORMAL | TRADE_DATE   | [types: [DATEV2]; keys: [2024-03-05]; ..types: [DATEV2]; keys: [2024-03-06]; ) | TRADE_DATE      | 10      | 1              | HDD           | 9999-12-31 23:59:59 |                     | NULL                     | 0.000    | false      | tag.location.default: 1 | true      | true               | NULL         |\\n| 577919      | p20240306000000 | 2              | 2024-06-01 08:53:49 | NORMAL | TRADE_DATE   | [types: [DATEV2]; keys: [2024-03-06]; ..types: [DATEV2]; keys: [2024-03-07]; ) | TRADE_DATE      | 10      | 1              | HDD           | 9999-12-31 23:59:59 |                     | NULL                     | 0.000    | false      | tag.location.default: 1 | true      | true               | NULL         |\\n+-------------+-----------------+----------------+---------------------+--------+--------------+--------------------------------------------------------------------------------+-----------------+---------+----------------+---------------+---------------------+---------------------+--------------------------+----------+------------+-------------------------+-----------+--------------------+--------------+\\n3 rows in set (0.10 sec)\\n```\\n\\n**NULL partition**\\n\\nWith Auto Partition by LIST, Doris supports storing NULL values in NULL partitions. For example:\\n\\n```sql\\nmysql> CREATE TABLE list_nullable\\n    -> (\\n    ->     `str` varchar NULL\\n    -> )\\n    -> AUTO PARTITION BY LIST (`str`)\\n    -> ()\\n    -> DISTRIBUTED BY HASH(`str`) BUCKETS auto\\n    -> PROPERTIES\\n    -> (\\n    ->     \\"replication_num\\" = \\"1\\"\\n    -> );\\nQuery OK, 0 rows affected (0.10 sec)\\n\\nmysql> insert into list_nullable values (\'123\'), (\'\'), (NULL);\\nQuery OK, 3 rows affected (0.24 sec)\\n{\'label\':\'label_f5489769c2f04f0d_bfb65510f9737fff\', \'status\':\'VISIBLE\', \'txnId\':\'85089\'}\\n\\nmysql> show partitions from list_nullable;\\n+-------------+---------------+----------------+---------------------+--------+--------------+------------------------------------+-----------------+---------+----------------+---------------+---------------------+---------------------+--------------------------+----------+------------+-------------------------+-----------+--------------------+--------------+\\n| PartitionId | PartitionName | VisibleVersion | VisibleVersionTime  | State  | PartitionKey | Range                              | DistributionKey | Buckets | ReplicationNum | StorageMedium | CooldownTime        | RemoteStoragePolicy | LastConsistencyCheckTime | DataSize | IsInMemory | ReplicaAllocation       | IsMutable | SyncWithBaseTables | UnsyncTables |\\n+-------------+---------------+----------------+---------------------+--------+--------------+------------------------------------+-----------------+---------+----------------+---------------+---------------------+---------------------+--------------------------+----------+------------+-------------------------+-----------+--------------------+--------------+\\n| 577297      | pX            | 2              | 2024-06-01 08:19:21 | NORMAL | str          | [types: [VARCHAR]; keys: [NULL]; ] | str             | 10      | 1              | HDD           | 9999-12-31 23:59:59 |                     | NULL                     | 0.000    | false      | tag.location.default: 1 | true      | true               | NULL         |\\n| 577276      | p0            | 2              | 2024-06-01 08:19:21 | NORMAL | str          | [types: [VARCHAR]; keys: []; ]     | str             | 10      | 1              | HDD           | 9999-12-31 23:59:59 |                     | NULL                     | 0.000    | false      | tag.location.default: 1 | true      | true               | NULL         |\\n| 577255      | p1233         | 2              | 2024-06-01 08:19:21 | NORMAL | str          | [types: [VARCHAR]; keys: [123]; ]  | str             | 10      | 1              | HDD           | 9999-12-31 23:59:59 |                     | NULL                     | 0.000    | false      | tag.location.default: 1 | true      | true               | NULL         |\\n+-------------+---------------+----------------+---------------------+--------+--------------+------------------------------------+-----------------+---------+----------------+---------------+---------------------+---------------------+--------------------------+----------+------------+-------------------------+-----------+--------------------+--------------+\\n3 rows in set (0.11 sec)\\n```\\n\\nHowever, Auto Partition by RANGE does not support NULL partitions, because the NULL values will be stored in the smallest `LESS THAN` partition, and it is impossible to reliably determine the appropriate range for it. If Auto Partition were to create a NULL partition with a range of (-INFINITY, MIN_VALUE), there would be a risk of this partition being inadvertently deleted in production, as the MIN_VALUE boundary may not accurately represent the intended business logic.\\n\\n### Summary\\n\\nAuto Partition covers most of the use cases of Dynamic Partition, while introducing the benefit of upfront partition rule definition. Once the rules are defined, the bulk of partition creation work is automatically handled by Doris instead of a DBA.\\n\\nBefore utilizing Auto Partition, it\'s important to understand the relevant limitations:\\n\\n1. Auto Partition by LIST supports partitioning based on **multiple columns**, but each automatically created partition only contains one single value, and the partition name cannot exceed 50 characters in length. Note that the partition names follow specific naming conventions, which have particular implications for metadata management. That means not all of the 50-character space is at the user\'s disposal.\\n\\n2. Auto Partition by RANGE only supports a **single partition column**, which must be of type **DATE** or **DATETIME**.\\n\\n3. Auto Partition by LIST supports **NULLABLE** partition column and inserting NULL values. Auto Partition by RANGE does not support NULLABLE partition column.\\n\\n4. It is not recommended to use Auto Partition in conjunction with Dynamic Partition after Apache Doris 2.1.3.\\n\\n## Performance comparison\\n\\nThe main functional differences between Auto Partition and Dynamic Partition lie in partition creation and deletion, supported partition types, and their impact on import performance.\\n\\nDynamic Partition uses fixed threads to periodically create and reclaim partitions. It only supports partitioning by RANGE. In contrast, Auto Partition supports both partitioning by RANGE and by LIST. It automatically creates partitions on-demand based on specific rules during data ingestion, providing a higher level of automation and flexibility.\\n\\nDynamic Partition does not slow down data ingestion speed, while Auto Partition causes certain time overheads because it firstly checks for existing partitions and then creates new ones on demand. We will present the performance test results.\\n\\n![Performance comparison](/images/performance-comparison.png)\\n\\n## Auto Partition: ingestion workflow\\n\\nThis part is about how data ingestion is implemented with the Auto Partition mechanism, and we use [Stream Load](https://doris.apache.org/docs/data-operate/import/stream-load-manual) as an example. When Doris initiates a data import, one of the Doris Backend nodes takes on the role of the Coordinator. It is responsible for the initial data processing work and then dispatching the data to the appropriate BE nodes, known as the Executors, for execution.\\n\\n![Auto Partition: ingestion workflow](/images/auto-partition-ingestion-workflow.png)\\n\\n\\nIn the final Datasink Node of the Coordinator\'s execution pipeline, the data needs to be routed to the correct partitions, buckets, and Doris Backend node locations before it can be successfully transmitted and stored.\\n\\nTo enable this data transfer, the Coordinator and Executor nodes establish a communication channels:\\n\\n- The sending end is called the Node Channel.\\n\\n- The receiving end is called the Tablets Channel.\\n\\nThis is how Auto Partition comes into play during the process of determining the correct partitions for the data: \\n\\n![Auto Partition: ingestion workflow](/images/auto-partition-ingestion-workflow-2.png)\\n\\n\\nPreviously, without Auto Partition, when a table does not have the required partition, the behavior in Doris is for the BE nodes to accumulate errors until a `DATA_QUALITY_ERROR` is reported. Now, with Auto Partition enabled, a request will be initiated to the Doris Frontend to create the necessary partition on-the-fly. After the partition creation transaction is completed, the Doris Frontend responds to the Coordinator, which then opens the corresponding communication channels (Node Channel and Tablets Channel) to continue the data ingestion process. This is a seamless experience for users. \\n\\nIn a real-world cluster environment, the time spent by the Coordinator waiting for the Doris Frontend to complete partition creation can incur large overheads. This is due to the inherent latency of Thrift RPC calls, as well as lock contention on the Frontend under high load conditions.\\n\\nTo improve the data ingestion efficiency in Auto Partition, Doris has implemented batching to largely reduce the number of RPC calls made to the FE. This brings a notable performance enhancement for data write operations.\\n\\nNote that when the FE Master completes the partition creation transaction, the new partition becomes immediately visible. However, if the import process ultimately fails or is canceled, the created partitions are not automatically reclaimed.\\n\\n## Auto Partition performance\\n\\nWe tested the performance and stability of Auto Partition in Doris, covering different use cases:\\n\\n**Case 1**: 1 Frontend + 3 Backend; 6 randomly generated datasets, each having 100 million rows and 2,000 partitions; ingested the 6 datasets concurrently into 6 tables\\n\\n- **Objective**: Evaluate the performance of Auto Partition under high pressure and check for any performance degradation.\\n\\n- **Results**: Auto Partition brings an **average performance loss less than 5%**, with all import transactions running stably.\\n\\n![Auto Partition performance](/images/auto-partition-performance.png)\\n\\n**Case 2**: 1 Frontend + 3 Backend; ingesting 100 rows per second from Flink by Routine Load; testing with 1, 10, and 20 concurrent transactions (tables), respectively\\n\\n- **Objective**: Identify any potential  or data backlog issues that could arise with Auto Partition under different concurrency levels.\\n\\n- **Results**: With or without Auto Partition enabled, the data ingestion was successful without any backpressure issues across all the concurrency levels tested, even at 20 concurrent transactions when the CPU utilization reached close to 100%.\\n\\n![Auto Partition performance](/images/auto-partition-performance-2.png)\\n\\n\\nTo conclude the results of these tests, the impact of enabling Auto Partition on data ingestion performance is minimal.\\n\\n## Conclusion and future plans\\n\\nAuto Partition has simplified DDL and partition management since Apache Doris 2.1.0. It is useful in large-scale data processing and makes it easy for users to migrate from other database systems to Apache Doris. \\n\\nMoreover, we are committed to expanding the capabilities of Auto Partition to support more complex data types.\\n\\nPlans for Auto Partition by RANGE:\\n\\n- Support numeric values;\\n\\n- Allowing users to specify the left and right boundaries of the partition range.\\n\\nPlans for Auto Partition by LIST:\\n\\n- Allow merging multiple values into the same partition based on specific rules.\\n\\nJoin [Apache Doris open-source community](https://join.slack.com/t/apachedoriscommunity/shared_invite/zt-2gmq5o30h-455W226d79zP3L96ZhXIoQ) for more information and further guidance."},{"id":"/release-note-2.0.14","metadata":{"permalink":"/blog/release-note-2.0.14","source":"@site/blog/release-note-2.0.14.md","title":"Apache Doris version 2.0.14 has been released","description":"Thanks to our community users and developers, about 110 improvements and bug fixes have been made in Doris 2.0.14 version","date":"2024-08-07T00:00:00.000Z","tags":[{"inline":true,"label":"Release Notes","permalink":"/blog/tags/release-notes"}],"hasTruncateMarker":false,"authors":[{"name":"Apache Doris","key":null,"page":null}],"frontMatter":{"title":"Apache Doris version 2.0.14 has been released","summary":"Thanks to our community users and developers, about 110 improvements and bug fixes have been made in Doris 2.0.14 version","description":"Thanks to our community users and developers, about 110 improvements and bug fixes have been made in Doris 2.0.14 version","date":"2024-08-07","author":"Apache Doris","tags":["Release Notes"],"image":"/images/2.0.14.jpg"},"unlisted":false,"prevItem":{"title":"Automatic and flexible data sharding: Auto Partition in Apache Doris","permalink":"/blog/auto-partition-in-apache-doris"},"nextItem":{"title":"Migrate data lakehouse from BigQuery to Apache Doris, saving $4,500 per month","permalink":"/blog/migrate-lakehouse-from-bigquery-to-doris"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n  http://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n\\n\\nThanks to our community users and developers, about 110 improvements and bug fixes have been made in Doris 2.0.14 version\\n\\n[Quick Download](https://doris.apache.org/download/)\\n\\n## 1 New features\\n\\n- Adds a REST interface to retrieve the most recent query profile: `curl http://user:password@127.0.0.1:8030/api/profile/text` [#38268](https://github.com/apache/doris/pull/38268)\\n\\n## 2 Improvements\\n\\n- Optimizes the primary key point query performance for MOW tables with sequence columns [#38287](https://github.com/apache/doris/pull/38287)\\n\\n- Enhances the performance of inverted index queries with many conditions  [#35346](https://github.com/apache/doris/pull/35346)\\n\\n- Automatically enables the   `support_phrase` option when creating a tokenized inverted index to accelerate  `match_phrase` phrase queries [#37949](https://github.com/apache/doris/pull/37949)\\n\\n- Supports simplified SQL hints, for example: `SELECT /*+ query_timeout(3000) */ * FROM t;` [#37720](https://github.com/apache/doris/pull/37720)\\n\\n- Automatically retries reading from object storage when encountering a   `429` error to improve stability [#35396](https://github.com/apache/doris/pull/35396)\\n\\n- LEFT SEMI / ANTI JOIN terminates subsequent matching execution upon matching a qualifying data row to enhance performance. [#34703](https://github.com/apache/doris/pull/34703)\\n\\n- Prevents coredump when returning illegal data to MySQL results. [#28069](https://github.com/apache/doris/pull/28069)\\n\\n- Unifies the output of type names in lowercase to maintain compatibility with MySQL and be more friendly to BI tools. [#38521](https://github.com/apache/doris/pull/38521)\\n\\n\\nYou can access the full list through the GitHub [link](https://github.com/apache/doris/compare/2.0.13...2.0.14) , with the key features and improvements highlighted below.\\n\\n## Credits\\n\\nThanks all who contribute to this release:\\n\\n@ByteYue, @CalvinKirs, @GoGoWen, @HappenLee, @Jibing-Li, @Lchangliang, @LiBinfeng-01, @Mryange, @XieJiann, @Yukang-Lian, @Yulei-Yang, @airborne12, @amorynan, @biohazard4321, @cambyzju, @csun5285, @eldenmoon, @englefly, @freemandealer, @hello-stephen, @hubgeter, @kaijchen, @liaoxin01, @luwei16, @morningman, @morrySnow, @mymeiyi, @qidaye, @sollhui, @starocean999, @w41ter, @wuwenchi, @xiaokang, @xy720, @yujun777, @zclllyybb, @zddr, @zhangstar333, @zhiqiang-hhhh, @zy-kkk, @zzzxl1993"},{"id":"/migrate-lakehouse-from-bigquery-to-doris","metadata":{"permalink":"/blog/migrate-lakehouse-from-bigquery-to-doris","source":"@site/blog/migrate-lakehouse-from-bigquery-to-doris.md","title":"Migrate data lakehouse from BigQuery to Apache Doris, saving $4,500 per month","description":"Dien Tran Thanh shares his firsthand experience, proven best practices, and insightful suggestions for migrating from BigQuery to Apache Doris to build a more cost-efficient data warehouse.","date":"2024-07-19T00:00:00.000Z","tags":[{"inline":true,"label":"Best Practice","permalink":"/blog/tags/best-practice"}],"hasTruncateMarker":false,"authors":[{"name":"velodb.io \xb7 Dien, Tran Thanh","key":null,"page":null}],"frontMatter":{"title":"Migrate data lakehouse from BigQuery to Apache Doris, saving $4,500 per month","summary":"Dien Tran Thanh shares his firsthand experience, proven best practices, and insightful suggestions for migrating from BigQuery to Apache Doris to build a more cost-efficient data warehouse.","description":"Dien Tran Thanh shares his firsthand experience, proven best practices, and insightful suggestions for migrating from BigQuery to Apache Doris to build a more cost-efficient data warehouse.","date":"2024-07-19","author":"velodb.io \xb7 Dien, Tran Thanh","tags":["Best Practice"],"externalLink":"https://www.velodb.io/blog/689","image":"/images/migrate-lakehouse-from-bigquery-to-apache-doris.jpg"},"unlisted":false,"prevItem":{"title":"Apache Doris version 2.0.14 has been released","permalink":"/blog/release-note-2.0.14"},"nextItem":{"title":"Apache Doris version 2.0.13 has been released","permalink":"/blog/release-note-2.0.13"}},"content":"\x3c!-- \\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n:::tip Special Thanks\\nThe Apache Doris community would like to extend our gratitude to Dien for sharing his valuable experience and best practices in migrating from BigQuery to Apache Doris in this insightful and informative article. Dien is also an active member of the Apache Doris open-source [community](https://join.slack.com/t/apachedoriscommunity/shared_invite/zt-2gmq5o30h-455W226d79zP3L96ZhXIoQ), and we are truly appreciative of his ongoing support.\\n:::\\n\\nThis article is written by [Dien, Tran Thanh](https://www.linkedin.com/in/dien-tran-thanh-19275b14a/) and originally posted on [Medium](https://dientt.medium.com/migrate-data-platform-t\u1EEB-bigquery-sang-apache-doris-gi\xfap-gi\u1EA3m-chi-ph\xed-t\u1EEB-6-000-xu\u1ED1ng-c\xf2n-1-500-40ba9b22967e).\\n\\n## The problem posed\\n\\nTo cut BigQuery costs, my previous employer who worked at a retail company (Regrettably, I\'m not at liberty to disclose the name) asked me as an Advisor to research and design a data platform on-premises. In the data-driven company, it spent about $6,000 on BigQuery (The current scanning cost is $8.44 per TB \u2014 Pay-as-you-go, not including taxes and storage fees). \\n\\nI summarize the sources of the high BigQuery cost into the following:\\n\\n- **ETL**: More than 500 tables drawn from CRM system, OMS system, tracking on web/app, affiliates, marketplace, and social media... (Only the tables necessary for the current requirements are stored. For tables that are not actively needed, the data is stored as Parquet files in Google Cloud Storage (GCS)).\\n\\n- **Tables**: The departments built nearly 120 tables in the data mart, including the reports of accounting, marketing, sales, operations, and BOD.\\n\\n- **Dashboards**: There are 45 dashboards, each having 7\u201310 charts on average.\\n\\n- **Queries**: The systems running campaigns interact with BigQuery, as they need to query through a large volume of data (coded in Java, reading data from BigQuery).\\n\\nThe situation was complicated. After reviewing the data usage, I realized that the existing data lakehouse has already optimized the use of BigQuery. They have applied the techniques suggested by Google, such as partitioning, clustering, materialized views, denormalization, and caching. Additionally, they have workers monitoring the audit logs to detect expensive queries and alerts to ensure the queries follow the established rules. That is to say, there was little room for further optimization on the BigQuery side.\\n\\nGiven this scenario, using an open-source on-premises solution appeared to be the choice. However, this would come with the trade-off of increased operational costs. Those who have migrated from a cloud-based solution to an on-premises one will understand the challenges associated with this decision.\\n\\n## Approach to the problem\\n\\nThe key requirements for this migration process are:\\n\\n- Leverage the existing ETL pipeline with minimal changes.\\n\\n- Reuse the existing data marts that have been built by various teams. As 90% of them are built using SQL, the new data lakehouse system needs to have strong SQL support to enable the reuse of the existing workflows.\\n\\n- Ensure the current services (written in Java) can seamlessly connect to the new lakehouse, with minimal changes to the application logic.\\n\\n- Rebuild the existing dashboards on Metabase (a BI tool). This could be challenging as there might be differences in syntax or support for certain functions (e.g., window functions) between BigQuery and the new system.\\n\\n- Achieve good system performance, stability, flexibility, and easy scalability.\\n\\n- Hopefully, be able to read data from Apache Iceberg, as this is where the Machine Learning team exports their model results.\\n\\n- Hopefully, store vector data for the AI chatbot.\\n\\n- Ensure the new data lakehouse is easy for the existing teams to learn and adopt, especially in terms of SQL syntax.\\n\\n- The long-term maintenance costs of the new system should be low, with a reasonable initial effort to understand the operational aspects.\\n\\nBased on the information provided, I found [Apache Doris](https://doris.apache.org), an open source data warehouse, to be a suitable solution. (A friend introduced it to me before, when performing this migration, I had 5 months of experience working with Doris.) \\n\\nAround 20TB of data needs to be scanned daily. According to BigQuery charts, the peak hours are 00:00\u201306:00, 8:30\u201311:00 and 14:30\u201316 :00). I use the following hardware configuration:\\n\\n- 3 Follower nodes, each with 20GB RAM, 12 CPU, and 200GB SSD\\n\\n- 1 Observer node with 8GB RAM, 8 CPU, and 100GB SSD\\n\\n- 3 Backend nodes, each with 64GB RAM, 32 CPU, and 3TB SSD\\n\\n![apache-doris-architecture](/images/apache-doris-architecture.png)\\n\\nWith this configuration, the estimated monthly cost is around 37 million VND (using a server service provided by a Vietnamese company and the pricing may vary across different providers.).\\n\\nReasons to choose Apache Doris:\\n\\n- Doris is being actively developed by Baidu to meet their own needs. It is widely used by other major tech companies such as Alibaba, Tencent, and Xiaomi... and recently many technology companies in India.\\n\\n- The Doris community, while smaller compared to Clickhouse, has an active Slack group where users can get support and direct answers from the developers, including those who have previously encountered and resolved similar issues.\\n\\n- Doris supports the MySQL protocol, allowing applications and tools that can connect to MySQL to also connect to Doris.\\n\\n- Doris provides high availability, where data can still be queried even if a node goes down, as long as the table\'s replication settings are configured properly.\\n\\n- Doris can be scaled both horizontally and vertically. In cases where local storage is insufficient, data can be stored on HDFS, S3, GCS, and accessed using federated queries.\\n\\n- Doris\' performance is optimized through the Massive Parallel Processing architecture, and features like predicate pushdown, partitioning, various Indexing mechanisms, and rollup (very useful). It also uses a columnar data format and provides flexible join capabilities, including broadcast joins and local joins utilizing replicas to avoid data shuffling. It provides more flexible upsert than ClickHouse.\\n\\nThere are many more reasons, but I\'ve only highlighted a part of them above.\\n\\n## Problem solving\\n\\n### Migrate ETL streams from data sources\\n\\nBelow is the high-level data architecture of the data flow before migration:\\n\\n![Migrate ETL streams from data sources](/images/migrate-etl-streams-from-data-sources.png)\\n\\nThe company has saved costs by using open-source solutions for the ETL processing components, and it used only GCS and BigQuery. I will not delve into the details of the architecture here but keep the explanation simple to provide a high-level understanding of the migration process. If a more comprehensive solution is required, there are options like Apache Iceberg and Trino that can be leveraged for ad-hoc processing, A/B testing, etc.\\n\\nThis is the architecture after migration.\\n\\n![Migrate ETL streams from data sources](/images/migrate-etl-streams-from-data-sources-2.png)\\n\\nTo minimize the changes, we have used a similar architecture, where Apache Doris provides a connector that allows Apache Flink to directly upsert data into it.\\n\\nThe File Storage Service is provided by the server company. (The client connection is similar to S3, and I guess the underlying core could be MinIO.)\\n\\nApache Doris supports reading real-time data directly from Kafka and performing simple ETL, but to reduce the burden on Doris and leverage the processing capabilities of Apache Flink, we have only created an abstraction to push the data into Doris, which is quite straightforward.\\n\\nMigrating the old data is more challenging, and we have implemented the following steps:\\n\\n- **Step 1**: A script reads the schema for each table on BigQuery, and then creates corresponding tables with the same schema and partition columns on Doris (luckily, we don\'t have to deal with complex data types). Currently, Doris does not support clustering columns, but it does provide various indexes such as Bitmap Index, Prefix Index, Bloom Filter Index, and Inverted Index, depending on the use case.\\n\\n- **Step 2**: A script exports each table from BigQuery into Parquet files and stores them in GCS.\\n\\n- **Step 3**: A script directly reads the files from GCS and loads them into Doris (Doris supports reading data from File Storage System quite easily, requiring only a simple SQL statement).\\n\\nDuring the migration, we have prepared scripts and techniques to ensure data consistency between BigQuery and Doris when the new ETL flow on Doris starts consuming the new data.\\n\\n### Migrate data marts flow\\n\\nThis part is quite straightforward as the old flow used Apache Airflow, where they defined SQL (easy to manage SQL version when building Data Marts) and then used the Google Cloud BigQuery Operator to interact with BigQuery. In this part, I have replaced the BigQuery Operator with the MySQL Operator to connect to Doris. The Doris SQL is compatible with 90% of the old flow, so I can reuse the old SQL statements.\\n\\n### Migrate services pointing to BigQuery\\n\\nThis part is also easy as the majority of the services use simple custom SQL. The tables they use are mostly pre-calculated (denormalized, big tables, or materialized views), so there is almost no need for changes, except for the connection part to Doris, but we don\'t have to change the code logic.\\n\\n### Migrate dashboards in Metabase\\n\\nWhen Metabase connects to Doris, some dashboards will throw errors due to different function syntax. For example, Doris uses the Window Functions such as `LEAD(expr, offset, default) OVER (partition_by_clause order_by_clause)`, while in BigQuery, it is `LEAD(expr) OVER (partition_by_clause order_by_clause)`. Such difference can lead to logical errors, so we need to remove those charts and rebuild the new syntax on Doris.\\n\\n### Monitoring\\n\\nAfter all the migration, we use Airflow to check the data of both BigQuery and Doris for each table and each chart. When a table has a discrepancy, we will zoom in to handle it. Overall, we haven\'t seen any complex errors, and the discrepancy rate is less than 5%.\\n\\n## Evaluation\\n\\nAfter the ETL pipeline ran stably for a week, I provided the team with a trial version. After 4 weeks of testing, the performance remained stable, with occasional failures of a Frontend (FE) node. (When one FE node died, the system could still be queried though. Only backend failures would impact performance since that means the parallel server capacity could not be fully utilized. When a failure occurred, the system would self-recover within 2 seconds). During peak hours, queries were often slow because the ETL job consuming over 70% of the resources. The solution was to reconfigure the resource allocation to restrict the ETL account from consuming more than 40% of resources. (Doris provides a [mechanism](https://doris.apache.org/blog/multi-tenant-workload-isolation-in-apache-doris) to share resources between account groups, where normally unused resources can be borrowed, but during contention each group is limited to its allocated portion, so limiting ETL to 40% resolved the issue during high concurrent usage.)\\n\\nThe team plans to run the old and new systems in parallel for another 2-3 weeks. If everything checks out, they will then shut down the BigQuery pipeline.\\n\\nCurrently, the new system is not fully optimized, and the team hasn\'t utilized all the features that Apache Doris provides. With more time to explore, it seems they could solve many more problems.\\n\\nThe implementation was carried out by 1 Data Engineer, 1 Software Engineer, and 1 Data Analyst over 4 weeks.\\n\\n## Advantages of the new system\\n\\n- The biggest advantage is the cost savings. The monthly cost has been reduced from $6,000 to $1,500.\\n\\n- It supports seamless data import from Apache Iceberg. The Machine Learning and data mining team can directly import data without needing to create a separate pipeline like with BigQuery.\\n\\n- It supports [vector data storage](https://python.langchain.com/v0.2/docs/integrations/vectorstores/apache_doris/) for AI chatbots. Data can be directly imported from the File Store Service (S3) instead of having to push it to Redis as before.\\n\\n- It provides efficient data aggregation through the Rollup mechanism.\\n\\n- It allows hybrid hot and cold storage within a single table. The older, less frequently accessed data can be stored in cold storage on the File Store Service, with Doris automatically retrieving the cold data when needed (though this may incur a slight performance penalty).\\n\\n## Disadvantages of the new system\\n\\n- It is difficult to maintain, as it requires significant time to adjust the many configuration parameters (over 100) to ensure the system operates as expected.\\n\\n- Users may encounter OOM errors if too many users access the system at the same time, as different queries compete for RAM resources (in this case, some users may encounter errors while others do not, depending on the process and group_account).\\n\\n- Data replication between nodes can sometimes lose synchronization due to network issues or other reasons, and the automatic replication retry mechanism may not be successful. In such cases, it is necessary to set up an external worker mechanism for automatic handling (Doris manages a storage unit called Tablet. The metadata on each node records the ETL data into a specific table, and the metadata version gets updated. Doris provides a SQL-based method to handle desynchronization).\\n\\n- New versions may occasionally have bugs, so it\'s recommended to check if the [community](https://join.slack.com/t/apachedoriscommunity/shared_invite/zt-2gmq5o30h-455W226d79zP3L96ZhXIoQ) has reported any issues before deciding to upgrade.\\n\\n- Limited support for processing Vietnamese text, with the full-text search feature not performing well in Vietnamese.\\n\\n- The servers used are provided by a Vietnamese company, which seems to be using OpenStack, so they can occasionally be unstable. The occasional node disconnections require intervention from the provider. However, due to the high availability feature, Doris can still be used, though performance may be reduced during these periods (node disconnections are not frequent, occurring around once every few months). If the disconnection happens on a BE node, data rebalancing may be required if there are active ETL jobs at that time.\\n\\n## Key lessons from this migration process\\n\\n- The bandwidth requirements were not properly estimated, so Doris sometimes exceeds the read capacity when reading data from the old tables stored in GCS.\\n\\n- The migration team was not fully dedicated to the task, spending only 70% of their time on the migration and 30% on other responsibilities. This intermittent work led to lower-than-expected performance. (I worked part-time.)\\n\\n- For real-time data streams, the recorded storage files are very small in size, so it is necessary to adjust the compaction configuration so that Doris can merge files in groups, which will give better query performance.\\n\\n- We spent significant time manually inspecting the data mart tables to determine the appropriate index type. However, this process can be automated by leveraging a metadata management tool like Datahub.\\n\\n- The data quality validation process was not thorough enough. We should use a more comprehensive approach combining count, sum, and dimensional checks to ensure high data accuracy.\\n\\n- We realized that the old system, built by the management, was designed with extensibility, integration, and migration in mind, so I hardly changed the flow or logic but made the most of the old system. It is a valuable lesson to learn from the legacy system\'s architecture.\\n\\nThis article does not go into depth on the technical details of the new data lakehouse techniques, and the process of selecting the appropriate technologies is not discussed in detail, either.\\n\\nNow, the data lakehouse has been used stably by the teams for weeks, and both the CTO and CFO are happy with it. However, it will require more extensive usage to enable a more objective evaluation.\\n\\nNote: Using BigQuery is still more convenient than Doris, as Doris is an open-source solution and can have some minor issues. Doris requires more complex techniques to ensure a stable cluster, or it will require more effort in cluster maintenance. However, if you use Doris for an extended period and develop a good understanding of how to operate it, the maintenance costs are not particularly high.\\n\\nNevertheless, I still prefer using BigQuery. :D\\n\\n## References\\n\\n- Debezium: https://debezium.io/\\n\\n- Apache Doris: https://doris.apache.org/\\n\\n- Tencent blog: https://medium.com/geekculture/tencent-data-engineer-why-we-go-from-clickhouse-to-apache-doris-db120f324290"},{"id":"/release-note-2.0.13","metadata":{"permalink":"/blog/release-note-2.0.13","source":"@site/blog/release-note-2.0.13.md","title":"Apache Doris version 2.0.13 has been released","description":"Thanks to our community users and developers, about 112 improvements and bug fixes have been made in Doris 2.0.13 version.","date":"2024-07-17T00:00:00.000Z","tags":[{"inline":true,"label":"Release Notes","permalink":"/blog/tags/release-notes"}],"hasTruncateMarker":false,"authors":[{"name":"Apache Doris","key":null,"page":null}],"frontMatter":{"title":"Apache Doris version 2.0.13 has been released","summary":"Thanks to our community users and developers, about 112 improvements and bug fixes have been made in Doris 2.0.13 version.","description":"Thanks to our community users and developers, about 112 improvements and bug fixes have been made in Doris 2.0.13 version.","date":"2024-07-17","author":"Apache Doris","tags":["Release Notes"],"image":"/images/2.0.13.jpg"},"unlisted":false,"prevItem":{"title":"Migrate data lakehouse from BigQuery to Apache Doris, saving $4,500 per month","permalink":"/blog/migrate-lakehouse-from-bigquery-to-doris"},"nextItem":{"title":"Apache Doris version 2.0.12 has been released","permalink":"/blog/release-note-2.0.12"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n  http://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\nThanks to our community users and developers, about 112 improvements and bug fixes have been made in Doris 2.0.13 version\\n\\n[Quick Download](https://doris.apache.org/download/)\\n\\n## Behavior changes\\n\\nSQL input is treated as multiple statements only when the `CLIENT_MULTI_STATEMENTS` setting is enabled on the client side, enhancing compatibility with MySQL. [#36759](https://github.com/apache/doris/pull/36759)\\n\\n## New features\\n\\n- A new BE configuration `allow_zero_date` has been added, allowing dates with all zeros. When set to `false`, `0000-00-00` is parsed as `NULL`, and when set to `true`, it is parsed as `0000-01-01`. The default value is `false` to maintain consistency with previous behavior. [#34961](https://github.com/apache/doris/pull/34961)\\n\\n\\n- `LogicalWindow` and `LogicalPartitionTopN` support multi-field predicate pushdown to improve performance. [#36828](https://github.com/apache/doris/pull/36828)\\n\\n\\n- The ES Catalog now maps ES `nested` or `object` types to Doris `JSON` types. [#37101](https://github.com/apache/doris/pull/37101)\\n\\n\\n## Improvements\\n\\n- Queries with `LIMIT` end reading data earlier to reduce resource consumption and improve performance. [#36535](https://github.com/apache/doris/pull/36535)\\n\\n\\n- Special JSON data with empty keys is now supported. [#36762](https://github.com/apache/doris/pull/36762)\\n\\n\\n- Stability and usability of routine load have been improved, including load balancing, automatic recovery, exception handling, and more user-friendly error messages. [#36450](https://github.com/apache/doris/pull/36450) [#35376](https://github.com/apache/doris/pull/35376) [#35266](https://github.com/apache/doris/pull/35266) [ #33372](https://github.com/apache/doris/pull/33372) [#32282](https://github.com/apache/doris/pull/32282) [#32046](https://github.com/apache/doris/pull/32046) [#32021](https://github.com/apache/doris/pull/32021) [#31846](https://github.com/apache/doris/pull/31846) [#31273](https://github.com/apache/doris/pull/31273)\\n\\n\\n- BE load balancing selection of hard disk strategy and speed optimization. [#36826](https://github.com/apache/doris/pull/36826) [#36795](https://github.com/apache/doris/pull/36795) [#36509](https://github.com/apache/doris/pull/36509)\\n\\n\\n- Stability and usability of the JDBC catalog have been improved, including encryption, thread pool connection count configuration, and more user-friendly error messages. [#36940](https://github.com/apache/doris/pull/36940) [#36720](https://github.com/apache/doris/pull/36720) [#30880](https://github.com/apache/doris/pull/30880) [#35692](https://github.com/apache/doris/pull/35692)\\n\\n\\nYou can access the full list through the GitHub [link](https://github.com/apache/doris/compare/2.0.12...2.0.13) , with the key features and improvements highlighted below.\\n\\n## Credits\\n\\nThanks to all who contributed to this release:\\n\\n@Gabriel39, @Jibing-Li, @Johnnyssc, @Lchangliang, @LiBinfeng-01, @SWJTU-ZhangLei, @Thearas, @Yukang-Lian, @Yulei-Yang, @airborne12, @amorynan, @bobhan1, @cambyzju, @csun5285, @dataroaring, @deardeng, @eldenmoon, @englefly, @feiniaofeiafei, @hello-stephen, @jacktengg, @kaijchen, @liutang123, @luwei16, @morningman, @morrySnow, @mrhhsg, @mymeiyi, @platoneko, @qidaye, @sollhui, @starocean999, @w41ter, @xiaokang, @xy720, @yujun777, @zclllyybb, @zddr"},{"id":"/release-note-2.0.12","metadata":{"permalink":"/blog/release-note-2.0.12","source":"@site/blog/release-note-2.0.12.md","title":"Apache Doris version 2.0.12 has been released","description":"Thanks to our community developers and users for their contributions. Doris version 2.0.12 will bring 99 improvements and bug fixes.","date":"2024-06-27T00:00:00.000Z","tags":[{"inline":true,"label":"Release Notes","permalink":"/blog/tags/release-notes"}],"hasTruncateMarker":false,"authors":[{"name":"Apache Doris","key":null,"page":null}],"frontMatter":{"title":"Apache Doris version 2.0.12 has been released","summary":"Thanks to our community developers and users for their contributions. Doris version 2.0.12 will bring 99 improvements and bug fixes.","description":"Thanks to our community developers and users for their contributions. Doris version 2.0.12 will bring 99 improvements and bug fixes.","date":"2024-06-27","author":"Apache Doris","tags":["Release Notes"],"image":"/images/2.0.12.jpg"},"unlisted":false,"prevItem":{"title":"Apache Doris version 2.0.13 has been released","permalink":"/blog/release-note-2.0.13"},"nextItem":{"title":"Apache Doris 2.1.4 just released","permalink":"/blog/release-note-2.1.4"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n  http://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n\\nThanks to our community developers and users for their contributions. Doris version 2.0.12 will bring 99 improvements and bug fixes.\\n\\n**Quick Download:** https://doris.apache.org/download/\\n\\n**GitHub Release:** https://github.com/apache/doris/releases\\n\\n## Behavior changes\\n\\n- No longer set the default table comment to the table type. Instead, set it to be empty by default, for example, change COMMENT \'OLAP\' to COMMENT \' \'.  This new behavior is more friendly for BI software that relies on table comments. [#35855](https://github.com/apache/doris/pull/35855)\\n\\n- Change the type of the `@@autocommit` variable from `BOOLEAN` to `BIGINT` to prevent errors from certain MySQL clients (such as .NET MySQL.Data). [#33282](https://github.com/apache/doris/pull/33282)\\n\\n\\n## Improvements\\n\\n- Remove the `disable_nested_complex_type` parameter and allow the creation of nested `ARRAY`, `MAP`, and `STRUCT` types by default. [#36255](https://github.com/apache/doris/pull/36255)\\n\\n- The HMS catalog supports the `SHOW CREATE DATABASE` command. [#28145](https://github.com/apache/doris/pull/28145)\\n\\n- Add more inverted index metrics to the query profile. [#36545](https://github.com/apache/doris/pull/36545)\\n\\n- Cross-Cluster Replication (CCR) supports inverted indices. [#31743](https://github.com/apache/doris/pull/31743)\\n\\nYou can access the full list through the GitHub [link](https://github.com/apache/doris/compare/2.0.11...2.0.12) , with the key features and improvements highlighted below.\\n\\n\\n\\n## Credits\\n\\nThanks all who contribute to this release:\\n\\n@airborne12, D14@amorynan, D14@BiteTheDDDDt, D14@cambyzju, D14@caoliang-web, D14@dataroaring, D14@eldenmoon, D14@feiniaofeiafei, D14@felixwluo, D14@gavinchou, D14@HappenLee, D14@hello-stephen, D14@jacktengg, D14@Jibing-Li, D14@Johnnyssc, D14@liaoxin01, D14@LiBinfeng-01, D14@luwei16, D14@mongo360, D14@morningman, D14@morrySnow, D14@mrhhsg, D14@Mryange, D14@mymeiyi, D14@qidaye, D14@qzsee, D14@starocean999, D14@w41ter, D14@wangbo, D14@wsjz, D14@wuwenchi, D14@xiaokang, D14@XuPengfei-1020, D14@xy720, D14@yongjinhou, D14@yujun777, D14@Yukang-Lian, D14@Yulei-Yang, D14@zclllyybb, D14@zddr, D14@zhannngchen, D14@zhiqiang-hhhh, D14@zy-kkk, D14@zzzxl1993"},{"id":"/release-note-2.1.4","metadata":{"permalink":"/blog/release-note-2.1.4","source":"@site/blog/release-note-2.1.4.md","title":"Apache Doris 2.1.4 just released","description":"In this update, we have optimized various functional experiences for data lakehouse scenarios, with a focus on resolving the abnormal memory usage issue in the previous version.","date":"2024-06-26T00:00:00.000Z","tags":[{"inline":true,"label":"Release Notes","permalink":"/blog/tags/release-notes"}],"hasTruncateMarker":false,"authors":[{"name":"Apache Doris","key":null,"page":null}],"frontMatter":{"title":"Apache Doris 2.1.4 just released","summary":"In this update, we have optimized various functional experiences for data lakehouse, with a focus on resolving the abnormal memory usage issue in the previous version.","description":"In this update, we have optimized various functional experiences for data lakehouse scenarios, with a focus on resolving the abnormal memory usage issue in the previous version.","date":"2024-06-26","author":"Apache Doris","tags":["Release Notes"],"image":"/images/2.1.4.jpg"},"unlisted":false,"prevItem":{"title":"Apache Doris version 2.0.12 has been released","permalink":"/blog/release-note-2.0.12"},"nextItem":{"title":"Why Apache Doris is the best open source alternative to Rockset","permalink":"/blog/apache-doris-vs-rockset"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n  http://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\nDear community, Apache Doris version 2.1.4 was released on June 26, 2024. In this update, we have optimized various functional experiences for data lakehouse scenarios, with a focus on resolving the abnormal memory usage issue in the previous version. Additionally, we have implemented several improvemnents and bug fixes to enhance the stability.  Welcome to download and use it.\\n\\n**Quick Download:** https://doris.apache.org/download/\\n\\n**GitHub Release:** https://github.com/apache/doris/releases\\n\\n## Behavior changes\\n\\n- Non-existent files will be ignored when querying external tables such as Hive. [#35319](https://github.com/apache/doris/pull/35319)\\n\\n  The file list is obtained from the meta cache, and it may not be consistent with the actual file list.\\n\\n  Ignoring non-existent files helps to avoid query errors.\\n\\n- By default, creating a Bitmap Index will no longer be automatically changed to an Inverted Index. [#35521](https://github.com/apache/doris/pull/35521)\\n\\n  This behavior is controlled by the FE configuration item `enable_create_bitmap_index_as_inverted_index`, which defaults to false.\\n\\n- When starting FE and BE processes using `--console`, all logs will be output to the standard output and differentiated by prefixes indicating the log type. [#35679](https://github.com/apache/doris/pull/35679)\\n\\n  For more infomation, please see the documentations:\\n\\n  - [Log Management - FE Log](https://doris.apache.org/docs/admin-manual/log-management/fe-log)\\n\\n  - [Log Management - BE Log](https://doris.apache.org/docs/admin-manual/log-management/be-log)\\n\\n- If no table comment is provided when creating a table, the default comment will be empty instead of using the table type as the default comment. [#36025](https://github.com/apache/doris/pull/36025)\\n\\n- The default precision of DECIMALV3 has been adjusted from (9, 0) to (38, 9) to maintain compatibility with the version in which this feature was initially released. [#36316](https://github.com/apache/doris/pull/36316)\\n\\n## New features\\n\\n### Query Optimizer\\n\\n- Support FE flame graph tool\\n\\n  For more information, see the [documentation](https://doris.apache.org/community/developer-guide/fe-profiler)\\n\\n- Support `SELECT DISTINCT` to be used with aggregation.\\n\\n- Support single table query rewrite without `GROUP BY`. This is useful for complex filters or expressions. [#35242](https://github.com/apache/doris/pull/35242).\\n\\n- The new optimizer fully supports point query functionality [#36205](https://github.com/apache/doris/pull/36205).\\n\\n### Lakehouse\\n\\n- Support native reader of Apache Paimon deletion vector  [#35241](https://github.com/apache/doris/pull/35241)\\n\\n- Support using Resource in Table Valued Functions [#35139](https://github.com/apache/doris/pull/35139)\\n\\n- Access controller with Hive Ranger plugin supports Data Mask\\n\\n### Asynchronous Materialized Views\\n\\n- Support partition roll-up during construction. [#31812](https://github.com/apache/doris/pull/31812)\\n\\n- Support triggered updates during construction. [#34548](https://github.com/apache/doris/pull/34548)\\n\\n- Support specifying the `store_row_column` and `storage_medium` attribute during construction. [#35860](https://github.com/apache/doris/pull/35860)\\n\\n- Transparent rewrite supports single table asynchronous materialized views. [#34646](https://github.com/apache/doris/pull/34646)\\n\\n- Transparent rewrite supports `AGG_STATE` type aggregation roll-up. [#35026](https://github.com/apache/doris/pull/35026)\\n\\n### Others\\n\\n- Added function `replace_empty`. \\n\\n  For more information, see [documentation](https://doris.apache.org/docs/sql-manual/sql-functions/string-functions/replace_empty).\\n\\n- Support `show storage policy using` statement.\\n\\n  For more information, see [documentation](https://doris.apache.org/docs/sql-manual/sql-statements/Show-Statements/SHOW-STORAGE-POLICY-USING/).\\n\\n- Support JVM metrics on the BE side.\\n\\n  By setting `enable_jvm_monitor=true` in `be.conf` to enable this feature.\\n\\n## Improvements\\n\\n- Supported creating inverted indexes for columns with Chinese names. [#36321](https://github.com/apache/doris/pull/36321)\\n\\n- Estimate memory consumed by segment cache more accurately so that unused memory can be released more quickly. [#35751](https://github.com/apache/doris/pull/35751)\\n\\n- Filter empty partitions before exporting tables to remote storage. [#35542](https://github.com/apache/doris/pull/35542)\\n\\n- Optimize routine load task allocation algorithm to balance the load among Backends. [#34778](https://github.com/apache/doris/pull/34778)\\n\\n- Provide hints when a related variable is not found during a set operation. [#35775](https://github.com/apache/doris/pull/35775)\\n\\n- Support placing Java UDF jar files in the FE\'s `custom_lib` directory for default loading. [#35984](https://github.com/apache/doris/pull/35984)\\n\\n- Add a timeout global variable `audit_plugin_load_timeout` for audit log load jobs.\\n\\n- Optimize the performance of transparent rewrite planning for asynchronous materialized views.\\n\\n- Optimize the `INSERT` operation that when the source is empty, the BE will not execute. [#34418](https://github.com/apache/doris/pull/34418)\\n\\n- Support fetching file lists of Hive/Hudi tables in batches. [#35107](https://github.com/apache/doris/pull/35107)\\n\\n## Bug fixes\\n\\n### Query Optimizer\\n\\n- Fixed the issue where SQL cache returns old results after truncating a partition. [#34698](https://github.com/apache/doris/pull/34698)\\n\\n- Fixed the issue where casting from JSON to other types did not correctly handle nullable attributes. [#34707](https://github.com/apache/doris/pull/34707)\\n\\n- Fixed occasional DATETIMEV2 literal simplification errors. [#35153](https://github.com/apache/doris/pull/35153)\\n\\n- Fixed the issue where `COUNT(*)` could not be used in window functions. [#35220](https://github.com/apache/doris/pull/35220)\\n\\n- Fixed the issue where nullable attributes could be incorrect when all `SELECT` statements under `UNION ALL` have no `FROM` clause. [#35074](https://github.com/apache/doris/pull/35074)\\n\\n- Fixed the issue where `bitmap in join` and subquery unnesting could not be used simultaneously. [#35435](https://github.com/apache/doris/pull/35435)\\n\\n- Fixed the performance issue where filter conditions could not be pushed down to the CTE producer in specific situations. [#35463](https://github.com/apache/doris/pull/35463)\\n\\n- Fixed the issue where aggregate combinators written in uppercase could not be found. [#35540](https://github.com/apache/doris/pull/35540)\\n\\n- Fixed the performance issue where window functions were not properly pruned by column pruning. [#35504](https://github.com/apache/doris/pull/35504)\\n\\n- Fixed the issue where queries might parse incorrectly leading to wrong results when multiple tables with the same name but in different databases appeared simultaneously in the query. [#35571](https://github.com/apache/doris/pull/35571)\\n\\n- Fixed the query error caused by generating runtime filters during schema table scans. [#35655](https://github.com/apache/doris/pull/35655)\\n\\n- Fixed the issue where nested correlated subqueries could not execute because the join condition was folded into a null literal. [#35811](https://github.com/apache/doris/pull/35811)\\n\\n- Fixed the occasional issue where decimal literals were set with incorrect precision during planning. [#36055](https://github.com/apache/doris/pull/36055)\\n\\n- Fixed the occasional issue where multiple layers of aggregation were merged incorrectly during planning. [#36145](https://github.com/apache/doris/pull/36145)\\n\\n- Fixed the occasional issue where the input-output mismatch error occurred after aggregate expansion planning. [#36207](https://github.com/apache/doris/pull/36207)\\n\\n- Fixed the occasional issue where `<=>` was incorrectly converted to `=`. [#36521](https://github.com/apache/doris/pull/36521)\\n\\n### Query Execution\\n\\n- Fixed the issue where the query hangs if the limited rows are reached on the pipeline engine and memory is not released. [#35746](https://github.com/apache/doris/pull/35746)\\n\\n- Fixed the BE coredump when `enable_decimal256` is true but falls back to the old planner. [#35731](https://github.com/apache/doris/pull/35731)\\n\\n### Asynchronous Materialized Views\\n\\n- Fixed the issue where asynchronous materialized views caused backup and restore exceptions. [#35703](https://github.com/apache/doris/pull/35703)\\n\\n- Fixed the issue where partition rewrite could lead to incorrect results. [#35236](https://github.com/apache/doris/pull/35236)\\n\\n### Semi-structured\\n\\n- Fixed the core dump problem when a VARIANT with an empty key is used. [#35671](https://github.com/apache/doris/pull/35671)\\n- Bitmap and BloomFilter index should not perform light index changes. [#35225](https://github.com/apache/doris/pull/35225)\\n\\n### Primary Key\\n\\n- Fixed the issue where an exception BE restart occurred in the case of partial column updates during import, which could result in duplicate keys. [#35678](https://github.com/apache/doris/pull/35678)\\n\\n- Fixed the issue where BE might core dump during clone operations when memory is tight. [#34702](https://github.com/apache/doris/pull/34702)\\n\\n### Lakehouse\\n\\n- Fixed the issue where a Hive table could not be created with a fully qualified name such as `ctl.db.tbl` [#34984](https://github.com/apache/doris/pull/34984)\\n\\n- Fixed the issue where the Hive metastore connection did not close when refreshing [#35426](https://github.com/apache/doris/pull/35426)\\n\\n- Fixed a potential meta replay issue when upgrading from 2.0.x to 2.1.x. [#35532](https://github.com/apache/doris/pull/35532)\\n\\n- Fixed the issue where the Table Valued Function could not read an empty snappy compressed file. [#34926](https://github.com/apache/doris/pull/34926)\\n\\n- Fixed the issue where unable to read Parquet files with invalid min-max column statistics [#35041](https://github.com/apache/doris/pull/35041)\\n\\n- Fixed the issue where unable to handle pushdown predicates with null-aware functions in the Parquet/ORC reader [#35335](https://github.com/apache/doris/pull/35335)\\n\\n- Fixed the issue about the order of partition columns when creating a Hive table [#35347](https://github.com/apache/doris/pull/35347)\\n\\n- Fixed the issue where writing to a Hive table on S3 failed when partition values contained spaces [#35645](https://github.com/apache/doris/pull/35645)\\n\\n- Fixed the issue about incorrect scheme of Aliyun OSS endpoint [#34907](https://github.com/apache/doris/pull/34907)\\n\\n- Fixed the issue where the Parquet format Hive table written by Doris could not be read by Hive [#34981](https://github.com/apache/doris/pull/34981)\\n\\n- Fixed the issue where unable to read ORC files after the schema change of a Hive table [#35583](https://github.com/apache/doris/pull/35583)\\n\\n- Fixed the issue where unable to read Paimon tables via JNI after the schema change of the Paimon table [#35309](https://github.com/apache/doris/pull/35309)\\n\\n- Fixed the issue of too small Row Groups in Parquet format files written out. [#36042](https://github.com/apache/doris/pull/36042) [#36143](https://github.com/apache/doris/pull/36143)\\n\\n- Fixed the issue where unable to read Paimon tables after schema changes [#36049](https://github.com/apache/doris/pull/36049)\\n\\n- Fixed the issue where unable to read Hive Parquet format tables after schema changes [#36182](https://github.com/apache/doris/pull/36182)\\n\\n- Fixed the FE OOM issue caused by Hadoop FS cache [#36403](https://github.com/apache/doris/pull/36403)\\n\\n- Fixed the issue where FE could not start after enabling the Hive Metastore Listener [#36533](https://github.com/apache/doris/pull/36533)\\n\\n- Fixed the issue of query performance degradation with a large number of files [#36431](https://github.com/apache/doris/pull/36431)\\n\\n- Fixed the timezone issue when reading the timestamp column type in Iceberg [#36435](https://github.com/apache/doris/pull/36435)\\n\\n- Fixed DATETIME conversion error and data path error on Iceberg Table. [#35708](https://github.com/apache/doris/pull/35708)\\n\\n- Support retain and pass the additional user-defined properties fo Table Valued Functions to the S3 SDK. [#35515](https://github.com/apache/doris/pull/35515)\\n\\n\\n### Data Import\\n\\n- Fixed the issue where `CANCEL LOAD` did not work [#35352](https://github.com/apache/doris/pull/35352)\\n\\n- Fixed the issue where a null pointer error in the Publish phase of load transactions prevented the load from completing [#35977](https://github.com/apache/doris/pull/35977)\\n\\n- Fixed the issue with bRPC serializing large data files when sent via HTTP [#36169](https://github.com/apache/doris/pull/36169)\\n\\n### Data Management\\n\\n- Fixed the isseu that the resource tag in ConnectionContext was not set after forwarding DDL or DML to master FE. [#35618](https://github.com/apache/doris/pull/35618)\\n\\n- Fixed the issue where the restored table name was incorrect when `lower_case_table_names` was enabled [#35508](https://github.com/apache/doris/pull/35508)\\n\\n- Fixed the issue where `admin clean trash` could not work [#35271](https://github.com/apache/doris/pull/35271)\\n\\n- Fixed the issue where a storage policy could not be deleted from a partition [#35874](https://github.com/apache/doris/pull/35874)\\n\\n- Fixed the issue of data loss when importing into a multi-replica automatic partition table [#36586](https://github.com/apache/doris/pull/36586)\\n\\n- Fixed the issue where the partition column of a table changed when querying or inserting into an automatic partition table using the old optimizer [#36514](https://github.com/apache/doris/pull/36514)\\n\\n### Memory Management\\n\\n- Fixed the issue of frequent errors in the logs due to failure in obtaining Cgroup meminfo. [#35425](https://github.com/apache/doris/pull/35425)\\n\\n- Fixed the issue where the Segment cache size was uncontrolled when using BloomFilter, leading to abnormal process memory growth. [#34871](https://github.com/apache/doris/pull/34871)\\n\\n### Permissions\\n\\n- Fixed the issue where permission settings were ineffective after enabling case-insensitive table names. [#36557](https://github.com/apache/doris/pull/36557)\\n\\n- Fixed the issue where setting LDAP passwords through non-Master FE nodes did not take effect. [#36598](https://github.com/apache/doris/pull/36598)\\n\\n- Fixed the issue where authorization could not be checked for the `SELECT COUNT(*)` statement. [#35465](https://github.com/apache/doris/pull/35465)\\n\\n### Others\\n\\n- Fixed the issue where the client JDBC program could not close the connection if the MySQL connection was broken. [#36616](https://github.com/apache/doris/pull/36616)\\n\\n- Fixed MySQL protocol compatibility issue with the `SHOW PROCEDURE STATUS` statement. [#35350](https://github.com/apache/doris/pull/35350)\\n\\n- The `libevent` now forces Keepalive to solve the issue of connection leaks in certain situations. [#36088](https://github.com/apache/doris/pull/36088)\\n\\n## Credits\\n\\nThanks to every one who contributes to this release.\\n\\n@airborne12, @amorynan, @AshinGau, @BePPPower, @BiteTheDDDDt, @ByteYue, @caiconghui, @CalvinKirs, @cambyzju, @catpineapple, @cjj2010, @csun5285, @DarvenDuan, @dataroaring, @deardeng, @Doris-Extras, @eldenmoon, @englefly, @feiniaofeiafei, @felixwluo, @freemandealer, @Gabriel39, @gavinchou, @GoGoWen, @HappenLee, @hello-stephen, @hubgeter, @hust-hhb, @jacktengg, @jackwener, @jeffreys-cat, @Jibing-Li, @kaijchen, @kaka11chen, @Lchangliang, @liaoxin01, @LiBinfeng-01, @lide-reed, @luennng, @luwei16, @mongo360, @morningman, @morrySnow, @mrhhsg, @Mryange, @mymeiyi, @nextdreamblue, @platoneko, @qidaye, @qzsee, @seawinde, @shuke987, @sollhui, @starocean999, @suxiaogang223, @TangSiyang2001, @Thearas, @Vallishp, @w41ter, @wangbo, @whutpencil, @wsjz, @wuwenchi, @xiaokang, @xiedeyantu, @XieJiann, @xinyiZzz, @XuPengfei-1020, @xy720, @xzj7019, @yiguolei, @yongjinhou, @yujun777, @Yukang-Lian, @Yulei-Yang, @zclllyybb, @zddr, @zfr9527, @zgxme, @zhangbutao, @zhangstar333, @zhannngchen, @zhiqiang-hhhh, @zy-kkk, @zzzxl1993"},{"id":"/apache-doris-vs-rockset","metadata":{"permalink":"/blog/apache-doris-vs-rockset","source":"@site/blog/apache-doris-vs-rockset.md","title":"Why Apache Doris is the best open source alternative to Rockset","description":"Among of all the claim-to-be alternatives to Rockset, Apache Doris is one of the few that cover all the key features of Rockset.","date":"2024-06-24T00:00:00.000Z","tags":[{"inline":true,"label":"Tech Sharing","permalink":"/blog/tags/tech-sharing"}],"hasTruncateMarker":false,"authors":[{"name":"velodb.io \xb7 Zaki Lu, Apache Doris Committer","key":null,"page":null}],"frontMatter":{"title":"Why Apache Doris is the best open source alternative to Rockset","summary":"Among of all the claim-to-be alternatives to Rockset, Apache Doris is one of the few that cover all the key features of Rockset.","description":"Among of all the claim-to-be alternatives to Rockset, Apache Doris is one of the few that cover all the key features of Rockset.","date":"2024-06-24","author":"velodb.io \xb7 Zaki Lu, Apache Doris Committer","tags":["Tech Sharing"],"externalLink":"https://www.velodb.io/blog/567","image":"/images/doris-vs-rockset.jpeg"},"unlisted":false,"prevItem":{"title":"Apache Doris 2.1.4 just released","permalink":"/blog/release-note-2.1.4"},"nextItem":{"title":"Steps to industry-leading query speed: evolution of the Apache Doris execution engine","permalink":"/blog/evolution-of-the-apache-doris-execution-engine"}},"content":"\x3c!-- \\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\nOpenAI dropped a bomb on the data world by announcing [the acquisition of Rockset](https://openai.com/index/openai-acquires-rockset/), a cloud-based, fully managed analytical database. Among all the congratulating voices, one question is raised: **why [Rockset](https://rockset.com)**?\\n\\n![OpenAI acquisition Rockset](/images/openai-twitter-rockset.png)\\n\\nFounded in 2016 by Venkat Venkataramani, former Engineering Director at Meta, Rockset focuses on real-time search and data analytics. Compared to other DBMS, Rockset stands out by its:\\n\\n- **Real-time data updates**: Rockset ensures data freshness for users by its capabilities in fetching and delivering the latest data. It supports real-time updates at the granularity of data fields, which can be performed within milliseconds.\\n\\n- **Converged index**: It reaps the benefits of inverted index, columnar storage, and row-oriented storage, and provides efficient and flexible data querying services.\\n\\n- **Native support for semi-structured data**: Rockset is well-suited to the growing demand for semi-structured data processing, hash joins, and nested loop joins.\\n\\n- **SQL and JOIN compatibility**: The Search Index of Rockset is optimized for various join queries.\\n\\nThe news also gaves all Rockset users a ticking time bomb: they have to find an appropriate alternative to Rockset for their own use case within three months. This, of course, arises as an opportunity for other analytical databases on the market. However, of all the claim-to-be alternatives, only a few of them cover all the above-mentioned key features of Rockset. Among them, Apache Doris is worth looking into.\\n\\nAs an open-source real-time data warehouse, Apache Doris is trusted by over 4000 enterprise users worldwide with powerful functionalities including:\\n\\n- **Real-time data updates**: Apache Doris supports not only [real-time updates](https://doris.apache.org/docs/table-design/data-model/unique) and deletion, but also real-time partial column updates, making it particularly useful in cases involving frequent data updates.\\n\\n- **Row/column hybrid storage**: Apache Doris is a column-oriented data warehouse that achieves world-leading OLAP performance on [ClickBench](https://benchmark.clickhouse.com/). Additionally, it supports row-oriented storage to serve [high-concurrency point query scenarios](https://doris.apache.org/docs/query/high-concurrent-point-query/), which allows it to respond to almost a million query requests within milliseconds. \\n\\n- **[Inverted index](https://doris.apache.org/docs/table-design/index/inverted-index) and full-text searches**: Apache Doris provides high efficiency and flexibility in keyword searching. It allows index creation on all fields and a flexible combination of data fields for multi-dimensional data analysis.\\n\\n- **Native support for semi-structured data**: Apache Doris has introduced the [VARIANT](https://doris.apache.org/docs/sql-manual/sql-types/Data-Types/VARIANT) data type to accommodate semi-structured data. It enables flexible data schema and high query speed on top of cost-efficient data storage. Compared to traditional JSON methods, VARIANT can bring a 10x performance improvement.\\n\\n- **Support for various SQL and [join operations](https://doris.apache.org/docs/query/join-optimization/doris-join-optimization)**: Apache Doris is highly compatible with MySQL syntaxes and interfaces. It supports INNER JOIN, CROSS JOIN, and all types of OUTER JOIN. The best part is its capability of auto-optimization based on data types to guarantee optimal performance under different circumstances.\\n\\nAs a Top-Level Project of the Apache Software Foundation, Apache Doris is supported by a robust and fast-growing community. It has accumulated over 11.8K GitHub stars and 636 contributors so far.\\n\\nApache Doris is the best open-source alternative to Rockset. Feel free to contact dev@doris.apache.org for more assistance."},{"id":"/evolution-of-the-apache-doris-execution-engine","metadata":{"permalink":"/blog/evolution-of-the-apache-doris-execution-engine","source":"@site/blog/evolution-of-the-apache-doris-execution-engine.md","title":"Steps to industry-leading query speed: evolution of the Apache Doris execution engine","description":"From the Volcano Model to the Pipeline Execution Engine, and now PipelineX, Apache Doris brings its computation efficiency to a higher level with each iteration.","date":"2024-06-18T00:00:00.000Z","tags":[{"inline":true,"label":"Tech Sharing","permalink":"/blog/tags/tech-sharing"}],"hasTruncateMarker":false,"authors":[{"name":"velodb.io \xb7 VeloDB Engineering Team","key":null,"page":null}],"frontMatter":{"title":"Steps to industry-leading query speed: evolution of the Apache Doris execution engine","summary":"From the Volcano Model to the Pipeline Execution Engine, and now PipelineX, Apache Doris brings its computation efficiency to a higher level with each iteration.","description":"From the Volcano Model to the Pipeline Execution Engine, and now PipelineX, Apache Doris brings its computation efficiency to a higher level with each iteration.","date":"2024-06-18","author":"velodb.io \xb7 VeloDB Engineering Team","externalLink":"https://www.velodb.io/blog/573","tags":["Tech Sharing"],"image":"/images/evolution-of-the-apache-doris-execution-engine.jpg"},"unlisted":false,"prevItem":{"title":"Why Apache Doris is the best open source alternative to Rockset","permalink":"/blog/apache-doris-vs-rockset"},"nextItem":{"title":"Another lifesaver for data engineers: Apache Doris Job Scheduler for task automation","permalink":"/blog/job-scheduler-for-task-automation"}},"content":"\x3c!-- \\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\nWhat makes a modern database system? The three key modules are query optimizer, execution engine, and storage engine. Among them, the role of execution engine to the DBMS is like the chef to a restaurant. This article focuses on the execution engine of the [Apache Doris](https://doris.apache.org) data warehouse, explaining the secret to its high performance.\\n\\nTo illustrate the role of the execution engine, let\'s follow the execution process of an SQL statement: \\n\\n- Upon receiving an SQL query, the query optimizer performs syntax/lexical analysis and generates the optimal execution plan based on the cost model and optimization rules.\\n\\n\\n- The execution engine then schedules the plan to the nodes, which operate on data in the underlying storage engine and then return the query results.\\n\\nThe execution engine performs operations like data reading, filtering, sorting, and aggregation. The efficiency of these steps determines query performance and resource utilization. That\'s why different execution models bring distinction in query efficiency.\\n\\n## Volcano Model\\n\\nThe Volcano Model (originally known as the Iterator Model) predominates in analytical databases, followed by the Materialization Model and Vectorized Model. In a Volcano Model, each operation is abstracted as an operator, so the entire SQL query is an operator tree. During query execution, the tree is traversed top-down by calling the `next()` interface, and data is pulled and processed from the bottom up. This is called a **pull-based** execution model. \\n\\nThe Volcano Model is flexible, scalable, and easy to implement and optimize. It underpins Apache Doris before version 2.1.0. When a user initiates an SQL query, Doris parses the query, generates a distributed execution plan, and dispatches tasks to the nodes for execution. Each individual task is an **instance**. Take a simple query as an example: \\n\\n```SQL\\nselect age, sex from employees where age > 30\\n```\\n\\n![Volcano Model](/images/doris-volcano-model.png)\\n\\nIn an instance, data flows between operators are propelled by the `next()` method. If the `next()` method of an operator is called, it will first call the `next()` of its child operator, obtain data from it, and then process the data to produce output. \\n\\n`next()` is a synchronous method. In other words, the current operator will be blocked if its child operator does not provide data for it. In this case, the `next()` method of the root operator needs to be called in a loop until all data is processed, which is when the instance finishes its computation.\\n\\nSuch execution mechanism faces a few bottlenecks in single-node, multi-core use cases:\\n\\n- **Thread blocking**: In a fixed-size thread pool, if an instance occupies a thread and it is blocked, that will easily cause a deadlock when there are a large number of instances requesting execution simultaneously. This is especially the case when the current instance is dependent on other instances. Additionally, if a node is running more instances than the number of CPU cores it has, the system scheduling mechanism will be heavily relied upon and a huge context switching overhead can be produced. In a colocation scenario, this will lead to an even larger thread switching overhead.\\n\\n- **CPU contention**: The threads might compete for CPU resources so queries of different sizes and between different tenants might interfere with each other.\\n\\n- **Underutilization of the multi-core computing capabilities**: Execution concurrency relies heavily on data distribution. Specifically, the number of instances running on a node is limited by the number of data buckets on that node. In this case, it\'s important to set an appropriate number of buckets. If you shard the data into too many buckets, that will become a burden for the system and bring unnecessary overheads; if the buckets are too few, you will not be able to utilize your CPU cores to the fullest. However, in a production environment, it is not always easy to estimate the proper number of buckets you need, thus performance loss. \\n\\n## Pipeline Execution Engine\\n\\nBased on the known issues of Volcano Model, we\'ve replaced it with the Pipeline Execution Engine since Apache Doris 2.0.0. \\n\\nAs the name suggests, the Pipeline Execution Engine breaks down the execution plan into pipeline tasks, and schedules these pipeline tasks into a thread pool in a time-sharing manner. If a pipeline task is blocked, it will be put on hold to release the thread it is occupying. Meanwhile, it supports various scheduling strategies, meaning that you can allocate CPU resources to different queries and tenants more flexibly. \\n\\nAdditionally, the Pipeline Execution Engine pools together data within data buckets, so the number of running instances is no longer capped by the number of buckets. This not only enhances Apache Doris\' utilization of multi-core systems, but also improves system performance and stability by avoiding frequent thread creation and deletion.\\n\\n### Example\\n\\nThis is the execution plan of a join query. It includes two instances:\\n\\n\\n![Pipeline Execution Engine](/images/doris-pipeline-execution-engine.png)\\n\\nAs illustrated, the Probe operation can only be executed after the hash table is built, while the Build operation is reliant on the computation results of the Exchange operator. Each of the two instances is divided into two pipeline tasks as such. Then these tasks will be scheduled in the \\"ready\\" queue of the thread pool. Following the specified strategies, the threads obtain the tasks to process. In a pipeline task, after one data block is finished, if the relevant data is ready and its runtime stays within the maximum allowed duration, the thread will continue to compute the next data block. \\n\\n### Design & implementation\\n\\n**Avoid thread blocking**\\n\\nAs is mentioned earlier, the Volcano Model is faced with a few bottlenecks: \\n\\n1. If too many threads are blocked, the thread pool will be saturated and unable to respond to subsequent queries.\\n\\n\\n2. Thread scheduling is entirely managed by the operating system, without any user-level control or customization.\\n\\nHow does Pipeline Execution Engine avoid such issues?\\n\\n1. We fix the size of the thread pool to match the CPU core count. Then we split all operators that are prone to blocking into pipeline tasks. For example, we use individual threads for disk I/O operations and RPC operations.\\n\\n\\n2. We design a user-space polling scheduler. It continuously checks the state of all executable pipeline tasks and assigns executable tasks to threads. With this in place, the operating system doesn\'t have to frequently switch threads, thus less overheads. It also allows customized scheduling strategies, such as assigning priorities to tasks.\\n\\n![Design & implementation](/images/pipeline-design-implementation.png)\\n\\n**Parallelization**\\n\\nBefore version 2.0, Apache Doris requires users to set a concurrency parameter for the execution engine (`parallel_fragment_exec_instance_num`), which does not dynamically change based on the workloads. Therefore, it is a burden for users to figure out an appropriate concurrency level that leads to optimal performance.\\n\\nWhat\'s the industry\'s solution to this?\\n\\nPresto\'s idea is to shuffle the data into a reasonable number of partitions during execution, which requires minimal concurrency control from users. On the other hand, DuckDB introduces an extra synchronization mechanism instead of shuffling. We decide to follow Presto\'s track of Presto because the DuckDB solution inevitably involves the use of locks, which works against our purpose of avoiding blocking.\\n\\nUnlike Presto, Apache Doris doesn\'t need an extra Local Exchange mechanism to shards the data into an appropriate number of partitions. With its massively parallel processing (MPP) architecture, Doris already does so during shuffling. (In Presto\'s case, it re-partitions the data via Local Exchange for higher execution concurrency. For example, in hash aggregation, Doris further shards the data based on the aggregation key in order to fully utilize the CPU cores. Also, this can downsize the hash table that each execution thread has to build.)\\n\\n\\n![Design & implementation](/images/pipeline-design-implementation-2.png)\\n\\nBased on the MPP architecture, we only need two improvements before we achieve what we want in Doris:\\n\\n- **Increase the concurrency level during shuffling**. For this, we only need to have the frontend (FE) perceive the backend (BE) environment and then set a reasonable number of partitions.\\n\\n\\n- **Implement concurrent execution after data reading by the scan layer**. To do this, we need a logical restructuring of the scan layer to decouple the threads from the number of data tablets. This is a pooling process. We pool the data read by scanner threads, so it can be fetched by multiple pipeline tasks directly. \\n\\n![Design & implementation](/images/pipleine-design-implementation-3.jpeg)\\n\\n## PipelineX \\n\\nIntroduced in Apache Doris 2.0.0, the pipeline execution engine has been improving query performance and stability under hybrid workload scenarios (queries of different sizes and from different tenants). In [version 2.1.0](https://doris.apache.org/blog/release-note-2.1.0), we\'ve tackled the known issues and upgraded this from an experimental feature to a robust and reliable solution, which is what we call [PipelineX](https://doris.apache.org/docs/query/pipeline/pipeline-x-execution-engine).\\n\\nPipelineX has provided answers to the following issues that used to challenge the Pipeline Execution Engine:\\n\\n- **Limited execution concurrency**\\n\\n\\n- **High execution overhead**\\n\\n\\n- **High scheduling overhead**\\n\\n\\n- **Poor readability of operator profile**\\n\\n### Execution concurrency\\n\\nThe Pipeline Execution Engine remains under the restriction of the static concurrency parameter at FE and the tablet count at the storage layer, making itself unable to capitalize on the full computing resources. Plus, it is easily affected by data skew. \\n\\nFor example, suppose that Table A contains 100 million rows but it has only 1 tablet, which means it is not sharded enough, let\'s see what can happen when you perform an aggregation query on it: \\n\\n```C++\\n SELECT COUNT(*) FROM A GROUP BY A.COL_1;\\n```\\n\\nDuring query execution, the query plan is divided into two **fragments**. Each fragment, consisting of multiple operators, is dispatched by frontend (FE) to backend (BE). The BE starts threads to execute the fragments concurrently.\\n\\n![Pipeline Execution concurrency](/images/doris-pipelinex.png)\\n\\nNow, let\'s focus on Fragment 0 for further elaboration. Because there is only one tablet, Fragment 0 can only be executed by one thread. That means aggregation of 100 million rows by one single thread. If you have 16 CPU cores, ideally, the system can allocate 8 threads to execute Fragment 0. In this case, there is a concurrency disparity of 8 to 1. This is how **the number of tablets restricts execution concurrency** and also why we introduce the idea of **Local Shuffle mechanism to remove that restriction** in Apache Doris 2.1.0. So this is how it works in PipelineX: \\n\\n- The threads execute their own pipeline tasks, but the pipeline tasks only maintain their runtime state (known as **Local State**), while the information that shared across all pipeline tasks (known as **Global State**) is managed by one pipeline object.\\n\\n\\n- On a single BE, the Local Shuffle mechanism is responsible for data distribution and data balancing across pipeline tasks.\\n\\n\\n![Pipeline Execution concurrency](/images/doris-pipelinex-1.png)\\n\\nApart from decoupling execution concurrency from tablet count, Local Shuffle can avoid performance loss due to data skew. Again, we will explain with the foregoing example.\\n\\nThis time, we shard Table A into two tablets instead of one, but the data is not evenly distributed. Tablet 1 and Tablet 3 hold 10 million and 90 million rows, respectively. The Pipeline Execution Engine and PipelineX Execution Engine respond differently to such data skew:\\n\\n- **Pipeline Execution Engine**: Thread 1 and Thread 2 executes Fragment 1 concurrently. The latter takes 9 times as long as Thread 1 because of the different data sizes they deal with.\\n\\n\\n- **PipelineX Execution Engine**: With Local Shuffle, data is distributed evenly to the two threads, so they take almost equal time to finish.\\n\\n![Pipeline vs PipelineX execution engine](/images/doris-pipelinex-3.png)\\n\\n### Execution overhead\\n\\nUnder the Pipeline Execution Engine, because the expressions of different instances are individual, each instance is initialized individually. However, since the initialization parameters of instances share a lot in common, we can reuse the shared states to reduce execution overheads. This is what PipelineX does: it initializes the Global State at a time, and the Local State sequentially.\\n\\n![Execution overhead](/images/pipeline-execution-overhead.png)\\n\\n### Scheduling overhead\\n\\nIn the Pipeline Execution Engine, the blocked tasks are put into a blocked queue, where a dedicated thread takes polls and moves the executable tasks over to the runnable queue. This dedicated scheduling thread consumes a CPU core, and incurs overheads that can be particularly noticeable on systems with limited computing resources.\\n\\n**As a better solution, PipelineX encapsulates the blocking conditions as dependencies, and the task status (blocked or runnable) will be triggered to change by event notifications.** Specifically, when RPC data arrives, the relevant task will be considered as ready by the ExchangeSourceOperator and then moved to the runnable queue.\\n\\n![Scheduling overhead](/images/pipeline-scheduling-overhead.png)\\n\\nThat means **PipelineX implements event-driven scheduling**. A query execution plan can be depicted as a DAG, where the pipeline tasks are abstracted as nodes and the dependencies as edges. Whether a pipeline task gets executed depends on whether all its associated dependencies have satisfied the requisite conditions.\\n\\n![Scheduling overhead](/images/pipeline-scheduling-overhead-2.jpeg)\\n\\nFor simplicity of illustration, the above DAG only shows the dependencies between the upstream and downstream pipeline tasks. In fact, all blocking conditions are abstracted as dependencies. The complete execution workflow of a pipeline task is as follows:\\n\\n![Scheduling overhead](/images/pipeline-scheduling-overhead-3.png)\\n\\nIn event-driven execution, a pipeline task will only be executed after all its dependencies satisfy the conditions; otherwise, it will be added to the blocked queue. When an external event arrives, all blocked tasks will be re-evaluated to see if they\'re runnable.\\n\\nThe event-driven design of PipelineX eliminates the need for a polling thread and thus the consequential performance loss under high cluster loads. Moreover, the encapsulation of dependencies enables a more flexible scheduling framework, making it easier to spill data to disks.\\n\\n### Operator profile\\n\\nPipelineX has reorganized the metrics in the operator profiles, adding new ones and obsoleting irrelevant ones. Besides, with the dependencies encapsulated, we monitor how long the dependencies take to get ready by the metric `WaitForDependency`, so the profile can provide a clear picture of the time spent in each step. These are two examples:\\n\\n- **Scan Operator**: The total execution time of `OLAP_SCAN_OPERATOR` is 457.750ms, including that spent in data reading by the scanner (436.883ms) and that in actual execution.\\n\\n  ```C++\\n  OLAP_SCAN_OPERATOR  (id=4.  table  name  =  Z03_DI_MID):\\n      -  ExecTime:  457.750ms\\n      -  WaitForDependency[OLAP_SCAN_OPERATOR_DEPENDENCY]Time:  436.883ms\\n  ```\\n\\n- **Exchange Source Operator**: The execution time of `EXCHANGE_OPERATOR` is 86.691us. The time spent waiting for data from upstream is 409.256us.\\n\\n  ```C++\\n  EXCHANGE_OPERATOR  (id=3):\\n      -  ExecTime:  86.691us\\n      -  WaitForDependencyTime:  0ns\\n          -  WaitForData0:  409.256us\\n  ```\\n\\n## What\'s next\\n\\nFrom the Volcano Model to the Pipeline Execution Engine, Apache Doris 2.0.0 has overcome the deadlocks under high cluster load and greatly increased CPU utilization. Now, from the Pipeline Execution Engine to PipelineX, Apache Doris 2.1.0 is more production-friendly as it has ironed out the kinks in concurrency, overheads, and operator profile. \\n\\nWhat\'s next in our roadmap is to support spilling data to disk in PipelineX to further improve query speed and system reliability. We also plan to advance further in terms of automation, such as self-adaptive concurrency and auto execution plan optimization, accompanied by NUMA technologies to harvest better performance from hardware resources. \\n\\nIf you want to talk to the amazing Doris developers who lead these changes, you are more than welcome to join the [Apache Doris](https://join.slack.com/t/apachedoriscommunity/shared_invite/zt-2gmq5o30h-455W226d79zP3L96ZhXIoQ) community."},{"id":"/job-scheduler-for-task-automation","metadata":{"permalink":"/blog/job-scheduler-for-task-automation","source":"@site/blog/job-scheduler-for-task-automation.md","title":"Another lifesaver for data engineers: Apache Doris Job Scheduler for task automation","description":"The built-in Doris Job Scheduler triggers pre-defined operations efficiently and reliably. It is useful in many cases including ETL and data lake analytics.","date":"2024-06-06T00:00:00.000Z","tags":[{"inline":true,"label":"Tech Sharing","permalink":"/blog/tags/tech-sharing"}],"hasTruncateMarker":false,"authors":[{"name":"velodb.io \xb7 VeloDB Engineering Team","key":null,"page":null}],"frontMatter":{"title":"Another lifesaver for data engineers: Apache Doris Job Scheduler for task automation","summary":"The built-in Doris Job Scheduler triggers pre-defined operations efficiently and reliably. It is useful in many cases including ETL and data lake analytics.","description":"The built-in Doris Job Scheduler triggers pre-defined operations efficiently and reliably. It is useful in many cases including ETL and data lake analytics.","date":"2024-06-06","author":"velodb.io \xb7 VeloDB Engineering Team","externalLink":"https://www.velodb.io/blog/574","tags":["Tech Sharing"],"image":"/images/job-scheduler-for-task-automation.jpg"},"unlisted":false,"prevItem":{"title":"Steps to industry-leading query speed: evolution of the Apache Doris execution engine","permalink":"/blog/evolution-of-the-apache-doris-execution-engine"},"nextItem":{"title":"Apache Doris version 2.0.11 has been released","permalink":"/blog/release-note-2.0.11"}},"content":"\x3c!-- \\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\nJob scheduling is an important part of data management as it enables regular data updates and cleanups. In a data platform, it is often undertaken by workflow orchestration tools like [Apache Airflow](https://airflow.apache.org) and [Apache Dolphinscheduler](https://dolphinscheduler.apache.org/en-us). However, adding another component to the data architecture also means investing extra resources for management and maintenance. That\'s why [Apache Doris 2.1.0](https://doris.apache.org/blog/release-note-2.1.0) introduces a built-in Job Scheduler. It is strategically more tailored to Apache Doris, and brings higher scheduling flexibility and architectural simplicity. \\n\\nThe Doris Job Scheduler triggers the pre-defined operations at specific time points or intervals, thus allowing for efficient and reliable task automation. Its key capabilities include: \\n\\n- **Efficiency**: It adopts the TimeWheel algorithm to ensure that the triggering of tasks is precise to the second.\\n\\n- **Flexibility**: It supports both one-time jobs and regular jobs. For the latter, users can define the start/end time, and intervals of minutes, hours, days, or weeks.\\n\\n- **Execution thread pool and processing queue**: It is supported by a Disruptor-based single-producer, multi-consumer model to avoid task execution overload.\\n\\n- **Traceability**: It keeps track of the latest task execution records (configurable), which are queryable by a simple command. \\n\\n- **Availability**: Like Apache Doris itself, the Doris Job Scheduler is easily recoverable and highly available.\\n\\n## Syntax & examples\\n\\n### Syntax description\\n\\nA valid job statement consists of the following elements:\\n\\n- `CREATE JOB`: Specifies the job name as a unique identifier.\\n\\n- The `ON SCHEDULE` clause: Specifies the type, trigger time, and frequency of the job.\\n\\n  - `AT timestamp`: This is used to specify a one-time job. `AT CURRENT_TIMESTAMP` means that the job will run immediately upon creation. \\n\\n  - `EVERY`: This is used to specify a regular job. You can define the execution frequency of the job. The interval can be measured in weeks, days, hours, and minutes.\\n\\n    - The `EVERY` clause supports an optional `STARTS` clause  with a timestamp to define the start time of the recurring schedule. `CURRENT_TIMESTAMP` can be used. It also supports an optional `ENDS` clause to specify the end time for the job.\\n\\n- The `DO` clause defines the action to be performed when the job is executed. At this time, the only supported operation is INSERT.\\n\\n  ```sql\\n  CREATE\\n      JOB\\n      job_name\\n      ON SCHEDULE schedule\\n      [COMMENT \'string\']\\n      DO execute_sql;\\n\\n  schedule: {\\n      AT timestamp \\n    | EVERY interval\\n      [STARTS timestamp ]\\n      [ENDS timestamp ]\\n  }\\n\\n  interval:\\n      quantity { WEEK |DAY | HOUR | MINUTE\\n              }\\n              \\n  ```\\n\\n  Example:\\n\\n  ```sql\\n  CREATE JOB my_job ON SCHEDULE EVERY 1 MINUTE DO INSERT INTO db1.tbl1 SELECT * FROM db2.tbl2;\\n  ```\\n\\n  The above statement creates a job named `my_job`, which is to load data from `db2.tbl2` to `db1.tbl1` every minute.\\n\\n### More examples\\n\\n**Create a one-time job**: Load data from `db2.tbl2` to `db1.tbl1` at 2025-01-01 00:00:00.\\n\\n```sql\\nCREATE JOB my_job ON SCHEDULE AT \'2025-01-01 00:00:00\' DO INSERT INTO db1.tbl1 SELECT * FROM db2.tbl2;\\n```\\n\\n**Create a regular job without specifying the end time**: Load data from `db2.tbl2` to `db1.tbl1` once a day starting from 2025-01-01 00:00:00.\\n\\n```sql\\nCREATE JOB my_job ON SCHEDULE EVERY 1 DAY STARTS \'2025-01-01 00:00:00\' DO INSERT INTO db1.tbl1 SELECT * FROM db2.tbl2 WHERE  create_time >=  days_add(now(),-1);\\n```\\n\\n**Create a regular job within a specified period**: Load data from `db2.tbl2` to `db1.tbl1` once a day, beginning at 2025-01-01 00:00:00 and finishing at 2026-01-01 00:10:00.\\n\\n```sql\\nCREATE JOB my_job ON SCHEDULER EVERY 1 DAY STARTS \'2025-01-01 00:00:00\' ENDS \'2026-01-01 00:10:00\' DO INSERT INTO db1.tbl1 SELECT * FROM db2.tbl2 create_time >=  days_add(now(),-1);\\n```\\n\\n**Asynchronous execution**: Because jobs are executed in an asynchronous manner in Doris. Tasks that require asynchronous execution, such as `insert into select`, can be implemented by a job. \\n\\nFor example, to asynchronously execute data loading from `db2.tbl2` to `db1.tbl1`, simply create a one-time job for it and schedule it at `current_timestamp`.\\n\\n```shell\\nCREATE JOB my_job ON SCHEDULE AT current_timestamp DO INSERT INTO db1.tbl1 SELECT * FROM db2.tbl2;\\n```\\n\\n## Auto data synchronization\\n\\nThe combination of the Job Scheduler and the [Multi-Catalog](https://doris.apache.org/docs/lakehouse/lakehouse-overview#multi-catalog) feature of Apache Doris is an efficient way to implement regular data synchronization across data sources.\\n\\nThis is useful in many cases, such as for an e-commerce user who regularly needs to load business data from MySQL to Doris for analysis.\\n\\n**Example**: To filter consumers by total consumption amount, last visit time, sex, and city in the table below, and import the query results to Doris regularly.\\n\\n![Auto data synchronization](/images/auto-data-synchronization.png)\\n\\n**Step 1**: Create a table in Doris\\n\\n```sql\\nCREATE TABLE IF NOT EXISTS user_activity\\n(\\n    `user_id` LARGEINT NOT NULL COMMENT \\"User ID\\",\\n    `date` DATE NOT NULL COMMENT \\"Time of data import\\",\\n    `city` VARCHAR(20) COMMENT \\"User city\\",\\n    `age` SMALLINT COMMENT \\"User age\\",\\n    `sex` TINYINT COMMENT \\"User sex\\",\\n    `last_visit_date` DATETIME REPLACE DEFAULT \\"1970-01-01 00:00:00\\" COMMENT \\"Time of user\'s last visit\\",\\n    `cost` BIGINT SUM DEFAULT \\"0\\" COMMENT \\"User\'s total consumption amount\\",\\n    `max_dwell_time` INT MAX DEFAULT \\"0\\" COMMENT \\"Maximum dwell time of user\\",\\n    `min_dwell_time` INT MIN DEFAULT \\"99999\\" COMMENT \\"Minimum dwell time of user\\"\\n)\\nAGGREGATE KEY(`user_id`, `date`, `city`, `age`, `sex`)\\nDISTRIBUTED BY HASH(`user_id`) BUCKETS 1\\nPROPERTIES (\\n\\"replication_allocation\\" = \\"tag.location.default: 1\\"\\n);\\n```\\n\\n**Step 2**: Create a catalog in Doris to map to the data in MySQL\\n\\n```shell\\nCREATE CATALOG activity PROPERTIES (\\n    \\"type\\"=\\"jdbc\\",\\n    \\"user\\"=\\"root\\",\\n    \\"jdbc_url\\" = \\"jdbc:mysql://127.0.0.1:9734/user?useSSL=false\\",\\n    \\"driver_url\\" = \\"mysql-connector-java-5.1.49.jar\\",\\n    \\"driver_class\\" = \\"com.mysql.jdbc.Driver\\"\\n);\\n```\\n\\n**Step 3**: Ingest data from MySQL to Doris. Leverage the catalog mechanism and the Insert Into method for full data ingestion. (We recommend that such operations be executed during low-traffic hours to minimize potential service disruptions.)\\n\\n- **One-time job**: Schedule a one-time full-scale data loading that starts at 2024-8-10 03:00:00.\\n\\n  ```sql\\n  CREATE JOB one_time_load_job\\n  ON SCHEDULE \\n  AT \'2024-8-10 03:00:00\'\\n  DO\\n  INSERT INTO user_activity FROM SELECT * FROM activity.user.activity \\n  \\n  ```\\n\\n- **Regular job**: Create a regular job to update data periodically.\\n\\n  ```sql\\n  CREATE JOB schedule_load\\n  ON SCHEDULE EVERY 1 DAY\\n  DO\\n  INSERT INTO user_activity FROM SELECT * FROM activity.user.activity where create_time >=  days_add(now(),-1)\\n  ```\\n\\n## Technical design & implementation\\n\\nEfficient scheduling often comes at the cost of significant resource consumption, and high-precision scheduling is even more resource-intensive. To implement job scheduling, some people rely on the built-in scheduling capabilities of Java, while others employ job scheduling libraries. But what if we want higher precision and lower memory usage than these solutions can reach? For that, the Doris makers combine the TimingWheel algorithm with the Disruptor framework to achieve second-level job scheduling.\\n\\n![Technical design & implementation](/images/technical-design-and-implementation.png)\\n\\nTo implement the TimingWheel algorithm, we leverage the HashedWheelTimer in Netty. The Job Manager puts tasks every 10 minutes (by default) in the TimeWheel for scheduling. In order to ensure efficient task triggering and avoid high resource usage, we adopt a Disruptor-based single-producer, multi-consumer model. The TimeWheel only triggers tasks but does not execute jobs directly. Tasks that need to be triggered upon expiration will be put into a Dispatch thread and distributed to an appropriate execution thread pool. Tasks that need to be executed immediately will be directly submitted to the corresponding execution thread pool.\\n\\nThis is how we improve processing efficiency by reducing unnecessary traversal: For one-time tasks, their definition will be removed after execution. For recurring tasks, the system events in the TimeWheel will periodically fetch the next round of execution tasks. This helps to avoid the accumulation of tasks in a single bucket.\\n\\nIn addition, for transactional tasks, the Job Scheduler can ensure data consistency and integrity by the transaction association and transaction callback mechanisms. \\n\\n## Applicable scenarios\\n\\nThe Doris Job Scheduler is a Swiss Army Knife. It is not only useful in ETL and data lake analytics as we mentioned, but also critical for the implementation of [asynchronous materialized views](https://doris.apache.org/docs/query/view-materialized-view/async-materialized-view). An asynchronous materialized view is a pre-computed result set. Unlike normal materialized views, it can be built on multiple tables. Thus, as you can imagine, changes in any of the source tables will lead to the need for updates in the asynchronous materialized view. That\'s why we apply the job scheduling mechanism for periodic data refreshing in asynchronous materialized views, which is low-maintenance and also ensures data consistency.\\n\\nWhere are we going with the Doris Job Scheduler? The [Apache Doris developer community](https://join.slack.com/t/apachedoriscommunity/shared_invite/zt-2gmq5o30h-455W226d79zP3L96ZhXIoQ) is looking at:\\n\\n- Displaying the distribution of tasks executed in different time slots on the WebUI.\\n\\n- DAG jobs. This will allow data warehouse task orchestration within Apache Doris, which will unlock many possibilities when it is combined with the Multi-Catalog feature. \\n\\n- Support for more operations such as UPDATE and DELETE."},{"id":"/release-note-2.0.11","metadata":{"permalink":"/blog/release-note-2.0.11","source":"@site/blog/release-note-2.0.11.md","title":"Apache Doris version 2.0.11 has been released","description":"Thanks to our community users and developers, about 123 improvements and bug fixes have been made in Doris 2.0.11 version.","date":"2024-06-05T00:00:00.000Z","tags":[{"inline":true,"label":"Release Notes","permalink":"/blog/tags/release-notes"}],"hasTruncateMarker":false,"authors":[{"name":"Apache Doris","key":null,"page":null}],"frontMatter":{"title":"Apache Doris version 2.0.11 has been released","summary":"Thanks to our community users and developers, about 123 improvements and bug fixes have been made in Doris 2.0.11 version.","description":"Thanks to our community users and developers, about 123 improvements and bug fixes have been made in Doris 2.0.11 version.","date":"2024-06-05","author":"Apache Doris","tags":["Release Notes"],"image":"/images/2.0.11.jpg"},"unlisted":false,"prevItem":{"title":"Another lifesaver for data engineers: Apache Doris Job Scheduler for task automation","permalink":"/blog/job-scheduler-for-task-automation"},"nextItem":{"title":"Apache Doris for log and time series data analysis in NetEase, why not Elasticsearch and InfluxDB?","permalink":"/blog/apache-doris-for-log-and-time-series-data-analysis-in-netease"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n  http://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\nThanks to our community users and developers, about 123 improvements and bug fixes have been made in Doris 2.0.11 version.\\n\\n**Quick Download:** [https://doris.apache.org/download/](https://doris.apache.org/download/)\\n\\n**GitHub\uFF1A** [https://github.com/apache/doris/releases](https://github.com/apache/doris/releases)\\n\\n## Behavior changes\\n\\nSince the inverted index is now mature and stable, it can replace the old BITMAP INDEX. Therefore, any newly created `BITMAP INDEX` will automatically switch to an `INVERTED INDEX`, while existing `BITMAP INDEX` will remain unchanged. This entire switching process is transparent to the user, with no changes to writing or querying. Additionally, users can disable this automatic switch by setting the FE configuration `enable_create_bitmap_index_as_inverted_index` to false. [#35528](https://github.com/apache/doris/pull/35528)\\n\\n\\n## Improvements\\n\\n- Add Trino JDBC Catalog type mapping for JSON and TIME\\n\\n- FE exit when failed to transfer to (non) master to prevent unknown state and too many logs\\n\\n- Write audit log while doing drop stats table.\\n\\n- Ignore min/max column stats if table is partially analyzed to avoid inefficient query plan\\n\\n- Support minus operation for set like `set1 - set2`\\n\\n- Improve perfmance of LIKE and REGEXP clause with concat (col, pattern_str), eg. `col1 LIKE concat(\'%\', col2, \'%\')`\\n\\n- Add query options for short circuit queries for upgrade compatibility\\n\\nSee the complete list of improvements and bug fixes on [github](https://github.com/apache/doris/compare/2.0.10...2.0.11) .\\n\\n## Credits\\n\\nThanks all who contribute to this release:\\n\\n@AshinGau, @BePPPower, @BiteTheDDDDt, @ByteYue, @CalvinKirs, @cambyzju, @csun5285, @dataroaring, @eldenmoon, @englefly, @feiniaofeiafei, @Gabriel39, @GoGoWen, @HHoflittlefish777, @hubgeter, @jacktengg, @jackwener, @jeffreys-cat, @Jibing-Li, @kaka11chen, @kobe6th, @LiBinfeng-01, @mongo360, @morningman, @morrySnow, @mrhhsg, @Mryange, @nextdreamblue, @qidaye, @sjyango, @starocean999, @SWJTU-ZhangLei, @w41ter, @wangbo, @wsjz, @wuwenchi, @xiaokang, @XieJiann, @xy720, @yujun777, @Yukang-Lian, @Yulei-Yang, @zclllyybb, @zddr, @zhangstar333, @zhiqiang-hhhh, @zy-kkk, @zzzxl1993"},{"id":"/apache-doris-for-log-and-time-series-data-analysis-in-netease","metadata":{"permalink":"/blog/apache-doris-for-log-and-time-series-data-analysis-in-netease","source":"@site/blog/apache-doris-for-log-and-time-series-data-analysis-in-netease.md","title":"Apache Doris for log and time series data analysis in NetEase, why not Elasticsearch and InfluxDB?","description":"NetEase (NASDAQ: NTES) has replaced Elasticsearch and InfluxDB with Apache Doris in its monitoring and time series data analysis platforms, respectively, achieving 11X query performance and saving 70% of resources.","date":"2024-05-23T00:00:00.000Z","tags":[{"inline":true,"label":"Best Practice","permalink":"/blog/tags/best-practice"}],"hasTruncateMarker":false,"authors":[{"name":"velodb.io \xb7 VeloDB Engineering Team","key":null,"page":null}],"frontMatter":{"title":"Apache Doris for log and time series data analysis in NetEase, why not Elasticsearch and InfluxDB?","summary":"NetEase (NASDAQ: NTES) has replaced Elasticsearch and InfluxDB with Apache Doris in its monitoring and time series data analysis platforms, respectively, achieving 11X query performance and saving 70% of resources.","description":"NetEase (NASDAQ: NTES) has replaced Elasticsearch and InfluxDB with Apache Doris in its monitoring and time series data analysis platforms, respectively, achieving 11X query performance and saving 70% of resources.","date":"2024-05-23","author":"velodb.io \xb7 VeloDB Engineering Team","externalLink":"https://www.velodb.io/blog/437","tags":["Best Practice"],"image":"/images/doris-for-log-and-time-series-data-analysis-in-netease.jpg"},"unlisted":false,"prevItem":{"title":"Apache Doris version 2.0.11 has been released","permalink":"/blog/release-note-2.0.11"},"nextItem":{"title":"Apache Doris 2.1.3 just released","permalink":"/blog/release-note-2.1.3"}},"content":"\x3c!-- \\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\nFor most people looking for a log management and analytics solution, Elasticsearch is the go-to choice. The same applies to InfluxDB for time series data analysis. These were exactly the choices of [NetEase,Inc. *(NASDAQ: NTES)*](https://finance.yahoo.com/quote/NTES/), one of the world\'s highest-yielding game companies but more than that. As NetEase expands its business horizons, the logs and time series data it receives explode, and problems like surging storage costs and declining stability come. As NetEase\'s pick among all big data components for platform upgrades, [Apache Doris](https://doris.apache.org) fits into both scenarios and brings much faster query performance.  \\n\\nWe list the gains of NetEase after adopting Apache Doris in their monitoring platform and time series data platform, and share their best practice with users who have similar needs.\\n\\n## Monitoring platform: Elasticsearch -> Apache Doris\\n\\nNetEase provides a collaborative workspace platform that combines email, calendar, cloud-based documents, instant messaging, and customer management, etc. To oversee its performance and availability, NetEase builds the Eagle monitoring platform, which collects logs for analysis. Eagle was supported by Elasticsearch and Logstash. The data pipeline was simple: Logstash gathers log data, cleans and transforms it, and then outputs it to Elasticsearch, which handles real-time log retrieval and analysis requests from users.\\n\\n![Monitoring platform: Elasticsearch -> Apache Doris](/images/monitoring-platform-elasticsearch.PNG)\\n\\nDue to NetEase\'s increasingly sizable log dataset, Elastisearch\'s index design, and limited hardware resources, the monitoring platform exhibits **high latency** in daily queries. Additionally, Elasticsearch maintains high data redundancy for forward indexes, inverted indexes, and columnar storage. This adds to cost pressure.\\n\\nAfter migration to Apache Doris, NetEase achieves a 70% reduction in storage costs and an 11-fold increase in query speed. \\n\\n![Monitoring platform: Elasticsearch -> Apache Doris](/images/monitoring-platform-apache-doris.PNG)\\n\\n- **70% reduction in storage costs**: This means a dataset that takes up 100TB in Elasticsearch only requires 30TB in Apache Doris. Moreover, thanks to the much-reduced storage space usage, they can replace their HDDs with more expensive SSDs for hot data storage to achieve higher query performance while staying within the same budget.\\n\\n- **11-fold increase in query speed**: Apache Doris can deliver faster queries while consuming less CPU resources than Elasticsearch. As shown below, Doris has reliably low latency in queries of various sizes, while Elasticsearch demonstrates longer latency and greater fluctuations, and the smallest speed difference is 11-fold. \\n\\n![Apache Doris vs Elasticsearch](/images/doris-vs-elasticsearch-query-latency.PNG)\\n\\n## Time series data platform: InfluxDB -> Apache Doris \\n\\nNetEase is also an instant messaging (IM) PaaS provider. To support this, it builds a data platform to analyze time series data from their IM services. The platform was built on InfluxDB, a time series database. Data flowed into a Kafka message queue. After the fields were parsed and cleaned, they arrived in InfluxDB, ready to be queried. InfluxDB responded to both online and offline queries. The former was to generate metric monitoring reports and bills in real time, and the latter was to batch analyze data from a day ago. \\n\\n![Time series data platform: InfluxDB -> Apache Doris ](/images/time-series-data-platform-from-influxdb-to-apache-doris.PNG)\\n\\nThis platform was also challenged by the increasing data size and diversifying data sources.\\n\\n- **OOM**: Offline data analysis across multiple data sources was putting InfluxDB under huge pressure and causing OOM errors.\\n\\n- **High storage costs**: Cold data took up a large portion but it was stored the same way as hot data. That added up to huge expenditures.\\n\\n![Time series data platform: InfluxDB -> Apache Doris ](/images/time-series-data-platform-influxdb-to-apache-doris-2.PNG)\\n\\nReplacing InfluxDB with Apache Doris has brought higher cost efficiency to the data platform:\\n\\n- **Higher throughput**: Apache Doris maintains a writing throughput of 500MB/s and achieves a peak writing throughput of 1GB/s. With InfluxDB, they used to require 22 servers for a CPU utilization rate of 50%. Now, with Doris, it only takes them 11 servers at the same CPU utilization rate. That means Doris helps cut down resource consumption by half.\\n\\n- **67% less storage usage**: The same dataset used 150TB of storage space with InfluxDB but only took up 50TB with Doris. Thus, Doris helps reduce storage costs by 67%.\\n\\n- **Faster and more stable query performance**: The performance test was to select a random online query SQL and run it 99 consecutive times. As is shown below, Doris delivers generally faster response time and maintains stability throughout the 99 queries.\\n\\n![Doris vs InfluxDB](/images/doris-vs-influxdb-cost-effectivity.png)\\n\\n## Best practice\\n\\nAdopting a new product and putting it into a production environment is, after all, a big project. The NetEase engineers came across a few hiccups during the journey, and they are kind enough to share about how they solved these problems and save other users some detours.\\n\\n### Table creation\\n\\nTable schema design has a significant impact on database performance, and this holds for log and time series data processing as well. Apache Doris provides optimization options for these scenarios. These are some recommendations provided by NetEase.\\n\\n1. **Retrieval of the latest N logs**: Using a `DATETIME` type time field as the primary key can largely speed queries up.\\n\\n2. **Partitioning strategy**: Use `PARTITION BY RANGE` based on a time field and enable [dynamic partition](https://doris.apache.org/docs/2.0/table-design/data-partitioning/dynamic-partitioning). This allows for  auto-management of data partitions.\\n\\n3. **Bucketing strategy**: Adopt random bucketing and set the number of buckets to roughly three times the total number of disks in the cluster. (Apache Doris also provides an [auto bucket](https://doris.apache.org/docs/2.0/table-design/data-partitioning/auto-bucket) feature to avoid performance loss caused by improper data sharding.)\\n\\n4. **Indexing**: Create indexes for frequently searched fields to improve query efficiency. Pay attention to the parser for the fields that require full-text searching, because it determines query accuracy.\\n\\n5. **Compaction**: Optimize the compaction strategies based on your own business needs.\\n\\n6. **Data compression**: Enable `ZSTD` for better a higher compression ratio.\\n\\n```sql\\nCREATE TABLE log\\n(\\n    ts DATETIME,\\n    host VARCHAR(20),\\n    msg TEXT,\\n    status INT,\\n    size INT,\\n    INDEX idx_size (size) USING INVERTED,\\n    INDEX idx_status (status) USING INVERTED,\\n    INDEX idx_host (host) USING INVERTED,\\n    INDEX idx_msg (msg) USING INVERTED PROPERTIES(\\"parser\\" = \\"unicode\\")\\n)\\nENGINE = OLAP\\nDUPLICATE KEY(ts)\\nPARTITION BY RANGE(ts) ()\\nDISTRIBUTED BY RANDOM BUCKETS 250\\nPROPERTIES (\\n    \\"compression\\"=\\"zstd\\",\\n    \\"compaction_policy\\" = \\"time_series\\",\\n    \\"dynamic_partition.enable\\" = \\"true\\",\\n    \\"dynamic_partition.create_history_partition\\" = \\"true\\",\\n    \\"dynamic_partition.time_unit\\" = \\"DAY\\",\\n    \\"dynamic_partition.start\\" = \\"-7\\",\\n    \\"dynamic_partition.end\\" = \\"3\\",\\n    \\"dynamic_partition.prefix\\" = \\"p\\",\\n    \\"dynamic_partition.buckets\\" = \\"250\\"\\n);\\n```\\n\\n### Cluster configuration\\n\\n**Frontend (FE) configuration**\\n\\n```sql\\n# For higher data ingestion performance:\\nenable_single_replica_load = true\\n\\n# For more balanced tablet distribution:\\nenable_round_robin_create_tablet = true\\ntablet_rebalancer_type = partition\\n\\n# Memory optimization for frequent imports:\\nmax_running_txn_num_per_db = 10000\\nstreaming_label_keep_max_second = 300\\nlabel_clean_interval_second = 300\\n```\\n\\n**Backend (BE) configuration**\\n\\n```SQL\\nwrite_buffer_size=1073741824\\nmax_tablet_version_num = 20000\\nmax_cumu_compaction_threads = 10 (Half of the total number of CPUs)\\nenable_write_index_searcher_cache = false\\ndisable_storage_page_cache = true\\nenable_single_replica_load = true\\nstreaming_load_json_max_mb=250\\n```\\n\\n### Stream Load optimization\\n\\nDuring peak times, the data platform is undertaking up to 1 million TPS and a writing throughput of 1GB/s. This is demanding for the system. Meanwhile, at peak time, a large number of concurrent write operations are loading data into lots of tables, but each individual write operation only involves a small amount of data. Thus, it takes a long time to accumulate a batch, which is contradictory to the data freshness requirement from the query side.\\n\\nAs a result, the data platform was bottlenecked by data backlogs in Apache Kafka. NetEase adopts the [Stream Load](https://doris.apache.org/docs/2.0/data-operate/import/stream-load-manual) method to ingest data from Kafka to Doris. So the key was to accelerate Stream Load. After talking to the [Apache Doris developers](https://join.slack.com/t/apachedoriscommunity/shared_invite/zt-2unfw3a3q-MtjGX4pAd8bCGC1UV0sKcw), NetEase adopted two optimizations for their log and time series data analysis:\\n\\n- **Single replica data loading**: Load one data replica and pull data from it to generate more replicas. This avoids the overhead of ranking and creating indexes for multiple replicas.\\n\\n- **Single tablet data loading** (`load_to_single_tablet=true`): Compared to writing data to multiple tablets, this reduces the I/O overhead and the number the small files generated during data loading. \\n\\n\\nThe above measures are effective in improving data loading performance:\\n\\n- **2X data consumption speed from Kafka**\\n\\n![2X data consumption speed from Kafka](/images/doris-data-loading-performance-1.png)\\n\\n- **75% lower data latency**\\n\\n![75% lower data latency](/images/doris-data-loading-performance-2.png)\\n\\n- **70% faster response of Stream Load**\\n\\n![70% faster response of Stream Load](/images/doris-data-loading-performance-3.png)\\n\\nBefore putting the upgraded data platform in their production environment, NetEase has conducted extensive stress testing and grayscale testing. This is their experience in tackling errors along the way.\\n\\n**1. Stream Load timeout:**\\n   \\n  The early stage of stress testing often reported frequent timeout errors during data import. Additionally, despite the processes and cluster status being normal, the monitoring system couldn\'t collect the correct BE metrics. The engineers obtained the Doris BE stack using Pstack and analyzed it with PT-PMT. They discovered that the root cause was the lack of HTTP chunked encoding or content-length settings when initiating requests. This led Doris to mistakenly consider the data transfer as incomplete, causing it to remain in a waiting state. The solution was to simply add a chunked encoding setting on the client side.\\n\\n**2. Data size in a single Stream Load exceeding threshold:** \\n\\n  The default limit is 100 MB. The solution was to increase `streaming_load_json_max_mb` to 250 MB.\\n\\n**3. Error:** `alive replica num 0 < quorum replica num 1`\\n\\n  By the `show backends` command, it was discovered that one BE node was in OFFLINE state. A lookup in the `be_custom` configuration file revealed a `broken_storage_path`. Further inspection of the BE logs located an error message \\"too many open files,\\" meaning the number of file handles opened by the BE process had exceeded the system\'s limit, and this caused I/O operations to fail. When Doris detected such an abnormality, it marked the disk as unavailable. Because the table was configured with one single replica, when the disk holding the only replica was unavailable, data writing failed.\\n\\n  The solution was to increase the maximum open file descriptor limit for the process to 1 million, delete the `be_custom.conf` file, and restart the BE node.\\n\\n**4. FE memory jitter**\\n  \\n  During grayscale testing, the FE could not be connected. The monitoring data showed that the JVM\'s 32 GB was exhausted, and the `bdb` directory under the FE\'s meta directory had ballooned to 50 GB. Memory jitter occurred every hour, with peak memory usage reaching 80%\\n\\n  The root cause was improper parameter configuration. During high-concurrency Stream Load operations, the FE records the related Load information. Each import adds about 200 KB of information to the memory. The cleanup time for such information is controlled by the `streaming_label_keep_max_second` parameter, which by default is 12 hours. Reducing this to 5 minutes can prevent the FE memory from being exhausted. However, they didn\'t modify the `label_clean_interval_second` parameter, which controls the interval of the label cleanup thread. The default value of this parameter is 1 hour, which explains the hourly memory jitter. \\n\\n  The solution was to dial down `label_clean_interval_second` to 5 minutes.\\n\\n### Query\\n\\nThe engineers found results that did not match the filtering conditions in a query on the Eagle monitoring platform. \\n\\n![Dorsi Query Optimization](/images/doris-query-optimization.png)\\n\\nThis was due to the engineers\' misconception of `match_all` in Apache Doris. `match_all` identifies data records that include all the specified tokens while tokenization is based on space and punctuation marks. In the unqualified result, although the timestamp did not match, the message included \\"29\\", which compensated for the unmatched part in the timestamp. That\'s why this data record was included as a query result.\\n\\n![Dorsi Query Optimization](/images/doris-query-optimization-2.png)\\n\\nFor Doris to produce what the engineers wanted in this query, `MATCH_PHRASE` should be used instead, because it also identifies the sequence of texts. \\n\\n```sql\\nSELECT * FROM table_name WHERE logmsg MATCH_PHRASE \'keyword1 keyword2\';\\n```\\n\\nNote that when using `MATCH_PHRASE`, you should enable `support_phrase` during index creation. Otherwise, the system will perform a full table scan and a hard match, resulting in poor query efficiency.\\n\\n```sql\\nINDEX idx_name4(column_name4) USING INVERTED PROPERTIES(\\"parser\\" = \\"english\\", \\"support_phrase\\" = \\"true\\")\\n```\\n\\nIf you want to enable `support_phrase` for existing tables that have already been populated with data, you can execute `DROP INDEX` and then `ADD INDEX` to replace the old index with a new one. This process is incremental and does not require rewriting the entire table.\\n\\n**This is another advantage of Doris compared to Elasticsearch: It supports more flexible index management and allows easy addition and removal of indexes.**\\n\\n## Conclusion\\n\\nApache Doris supports the log and time series data analytic workloads of NetEase with higher query performance and less storage consumption. Beyond these, Apache Doris has other capabilities such as data lake analysis since it is designed as an all-in-one big data analytic platform. If you want a quick evaluation of whether Doris is right for your use case, come talk to the Doris makers on [Slack](https://join.slack.com/t/apachedoriscommunity/shared_invite/zt-2unfw3a3q-MtjGX4pAd8bCGC1UV0sKcw)."},{"id":"/release-note-2.1.3","metadata":{"permalink":"/blog/release-note-2.1.3","source":"@site/blog/release-note-2.1.3.md","title":"Apache Doris 2.1.3 just released","description":"This version has updated several improvements, including writing data back to Hive, materialized view, permission management and bug fixes. It further enhances the performance and stability of the system.","date":"2024-05-21T00:00:00.000Z","tags":[{"inline":true,"label":"Release Notes","permalink":"/blog/tags/release-notes"}],"hasTruncateMarker":false,"authors":[{"name":"Apache Doris","key":null,"page":null}],"frontMatter":{"title":"Apache Doris 2.1.3 just released","description":"This version has updated several improvements, including writing data back to Hive, materialized view, permission management and bug fixes. It further enhances the performance and stability of the system.","date":"2024-05-21","author":"Apache Doris","tags":["Release Notes"],"image":"/images/2.1.3.jpg"},"unlisted":false,"prevItem":{"title":"Apache Doris for log and time series data analysis in NetEase, why not Elasticsearch and InfluxDB?","permalink":"/blog/apache-doris-for-log-and-time-series-data-analysis-in-netease"},"nextItem":{"title":"Apache Doris version 2.0.10 has been released","permalink":"/blog/release-note-2.0.10"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n  http://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\nApache Doris 2.1.3 was officially released on May 21, 2024. This version has updated several improvements, including writing data back to Hive, materialized view, permission management and bug fixes. It further enhances the performance and stability of the system.\\n\\n**Quick Download:** https://doris.apache.org/download/\\n\\n**GitHub Release:** https://github.com/apache/doris/releases\\n\\n\\n\\n## New features\\n\\n**1. Support writing back data to hive tables via Hive Catalog**\\n\\nStarting from version 2.1.3, Apache Doris supports DDL and DML operations on Hive. Users can directly create libraries and tables in Hive through Apache Doris and write data to Hive tables by executing `INSERT INTO` statements. This feature allows users to perform complete data query and write operations on Hive through Apache Doris, further simplifying the integrated lakehouse architecture.\\n\\nPlease refer: [https://doris.apache.org/docs/lakehouse/datalake-building/hive-build/](https://doris.apache.org/docs/lakehouse/datalake-building/hive-build/)\\n\\n**2. Support building new asynchronous materialized views on top of existing ones**\\n\\nUsers can create new asynchronous materialized views on top of existing ones, directly reusing pre-computed intermediate results for data processing. This simplifies complex aggregation and computation operations, reducing resource consumption and maintenance costs while further accelerating query performance and improving data availability.\\n\\n**3. Support rewriting through nested materialized views**\\n\\nMaterialized View (MV) is a database object used to store query results. Now, Apache Doris supports rewriting through nested materialized views, which helps optimize query performance.\\n\\n**4. New `SHOW VIEWS` statement**\\n\\nThe `SHOW VIEWS` statement can be used to query views in the database, facilitating better management and understanding of view objects in the database.\\n\\n**5. Workload Group supports binding to specific BE nodes**\\n\\nWorkload Group can be bound to specific BE nodes, enabling more refined control over query execution to optimize resource usage and improve performance.\\n\\n**6. Broker Load supports compressed JSON format**\\n\\nBroker Load now supports importing compressed JSON format data, significantly reducing bandwidth requirements for data transmission and accelerating data import performance.\\n\\n**7. TRUNCATE Function can use columns as scale parameters**\\n\\nThe TRUNCATE function can now accept columns as scale parameters, providing more flexibility when processing numerical data.\\n\\n**8. Add new functions `uuid_to_int` and `int_to_uuid`**\\n\\nThese two functions allow users to convert between UUID and integer, significantly helping in scenarios that require handling UUID data.\\n\\n**9. Add `bypass_workload_group` session variable to bypass query queue**\\n\\nThe `bypass_workload_group` session variable allows certain queries to bypass the Workload Group queue and execute directly, which is useful for handling critical queries that require quick responses.\\n\\n**10. Add strcmp function**\\n\\nThe strcmp function compares two strings and returns their comparison result, simplifying text data processing.\\n\\n**11. Support HLL functions `hll_from_base64` and `hll_to_base64`**\\n\\nHyperLogLog (HLL) is an algorithm for cardinality estimation. These two functions allow users to decode HLL data from a Base64-encoded string or encode HLL data as a Base64 string, which is very useful for storing and transmitting HLL data.\\n\\n## Improvements\\n\\n**1. Replace SipHash with XXHash to improve shuffle performance**\\n\\nBoth SipHash and XXHash are hashing functions, but XXHash may provide faster hashing speeds and better performance in certain scenarios. This optimization aims to improve performance during data shuffling by adopting XXHash.\\n\\n**2. Asynchronous materialized views support NULL partition columns in OLAP tables**\\n\\nThis enhancement allows asynchronous materialized views to support NULL partition columns in OLAP tables, enhancing data processing flexibility.\\n\\n**3. Limit maximum string length to 1024 when collecting column statistics to control BE memory usage**\\n\\nLimiting the string length when collecting column statistics prevents excessive data from consuming too much BE memory, helping maintain system stability and performance.\\n\\n**4. Support dynamic deletion of Bitmap cache to improve performance**\\n\\nDynamically deleting no longer needed Bitmap Cache can free up memory and improve system performance.\\n\\n**5. Reduce memory usage during ALTER operations**\\n\\nReducing memory usage during ALTER operations improves the efficiency of system resource utilization.\\n\\n**6. Support constant folding for complex types**\\n\\nSupports constant folding for Array/Map/Struct complex types.\\n\\n**7. Add support for Variant type in Aggregate Key Model**\\n\\nThe Variant data type can store multiple data types. This optimization allows aggregation operations on Variant type data, enhancing the flexibility of semi-structured data analysis.\\n\\n**8. Support new inverted index format in CCR**\\n\\n**9. Optimize rewriting performance for nested materialized views**\\n\\n**10. Support decimal256 type in row-based storage format**\\n\\nSupporting the decimal256 type in row-based storage extends the system\'s ability to handle high-precision numerical data.\\n\\n## Behavior changes\\n\\n**1. Authorization**\\n\\n- **Grant_priv permission changes**: `Grant_priv` can no longer be arbitrarily granted. When performing a `GRANT` operation, the user not only needs to have `Grant_priv` but also the permissions to be granted. For example, to grant `SELECT` permission on `table1`, the user needs both `GRANT` permission and `SELECT` permission on `table1`, enhancing security and consistency in permission management.\\n\\n- **Workload group and resource usage_priv**: `Usage_priv` for Workload Group and Resource is no longer global but limited to Resource and Workload Group, making permission granting and usage more specific.\\n\\n- **Authorization for operations**: Operations that were previously unauthorized now have corresponding authorizations for more detailed and comprehensive operational permission control.\\n\\n**2. LOG directory configuration**\\n\\nThe log directory configuration for FE and BE now uniformly uses the `LOG_DIR` environment variable. All other different types of logs will be stored with `LOG_DIR` as the root directory. To maintain compatibility between versions, the previous configuration item `sys_log_dir` can still be used.\\n\\n**3. S3 Table Function (TVF)**\\n\\nDue to issues with correctly recognizing or processing S3 URLs in certain cases, the parsing logic for object storage paths has been refactored. For file paths in S3 table functions, the `force_parsing_by_standard_uri` parameter needs to be passed to ensure correct parsing.\\n\\n## Upgrade Issues\\n\\nSince many users use certain keywords as column names or attribute values, the following keywords have been set as non-reserved, allowing users to use them as identifiers.\\n\\n## Bug fixes\\n\\n**1. Fix no data error when reading Hive tables on Tencent Cloud COSN**\\n\\nResolved the no data error that could occur when reading Hive tables on Tencent Cloud COSN, enhancing compatibility with Tencent Cloud storage services.\\n\\n**2. Fix incorrect results returned by `milliseconds_diff` function**\\n\\nFixed an issue where the `milliseconds_diff` function returned incorrect results in some cases, ensuring the accuracy of time difference calculations.\\n\\n**3. User-defined variables should be rorwarded to the Master node**\\n\\nEnsured that user-defined variables are correctly passed to the Master node for consistency and correct execution logic across the entire system.\\n\\n**4. Fix Schema Change issues when adding complex type columns**\\n\\nResolved Schema Change issues that could arise when adding complex type columns, ensuring the correctness of Schema Changes.\\n\\n**5. Fix data loss issue in Routine Load when FE Master node changes**\\n\\n`Routine Load` is often used to subscribe to Kafka message queues. This fix addresses potential data loss issues that may occur during FE Master node changes.\\n\\n**6. Fix Routine Load failure when Workload Group cannot be found**\\n\\nResolved an issue where `Routine Load` would fail if the specified Workload Group could not be found.\\n\\n**7. Support column string64 to avoid join failures when string size overflows unit32**\\n\\nIn some cases, string sizes may exceed the unit32 limit. Supporting the `string64` type ensures correct execution of string JOIN operations.\\n\\n**8. Allow Hadoop users to create Paimon Catalog**\\n\\nPermitted authorized Hadoop users to create Paimon Catalogs.\\n\\n**9. Fix `function_ipxx_cidr` function issues with constant parameters**\\n\\nResolved problems with the `function_ipxx_cidr` function when handling constant parameters, ensuring the correctness of function execution.\\n\\n**10. Fix file download errors when restoring using HDFS**\\n\\nResolved \\"failed to download\\" errors encountered during data restoration using HDFS, ensuring the accuracy and reliability of data recovery.\\n\\n**11. Fix column permission issues related to hidden columns**\\n\\nIn some cases, permission settings for hidden columns may be incorrect. This fix ensures the correctness and security of column permission settings.\\n\\n**12. Fix issue where Arrow Flight cannot obtain the correct IP in K8s deployments**\\n\\nThis fix resolves an issue where Arrow Flight cannot correctly obtain the IP address in Kubernetes deployment environments."},{"id":"/release-note-2.0.10","metadata":{"permalink":"/blog/release-note-2.0.10","source":"@site/blog/release-note-2.0.10.md","title":"Apache Doris version 2.0.10 has been released","description":"Thanks to our community users and developers, about 83 improvements and bug fixes have been made in Doris 2.0.10 version.","date":"2024-05-16T00:00:00.000Z","tags":[{"inline":true,"label":"Release Notes","permalink":"/blog/tags/release-notes"}],"hasTruncateMarker":false,"authors":[{"name":"Apache Doris","key":null,"page":null}],"frontMatter":{"title":"Apache Doris version 2.0.10 has been released","description":"Thanks to our community users and developers, about 83 improvements and bug fixes have been made in Doris 2.0.10 version.","date":"2024-05-16","author":"Apache Doris","tags":["Release Notes"],"image":"/images/2.0.10.jpg"},"unlisted":false,"prevItem":{"title":"Apache Doris 2.1.3 just released","permalink":"/blog/release-note-2.1.3"},"nextItem":{"title":"Multi-tenant workload isolation: a better balance between isolation and utilization","permalink":"/blog/multi-tenant-workload-isolation-in-apache-doris"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n  http://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n\\nThanks to our community users and developers, about 83 improvements and bug fixes have been made in Doris 2.0.10 version.\\n\\n**Quick Download:** [https://doris.apache.org/download/](https://doris.apache.org/download/)\\n\\n**GitHub\uFF1A** [https://github.com/apache/doris/releases](https://github.com/apache/doris/releases)\\n\\n\\n## Improvements\\n\\n- This enhancement introduces the `read_only` and `super_read_only` variables to the database system, ensuring compatibility with MySQL\'s read-only modes.\\n\\n- When the check status is not IO_ERROR, the disk path should not be added to the broken list. This ensures that only disks with actual I/O errors are marked as broken.\\n\\n- When performing a Create Table As Select (CTAS) operation from an external table, convert the `VARCHAR` column to `STRING` type.\\n\\n- Support mapping Paimon column type \\"ROW\\" to Doris type \\"STRUCT\\"\\n\\n- Choose disk tolerate with little skew when creating tablet\\n\\n- Write editlog to `set replica drop` to avoid confusing status on follower FE\\n\\n- Make the schema change memory space adaptive to avoid memory over limit\\n\\n- Inverted index \'unicode\' tokenizer supports configuration to exclude stop words\\n\\nSee the complete list of improvements and bug fixes on [GitHub](https://github.com/apache/doris/compare/2.0.9...2.0.10) .\\n\\n\\n## Credits\\n\\nThanks to all who contributed to this release\\n\\n@airborne12, @BePPPower, @ByteYue, @CalvinKirs, @cambyzju, @csun5285, @dataroaring, @deardeng, @DongLiang-0, @eldenmoon, @felixwluo, @HappenLee, @hubgeter, @jackwener, @kaijchen, @kaka11chen, @Lchangliang, @liaoxin01, @LiBinfeng-01, @luennng, @morningman, @morrySnow, @Mryange, @nextdreamblue, @qidaye, @starocean999, @suxiaogang223, @SWJTU-ZhangLei, @w41ter, @xiaokang, @xy720, @yujun777, @Yukang-Lian, @zhangstar333, @zxealous, @zy-kkk, @zzzxl1993"},{"id":"/multi-tenant-workload-isolation-in-apache-doris","metadata":{"permalink":"/blog/multi-tenant-workload-isolation-in-apache-doris","source":"@site/blog/multi-tenant-workload-isolation-in-apache-doris.md","title":"Multi-tenant workload isolation: a better balance between isolation and utilization","description":"Apache Doris supports workload isolation based on Resource Tag and Workload Group. It provides solutions for different tradeoffs among the level of isolation, resource utilization, and stable performance.","date":"2024-05-14T00:00:00.000Z","tags":[{"inline":true,"label":"Tech Sharing","permalink":"/blog/tags/tech-sharing"}],"hasTruncateMarker":false,"authors":[{"name":"velodb.io \xb7 VeloDB Engineering Team","key":null,"page":null}],"frontMatter":{"title":"Multi-tenant workload isolation: a better balance between isolation and utilization","summary":"Apache Doris supports workload isolation based on Resource Tag and Workload Group. It provides solutions for different tradeoffs among the level of isolation, resource utilization, and stable performance.","description":"Apache Doris supports workload isolation based on Resource Tag and Workload Group. It provides solutions for different tradeoffs among the level of isolation, resource utilization, and stable performance.","date":"2024-05-14","author":"velodb.io \xb7 VeloDB Engineering Team","externalLink":"https://www.velodb.io/blog/403","tags":["Tech Sharing"],"image":"/images/multi-tenant-workload-group.jpg"},"unlisted":false,"prevItem":{"title":"Apache Doris version 2.0.10 has been released","permalink":"/blog/release-note-2.0.10"},"nextItem":{"title":"From Presto, Trino, ClickHouse, and Hive to Apache Doris: SQL convertor for easy migration","permalink":"/blog/from-presto-trino-clickhouse-and-hive-to-apache-doris-sql-convertor-for-easy-migration"}},"content":"\x3c!-- \\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n\\nThis is an in-depth introduction to the workload isolation capabilities of [Apache Doris](https://doris.apache.org). But first of all, why and when do you need workload isolation? If you relate to any of the following situations, read on and you will end up with a solution:\\n\\n- You have different business departments or tenants sharing the same cluster and you want to prevent the interference of workloads among them.\\n\\n- You have query tasks of varying priority levels and you want to give priority to your critical tasks (such as real-time data analytics and online transactions) in terms of resources and execution. \\n\\n- You need workload isolation but also want high cost-effectiveness and resource utilization rates.\\n\\nApache Doris supports workload isolation based on Resource Tag and Workload Group. Resource Tag isolates the CPU and memory resources for different workloads at the level of backend nodes, while the Workload Group mechanism can further divide the resources within a backend node for higher resource utilization.  \\n\\n:::tip\\n\\n[Demo](https://www.youtube.com/watch?v=Wd3l5C4k8Ok&t=1s) of using the Workload Manager in Apache Doris to set a CPU soft/hard limit for Workload Groups.\\n\\n:::\\n\\n## Resource isolation based on Resource Tag\\n\\nLet\'s begin with the architecture of Apache Doris. Doris has two [types of nodes](https://doris.apache.org/docs/gettingStarted/what-is-apache-doris): frontends (FEs) and backends (BEs). FE nodes store metadata, manage clusters, process user requests, and parse query plans, while BE nodes are responsible for computation and data storage. Thus, BE nodes are the major resource consumers. \\n\\nThe main idea of a Resource Tag-based isolation solution is to divide computing resources into groups by assigning tags to BE nodes in a cluster, where BE nodes of the same tag constitute a Resource Group. A Resource Group can be deemed as a unit for data storage and computation. For data ingested into Doris, the system will write data replicas into different Resource Groups according to the configurations. Queries will also be assigned to their corresponding [Resource Groups](https://doris.apache.org/docs/admin-manual/resource-admin/multi-tenant#tag-division-and-cpu-limitation-are-new-features-in-version-015-in-order-to-ensure-a-smooth-upgrade-from-the-old-version-doris-has-made-the-following-forward-compatibility) for execution. \\n\\nFor example, if you want to separate read and write workloads in a 3-BE cluster, you can follow these steps:\\n\\n1. **Assign Resource Tags to BE nodes**: Bind 2 BEs to the \\"Read\\" tag and 1 BE to the \\"Write\\" tag. \\n\\n2. **Assign Resource Tags to data replicas**: Assuming that Table 1 has 3 replicas, bind 2 of them to the \\"Read\\" tag and 1 to the \\"Write\\" tag. Data written into Replica 3 will be synchronized to Replica 1 and Replica 2 and the data synchronization process consumes few resources of BE 1 and BE2.\\n\\n3. **Assign workload groups to Resource Tags**: Queries that include the \\"Read\\" tag in their SQLs will be automatically routed to the nodes tagged with \\"Read\\" (in this case, BE 1 and BE 2). For data writing tasks, you also need to assign them with the \\"Write\\" tag, so they can be routed to the corresponding node (BE 3). In this way, there will be no resource contention between read and write workloads except the data synchronization overheads from replica 3 to replicate 1 and 2.\\n\\n![Resource isolation based on Resource Tag](/images/resource-isolation-based-on-resource-tag.PNG)\\n\\nResource Tag also enables **multi-tenancy** in Apache Doris. For example, computing and storage resources tagged with \\"User A\\" are for User A only, while those tagged with \\"User B\\" are exclusive to User B. This is how Doris implements multi-tenant resource isolation with Resource Tags at the BE side.\\n\\n![Resource isolation based on Resource Tag](/images/resource-isolation-based-on-resource-tag-2.PNG)\\n\\nDividing the BE nodes into groups ensures **a high level of isolation**:\\n\\n- CPU, memory, and I/O of different tenants are physically isolated.\\n\\n- One tenant will never be affected by the failures (such as process crashes) of another tenant.\\n\\nBut it has a few downsides:\\n\\n- In read-write separation, when the data writing stops, the BE nodes tagged with \\"Write\\" become idle. This reduces overall cluster utilization.\\n\\n- Under multi-tenancy, if you want to further isolate different workloads of the same tenant by assigning separate BE nodes to each of them, you will need to endure significant costs and low resource utilization.\\n\\n- The number of tenants is tied to the number of data replicas. So if you have 5 tenants, you will need 5 data replicas. That\'s huge storage redundancy.\\n\\n**To improve on this,we provide a workload isolation solution based on Workload Group in Apache Doris 2.0.0, and enhanced it in [Apache Doris 2.1.0](https://doris.apache.org/blog/release-note-2.1.0)**\\n\\n## Workload isolation based on Workload Group\\n\\nThe [Workload Group](https://doris.apache.org/docs/admin-manual/resource-admin/workload-group)-based solution realizes a more granular division of resources. It further divides CPU and memory resources within processes on BE nodes, meaning that the queries in one BE node can be isolated from each other to some extent. This avoids resource competition within BE processes and optimizes resource utilization.\\n\\nUsers can relate queries to Workload Groups, and thus limit the percentage of CPU and memory resources that a query can use. Under high cluster loads, Doris can automatically kill the most resource-consuming queries in a Workload Group. Under low cluster loads, Doris can allow multiple Workload Groups to share idle resources. \\n\\nDoris supports both CPU soft limit and CPU hard limit. The soft limit allows Workload Groups to break the limit and utilize idle resources, enabling more efficient utilization. The hard limit is a hard guarantee of stable performance because it prevents the mutual impact of Workload Groups. \\n\\n*(CPU soft limit and CPU hard limit are contradictory to each other. You can choose between them based on your own use case.)*\\n\\n![Workload isolation based on Workload Group](/images/workload-isolation-based-on-workload-group.png)\\n\\nIts differences from the Resource Tag-based solution include: \\n\\n- Workload Groups are formed within processes. Multiple Workload Groups compete for resources within the same BE node.\\n\\n- The consideration of data replica distribution is out of the picture because Workload Group is only a way of resource management.\\n\\n### CPU soft limit\\n\\nCPU soft limit is implemented by the `cpu_share` parameter, which is similar to weights conceptually. Workload Groups with higher `cpu_share` will be allocated more CPU time during a time slot. \\n\\nFor example, if Group A is configured with a `cpu_share` of 1, and Group B, 9. In a time slot of 10 seconds, when both Group A and Group B are fully loaded, Group A and Group B will be able to consume 1s and 9s of CPU time, respectively.  \\n\\nWhat happens in real-world cases is that, not all workloads in the cluster run at full capacity. Under the soft limit, if Group B has low or zero workload, then Group A will be able to use all 10s of CPU time, thus increasing the overall CPU utilization in the cluster.\\n\\n![CPU soft limit](/images/CPU-soft-limit.png)\\n\\nA soft limit brings flexibility and a higher resource utilization rate. On the flip side, it might cause performance fluctuations.\\n\\n### CPU hard limit\\n\\nCPU hard limit in Apache Doris 2.1.0 is designed for users who require stable performance. In simple terms, the CPU hard limit defines that a Workload Group cannot use more CPU resources than its limit whether there are idle CPU resources or not.\\n\\nThis is how it works: \\n\\nSuppose that Group A is set with `cpu_hard_limit=10%` and Group B with `cpu_hard_limit=90%`. If both Group A and Group B run at full load, Group A and Group B will respectively use 10% and 90% of the overall CPU time. The difference lies in when the workload of Group B decreases. In such cases, regardless of how high the query load of Group A is, it should not use more than the 10% CPU resources allocated to it. \\n\\n![CPU hard limit](/images/CPU-hard-limit.png)\\n\\nAs opposed to soft limit, a hard limit guarantees stable system performance at the cost of flexibility and the possibility of a higher resource utilization rate. \\n\\n### Memory resource limit\\n\\n> The memory of a BE node comprises the following parts:\\n>\\n> - Reserved memory for the operating system.\\n>\\n> - Memory consumed by non-queries, which is not considered in the Workload Group\'s memory statistics.\\n>\\n> - Memory consumed by queries, including data writing. This can be tracked and controlled by Workload Group.\\n\\nThe `memory_limit` parameter defines the maximum (%) memory available to a Workload Group within the BE process. It also affects the priority of Resource Groups.\\n\\nUnder initial status, a high-priority Resource Group will be allocated more memory. By setting `enable_memory_overcommit`, you can allow Resource Groups to occupy more memory than the limits when there is idle space. When memory is tight, Doris will cancel tasks to reclaim the memory resources that they commit.  In this case, the system will retain memory resources for high-priority resource groups as much as possible.\\n\\n\\n<div style={{textAlign:\'center\'}}><img src=\\"/images/memory-resource-limit.png\\" alt=\\"Memory resource limit\\" style={{display: \'inline-block\', width:300}}/></div >\\n\\n\\n### Query queue\\n\\nIt happens that the cluster is undertaking more loads than it can handle. In this case, submitting new query requests will not only be fruitless but also interruptive to the queries in progress.\\n\\nTo improve on this, Apache Doris provides the [query queue](https://doris.apache.org/docs/admin-manual/resource-admin/workload-group#query-queue) mechanism. Users can put a limit on the number of queries that can run concurrently in the cluster. A query will be rejected when the query queue is full or after a waiting timeout, thus ensuring system stability under high loads.\\n\\n![Query queue](/images/query-queue.png)\\n\\nThe query queue mechanism involves three parameters: `max_concurrency`, `max_queue_size`, and `queue_timeout`.\\n\\n## Tests\\n\\nTo demonstrate the effectiveness of the CPU soft limit and hard limit, we did a few tests.\\n\\n- Environment: single machine, 16 cores, 64GB \\n\\n- Deployment: 1 FE + 1 BE\\n\\n- Dataset: ClickBench, TPC-H\\n\\n- Load testing tool: Apache JMeter\\n\\n### CPU soft limit test\\n\\nStart two clients and continuously submit queries (ClickBench Q23) with and without using Workload Groups, respectively. Note that Page Cache should be disabled to prevent it from affecting the test results.\\n\\n![CPU soft limit test](/images/CPU-soft-limit-test.png)\\n\\nComparing the throughputs of the two clients in both tests, it can be concluded that:\\n\\n- **Without configuring Workload Groups**, the two clients consume the CPU resources on an equal basis.\\n\\n- **Configuring Workload Groups** and setting the `cpu_share` to 2:1, the throughput ratio of the two clients is 2:1. With a higher `cpu_share`, Client 1 is provided with a higher portion of CPU resources, and it delivers a higher throughput. \\n\\n### CPU hard limit test\\n\\nStart a client, set `cpu_hard_limit=50%` for the Workload Group, and execute ClickBench Q23 for 5 minutes under a concurrency level of 1, 2, and 4, respectively. \\n\\n![CPU hard limit test](/images/CPU-hard-limit-test.png)\\n\\nAs the query concurrency increases, the CPU utilization rate remains at around 800%, meaning that 8 cores are used. On a 16-core machine, that\'s **50% utilization**, which is as expected. In addition, since CPU hard limits are imposed, the increase in TP99 latency as concurrency rises is also an expected outcome.\\n\\n## Test in simulated production environment\\n\\nIn real-world usage, users are particularly concerned about query latency rather than just query throughput, since latency is more easily perceptible in user experience. That\'s why we decided to validate the effectiveness of Workload Group in a simulated production environment.\\n\\nWe picked out a SQL set consisting of queries that should be finished within 1s (ClickBench Q15, Q17, Q23 and TPC-H Q3, Q7, Q19), including single-table aggregations and join queries. The size of the TPC-H dataset is 100GB.\\n\\nSimilarly, we conduct tests with and without configuring Workload Groups.\\n\\n![Test in simulated production environment](/images/test-in-simulated-production-environment.png)\\n\\nAs the results show:\\n\\n- **Without Workload Group** (comparing Test 1 & 2): When dialing up the concurrency of Client 2, both clients experience a 2~3-time increase in query latency.\\n\\n- **Configuring Workload Group** (comparing Test 3 & 4): As the concurrency of Client 2 goes up, the performance fluctuation in Client 1 is much smaller, which is proof of how it is effectively protected by workload isolation.\\n\\n## Recommendations & plans\\n\\nThe Resource Tag-based solution is a thorough workload isolation plan. The Workload Group-based solution realizes a better balance between resource isolation and utilization, and it is complemented by the query queue mechanism for stability guarantee.\\n\\nSo which one to choose for your use case? Here is our recommendation:\\n\\n- **Resource Tag**: for use cases where different business lines of departments share the same cluster, so the resources and data are physically isolated for different tenants.\\n\\n- **Workload Group**: for use cases where one cluster undertakes various query workloads for flexible resource allocation.\\n\\nIn future releases, we will keep improving user experience of the Workload Group and query queue features:\\n\\n- Freeing up memory space by canceling queries is a brutal method. We plan to implement that by disk spilling, which will bring higher stability in query performance.\\n\\n- Since memory consumed by non-queries in the BE is not included in Workload Group\'s memory statistics, users might observe a disparity between the BE process memory usage and Workload Group memory usage. We will address this issue to avoid confusion.\\n\\n- In the query queue mechanism,  cluster load is controlled by setting the maximum query concurrency. We plan to enable dynamic maximum query concurrency based on resource availability at the BE. This is to create backpressure on the client side and thus improve the availability of Doris when clients keep submitting high loads.\\n\\n- The main idea of Resource Tag is to group the BE nodes, while that of Workload Group is to further divide the resources of a single BE node. For users to grasp these ideas, they need to learn about the concept of BE nodes in Doris first. However, from an operational perspective, users only need to understand the resource consumption percentage of each of their workloads and what priority they should have when cluster load is saturated. Thus, we will try and figure out a way to flatten the learning curve for users, such as keeping the concept of BE nodes in the black box. \\n\\nFor further assistance on workload isolation in Apache Doris, join the [Apache Doris community](https://join.slack.com/t/apachedoriscommunity/shared_invite/zt-2unfw3a3q-MtjGX4pAd8bCGC1UV0sKcw)."},{"id":"/from-presto-trino-clickhouse-and-hive-to-apache-doris-sql-convertor-for-easy-migration","metadata":{"permalink":"/blog/from-presto-trino-clickhouse-and-hive-to-apache-doris-sql-convertor-for-easy-migration","source":"@site/blog/from-presto-trino-clickhouse-and-hive-to-apache-doris-sql-convertor-for-easy-migration.md","title":"From Presto, Trino, ClickHouse, and Hive to Apache Doris: SQL convertor for easy migration","description":"Users can execute queries with their old SQL syntaxes directly in Doris or batch convert their existing SQL statements on the visual SQL conversion interface.","date":"2024-05-06T00:00:00.000Z","tags":[{"inline":true,"label":"Tech Sharing","permalink":"/blog/tags/tech-sharing"}],"hasTruncateMarker":false,"authors":[{"name":"velodb.io \xb7 VeloDB Engineering Team","key":null,"page":null}],"frontMatter":{"title":"From Presto, Trino, ClickHouse, and Hive to Apache Doris: SQL convertor for easy migration","description":"Users can execute queries with their old SQL syntaxes directly in Doris or batch convert their existing SQL statements on the visual SQL conversion interface.","date":"2024-05-06","author":"velodb.io \xb7 VeloDB Engineering Team","externalLink":"https://www.velodb.io/blog/377","tags":["Tech Sharing"],"image":"/images/sql-convertor-feature.jpeg"},"unlisted":false,"prevItem":{"title":"Multi-tenant workload isolation: a better balance between isolation and utilization","permalink":"/blog/multi-tenant-workload-isolation-in-apache-doris"},"nextItem":{"title":"Cross-cluster replication for read-write separation: story of a grocery store brand","permalink":"/blog/cross-cluster-replication-for-read-write"}},"content":"\x3c!-- \\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n[Apache Doris](https://doris.apache.org/) is an all-in-one data platform that is capable of real-time reporting, ad-hoc queries, data lakehousing, log management and analysis, and batch data processing. As more and more companies have been replacing their component-heavy data architecture with Apache Doris, there is an increasing need for a more convenient data migration solution. **That\'s why the Doris SQL Convertor is made.**\\n\\nMost database systems run their own SQL dialects. Thus, migration between systems often entails modifications of SQL syntaxes. Since SQLs work closely with a company\'s business logic, in many cases, users have to modify their business logic, too. To reduce the transition pain for users, Apache Doris 2.1 provides the Doris SQL Convertor. It supports the SQL syntaxes of Presto, Trino, Hive, ClickHouse, and PostgreSQL. With it, users can execute queries with their old SQL syntaxes directly in Doris or batch convert their existing SQL statements on the visual interface.\\n\\n## Doris SQL Convertor\\n\\nThe Doris SQL Convertor requires **zero rewriting** of SQL. Simply `set sql_dialect = \\"trino\\"` in the session variable, then you can execute queries in Doris using Trino SQLs. \\n\\nThe SQL compatibility of it has been proven by extensive tests. For example, a user tested the Doris SQL Convertor with over 30,000 SQL queries from their production environment. Turned out that the Convertor successfully converted 99.6% of the Trino SQLs and 98% of the ClickHouse SQLs.\\n\\nCurrently, Presto, Trino, Hive, ClickHouse, and PostgreSQL dialects are supported. We are working to add Teradata, SQL Server, and Snowflake to the list, and consistently increase the compatibility level of each SQL dialect.\\n\\n## Installation & usage\\n\\n### SQL conversion service\\n\\n1. **Download** **[Doris SQL Convertor](https://selectdb-doris-1308700295.cos.ap-beijing.myqcloud.com/doris-sql-convertor/doris-sql-convertor-1.0.3-bin-x86.tar.gz)**\\n\\n2. On any frontend (FE) node, start the service using the following command.\\n\\n- The SQL conversion service is stateless and can be started or stopped at any time.\\n\\n- `port=5001` in the command specifies the service port. (You can use any available port.)\\n\\n- It is advisable to start a service individually for each FE node.\\n\\n```Shell\\nnohup ./doris-sql-convertor-1.0.1-bin-x86 run --host=0.0.0.0 --port=5001 &\\n```\\n\\n3. Start a Doris cluster **(Use Doris 2.1.0 or newer)**.\\n\\n4. Set the URL for SQL conversion service in Doris. `127.0.0.1:5001` in the command represents the IP and port number of the node where the service is deployed.\\n\\n```Shell\\nMySQL> set global sql_converter_service_url = \\"http://127.0.0.1:5001/api/v1/convert\\"\\n```\\n\\nAfter deployment, you can execute SQL directly in the command line. You can start the service by `set sql_dialect = XXX`. The following examples are based on ClickHouse SQL dialects.\\n\\n- Presto\\n\\n```sql\\nmysql> set sql_dialect=presto;                                                                                                                                                                                                             \\nQuery OK, 0 rows affected (0.00 sec) \\n\\nmysql> SELECT cast(start_time as varchar(20)) as col1,                                                                                                                                                                                     \\n            array_distinct(arr_int) as col2,                                                                                                                                                                                             \\n            FILTER(arr_str, x -> x LIKE \'%World%\') as col3,                                                                                                                                                                              \\n            to_date(value,\'%Y-%m-%d\') as col4,                                                                                                                                                                                           \\n            YEAR(start_time) as col5,                                                                                                                                                                                                    \\n            date_add(\'month\', 1, start_time) as col6,                                                                                                                                                                                    \\n            REGEXP_EXTRACT_ALL(value, \'-.\') as col7,                                                                                                                                                                                     \\n            JSON_EXTRACT(\'{\\"id\\": \\"33\\"}\', \'$.id\')as col8,                                                                                                                                                                                 \\n            element_at(arr_int, 1) as col9,                                                                                                                                                                                              \\n            date_trunc(\'day\',start_time) as col10                                                                                                                                                                                        \\n         FROM test_sqlconvert                                                                                                                                                                                                            \\n         where date_trunc(\'day\',start_time)= DATE\'2024-05-20\'                                                                                                                                                                            \\n     order by id;                                                                                                                                                                                                                        \\n+---------------------+-----------+-----------+------------+------+---------------------+-------------+------+------+---------------------+                                                                                                \\n| col1                | col2      | col3      | col4       | col5 | col6                | col7        | col8 | col9 | col10               |                                                                                                \\n+---------------------+-----------+-----------+------------+------+---------------------+-------------+------+------+---------------------+                                                                                                \\n| 2024-05-20 13:14:52 | [1, 2, 3] | [\\"World\\"] | 2024-01-14 | 2024 | 2024-06-20 13:14:52 | [\'-0\',\'-1\'] | \\"33\\" |    1 | 2024-05-20 00:00:00 |                                                                                                \\n+---------------------+-----------+-----------+------------+------+---------------------+-------------+------+------+---------------------+                                                                                                \\n1 row in set (0.03 sec)    \\n```\\n\\n- ClickHouse\\n\\n```sql\\nmysql> set sql_dialect=clickhouse;                                                                                                                                             \\nQuery OK, 0 rows affected (0.00 sec)                                                                                                                                           \\n                                                                                                                                                                               \\nmysql> select  toString(start_time) as col1,                                                                                                                                   \\n             arrayCompact(arr_int) as col2,                                                                                                                                  \\n             arrayFilter(x -> x like \'%World%\',arr_str)as col3,                                                                                                              \\n             toDate(value) as col4,                                                                                                                                          \\n             toYear(start_time)as col5,                                                                                                                                      \\n             addMonths(start_time, 1)as col6,                                                                                                                                \\n             extractAll(value, \'-.\')as col7,                                                                                                                                 \\n             JSONExtractString(\'{\\"id\\": \\"33\\"}\' , \'id\')as col8,                                                                                                                \\n             arrayElement(arr_int, 1) as col9,                                                                                                                               \\n             date_trunc(\'day\',start_time) as col10                                                                                                                           \\n          FROM test_sqlconvert                                                                                                                                               \\n          where date_trunc(\'day\',start_time)= \'2024-05-20 00:00:00\'                                                                                                          \\n     order by id;                                                                                                                                                   \\n+---------------------+-----------+-----------+------------+------+---------------------+-------------+------+------+---------------------+                                    \\n| col1                | col2      | col3      | col4       | col5 | col6                | col7        | col8 | col9 | col10               |                                    \\n+---------------------+-----------+-----------+------------+------+---------------------+-------------+------+------+---------------------+                                    \\n| 2024-05-20 13:14:52 | [1, 2, 3] | [\\"World\\"] | 2024-01-14 | 2024 | 2024-06-20 13:14:52 | [\'-0\',\'-1\'] | \\"33\\" |    1 | 2024-05-20 00:00:00 |                                    \\n+---------------------+-----------+-----------+------------+------+---------------------+-------------+------+------+---------------------+                                    \\n1 row in set (0.02 sec)\\n```\\n\\n### Visual interface\\n\\nFor large-scale conversion, it is recommended to use the visual interface, on which you can batch upload the files for dialect conversion.\\n\\nFollow these steps to deploy the visual conversion interface:\\n\\n1. Environment: Docker, Docker-Compose\\n\\n2. Get Doris-SQL-Convertor Docker image\\n\\n3. Create a network for the image\\n\\n```shell\\ndocker network create app_network\\n```\\n\\n4. Decompress the package\\n\\n```shell\\ntar xzvf doris-sql-convertor-1.0.1.tar.gz\\n\\ncd doris-sql-convertor\\n```\\n\\n5. Edit the environment variables\\n   \\n```shell\\nFLASK_APP=server/app.py\\nFLASK_DEBUG=1\\nAPI_HOST=http://doris-sql-convertor-api:5000\\n\\n# DOCKER TAG\\nAPI_TAG=latest\\nWEB_TAG=latest\\n```\\n\\n6. Start it up\\n\\n```shell\\nsh start.sh\\n```\\n\\nAfter deployment, you can access the service by `ip:8080` via your local browser. `8080` is the default port. You can change the mapping port. On the visual interface, you can select the source dialect type and target dialect type, and then click \\"Convert\\".\\n\\n:::info Note\\n1. For batch conversion, each SQL statement should end with `; `.\\n\\n2. The Doris SQL Convertor supports 239 UNION ALL conversions at most.\\n:::\\n\\nJoin the [Apache Doris community](https://join.slack.com/t/apachedoriscommunity/shared_invite/zt-2unfw3a3q-MtjGX4pAd8bCGC1UV0sKcw) to seek guidance from the Doris makers or provide your feedback!"},{"id":"/cross-cluster-replication-for-read-write","metadata":{"permalink":"/blog/cross-cluster-replication-for-read-write","source":"@site/blog/cross-cluster-replication-for-read-write.md","title":"Cross-cluster replication for read-write separation: story of a grocery store brand","description":"Cross-cluster replication (CCR) in Apache Doris is proven to be fast, stable, and easy to use. It secures a real-time data synchronization latency of 1 second.","date":"2024-04-25T00:00:00.000Z","tags":[{"inline":true,"label":"Best Practice","permalink":"/blog/tags/best-practice"}],"hasTruncateMarker":false,"authors":[{"name":"velodb.io \xb7 VeloDB Engineering Team","key":null,"page":null}],"frontMatter":{"title":"Cross-cluster replication for read-write separation: story of a grocery store brand","description":"Cross-cluster replication (CCR) in Apache Doris is proven to be fast, stable, and easy to use. It secures a real-time data synchronization latency of 1 second.","date":"2024-04-25","author":"velodb.io \xb7 VeloDB Engineering Team","externalLink":"https://www.velodb.io/blog/339","tags":["Best Practice"],"image":"/images/ccr-for-read-write-separation.jpg"},"unlisted":false,"prevItem":{"title":"From Presto, Trino, ClickHouse, and Hive to Apache Doris: SQL convertor for easy migration","permalink":"/blog/from-presto-trino-clickhouse-and-hive-to-apache-doris-sql-convertor-for-easy-migration"},"nextItem":{"title":"Apache Doris version 2.0.9 has been released","permalink":"/blog/release-note-2.0.9"}},"content":"\x3c!-- \\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\nThis is about how a grocery store brand leverages the [Cross-Cluster Replication (CCR)](https://doris.apache.org/docs/2.0/admin-manual/data-admin/ccr) capability of Apache Doris to separate their data reading and writing workloads. In this case, where the freshness of groceries is guaranteed by the freshness of data, they use Apache Doris as their data warehouse to monitor and analyze their procurement, sale, and stock in real time for all their stores and supply chains. \\n\\n## Why they need CCR\\n\\nA major part of the user\'s data warehouse (including the ODS, DWD, DWS, and ADS layers) is built within Apache Doris, which employs a micro-batch scheduling mechanism to coordinate data across the data warehouse layers. However, this is pressured by the burgeoning business of the grocery store brand. The data size they have to receive, store, and analyze is getting larger and larger. That means their data warehouse has to handle bigger data writing batches and more frequent data queries. However, task scheduling during query execution might lead to resource preemption, so any resource shortage can easily compromise performance or even cause task failure or system disruption.\\n\\n Naturally, the user thought of **separating the reading and writing workloads.** Specifically, they want to replicate data from the ADS layer (which is cleaned, transformed, aggregated, and ready to be queried) to a backup cluster dedicated to query services. **This is implemented by the CCR in Apache Doris.** It prevents abnormal queries from interrupting data writing and ensures cluster stability.   \\n\\n## Before CCR\\n\\nBefore CCR was available, they innovatively adopted the [Multi-Catalog](https://doris.apache.org/docs/2.0/lakehouse/lakehouse-overview#multi-catalog) feature of Doris for the same purpose. Multi-Catalog allows users to connect Doris to various data sources conveniently. It is actually designed for federated querying, but the user drew inspiration from it. They wrote a script and tried to pull incremental data via Catalog. Their data synchronization pipeline is as follows:\\n\\n![Before CCR](/images/before-ccr.jpeg)\\n\\nThey loaded data from the source cluster to the target cluster by regular scheduling tasks. To identify incremental data, they added a `last_update_time` field to the tables. There were two downsides to this. Firstly, the data freshness of the target cluster was reliant on and hindered by the scheduling tasks. Secondly, for incremental data ingestion, in order to identify incremental data, the import SQL statement for every table has to include the logic to check the `last_update_time` field, otherwise the system just deletes and re-imports the entire table. Such requirement increases development complexity and data error rate. \\n\\n## CCR in Apache Doris\\n\\nJust when they were looking for a better solution, Apache Doris released CCR in version 2.0. Compared to the alternatives they\'ve tried, CCR in Apache Doris is:\\n\\n- **Lightweight in design**: The data synchronization tasks consume very few machine resources. They run smoothly without reducing the overall performance of Apache Doris.\\n\\n- **Easy to use**: It can be configured by one simple `POST` request.\\n\\n- **Unlimited in migration**: Users can raise the upper limit of the data migration capabilities in CCR by optimizing their cluster configuration. \\n\\n- **Consistent in data**: The DDL statements executed in the source cluster can be automatically synchronized into the target cluster, ensuring data consistency.\\n\\n- **Flexible in synchronization**: It is able to perform both full data synchronization and incremental data synchronization.\\n\\nTo start CCR in Doris simply requires two steps. Step one is to enable binlogs in both the source cluster and the target cluster. Step two is to send the name of the database or table to be replicated. Then the system will start synchronizing full or incremental data. The detailed workflow is as follows: \\n\\n![CCR in Apache Doris](/images/ccr-in-apache-doris.jpeg)\\n\\n\\nIn the grocery store brand\'s case, they need to synchronize a few tables from the source cluster to the target cluster, each table having an incremental data size of about 50 million rows. After a month\'s trial run, the Doris CCR mechanism is proven to be stable and performant:\\n\\n- **Higher stability and data accuracy**: No replication failure has ever occurred during the trial period. Every data row is transferred and landed in the target cluster accurately. \\n\\n- **Streamlined workflows:**\\n\\n  - **Before CCR**: The user had to write SQL for each table and write data via Catalog; For tables without a `last_update_time` field, incremental data synchronization can only be implemented by full-table deletion and re-import.\\n\\n  ```sql\\n  Insert into catalog1.db.destination_table_1 select * from catalog1.db.source_table1 where time > xxx\\n  Insert into catalog1.db.destination_table_2 select * from catalog1.db.source_table2 where time > xxx\\n  \u2026\\n  Insert into catalog1.db.destination_table_x select * from catalog1.db.source_table_x\\n  ```\\n\\n  - **After CCR**: It only requires one `POST` request to synchronize an entire database.\\n\\n  ```sql\\n  curl -X POST -H \\"Content-Type: application/json\\" -d \'{\\n      \\"name\\": \\"ccr_test\\",\\n      \\"src\\": {\\n      \\"host\\": \\"localhost\\",\\n      \\"port\\": \\"9030\\",\\n      \\"thrift_port\\": \\"9020\\",\\n      \\"user\\": \\"root\\",\\n      \\"password\\": \\"\\",\\n      \\"database\\": \\"demo\\",\\n      \\"table\\": \\"\\"\\n      },\\n      \\"dest\\": {\\n      \\"host\\": \\"localhost\\",\\n      \\"port\\": \\"9030\\",\\n      \\"thrift_port\\": \\"9020\\",\\n      \\"user\\": \\"root\\",\\n      \\"password\\": \\"\\",\\n      \\"database\\": \\"ccrt\\",\\n      \\"table\\": \\"\\"\\n      }\\n  }\' http://127.0.0.1:9190/create_ccr\\n  ```\\n\\n- **Faster data loading**: With CCR, it only takes **3~4 seconds** to ingest a day\'s incremental data, as compared to more than 30 seconds with the Catalog method. As for real-time synchronization, CCR can finish data ingestion in 1 second, without reliance on manual updates or regular scheduling.\\n\\n## Conclusion\\n\\nUsing CCR in Apache Doris, the grocery store brand separates reading and writing workloads into different clusters and thus improves overall system stability. This solution delivers a real-time data synchronization latency of about 1 second. To further ensure normal functioning, it has a real-time monitoring and alerting mechanism so any issue will be notified and attended to instantly, and a contingency plan to guarantee uninterrupted query services. It also supports partition-based data synchronization (e.g. `ALTER TABLE tbl1 REPLACE PARTITION`). With demonstrated effectiveness of CCR, they are planning to replicate more of their data assets for efficient and secure data usage.\\n\\nCCR is also applicable when you need to build multiple data centers or derive a test dataset from your production environment. For further guidance on CCR, join the [Apache Doris community](https://join.slack.com/t/apachedoriscommunity/shared_invite/zt-2unfw3a3q-MtjGX4pAd8bCGC1UV0sKcw)."},{"id":"/release-note-2.0.9","metadata":{"permalink":"/blog/release-note-2.0.9","source":"@site/blog/release-note-2.0.9.md","title":"Apache Doris version 2.0.9 has been released","description":"Thanks to our community users and developers, about 68 improvements and bug fixes have been made in Doris 2.0.9 version.","date":"2024-04-23T00:00:00.000Z","tags":[{"inline":true,"label":"Release Notes","permalink":"/blog/tags/release-notes"}],"hasTruncateMarker":false,"authors":[{"name":"Apache Doris","key":null,"page":null}],"frontMatter":{"title":"Apache Doris version 2.0.9 has been released","description":"Thanks to our community users and developers, about 68 improvements and bug fixes have been made in Doris 2.0.9 version.","date":"2024-04-23","author":"Apache Doris","tags":["Release Notes"],"image":"/images/2.0.9.png"},"unlisted":false,"prevItem":{"title":"Cross-cluster replication for read-write separation: story of a grocery store brand","permalink":"/blog/cross-cluster-replication-for-read-write"},"nextItem":{"title":"Arrow Flight SQL for 10X faster data transfer","permalink":"/blog/arrow-flight-sql-in-apache-doris-for-10x-faster-data-transfer"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n  http://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n\\n\\nThanks to our community users and developers, about 68 improvements and bug fixes have been made in Doris 2.0.9 version.\\n\\n- **Quick Download** : [https://doris.apache.org/download/](https://doris.apache.org/download/)\\n\\n- **GitHub** : [https://github.com/apache/doris/releases](https://github.com/apache/doris/releases)\\n\\n\\n## 1 Behavior changes\\n\\nNA\\n\\n## 2 New features\\n\\n- Support predicate apprear both on key and value mv column\\n\\n- Support mv with `bitmap_union(bitmap_from_array())`\\n\\n- Add a FE config to force replicate allocation for OLAP tables in the cluster\\n\\n- Support date literal support timezone in new optimizer Nereids\\n\\n- Support slop in fulltext search `match_phrase` to specify word distence\\n\\n- Show index id in `SHOW PROC INDEXES`\\n\\n## 3 Improvements\\n\\n- Sdd a secondary argument in `first_value` / `last_value` to ignore NULL values\\n\\n- the offset params in `LEAD`/ `LAG` function could use 0\\n\\n- Adjust priority of materialized view match rule\\n\\n- TopN opt reads only limit number of records for better performance\\n\\n- Add profile for delete_bitmap get_agg function\\n\\n- Refine the Meta cache to get better performance\\n\\n- Add FE config `autobucket_max_buckets`\\n\\nSee the complete list of improvements and bug fixes on [GitHub](https://github.com/apache/doris/compare/2.0.8...2.0.9) ."},{"id":"/arrow-flight-sql-in-apache-doris-for-10x-faster-data-transfer","metadata":{"permalink":"/blog/arrow-flight-sql-in-apache-doris-for-10x-faster-data-transfer","source":"@site/blog/arrow-flight-sql-in-apache-doris-for-10x-faster-data-transfer.md","title":"Arrow Flight SQL for 10X faster data transfer","description":"Apache Doris 2.1 supports Arrow Flight SQL protocol for reading data from Doris. It delivers tens-fold speedups compared to PyMySQL and Pandas.","date":"2024-04-16T00:00:00.000Z","tags":[{"inline":true,"label":"Tech Sharing","permalink":"/blog/tags/tech-sharing"}],"hasTruncateMarker":false,"authors":[{"name":"velodb.io \xb7 VeloDB Engineering Team","key":null,"page":null}],"frontMatter":{"title":"Arrow Flight SQL for 10X faster data transfer","description":"Apache Doris 2.1 supports Arrow Flight SQL protocol for reading data from Doris. It delivers tens-fold speedups compared to PyMySQL and Pandas.","date":"2024-04-16","author":"velodb.io \xb7 VeloDB Engineering Team","tags":["Tech Sharing"],"externalLink":"https://www.velodb.io/blog/295","image":"/images/arrow-flight-sql-in-apache-doris-for-10x-faster-data-transfer.png"},"unlisted":false,"prevItem":{"title":"Apache Doris version 2.0.9 has been released","permalink":"/blog/release-note-2.0.9"},"nextItem":{"title":"Apache Doris 2.1.2 just released","permalink":"/blog/release-note-2.1.2"}},"content":"\x3c!-- \\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n\\nFor years, JDBC and ODBC have been commonly adopted norms for database interaction. Now, as we gaze upon the vast expanse of the data realm, the rise of data science and data lake analytics brings bigger and bigger datasets. Correspondingly, we need faster and faster data reading and transmission, so we start to look for better answers than JDBC and ODBC. Thus, we include **Arrow Flight SQL protocol** into [Apache Doris 2.1](https://doris.apache.org), which provides **tens-fold speedups for data transfer**. \\n\\n:::tip Tip\\nA [demo](https://www.youtube.com/watch?v=zIqy24gI8DE) of loading data from Apache Doris to Python using Arrow Flight SQL.\\n:::\\n\\n## High-speed data transfer based on Arrow Flight SQL\\n\\nAs a column-oriented data warehouse, Apache Doris arranges its query results in the form of data Blocks in a columnar format. Before version 2.1, the Blocks must be serialized into bytes in row-oriented formats before they can be transferred to a target client via a MySQL client or JDBC/ODBC driver. Moreover, if the target client is a columnar database or a column-oriented data science component like Pandas, the data should then be de-serialized. The serialization-deserialization process is a speed bump for data transmission.\\n\\nApache Doris 2.1 has a data transmission channel built on [Arrow Flight SQL](https://arrow.apache.org/docs/format/FlightSql.html). ([Apache Arrow](https://arrow.apache.org/) is a software development platform designed for high data movement efficiency across systems and languages, and the Arrow format aims for high-performance, lossless data exchange.) It allows **high-speed, large-scale data reading from Doris via SQL in various mainstream programming languages**. For target clients that also support the Arrow format, the whole process will be free of serialization/deserialization, thus no performance loss. Another upside is, Arrow Flight can make full use of multi-node and multi-core architecture and implement parallel data transfer, which is another enabler of high data throughput.\\n\\nFor example, if a Python client reads data from Apache Doris, Doris will first convert the column-oriented Blocks to Arrow RecordBatch. Then in the Python client, Arrow RecordBatch will be converted to Pandas DataFrame. Both conversions are fast because the Doris Blocks, Arrow RecordBatch, and Pandas DataFrame are all column-oriented. \\n\\n![img](/images/high-speed-data-transfer-based-on-doris-arrow-flight-sql.png)\\n\\n\\nIn addition, Arrow Flight SQL provides a general JDBC driver to facilitate seamless communication between databases that supports the Arrow Flight SQL protocol. This unlocks the the potential of Doris to be connected to a wider ecosystem and to be used in more cases. \\n\\n## Performance test\\n\\nThe \\"tens-fold speedups\\" conclusion is based on our benchmark tests. We tried reading data from Doris using PyMySQL, Pandas, and Arrow Flight SQL, and jotted down the durations, respectively. The test data is the ClickBench dataset.\\n\\n![Performance test](/images/doris-performance-test.png)\\n\\n\\nResults on various data types are as follows: \\n\\n![Performance test results](/images/doris-performance-test-2.png)\\n\\n**As shown, Arrow Flight SQL outperforms PyMySQL and Pandas in all data types by a factor ranging from 20 to several hundreds**. \\n\\n![Arrow Flight SQL outperforms PyMySQL and Pandas](/images/doris-performance-test-3.png)\\n\\n## Usage\\n\\nWith support for Arrow Flight SQL, Apache Doris can leverage the Python ADBC Driver for fast data reading. I will showcase a few frequently executed database operations using the Python ADBC Driver (version 3.9 or later), including DDL, DML, session variable setting, and `show` statements.\\n\\n### 01  Install library\\n\\nThe relevant library is already published on PyPI. It can be installed simply as follows: \\n\\n```C++\\npip install adbc_driver_manager\\npip install adbc_driver_flightsql\\n```\\n\\nImport the following module/library to interact with the installed library: \\n\\n```Python\\nimport adbc_driver_manager\\nimport adbc_driver_flightsql.dbapi as flight_sql\\n\\n>>> print(adbc_driver_manager.__version__)\\n1.1.0\\n>>> print(adbc_driver_flightsql.__version__)\\n1.1.0\\n```\\n\\n### 02  Connect to Doris\\n\\nCreate a client for interacting with the Doris Arrow Flight SQL service. Prerequisites include: Doris frontend (FE) host, Arrow Flight port, and login username/password.\\n\\nConfigure parameters for Doris frontend (FE) and backend (BE):\\n\\n- In `fe/conf/fe.conf`, set `arrow_flight_sql_port ` to an available port, such as 8070.\\n\\n- In `be/conf/be.conf`, set `arrow_flight_sql_port ` to an available port, such as 8050.\\n\\n`Note: The arrow_flight_sql_port port number configured in fe.conf and be.conf is different`\\n\\nAfter modifying the configuration and restarting the cluster, searching for `Arrow Flight SQL service is started` in the fe/log/fe.log file indicates that the Arrow Flight Server of FE has been successfully started; searching for `Arrow Flight Service bind to host` in the be/log/be.INFO file indicates that the Arrow Flight Server of BE has been successfully started.\\n\\nSuppose that the Arrow Flight SQL services for the Doris instance will run on ports 8070 and 8050 for FE and BE respectively, and the Doris username/password is \\"user\\" and \\"pass\\", the connection process would be:\\n\\n```C++\\nconn = flight_sql.connect(uri=\\"grpc://{FE_HOST}:{fe.conf:arrow_flight_sql_port}\\", db_kwargs={\\n            adbc_driver_manager.DatabaseOptions.USERNAME.value: \\"user\\",\\n            adbc_driver_manager.DatabaseOptions.PASSWORD.value: \\"pass\\",\\n        })\\ncursor = conn.cursor()\\n```\\n\\nOnce the connection is established, you can interact with Doris using SQL statements through the returned cursor object. This allows you to perform various operations such as table creation, metadata retrieval, data import, and query execution.\\n\\n### 03  Create table and retrieve metadata\\n\\nPass the query to the `cursor.execute()` function, which creates tables and retrieves metadata.\\n\\n```C++\\ncursor.execute(\\"DROP DATABASE IF EXISTS arrow_flight_sql FORCE;\\")\\nprint(cursor.fetchallarrow().to_pandas())\\n\\ncursor.execute(\\"create database arrow_flight_sql;\\")\\nprint(cursor.fetchallarrow().to_pandas())\\n\\ncursor.execute(\\"show databases;\\")\\nprint(cursor.fetchallarrow().to_pandas())\\n\\ncursor.execute(\\"use arrow_flight_sql;\\")\\nprint(cursor.fetchallarrow().to_pandas())\\n\\ncursor.execute(\\"\\"\\"CREATE TABLE arrow_flight_sql_test\\n    (\\n         k0 INT,\\n         k1 DOUBLE,\\n         K2 varchar(32) NULL DEFAULT \\"\\" COMMENT \\"\\",\\n         k3 DECIMAL(27,9) DEFAULT \\"0\\",\\n         k4 BIGINT NULL DEFAULT \'10\',\\n         k5 DATE,\\n    )\\n    DISTRIBUTED BY HASH(k5) BUCKETS 5\\n    PROPERTIES(\\"replication_num\\" = \\"1\\");\\"\\"\\")\\nprint(cursor.fetchallarrow().to_pandas())\\n\\ncursor.execute(\\"show create table arrow_flight_sql_test;\\")\\nprint(cursor.fetchallarrow().to_pandas())\\n```\\n\\nIf the returned `StatusResult` is 0, that means the query is executed successfully. (Such design is to ensure compatibility with JDBC.)\\n\\n```C++\\n  StatusResult\\n0            0\\n\\n  StatusResult\\n0            0\\n\\n                   Database\\n0         __internal_schema\\n1          arrow_flight_sql\\n..                      ...\\n507             udf_auth_db\\n\\n[508 rows x 1 columns]\\n\\n  StatusResult\\n0            0\\n\\n  StatusResult\\n0            0\\n                   Table                                       Create Table\\n0  arrow_flight_sql_test  CREATE TABLE `arrow_flight_sql_test` (\\\\n  `k0`...\\n```\\n\\n### 04  Ingest data\\n\\nExecute an INSERT INTO statement to load test data into the table created:\\n\\n```C++\\ncursor.execute(\\"\\"\\"INSERT INTO arrow_flight_sql_test VALUES\\n        (\'0\', 0.1, \\"ID\\", 0.0001, 9999999999, \'2023-10-21\'),\\n        (\'1\', 0.20, \\"ID_1\\", 1.00000001, 0, \'2023-10-21\'),\\n        (\'2\', 3.4, \\"ID_1\\", 3.1, 123456, \'2023-10-22\'),\\n        (\'3\', 4, \\"ID\\", 4, 4, \'2023-10-22\'),\\n        (\'4\', 122345.54321, \\"ID\\", 122345.54321, 5, \'2023-10-22\');\\"\\"\\")\\nprint(cursor.fetchallarrow().to_pandas())\\n```\\n\\nIf you see the following returned result, the data ingestion is successful.\\n\\n```C++\\n  StatusResult\\n0            0\\n```\\n\\nIf the data size to ingest is huge, you can apply the Stream Load method using [pydoris](https://pypi.org/project/pydoris/).\\n\\n### 05  Execute queries\\n\\nPerform queries on the above table, such as aggregation, sorting, and session variable setting.\\n\\n```C++\\ncursor.execute(\\"select * from arrow_flight_sql_test order by k0;\\")\\nprint(cursor.fetchallarrow().to_pandas())\\n\\ncursor.execute(\\"set exec_mem_limit=2000;\\")\\nprint(cursor.fetchallarrow().to_pandas())\\n\\ncursor.execute(\\"show variables like \\\\\\"%exec_mem_limit%\\\\\\";\\")\\nprint(cursor.fetchallarrow().to_pandas())\\n\\ncursor.execute(\\"select k5, sum(k1), count(1), avg(k3) from arrow_flight_sql_test group by k5;\\")\\nprint(cursor.fetchallarrow().to_pandas())\\n```\\n\\nThe results are as follows:\\n\\n```C++\\n   k0            k1    K2                k3          k4          k5\\n0   0       0.10000    ID       0.000100000  9999999999  2023-10-21\\n1   1       0.20000  ID_1       1.000000010           0  2023-10-21\\n2   2       3.40000  ID_1       3.100000000      123456  2023-10-22\\n3   3       4.00000    ID       4.000000000           4  2023-10-22\\n4   4  122345.54321    ID  122345.543210000           5  2023-10-22\\n\\n[5 rows x 6 columns]\\n\\n  StatusResult\\n0            0\\n\\n    Variable_name Value Default_Value Changed\\n0  exec_mem_limit  2000    2147483648       1\\n\\n           k5  Nullable(Float64)_1  Int64_2 Nullable(Decimal(38, 9))_3\\n0  2023-10-22         122352.94321        3            40784.214403333\\n1  2023-10-21              0.30000        2                0.500050005\\n\\n[2 rows x 5 columns]\\n```\\n\\n### 06  Complete code\\n\\n```Python\\n# Doris Arrow Flight SQL Test\\n\\n# step 1, library is released on PyPI and can be easily installed.\\n# pip install adbc_driver_manager\\n# pip install adbc_driver_flightsql\\nimport adbc_driver_manager\\nimport adbc_driver_flightsql.dbapi as flight_sql\\n\\n# step 2, create a client that interacts with the Doris Arrow Flight SQL service.\\n# Modify arrow_flight_sql_port in fe/conf/fe.conf to an available port, such as 8070.\\n# Modify arrow_flight_sql_port in be/conf/be.conf to an available port, such as 8050.\\nconn = flight_sql.connect(uri=\\"grpc://{FE_HOST}:{fe.conf:arrow_flight_sql_port}\\", db_kwargs={\\n            adbc_driver_manager.DatabaseOptions.USERNAME.value: \\"root\\",\\n            adbc_driver_manager.DatabaseOptions.PASSWORD.value: \\"\\",\\n        })\\ncursor = conn.cursor()\\n\\n# interacting with Doris via SQL using Cursor\\ndef execute(sql):\\n    print(\\"\\\\n### execute query: ###\\\\n \\" + sql)\\n    cursor.execute(sql)\\n    print(\\"### result: ###\\")\\n    print(cursor.fetchallarrow().to_pandas())\\n\\n# step3, execute DDL statements, create database/table, show stmt.\\nexecute(\\"DROP DATABASE IF EXISTS arrow_flight_sql FORCE;\\")\\nexecute(\\"show databases;\\")\\nexecute(\\"create database arrow_flight_sql;\\")\\nexecute(\\"show databases;\\")\\nexecute(\\"use arrow_flight_sql;\\")\\nexecute(\\"\\"\\"CREATE TABLE arrow_flight_sql_test\\n    (\\n         k0 INT,\\n         k1 DOUBLE,\\n         K2 varchar(32) NULL DEFAULT \\"\\" COMMENT \\"\\",\\n         k3 DECIMAL(27,9) DEFAULT \\"0\\",\\n         k4 BIGINT NULL DEFAULT \'10\',\\n         k5 DATE,\\n    )\\n    DISTRIBUTED BY HASH(k5) BUCKETS 5\\n    PROPERTIES(\\"replication_num\\" = \\"1\\");\\"\\"\\")\\nexecute(\\"show create table arrow_flight_sql_test;\\")\\n\\n\\n# step4, insert into\\nexecute(\\"\\"\\"INSERT INTO arrow_flight_sql_test VALUES\\n        (\'0\', 0.1, \\"ID\\", 0.0001, 9999999999, \'2023-10-21\'),\\n        (\'1\', 0.20, \\"ID_1\\", 1.00000001, 0, \'2023-10-21\'),\\n        (\'2\', 3.4, \\"ID_1\\", 3.1, 123456, \'2023-10-22\'),\\n        (\'3\', 4, \\"ID\\", 4, 4, \'2023-10-22\'),\\n        (\'4\', 122345.54321, \\"ID\\", 122345.54321, 5, \'2023-10-22\');\\"\\"\\")\\n\\n\\n# step5, execute queries, aggregation, sort, set session variable\\nexecute(\\"select * from arrow_flight_sql_test order by k0;\\")\\nexecute(\\"set exec_mem_limit=2000;\\")\\nexecute(\\"show variables like \\\\\\"%exec_mem_limit%\\\\\\";\\")\\nexecute(\\"select k5, sum(k1), count(1), avg(k3) from arrow_flight_sql_test group by k5;\\")\\n\\n# step6, close cursor \\ncursor.close()\\n```\\n\\n## Examples of data transmission at scale\\n\\n### 01  Python\\n\\nIn Python, after connecting to Doris using the ADBC Driver, you can use various ADBC APIs to load the Clickbench dataset from Doris into Python. Here\'s how:\\n\\n```Python\\n#!/usr/bin/env python\\n# -*- coding: utf-8 -*-\\n\\nimport adbc_driver_manager\\nimport adbc_driver_flightsql.dbapi as flight_sql\\nimport pandas\\nfrom datetime import datetime\\n\\nmy_uri = \\"grpc://0.0.0.0:`fe.conf_arrow_flight_sql_port`\\"\\nmy_db_kwargs = {\\n    adbc_driver_manager.DatabaseOptions.USERNAME.value: \\"root\\",\\n    adbc_driver_manager.DatabaseOptions.PASSWORD.value: \\"\\",\\n}\\nsql = \\"select * from clickbench.hits limit 1000000;\\"\\n\\n# PEP 249 (DB-API 2.0) API wrapper for the ADBC Driver Manager.\\ndef dbapi_adbc_execute_fetchallarrow():\\n    conn = flight_sql.connect(uri=my_uri, db_kwargs=my_db_kwargs)\\n    cursor = conn.cursor()\\n    start_time = datetime.now()\\n    cursor.execute(sql)\\n    arrow_data = cursor.fetchallarrow()\\n    dataframe = arrow_data.to_pandas()\\n    print(\\"\\\\n##################\\\\n dbapi_adbc_execute_fetchallarrow\\" + \\", cost:\\" + str(datetime.now() - start_time) + \\", bytes:\\" + str(arrow_data.nbytes) + \\", len(arrow_data):\\" + str(len(arrow_data)))\\n    print(dataframe.info(memory_usage=\'deep\'))\\n    print(dataframe)\\n\\n# ADBC reads data into pandas dataframe, which is faster than fetchallarrow first and then to_pandas.\\ndef dbapi_adbc_execute_fetch_df():\\n    conn = flight_sql.connect(uri=my_uri, db_kwargs=my_db_kwargs)\\n    cursor = conn.cursor()\\n    start_time = datetime.now()\\n    cursor.execute(sql)\\n    dataframe = cursor.fetch_df()    \\n    print(\\"\\\\n##################\\\\n dbapi_adbc_execute_fetch_df\\" + \\", cost:\\" + str(datetime.now() - start_time))\\n    print(dataframe.info(memory_usage=\'deep\'))\\n    print(dataframe)\\n\\n# Can read multiple partitions in parallel.\\ndef dbapi_adbc_execute_partitions():\\n    conn = flight_sql.connect(uri=my_uri, db_kwargs=my_db_kwargs)\\n    cursor = conn.cursor()\\n    start_time = datetime.now()\\n    partitions, schema = cursor.adbc_execute_partitions(sql)\\n    cursor.adbc_read_partition(partitions[0])\\n    arrow_data = cursor.fetchallarrow()\\n    dataframe = arrow_data.to_pandas()\\n    print(\\"\\\\n##################\\\\n dbapi_adbc_execute_partitions\\" + \\", cost:\\" + str(datetime.now() - start_time) + \\", len(partitions):\\" + str(len(partitions)))\\n    print(dataframe.info(memory_usage=\'deep\'))\\n    print(dataframe)\\n\\ndbapi_adbc_execute_fetchallarrow()\\ndbapi_adbc_execute_fetch_df()\\ndbapi_adbc_execute_partitions()\\n```\\n\\nResults are as follows (omitting the repeated outputs). **It only takes 3s** to load a Clickbench dataset containing 1 million rows and 105 columns. \\n\\n```Python\\n##################\\n dbapi_adbc_execute_fetchallarrow, cost:0:00:03.548080, bytes:784372793, len(arrow_data):1000000\\n<class \'pandas.core.frame.DataFrame\'>\\nRangeIndex: 1000000 entries, 0 to 999999\\nColumns: 105 entries, CounterID to CLID\\ndtypes: int16(48), int32(19), int64(6), object(32)\\nmemory usage: 2.4 GB\\nNone\\n        CounterID   EventDate               UserID            EventTime              WatchID  JavaEnable                                              Title  GoodEvent  ...  UTMCampaign  UTMContent  UTMTerm  FromTag  HasGCLID          RefererHash              URLHash  CLID\\n0          245620  2013-07-09  2178958239546411410  2013-07-09 19:30:27  8302242799508478680           1  OWAProfessionov \u2014 \u041C\u043E\u0439 \u041A\u0440\u0443\u0433 (\u0421\u0412\u0410\u041E \u0418\u043D\u0442\u0435\u0440\u043D\u0435\u0442-\u043C\u0430\u0433\u0430\u0437\u0438\u043D          1  ...                                                    0 -7861356476484644683 -2933046165847566158     0\\n999999       1095  2013-07-03  4224919145474070397  2013-07-03 14:36:17  6301487284302774604           0  @\u0434\u043D\u0435\u0432\u043D\u0438\u043A\u0438 Sinatra (\u041B\u0410\u0414\u0410, \u0446\u0435\u043D\u0430 \u0434\u043B\u044F \u0434\u0435\u0442\u0430\u043B\u043B\u0438 \u043A\u0442\u043E ...          1  ...                                                    0  -296158784638538920  1335027772388499430     0\\n\\n[1000000 rows x 105 columns]\\n\\n##################\\n dbapi_adbc_execute_fetch_df, cost:0:00:03.611664\\n##################\\n dbapi_adbc_execute_partitions, cost:0:00:03.483436, len(partitions):1\\n##################\\n low_level_api_execute_query, cost:0:00:03.523598, stream.address:139992182177600, rows:-1, bytes:784322926, len(arrow_data):1000000\\n##################\\n low_level_api_execute_partitions, cost:0:00:03.738128streams.size:3, 1, -1\\n```\\n\\n### 02  JDBC\\n\\nThe open-source JDBC driver for the Arrow Flight SQL protocol provides compatibility with the standard JDBC API. It allows most BI tools to access Doris via JDBC and supports high-speed transfer of Apache Arrow data. \\n\\nUsage of this driver is similar to using that for the MySQL protocol. You just need to replace `jdbc:mysql` in the connection URL with `jdbc:arrow-flight-sql`. The returned result will be in the JDBC ResultSet data structure. \\n\\n```Java\\nimport java.sql.Connection;\\nimport java.sql.DriverManager;\\nimport java.sql.ResultSet;\\nimport java.sql.Statement;\\n\\nClass.forName(\\"org.apache.arrow.driver.jdbc.ArrowFlightJdbcDriver\\");\\nString DB_URL = \\"jdbc:arrow-flight-sql://{FE_HOST}:{fe.conf:arrow_flight_sql_port}?useServerPrepStmts=false\\"\\n        + \\"&cachePrepStmts=true&useSSL=false&useEncryption=false\\";\\nString USER = \\"root\\";\\nString PASS = \\"\\";\\n\\nConnection conn = DriverManager.getConnection(DB_URL, USER, PASS);\\nStatement stmt = conn.createStatement();\\nResultSet resultSet = stmt.executeQuery(\\"show tables;\\");\\nwhile (resultSet.next()) {\\n    String col1 = resultSet.getString(1);\\n    System.out.println(col1);\\n}\\n\\nresultSet.close();\\nstmt.close();\\nconn.close();\\n```\\n\\n### 03  JAVA\\n\\nSimilar to that with Python, you can directly create an ADBC client with JAVA to read data from Doris. Firstly, you need to obtain the FlightInfo. Then, you connect to each endpoint to pull the data.\\n\\n```Java\\n// method one\\nAdbcStatement stmt = connection.createStatement()\\nstmt.setSqlQuery(\\"SELECT * FROM \\" + tableName)\\n// executeQuery, two steps:\\n// 1. Execute Query and get returned FlightInfo;\\n// 2. Create FlightInfoReader to sequentially traverse each Endpoint;\\nQueryResult queryResult = stmt.executeQuery()\\n\\n\\n// method two\\nAdbcStatement stmt = connection.createStatement()\\nstmt.setSqlQuery(\\"SELECT * FROM \\" + tableName)\\n// Execute Query and parse each Endpoint in FlightInfo, and use the Location and Ticket to construct a PartitionDescriptor\\npartitionResult = stmt.executePartitioned();\\npartitionResult.getPartitionDescriptors()\\n//Create ArrowReader for each PartitionDescriptor to read data\\nArrowReader reader = connection2.readPartition(partitionResult.getPartitionDescriptors().get(0).getDescriptor()))\\n```\\n\\n### 04  Spark\\n\\nFor Spark users, apart from connecting to Flight SQL Server using JDBC and JAVA, you can apply the [Spark-Flight-Connector](https://github.com/qwshen/spark-flight-connector), which enables Spark to act as a client for reading and writing data from/to a Flight SQL Server. This is made possible by the fast data conversion between the Arrow format and the Block in Apache Doris, which is **10 times faster than the conversion between CSV and Block**. Moreover, the Arrow data format provides more comprehensive and robust support for complex data types such as Map and Array.\\n\\n## Hop on the trend train\\n\\nA number of enterprise users of Doris has tried loading data from Doris to Python, Spark, and Flink using Arrow Flight SQL and enjoyed much faster data reading speed. In the future, we plan to include the support for Arrow Flight SQL in data writing, too. By then, most systems built with mainstream programming languages will be able to read and write data from/to Apache Doris by an ADBC client. That\'s high-speed data interaction which opens up numerous possibilities. On our to-do list, we also envision leveraging Arrow Flight to implement parallel data reading by multiple backends and facilitate federated queries across Doris and Spark. \\n\\nDownload [Apache Doris 2.1](https://doris.apache.org/download/) and get a taste of 100 times faster data transfer powered by Arrow Flight SQL. If you need assistance, come find us in the [Apache Doris developer and user community](https://join.slack.com/t/apachedoriscommunity/shared_invite/zt-2unfw3a3q-MtjGX4pAd8bCGC1UV0sKcw)."},{"id":"/release-note-2.1.2","metadata":{"permalink":"/blog/release-note-2.1.2","source":"@site/blog/release-note-2.1.2.md","title":"Apache Doris 2.1.2 just released","description":"Dear community, Apache Doris 2.1.2 has been officially released on April 12, 2024. This version submits several enhancements and bug fixes to further improve the performance and stability.","date":"2024-04-12T00:00:00.000Z","tags":[{"inline":true,"label":"Release Notes","permalink":"/blog/tags/release-notes"}],"hasTruncateMarker":false,"authors":[{"name":"Apache Doris","key":null,"page":null}],"frontMatter":{"title":"Apache Doris 2.1.2 just released","description":"Dear community, Apache Doris 2.1.2 has been officially released on April 12, 2024. This version submits several enhancements and bug fixes to further improve the performance and stability.","date":"2024-04-12","author":"Apache Doris","tags":["Release Notes"],"image":"/images/2.1.2.png"},"unlisted":false,"prevItem":{"title":"Arrow Flight SQL for 10X faster data transfer","permalink":"/blog/arrow-flight-sql-in-apache-doris-for-10x-faster-data-transfer"},"nextItem":{"title":"Apache Doris 2.0.8 just released","permalink":"/blog/release-note-2.0.8"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n  http://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\nDear community, Apache Doris 2.1.2 has been officially released on April 12, 2024. This version submits several enhancements and bug fixes to further improve the performance and stability.\\n\\n**Quick Download:** https://doris.apache.org/download/\\n\\n**GitHub Release:** https://github.com/apache/doris/releases\\n\\n## Behavior changes\\n\\n1. Set the default value of the `data_consistence` property of EXPORT to partition to make export more stable during load. \\n\\n- https://github.com/apache/doris/pull/32830\\n\\n2. Some of MySQL Connector (eg, dotnet MySQL.Data) rely on variable\'s column type to make connection.\\n\\n   eg, select @[@autocommit]([@autocommit](https://github.com/autocommit)) should with column type BIGINT, not BIT, otherwise it will throw error. So we change column type of @[@autocommit](https://github.com/autocommit) to BIGINT. \\n\\n- https://github.com/apache/doris/pull/33282\\n\\n3. Auto Partition syntax changes, see https://doris.apache.org/zh-CN/docs/table-design/data-partition#%E8%87%AA%E5%8A%A8%E5%88%86%E5%8C%BA\\n\\n- https://github.com/apache/doris/pull/32737\\n\\n4. Auto Partition prohibits the simultaneous use of Dynamic Partition on a single table.\\n\\n- https://github.com/apache/doris/pull/33736\\n\\n## Upgrade problem\\n\\n1. Normal workload group is not created when upgrade from 2.0 or other old versions. \\n\\n  - https://github.com/apache/doris/pull/33197\\n\\n##  New features\\n\\n\\n1. Add processlist table in information_schema database, users could use this table to query active connections. \\n\\n  - https://github.com/apache/doris/pull/32511\\n\\n2. Add a new table valued function `LOCAL` to allow access file system like shared storage. \\n\\n  - https://github.com/apache/doris-website/pull/494\\n\\n\\n## Improvements\\n\\n1. Skip some useless process to make graceful stop more quickly in K8s env. \\n\\n  - https://github.com/apache/doris/pull/33212\\n\\n2. Add rollup table name in profile to help find the mv selection problem. \\n\\n  - https://github.com/apache/doris/pull/33137\\n\\n3. Add test connection function to DB2 database to allow user check the connection when create DB2 Catalog. \\n\\n  - https://github.com/apache/doris/pull/33335\\n\\n4. Add DNS Cache for FQDN to accelerate the connect process among BEs in K8s env. \\n\\n  - https://github.com/apache/doris/pull/32869\\n\\n5. Refresh external table\'s rowcount async to make the query plan more stable. \\n\\n  - https://github.com/apache/doris/pull/32997\\n\\n\\n## Bug fixes\\n\\n\\n1. Fix Iceberg Catalog of HMS and Hadoop do not support Iceberg properties like \\"io.manifest.cache-enabled\\" to enable manifest cache in Iceberg. \\n\\n  - https://github.com/apache/doris/pull/33113\\n\\n2. The offset params in `LEAD`/`LAG` function could use 0 as offset. \\n\\n  - https://github.com/apache/doris/pull/33174\\n\\n3. Fix some timeout issues with load. \\n\\n  - https://github.com/apache/doris/pull/33077\\n\\n  - https://github.com/apache/doris/pull/33260\\n\\n4. Fix core problem related with `ARRAY`/`MAP`/`STRUCT` compaction process. \\n\\n  - https://github.com/apache/doris/pull/33130\\n\\n  - https://github.com/apache/doris/pull/33295\\n\\n5. Fix runtime filter wait timeout. \\n\\n  - https://github.com/apache/doris/pull/33369\\n\\n6. Fix `unix_timestamp` core for string input in auto partition. \\n\\n  - https://github.com/apache/doris/pull/32871"},{"id":"/release-note-2.0.8","metadata":{"permalink":"/blog/release-note-2.0.8","source":"@site/blog/release-note-2.0.8.md","title":"Apache Doris 2.0.8 just released","description":"Thanks to our community users and developers, about 65 improvements and bug fixes have been made in Doris 2.0.8 version.","date":"2024-04-09T00:00:00.000Z","tags":[{"inline":true,"label":"Release Notes","permalink":"/blog/tags/release-notes"}],"hasTruncateMarker":false,"authors":[{"name":"Apache Doris","key":null,"page":null}],"frontMatter":{"title":"Apache Doris 2.0.8 just released","description":"Thanks to our community users and developers, about 65 improvements and bug fixes have been made in Doris 2.0.8 version.","date":"2024-04-09","author":"Apache Doris","tags":["Release Notes"],"image":"/images/2.0.8.png"},"unlisted":false,"prevItem":{"title":"Apache Doris 2.1.2 just released","permalink":"/blog/release-note-2.1.2"},"nextItem":{"title":"Auto-increment columns in databases: a simple magic that makes a big difference","permalink":"/blog/auto-increment-columns-in-databases"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n  http://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n\\nThanks to our community users and developers, about 65 improvements and bug fixes have been made in Doris 2.0.8 version.\\n\\n- **Quick Download** : [https://doris.apache.org/download/](https://doris.apache.org/download/)\\n\\n- **GitHub** : [https://github.com/apache/doris/releases](https://github.com/apache/doris/releases)\\n\\n\\n\\n## Behavior changes\\n\\nThe `ADMIN SHOW` statement can not be executed with high version of MySQL 8.x jdbc driver. So rename these statement, remove the `ADMIN` keywords. \\n\\n- https://github.com/apache/doris/pull/29492\\n\\n```sql\\nADMIN SHOW CONFIG -> SHOW CONFIG\\nADMIN SHOW REPLICA -> SHOW REPLICA\\nADMIN DIAGNOSE TABLET -> SHOW TABLET DIAGNOSIS\\nADMIN SHOW TABLET -> SHOW TABLET\\n```\\n\\n\\n## New features\\n\\nN/A\\n\\n\\n\\n## Improvement\\n\\n- Make Inverted Index work with TopN opt in Nereids\\n\\n- Limit the max string length to 1024 while collecting column stats to control BE memory usage\\n\\n- JDBC Catalog close when JDBC client is not empty\\n\\n- Accept all Iceberg database and do not check the name format of database\\n\\n- Refresh external table\'s rowcount async to avoid cache miss and unstable query plan\\n\\n- Simplify the isSplitable method of hive external table to avoid too many hadoop metrics\\n\\nSee the complete list of improvements and bug fixes on [GitHub](https://github.com/apache/doris/compare/2.0.7...2.0.8) .\\n\\n\\n## Credits\\n\\nThanks all who contribute to this release:\\n\\n924060929,  AcKing-Sam, amorynan, AshinGau, BePPPower, BiteTheDDDDt, ByteYue, cambyzju,  dongsilun, eldenmoon, feiniaofeiafei, gnehil, Jibing-Li, liaoxin01, luwei16,  morningman, morrySnow, mrhhsg, Mryange, nextdreamblue, platoneko,  starocean999, SWJTU-ZhangLei, wuwenchi, xiaokang, xinyiZzz, Yukang-Lian,  Yulei-Yang, zclllyybb, zddr, zhangstar333, zhiqiang-hhhh, ziyanTOP, zy-kkk,  zzzxl1993"},{"id":"/auto-increment-columns-in-databases","metadata":{"permalink":"/blog/auto-increment-columns-in-databases","source":"@site/blog/auto-increment-columns-in-databases.md","title":"Auto-increment columns in databases: a simple magic that makes a big difference","description":"Auto-increment columns in Apache Doris accelerates dictionary encoding and pagination without damaging data writing performance. This is an introduction to its usage, applicable scenarios, and implementation details.","date":"2024-04-08T00:00:00.000Z","tags":[{"inline":true,"label":"Tech Sharing","permalink":"/blog/tags/tech-sharing"}],"hasTruncateMarker":false,"authors":[{"name":"velodb.io \xb7 VeloDB Engineering Team","key":null,"page":null}],"frontMatter":{"title":"Auto-increment columns in databases: a simple magic that makes a big difference","description":"Auto-increment columns in Apache Doris accelerates dictionary encoding and pagination without damaging data writing performance. This is an introduction to its usage, applicable scenarios, and implementation details.","date":"2024-04-08","author":"velodb.io \xb7 VeloDB Engineering Team","externalLink":"https://www.velodb.io/blog/252","tags":["Tech Sharing"],"image":"/images/auto-increment-columns-in-databases.png"},"unlisted":false,"prevItem":{"title":"Apache Doris 2.0.8 just released","permalink":"/blog/release-note-2.0.8"},"nextItem":{"title":"Apache Doris 2.1.1 just released","permalink":"/blog/release-note-2.1.1"}},"content":"\x3c!-- \\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\nAuto-increment column is a bread-and-butter feature of single-node transactional databases. It assigns a unique identifier for each row in a way that requires the least manual effort from users. With an auto-increment column in the table, whenever a new row is inserted into the table, the new row will be assigned with the next available value from the auto-increment sequence. This is an automated mechanism that makes database maintenance easy and reliable.\\n\\nAuto-increment column is the bedrock of many features in databases:\\n\\n- **Dictionary encoding**: User IDs and Order IDs are often stored as strings. However, strings are not friendly to precise deduplication query execution. So for optimal performance, a common practice is to perform dictionary encoding on the strings and then construct a bitmap for aggregation operations. The role of an auto-increment column in this process is that **it speeds up dictionary encoding and thus accelerates string deduplication**.\\n\\n- **Primary key generation**: An auto-increment column is the perfect candidate for the primary key of a table. Primary keys must be unique and not empty, while auto-increment columns guarantee a unique identifier for each row. \\n\\n- **Detailed data updates**: Updating detail tables is tricky, but it can be easy if you add a auto-increment table to it. It gives each data record in the database a unique ID, which can work as the primary key, and then data updates can be done based on the primary key.\\n\\n- **Efficient pagination**: Pagination is often required in data display. It is typically implemented by the `limit` or `offset` + `order by` statement in SQL queries. However, such implementation involves full data reading and sorting, which doesn\'t make so much sense in deep pagination queries (those with large offsets). This is when auto-increment columns come to the rescue. Like I said, it gives a unique identifier to each row, so the maximum identifier of the last page can be used as the filtering condition for the next page. Thus, it can avoid a lot of unnecessary data scanning and increase pagination efficiency.\\n\\nThe idea of auto-increment columns is intuitive, but when it comes to distributed databases, it becomes a different game, because it has to consider global transactions. As a distributed DBMS, Apache Doris provides an innovative and efficient auto-increment solution that does no harm to data writing performance.\\n\\n:::tip\\nTo give AUTO_INCREMENT column a spin, follow this quick [demo](https://www.youtube.com/watch?v=FGVp2RQvGBo).\\n:::\\n\\n## Syntax & usage\\n\\nTo enable an auto-increment column in Doris, add `AUTO_INCREMENT` property to the column in the table creation statement ([CREAT-TABLE](https://doris.apache.org/zh-CN/docs/sql-manual/sql-statements/Data-Definition-Statements/Create/CREATE-TABLE)). You can specify a starting value for the auto-increment column via `AUTO_INCREMENT(start_value)`; if not, the default starting value is 1.\\n\\nFor example, you can create a table in the [Duplicate Key model](https://doris.apache.org/docs/data-table/data-model#duplicate-model), with one of the key columns being an auto-increment column. \\n\\n```sql\\nCREATE TABLE `demo`.`tbl` (\\n  // highlight-next-line\\n      `id` BIGINT NOT NULL AUTO_INCREMENT,\\n      `value` BIGINT NOT NULL\\n) ENGINE=OLAP\\nDUPLICATE KEY(`id`)\\nDISTRIBUTED BY HASH(`id`) BUCKETS 10\\nPROPERTIES (\\n\\"replication_allocation\\" = \\"tag.location.default: 3\\"\\n);\\n```\\n\\nApart from a key column, you can also specify a value column as an auto-increment column (example below):\\n\\n```sql\\nCREATE TABLE `demo`.`tbl` (\\n      `uid` BIGINT NOT NULL,\\n      `name` BIGINT NOT NULL,\\n      // highlight-next-line\\n      `id` BIGINT NOT NULL AUTO_INCREMENT,\\n      `value` BIGINT NOT NULL\\n) ENGINE=OLAP\\nDUPLICATE KEY(`uid`, `name`)\\nDISTRIBUTED BY HASH(`uid`) BUCKETS 10\\nPROPERTIES (\\n\\"replication_allocation\\" = \\"tag.location.default: 3\\"\\n);\\n```\\n\\nAUTO_INCREMENT is supported in both the Duplicate Key model and the [Unique Key model](https://doris.apache.org/docs/data-table/data-model/#unique-model). Usage in the latter is similar.\\n\\nI will walk you down the rest of the road with the table below as an example: \\n\\n```sql\\nCREATE TABLE `demo`.`tbl` (\\n    `id` BIGINT NOT NULL AUTO_INCREMENT,\\n    `name` varchar(65533) NOT NULL,\\n    `value` int(11) NOT NULL\\n) ENGINE=OLAP\\nUNIQUE KEY(`id`)\\nDISTRIBUTED BY HASH(`id`) BUCKETS 10\\nPROPERTIES (\\n\\"replication_allocation\\" = \\"tag.location.default: 3\\"\\n);\\n```\\n\\nWhen you ingest data into this table using an `insert into` statement, if the `id` column has no specified value in the original data file, it will be auto-filled with auto-increment values.\\n\\n```sql\\nmysql> insert into tbl(name, value) values(\\"Bob\\", 10), (\\"Alice\\", 20), (\\"Jack\\", 30);\\nQuery OK, 3 rows affected (0.09 sec)\\n{\'label\':\'label_183babcb84ad4023_a2d6266ab73fb5aa\', \'status\':\'VISIBLE\', \'txnId\':\'7\'}\\n\\nmysql> select * from tbl order by id;\\n+------+-------+-------+\\n| id   | name  | value |\\n+------+-------+-------+\\n|    1 | Bob   |    10 |\\n|    2 | Alice |    20 |\\n|    3 | Jack  |    30 |\\n+------+-------+-------+\\n3 rows in set (0.05 sec)\\n```\\n\\nSimilarly, when you ingest a data file `test.csv` by Stream Load, the `id` column will be auto-filled with auto-increment values, too.\\n\\n```sql\\ntest.csv:\\nTom,40\\nJohn,50\\ncurl --location-trusted -u user:passwd -H \\"columns:name,value\\" -H \\"column_separator:,\\" -T ./test.csv http://{host}:{port}/api/{db}/tbl/_stream_load\\nselect * from tbl order by id;\\n+------+-------+-------+\\n| id   | name  | value |\\n+------+-------+-------+\\n|    1 | Bob   |    10 |\\n|    2 | Alice |    20 |\\n|    3 | Jack  |    30 |\\n|    4 | Tom   |    40 |\\n|    5 | John  |    50 |\\n+------+-------+-------+\\n5 rows in set (0.04 sec)\\n```\\n\\n## Applicable scenarios\\n\\n### 01 Dictionary encoding\\n\\nIn Apache Doris, the bitmap data type and the bitmap-related aggregations are implemented with RoaringBitmap, which can deliver high performance especially when dictionary encoding produces dense values. \\n\\nAs is mentioned, auto-increment columns enable fast dictionary encoding. I will put you into the context of user profiling to show you how that works.\\n\\nFor analysis of offline page views (PV) and unique visitors (UV), store the details in a user behavior table: \\n\\n```sql\\nCREATE TABLE `demo`.`dwd_dup_tbl` (\\n    `user_id` varchar(50) NOT NULL,\\n    `dim1` varchar(50) NOT NULL,\\n    `dim2` varchar(50) NOT NULL,\\n    `dim3` varchar(50) NOT NULL,\\n    `dim4` varchar(50) NOT NULL,\\n    `dim5` varchar(50) NOT NULL,\\n    `visit_time` DATE NOT NULL\\n) ENGINE=OLAP\\nDUPLICATE KEY(`user_id`)\\nDISTRIBUTED BY HASH(`user_id`) BUCKETS 32\\nPROPERTIES (\\n\\"replication_allocation\\" = \\"tag.location.default: 3\\"\\n);\\n```\\n\\nCreate a dictionary table as follows leveraging AUTO_INCREMENT:\\n\\n```sql\\nCREATE TABLE `demo`.`dictionary_tbl` (\\n    `user_id` varchar(50) NOT NULL,\\n    `aid` BIGINT NOT NULL AUTO_INCREMENT\\n) ENGINE=OLAP\\nUNIQUE KEY(`user_id`)\\nDISTRIBUTED BY HASH(`user_id`) BUCKETS 32\\nPROPERTIES (\\n\\"replication_allocation\\" = \\"tag.location.default: 3\\"\\n);\\n```\\n\\nLoad the existing `user_id` into the dictionary table, and create mappings from `user_id` to integer values.\\n\\n```sql\\ninsert into dictionary_tbl(user_id)\\nselect user_id from dwd_dup_tbl group by user_id;\\n```\\n\\nIf you only need to load the incremental `user_id` into the dictionary table, you can use the following command. In practice, you can also use the [Flink Doris Connector](https://doris.apache.org/docs/ecosystem/flink-doris-connector/) for data writing. \\n\\n```sql\\ninsert into dictionary_tbl(user_id)\\nselect dwd_dup_tbl.user_id from dwd_dup_tbl left join dictionary_tbl\\non dwd_dup_tbl.user_id = dictionary_tbl.user_id where dwd_dup_tbl.visit_time  \'2023-12-10\' and dictionary_tbl.user_id is NULL;\\n```\\n\\nSuppose you have your analytic dimensions as `dim1`,  `dim3`,  `dim5`, create a table in the [Aggregate Key model](https://doris.apache.org/docs/data-table/data-model#aggregate-model) to accommodate the results of data aggregation:\\n\\n```sql\\nCREATE TABLE `demo`.`dws_agg_tbl` (\\n    `dim1` varchar(50) NOT NULL,\\n    `dim3` varchar(50) NOT NULL,\\n    `dim5` varchar(50) NOT NULL,\\n    `user_id_bitmap` BITMAP BITMAP_UNION NOT NULL,\\n    `pv` BIGINT SUM NOT NULL \\n) ENGINE=OLAP\\nAGGREGATE KEY(`dim1`,`dim3`,`dim5`)\\nDISTRIBUTED BY HASH(`dim1`) BUCKETS 32\\nPROPERTIES (\\n\\"replication_allocation\\" = \\"tag.location.default: 3\\"\\n);\\n```\\n\\nLoad the aggregated results into the table:\\n\\n```sql\\ninsert into dws_agg_tbl\\nselect dwd_dup_tbl.dim1, dwd_dup_tbl.dim3, dwd_dup_tbl.dim5, BITMAP_UNION(TO_BITMAP(dictionary_tbl.aid)), COUNT(1)\\nfrom dwd_dup_tbl INNER JOIN dictionary_tbl on dwd_dup_tbl.user_id = dictionary_tbl.user_id\\ngroup by dwd_dup_tbl.dim1, dwd_dup_tbl.dim3, dwd_dup_tbl.dim5;\\n```\\n\\nThen you query PV/UV using the following statement:\\n\\n```sql\\nselect dim1, dim3, dim5, bitmap_count(user_id_bitmap) as uv, pv from dws_agg_tbl;\\n```\\n\\n### 02 Detailed data updates\\n\\nIn Doris, the Unique Key model is applicable to use cases with frequent data updates, while the Duplicate Key model is designed for detailed data storage with no data updating requirements.\\n\\nHowever, in real life, users might need to update their detailed data sometimes, which can be hard to implement because the data tables don\'t come with unique key columns.\\n\\nIn this case, you can **use an auto-increment column as the primary key for the detailed data**.\\n\\nFor example, a financial institution keeps record of customer loans and writes it into a Duplicate Key table, in which one single user might have multiple borrowing records. \\n\\n```Python\\nCREATE TABLE loan_records (\\n    `user_id` VARCHAR(20) DEFAULT NULL COMMENT \'Customer ID\',\\n    `loan_amount` DECIMAL(10, 2) DEFAULT NULL COMMENT \'Amount of loan\',\\n    `interest_rate` DECIMAL(10, 2) DEFAULT NULL COMMENT \'Interest rate\',\\n    `loan_start_date` DATE DEFAULT NULL COMMENT \'Start date of the loan\',\\n    `loan_end_date` DATE DEFAULT NULL COMMENT \'End date of the loan\',\\n    `total_debt` DECIMAL(10, 2) DEFAULT NULL COMMENT \'Amount of debt\'\\n) DUPLICATE KEY(`user_id`)\\nDISTRIBUTED BY HASH(`user_id`) BUCKETS 10\\nPROPERTIES (\\n    \\"replication_allocation\\" = \\"tag.location.default: 3\\"\\n);\\n```\\n\\nSuppose that in a promotional campaign, the institution offers a 10% discount on interest rates to its existing customers. Correspondingly, there is a need to update the `interest_rate` and `total_debt` in the table.\\n\\nFor that sake, you can create a Unique Key table for the same data, but add an `auto_id` field and set it as the primary key. \\n\\n```Python\\nCREATE TABLE loan_records (\\n    `auto_id` BIGINT NOT NULL AUTO_INCREMENT,\\n    `user_id` VARCHAR(20) DEFAULT NULL COMMENT \'Customer ID\',\\n    `loan_amount` DECIMAL(10, 2) DEFAULT NULL COMMENT \'Amount of loan\',\\n    `interest_rate` DECIMAL(10, 2) DEFAULT NULL COMMENT \'Interest rate\',\\n    `loan_start_date` DATE DEFAULT NULL COMMENT \'Start date of the loan\',\\n    `loan_end_date` DATE DEFAULT NULL COMMENT \'End date of the loan\',\\n    `total_debt` DECIMAL(10, 2) DEFAULT NULL COMMENT \'Amount of debt\'\\n) UNIQUE KEY(`auto_id`)\\nDISTRIBUTED BY HASH(`auto_id`) BUCKETS 10\\nPROPERTIES (\\n    \\"replication_allocation\\" = \\"tag.location.default: 3\\"\\n);\\n```\\n\\nNow, write a few new records into the table and see what happens. (Note that you don\'t have to write in the `auto_id` field.)\\n\\n```Python\\nINSERT INTO loan_records (user_id, loan_amount, interest_rate, loan_start_date, loan_end_date, total_debt) VALUES\\n(\'10001\', 5000.00, 5.00, \'2024-03-01\', \'2024-03-31\', 5020.55),\\n(\'10002\', 10000.00, 5.00, \'2024-03-01\', \'2024-05-01\', 10082.56),\\n(\'10003\', 2000.00, 5.00, \'2024-03-01\', \'2024-03-15\', 2003.84),\\n(\'10004\', 7500.00, 5.00, \'2024-03-01\', \'2024-04-15\', 7546.23),\\n(\'10005\', 3000.00, 5.00, \'2024-03-01\', \'2024-03-21\', 3008.22),\\n(\'10002\', 8000.00, 5.00, \'2024-03-01\', \'2024-06-01\', 8100.82),\\n(\'10007\', 6000.00, 5.00, \'2024-03-01\', \'2024-04-10\', 6032.88),\\n(\'10008\', 4000.00, 5.00, \'2024-03-01\', \'2024-03-26\', 4013.70),\\n(\'10001\', 5500.00, 5.00, \'2024-03-01\', \'2024-04-05\', 5526.37),\\n(\'10010\', 9000.00, 5.00, \'2024-03-01\', \'2024-05-10\', 9086.30);\\n```\\n\\nCheck with the `select * from loan_records` statement, and you can see a unique ID is already in place for each newly-ingested record:\\n\\n```Python\\nmysql> select * from loan_records;\\n+---------+---------+-------------+---------------+-----------------+---------------+------------+\\n| auto_id | user_id | loan_amount | interest_rate | loan_start_date | loan_end_date | total_debt |\\n+---------+---------+-------------+---------------+-----------------+---------------+------------+\\n|       1 | 10001   |     5000.00 |          5.00 | 2024-03-01      | 2024-03-31    |    5020.55 |\\n|       4 | 10004   |     7500.00 |          5.00 | 2024-03-01      | 2024-04-15    |    7546.23 |\\n|       2 | 10002   |    10000.00 |          5.00 | 2024-03-01      | 2024-05-01    |   10082.56 |\\n|       3 | 10003   |     2000.00 |          5.00 | 2024-03-01      | 2024-03-15    |    2003.84 |\\n|       6 | 10002   |     8000.00 |          5.00 | 2024-03-01      | 2024-06-01    |    8100.82 |\\n|       8 | 10008   |     4000.00 |          5.00 | 2024-03-01      | 2024-03-26    |    4013.70 |\\n|       7 | 10007   |     6000.00 |          5.00 | 2024-03-01      | 2024-04-10    |    6032.88 |\\n|       9 | 10001   |     5500.00 |          5.00 | 2024-03-01      | 2024-04-05    |    5526.37 |\\n|       5 | 10005   |     3000.00 |          5.00 | 2024-03-01      | 2024-03-21    |    3008.22 |\\n|      10 | 10010   |     9000.00 |          5.00 | 2024-03-01      | 2024-05-10    |    9086.30 |\\n+---------+---------+-------------+---------------+-----------------+---------------+------------+\\n10 rows in set (0.01 sec)\\n```\\n\\nExecute these two SQL statements to update `interest_rate` and `total_debt`, respectively:\\n\\n```Python\\nupdate loan_records set interest_rate = interest_rate * 0.9 where user_id <= 10005;\\nupdate loan_records set total_debt = loan_amount + (loan_amount * (interest_rate / 100) * DATEDIFF(loan_end_date, loan_start_date) / 365);\\n```\\n\\nCheck again to see if the old records have been replaced by the new ones:\\n\\n```Python\\nmysql> select * from loan_records order by auto_id;\\n+---------+---------+-------------+---------------+-----------------+---------------+------------+\\n| auto_id | user_id | loan_amount | interest_rate | loan_start_date | loan_end_date | total_debt |\\n+---------+---------+-------------+---------------+-----------------+---------------+------------+\\n|       1 | 10001   |     5000.00 |          4.50 | 2024-03-01      | 2024-03-31    |    5018.49 |\\n|       2 | 10002   |    10000.00 |          4.50 | 2024-03-01      | 2024-05-01    |   10075.21 |\\n|       3 | 10003   |     2000.00 |          4.50 | 2024-03-01      | 2024-03-15    |    2003.45 |\\n|       4 | 10004   |     7500.00 |          4.50 | 2024-03-01      | 2024-04-15    |    7541.61 |\\n|       5 | 10005   |     3000.00 |          4.50 | 2024-03-01      | 2024-03-21    |    3007.40 |\\n|       6 | 10002   |     8000.00 |          4.50 | 2024-03-01      | 2024-06-01    |    8090.74 |\\n|       7 | 10007   |     6000.00 |          5.00 | 2024-03-01      | 2024-04-10    |    6032.88 |\\n|       8 | 10008   |     4000.00 |          5.00 | 2024-03-01      | 2024-03-26    |    4013.70 |\\n|       9 | 10001   |     5500.00 |          4.50 | 2024-03-01      | 2024-04-05    |    5523.73 |\\n|      10 | 10010   |     9000.00 |          5.00 | 2024-03-01      | 2024-05-10    |    9086.30 |\\n+---------+---------+-------------+---------------+-----------------+---------------+------------+\\n10 rows in set (0.01 sec)\\n```\\n\\n### 03 Efficient pagination\\n\\nImagine that you need to sort the data in a specific order and then retrieve record No. 90,001 to record No. 90,010. This means you have a large offset of 90,000. This is what we call a deep pagination query. Even though you only require a result set of 10 rows, the database system still has to read the entire dataset into memory and perform a full sorting.\\n\\n**For higher execution efficiency in deep pagination queries, you can harness the power of auto-increment columns**. The main idea is to record the `max_value` from the `unique_value` column of the previous page, and push down predicates by `where unique_value > max_value limit rows_per_page`.\\n\\nFor example, during table creation, you enable an auto-increment column: `unique_value`, which gives each row an identifier.\\n\\n```sql\\nCREATE TABLE `demo`.`records_tbl` (\\n    `user_id` int(11) NOT NULL COMMENT \\"\\",\\n    `name` varchar(26) NOT NULL COMMENT \\"\\",\\n    `address` varchar(41) NOT NULL COMMENT \\"\\",\\n    `city` varchar(11) NOT NULL COMMENT \\"\\",\\n    `nation` varchar(16) NOT NULL COMMENT \\"\\",\\n    `region` varchar(13) NOT NULL COMMENT \\"\\",\\n    `phone` varchar(16) NOT NULL COMMENT \\"\\",\\n    `mktsegment` varchar(11) NOT NULL COMMENT \\"\\",\\n    `unique_value` BIGINT NOT NULL AUTO_INCREMENT\\n) DUPLICATE KEY (`user_id`, `name`)\\nDISTRIBUTED BY HASH(`user_id`) BUCKETS 10\\nPROPERTIES (\\n    \\"replication_allocation\\" = \\"tag.location.default: 3\\"\\n);\\n```\\n\\nIn pagination queries, suppose that each page displays 100 results, this is how you retrieve the first page of the result set. \\n\\n```sql\\nselect * from records_tbl order by unique_value limit 100;\\n```\\n\\nUse programs to record the maximum `unique_value` in the returned result. Suppose that the maximum is 99, you can query data from the second page using the following statement:\\n\\n```sql\\nselect * from records_tbl where unique_value > 99 order by unique_value limit 100;\\n```\\n\\nIf you need to query data from a deeper page, for example, page 101, which means it\'s hard to get the maximum `unique_value` from the previous page directly, then you can use the statement as follows:\\n\\n```sql\\nselect user_id, name, address, city, nation, region, phone, mktsegment\\nfrom records_tbl, (select unique_value as max_value from records_tbl order by unique_value limit 1 offset 9999) as previous_data\\nwhere records_tbl.unique_value > previous_data.max_value\\norder by unique_value limit 100;\\n```\\n\\n## Implementation\\n\\nTypical OLTP databases perform incremental ID matching by their transaction mechanisms. However, in an MPP-based distributed database system like Apache Doris, such an approach can easily suffocate data writing performance. \\n\\nThat\'s why Apache Doris 2.1 innovates the implementation of auto-increment IDs. In a data ingestion task, one of the backend (BE) nodes will work as the coordinator, which is responsible for the allocation of auto-increment IDs. The coordinator BE requests a range of IDs in bulk from the frontend (FE). The FE makes sure that the ID ranges allocated to each BE do not overlap, thus guaranteeing the uniqueness of IDs.\\n\\nI illustrate the process with the figure below. StreamLoad1 has BE1 as the coordinator. BE1 requests a batch of IDs (range: 1-1000) from the FE and caches the IDs locally. Once all 1000 IDs are allocated, BE1 will request a new batch from the FE. At the same time, StreamLoad 2 selects BE3 as the coordinator, and BE3 also requests IDs from the FE. Since IDs 1-1000 have already been allocated to BE1, the FE assigns IDs 1001-2000 to BE3.\\n\\n![the implementation of auto-increment IDs](/images/the-implementation-of-auto-increment-IDs.png)\\n\\nSuppose that StreamLoad1 and StreamLoad2 each write in 50 new data records, the auto-increment IDs assigned to them will be 1-50 and 1001-1050. \\n\\nSuppose that StreamLoad3 arises later and selects BE1 as the coordinator, BE1 will assign IDs starting from 51 to the data written by StreamLoad3. From the user\'s side, they will see that rows written by StreamLoad3 get smaller ID numbers than those by StreamLoad2, even though StreamLoad2 precedes StreamLoad3 in time.\\n\\n## Note\\n\\nAttention is required regarding: \\n\\n- **Scope of uniqueness guarantee**: Doris ensures that the values generated on an auto-increment column are unique within the table, but this only applies to values auto-filled by Doris. If a user explicitly inserts values into the auto-increment column, Doris cannot guarantee the uniqueness of those values.\\n\\n- **Density and continuity of values**: Doris ensures that the values generated by the auto-increment column are dense. However, for performance reasons, it cannot guarantee that the auto-filled values are continuous. This means there may be occurrences of value jumps in the auto-increment column. Additionally, since the auto-increment values are pre-allocated and cached in BE, the magnitude of the auto-increment values cannot reflect the order of data import.\\n\\n## Conclusion\\n\\nAUTO_INCREMENT brings higher stability and reliability for Doris in large-scale data processing. If it sounds like something you need, download [Apache Doris](https://doris.apache.org/download/) and try it out. For issues you come across along the way, join us in the [Apache Doris developer and user community](https://join.slack.com/t/apachedoriscommunity/shared_invite/zt-2unfw3a3q-MtjGX4pAd8bCGC1UV0sKcw) and we are happy to help."},{"id":"/release-note-2.1.1","metadata":{"permalink":"/blog/release-note-2.1.1","source":"@site/blog/release-note-2.1.1.md","title":"Apache Doris 2.1.1 just released","description":"Dear community, Apache Doris 2.1.1 is now available, with several enhancements and bug fixes based on 2.1.0, enabling smoother user experience.","date":"2024-04-03T00:00:00.000Z","tags":[{"inline":true,"label":"Release Notes","permalink":"/blog/tags/release-notes"}],"hasTruncateMarker":false,"authors":[{"name":"Apache Doris","key":null,"page":null}],"frontMatter":{"title":"Apache Doris 2.1.1 just released","description":"Dear community, Apache Doris 2.1.1 is now available, with several enhancements and bug fixes based on 2.1.0, enabling smoother user experience.","date":"2024-04-03","author":"Apache Doris","tags":["Release Notes"],"image":"/images/2.1.1.png"},"unlisted":false,"prevItem":{"title":"Auto-increment columns in databases: a simple magic that makes a big difference","permalink":"/blog/auto-increment-columns-in-databases"},"nextItem":{"title":"Apache Doris 2.0.7 just released","permalink":"/blog/release-note-2.0.7"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n  http://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n\\nDear community members, Apache Doris 2.1.1 has been officially released on April 3, 2024, with several enhancements and bug fixes based on 2.1.0, enabling smoother user experience.\\n\\n\\n- **Quick Download:** [https://doris.apache.org/download/](https://doris.apache.org/download/)\\n\\n- **GitHub\uFF1A** [https://github.com/apache/doris/releases](https://github.com/apache/doris/releases)\\n\\n\\n\\n## Behavior changes\\n\\n1. Change float type output format to improve float type serialization performance.\\n\\n- https://github.com/apache/doris/pull/32049\\n\\n2. Change system table value functions active_queries(), workload_groups() to system tables. \\n\\n- https://github.com/apache/doris/pull/32314\\n\\n3. Disable show query/load profile stmt because there are not so many developers use it and the pipeline and pipelinex engine not support it. \\n\\n- https://github.com/apache/doris/pull/32467\\n\\n4. Upgrade arrow flight version to 15.0.2 to fix some bugs, so that please use ADBC 15.0.2 version to access Doris. \\n\\n- https://github.com/apache/doris/pull/32827.\\n\\n\\n\\n## Upgrade Problem\\n\\n1. BE will core when rolling pgrade problem from 2.0.x to 2.1.x.\\n\\n- https://github.com/apache/doris/pull/32672\\n\\n- https://github.com/apache/doris/pull/32444\\n\\n- https://github.com/apache/doris/pull/32162\\n\\n2. JDBC Catalog will have query errors when rolling grade rom 2.0.x to 2.1.x. \\n\\n- https://github.com/apache/doris/pull/32618\\n\\n\\n\\n## New Features\\n\\n1. Enable column auth by default.\\n\\n- https://github.com/apache/doris/pull/32659\\n\\n\\n2. Get correct cores for pipeline and pipelinex engine when running within docker or k8s. \\n\\n- https://github.com/apache/doris/pull/32370\\n\\n3. Support read parquet int96 type. \\n\\n- https://github.com/apache/doris/pull/32394\\n\\n4. Enable proxy protocol to support IP transparency. Using this protocol, IP transparency for load balancing can be achieved, so that after load balancing, Doris can still obtain the client\'s real IP and implement permission control such as whitelisting. \\n\\n- https://github.com/apache/doris/pull/32338/files\\n\\n5. Add workload group queue related columns for active_queries system table. Uses could use this system to monitor the workload queue usage. \\n\\n- https://github.com/apache/doris/pull/32259\\n\\n6. Add new system table backend_active_tasks to monitor the realtime query statics on every BE. \\n\\n- https://github.com/apache/doris/pull/31945\\n\\n7. Add ipv4 and ipv6 support for spark-doris connector. \\n\\n- https://github.com/apache/doris/pull/32240\\n\\n8. Add inverted index support for CCR. \\n\\n- https://github.com/apache/doris/pull/32101\\n\\n9. Support select experimental session variable. \\n\\n- https://github.com/apache/doris/pull/31837\\n\\n10. Support materialized view with bitmap_union(bitmap_from_array()) case. \\n\\n- https://github.com/apache/doris/pull/31962\\n\\n11. Support partition prune for `HIVE_DEFAULT_PARTITION`. \\n\\n- https://github.com/apache/doris/pull/31736\\n\\n12. Support function in set variable statement. \\n\\n- https://github.com/apache/doris/pull/32492\\n\\n13. Support arrow serialization for varint type. \\n\\n- https://github.com/apache/doris/pull/32809\\n\\n\\n\\n## Improvements\\n\\n1. Auto resume routine load when be restart or during upgrade. And keep the routine load stable. \\n\\n- https://github.com/apache/doris/pull/32239\\n\\n2. Routine Load: optimize allocate task to be algorithm for load balance. \\n\\n- https://github.com/apache/doris/pull/32021\\n\\n3. Spark Load: update spark version for spark load to resolve cve problem. \\n\\n- https://github.com/apache/doris/pull/30368\\n\\n4. Skip cooldown if the tablet is dropped. \\n\\n- https://github.com/apache/doris/pull/32079\\n\\n5. Support using workload group to manage routine load. \\n\\n- https://github.com/apache/doris/pull/31671\\n\\n6. [MTMV ]Improve the performance for query rewritting by materialized view. \\n\\n- https://github.com/apache/doris/pull/31886\\n\\n7. Reduce jvm heap memory consumed by profiles of BrokerLoadJob. \\n\\n- https://github.com/apache/doris/pull/31985\\n\\n8. Imporve the high QPS query by speed up PartitionPrunner. \\n\\n- https://github.com/apache/doris/pull/31970\\n\\n9. Reduce duplicated memory consumption for column name and column path for schema cache. \\n\\n- https://github.com/apache/doris/pull/31141\\n\\n10. Support more join types for query rewriting by materialized view such as INNER JOIN, LEFT OUTER JOIN, RIGHT OUTER JOIN, FULL OUTER JOIN, LEFT SEMI JOIN, RIGHT SEMI JOIN, LEFT ANTI JOIN, RIGHT ANTI JOIN.\\n\\n- https://github.com/apache/doris/pull/32909\\n\\n\\n\\n## Bug fixes\\n\\n\\n1. Do not push down topn-filter through right/full outer join if the first orderkey is nulls first. \\n\\n- https://github.com/apache/doris/pull/32633\\n\\n2. Fix memory leak in Java UDF.\\n\\n- https://github.com/apache/doris/pull/32630\\n\\n3. If some odbc tables use the same resource, and restore not all odbc tables, it will not retain the resource.\\nand check some conf for backup/restore \\n\\n- https://github.com/apache/doris/pull/31989\\n\\n4. Fold constant will core for variant type. \\n\\n- https://github.com/apache/doris/pull/32265\\n\\n5. Routine load will pause when transaction fail in some cases. \\n\\n- https://github.com/apache/doris/pull/32638\\n\\n6. the result of left semi join with empty right side should be false instead of null. \\n\\n- https://github.com/apache/doris/pull/32477\\n\\n7. Fix core when build inverted index for a new column with no data. \\n\\n- https://github.com/apache/doris/pull/32669\\n\\n8. Fix be core caused by null-safe-equal join. \\n\\n- https://github.com/apache/doris/pull/32623\\n\\n9. Partial update: fix data correctness risk when load delete sign data into a table with sequence col. \\n\\n- https://github.com/apache/doris/pull/32574\\n\\n10. Select outfile: Fix the column type mapping in the orc/parquet file format. \\n\\n- https://github.com/apache/doris/pull/32281\\n\\n11. Fix BE core during restore stage. \\n\\n- https://github.com/apache/doris/pull/32489\\n\\n12. Use array_agg func after other agg func like count, sum, may make be core. \\n\\n- https://github.com/apache/doris/pull/32387\\n\\n13. Variant type should always nullable or there will some bugs. \\n\\n- https://github.com/apache/doris/pull/32248\\n\\n14. Fix the bug of handling empty blocks in schema change. \\n\\n- https://github.com/apache/doris/pull/32396\\n\\n15. Fix BE will core when use json_length() in some cases. \\n\\n- https://github.com/apache/doris/pull/32145\\n\\n16. Fix error when query iceberg table using date cast predicate \\n\\n- https://github.com/apache/doris/pull/32194\\n\\n17. Fix some bugs when build inverted index for variant type. \\n\\n- https://github.com/apache/doris/pull/31992\\n\\n18. Wrong result of two or more map_agg functions in query. \\n\\n- https://github.com/apache/doris/pull/31928\\n\\n19. Fix wrong result of money_format function. \\n\\n- https://github.com/apache/doris/pull/31883\\n\\n20. Fix connection hang after too many connections. \\n\\n- https://github.com/apache/doris/pull/31594"},{"id":"/release-note-2.0.7","metadata":{"permalink":"/blog/release-note-2.0.7","source":"@site/blog/release-note-2.0.7.md","title":"Apache Doris 2.0.7 just released","description":"Thanks to our community users and developers, about 80 improvements and bug fixes have been made in Doris 2.0.7 version.","date":"2024-03-26T00:00:00.000Z","tags":[{"inline":true,"label":"Release Notes","permalink":"/blog/tags/release-notes"}],"hasTruncateMarker":false,"authors":[{"name":"Apache Doris","key":null,"page":null}],"frontMatter":{"title":"Apache Doris 2.0.7 just released","description":"Thanks to our community users and developers, about 80 improvements and bug fixes have been made in Doris 2.0.7 version.","date":"2024-03-26","author":"Apache Doris","tags":["Release Notes"],"image":"/images/2.0.7.png"},"unlisted":false,"prevItem":{"title":"Apache Doris 2.1.1 just released","permalink":"/blog/release-note-2.1.1"},"nextItem":{"title":"Variant in Apache Doris 2.1.0: a new data type 8 times faster than JSON for semi-structured data analysis","permalink":"/blog/variant-in-apache-doris-2.1"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n  http://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n\\nThanks to our community users and developers, about 80 improvements and bug fixes have been made in Doris 2.0.7 version.\\n\\n**Quick Download:** [https://doris.apache.org/download/](https://doris.apache.org/download/)\\n\\n**GitHub\uFF1A** [https://github.com/apache/doris/releases](https://github.com/apache/doris/releases)\\n\\n\\n## Behavior changes\\n\\n- `round` function defaults to rounding normally as MySQL, eg. round(5/2) return 3 instead of 2.\\n  \\n  - https://github.com/apache/doris/pull/31583\\n\\n- `round` datetime with scale from string literal as MySQL, eg. round \'2023-10-12 14:31:49.666\' to \'2023-10-12 14:31:50\' .\\n\\n  - https://github.com/apache/doris/pull/27965 \\n\\n\\n## New features\\n- Support make miss slot as null alias when converting outer join to anti join to speed up query\\n\\n  - https://github.com/apache/doris/pull/31854\\n\\n- Enable proxy protocol to support IP transparency for Nginx and HAProxy.\\n\\n  - https://github.com/apache/doris/pull/32338\\n\\n\\n## Improvements\\n\\n- Add DEFAULT_ENCRYPTION column in `information_schema` table and add `processlist` table for better compatibility for BI tools\\n\\n- Automatically test connectivity by default when creating a JDBC Catalog.\\n\\n- Enhance auto resume to keep routine load stable\\n\\n- Use lowercase by default for Chinese tokenizer in inverted index\\n\\n- Add error msg if exceeded maximum default value in repeat function\\n\\n- Skip hidden file and dir in Hive table\\n\\n- Reduce file meta cache size and disable cache for some cases to avoid OOM\\n\\n- Reduce jvm heap memory consumed by profiles of BrokerLoadJob\\n\\n- Remove sort which is under table sink to speed up query like `INSERT INTO t1 SELECT * FROM t2 ORDER BY k`.\\n\\nSee the complete list of improvements and bug fixes on [github](https://github.com/apache/doris/compare/2.0.6...2.0.7) .\\n\\n\\n## 4 Credits\\n\\nThanks all who contribute to this release:\\n\\n924060929,airborne12,amorynan,ByteYue,dataroaring,deardeng,feiniaofeiafei,felixwluo,freemandealer,gavinchou,hello-stephen,HHoflittlefish777,jacktengg,jackwener,jeffreys-cat,Jibing-Li,KassieZ,LiBinfeng-01,luwei16,morningman,mrhhsg,Mryange,nextdreamblue,platoneko,qidaye,rohitrs1983,seawinde,shuke987,starocean999,SWJTU-ZhangLei,w41ter,wsjz,wuwenchi,xiaokang,XieJiann,XuJianxu,yujun777,Yulei-Yang,zhangstar333,zhiqiang-hhhh,zy-kkk,zzzxl1993"},{"id":"/variant-in-apache-doris-2.1","metadata":{"permalink":"/blog/variant-in-apache-doris-2.1","source":"@site/blog/variant-in-apache-doris-2.1.md","title":"Variant in Apache Doris 2.1.0: a new data type 8 times faster than JSON for semi-structured data analysis","description":"Doris 2.1.0 provides a new data type: Variant, for semi-structured data analysis, which enables 8 times faster query performance than JSON with one-third storage space.","date":"2024-03-26T00:00:00.000Z","tags":[{"inline":true,"label":"Tech Sharing","permalink":"/blog/tags/tech-sharing"}],"hasTruncateMarker":false,"authors":[{"name":"velodb.io \xb7 VeloDB Engineering Team","key":null,"page":null}],"frontMatter":{"title":"Variant in Apache Doris 2.1.0: a new data type 8 times faster than JSON for semi-structured data analysis","description":"Doris 2.1.0 provides a new data type: Variant, for semi-structured data analysis, which enables 8 times faster query performance than JSON with one-third storage space.","date":"2024-03-26","author":"velodb.io \xb7 VeloDB Engineering Team","externalLink":"https://www.velodb.io/blog/224","tags":["Tech Sharing"],"image":"/images/variant-in-apache-doris-2.1.png"},"unlisted":false,"prevItem":{"title":"Apache Doris 2.0.7 just released","permalink":"/blog/release-note-2.0.7"},"nextItem":{"title":"Another big leap: Apache Doris 2.1.0 is released","permalink":"/blog/release-note-2.1.0"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n  http://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\nSemi-structured data is data arranged in flexible formats. Unlike structured data, it does not require data users to pre-define the table schema for it, so it provides convenience for data storage and analysis. Common forms of semi-structured data include XML, JSON, and log files. They are widely seen in the following industry scenarios:\\n\\n- **E-commerce** platforms store user reviews of products as semi-structured data for sentiment analysis and user behavior pattern mining.\\n\\n- **Telecommunication** use cases often require schemaless support for their network data and complicated nested JSON data.\\n\\n- **Mobile applications** keep records of user behavior in the form of semi-structured data, because after new features are introduced, the user behavior attributes can change. A non-fixed schema can adapt to these changes easily and save the trouble of frequent manual modification. \\n\\n- **Internet of Vehicles** (IoV) and **Internet of Things** (IoT) platforms receive real-time data from vehicle sensors, such as speed, location, and fuel consumption, based on which they perform real-time monitoring, fault alerting, and route planning. Such data is also stored as semi-structured data.\\n\\nAs an open-source real-time data warehouse, [Apache Doris](https://doris.apache.org) provides semi-structured data processing capabilities, and the newly-released [version 2.1.0](https://doris.apache.org/blog/release-note-2.1.0) makes a stride in this direction. Before V2.1, Apache Doris stores semi-structured data as JSON files. However, during query execution, the real-time parsing of JSON data leads to high CPU and I/O consumption in addition to high query latency, especially when the dataset is huge and complicated. Moreover, the lack of a pre-defined schema means there is no handle for query optimization. \\n\\n\\n## A newly-added data type: Variant\\n\\n:::tip\\nTo help you quickly learn and use Variant data type, we provide **[a hands-on demo](https://www.youtube.com/watch?v=FVfsnkZUBsU) **\\n:::\\n\\n\\nIn Apache Doris 2.1.0, we have introduced a new data type: [Variant](https://doris.apache.org/docs/sql-manual/sql-reference/Data-Types/VARIANT). Fields of the Variant data type can accommodate integers, strings, boolean values, and any combination of them. With Variant, you don\'t have to define the specific columns in the table schema in advance.\\n\\nThe Variant data type is well-suited to handle nested structures, which tend to change dynamically. Upon data writing, the Variant type automatically infers column information based on the data and its structure in the columns, and then merges it into the existing table schema. It stores the JSON keys and their corresponding values as dynamic sub-columns. \\n\\nMeanwhile, you can include both Variant columns and static columns of pre-defined data types in the same table. This Schema-on-Write method provides greater flexibility in storage and queries. Powered by the columnar storage, vectorized execution engine, and query optimizer of Doris, the Variant type delivers high efficiency in queries and storage. \\n\\nCompared to the JSON type, storage data in the Variant type can save up to 65% of disk space, and increase query speed by 8 times. (See details later in this post)\\n\\n## Usage guide\\n\\nCreate table: syntax keyword `variant`\\n\\n```sql\\n-- No index\\nCREATE TABLE IF NOT EXISTS ${table_name} (\\n    k BIGINT,\\n    v VARIANT\\n)\\ntable_properties;\\n\\n-- Create index for the v column, specify the parser\\nCREATE TABLE IF NOT EXISTS ${table_name} (\\n    k BIGINT,\\n    v VARIANT,\\n    INDEX idx_var(v) USING INVERTED [PROPERTIES(\\"parser\\" = \\"english|unicode|chinese\\")] [COMMENT \'your comment\']\\n)\\ntable_properties;\\n\\n-- Create Bloom Filter for the v column\\nCREATE TABLE IF NOT EXISTS ${table_name} (\\n    k BIGINT,\\n    v VARIANT\\n)\\n...\\nproperties(\\"replication_num\\" = \\"1\\", \\"bloom_filter_columns\\" = \\"v\\");\\n```\\n\\nQuery: access sub-column via `[]`. The sub-columns are also of the Variant type.\\n\\n```sql\\nSELECT v[\\"properties\\"][\\"title\\"] from ${table_name}\\n```\\n\\nNow, let\'s show you how to create a table containing the Variant data type and conduct data ingestion and queries to it. The dataset is Github Events records. This is one of the formatted records:\\n\\n```JSON\\n{\\n  \\"id\\": \\"14186154924\\",\\n  \\"type\\": \\"PushEvent\\",\\n  \\"actor\\": {\\n    \\"id\\": 282080,\\n    \\"login\\": \\"brianchandotcom\\",\\n    \\"display_login\\": \\"brianchandotcom\\",\\n    \\"gravatar_id\\": \\"\\",\\n    \\"url\\": \\"https://api.github.com/users/brianchandotcom\\",\\n    \\"avatar_url\\": \\"https://avatars.githubusercontent.com/u/282080?\\"\\n  },\\n  \\"repo\\": {\\n    \\"id\\": 1920851,\\n    \\"name\\": \\"brianchandotcom/liferay-portal\\",\\n    \\"url\\": \\"https://api.github.com/repos/brianchandotcom/liferay-portal\\"\\n  },\\n  \\"payload\\": {\\n    \\"push_id\\": 6027092734,\\n    \\"size\\": 4,\\n    \\"distinct_size\\": 4,\\n    \\"ref\\": \\"refs/heads/master\\",\\n    \\"head\\": \\"91edd3c8c98c214155191feb852831ec535580ba\\",\\n    \\"before\\": \\"abb58cc0db673a0bd5190000d2ff9c53bb51d04d\\",\\n    \\"commits\\": [\\"\\"]\\n  },\\n  \\"public\\": true,\\n  \\"created_at\\": \\"2020-11-13T18:00:00Z\\"\\n}\\n```\\n\\n### 01  Create table\\n\\n- Create 3 columns of the Variant type: `actor`, `repo` and `payload`\\n\\n- Meanwhile, create inverted index for the `payload` column: `idx_payload`\\n\\n- `USING INVERTED` specifies the index as inverted index, which accelerates conditional filtering on sub-columns\\n\\n```sql\\nCREATE DATABASE test_variant;\\n\\nUSE test_variant;\\n\\nCREATE TABLE IF NOT EXISTS github_events (\\n    id BIGINT NOT NULL,\\n    type VARCHAR(30) NULL,\\n    actor VARIANT NULL,\\n    repo VARIANT NULL,\\n    payload VARIANT NULL,\\n    public BOOLEAN NULL,\\n    created_at DATETIME NULL,\\n    INDEX idx_payload (`payload`) USING INVERTED PROPERTIES(\\"parser\\" = \\"english\\") COMMENT \'inverted index for payload\'\\n)\\nDUPLICATE KEY(`id`)\\nDISTRIBUTED BY HASH(id) BUCKETS 10\\nproperties(\\"replication_num\\" = \\"1\\");\\n```\\n\\n> Note: If the `Payload` column has too many sub-columns, creating indexes on it may lead to an excessive number of index columns and decrease data writing performance. If the data analysis only involves equivalence queries, it is advisable to build Bloom Filter index on the Variant columns. This can bring better performance than inverted index. For a single Variant column, if the parsing properties are the same but you have multiple parsing requirements, you can replicate the column and specify various indexes for each of them.\\n\\n### 02  Ingest data by Stream Load\\n\\nLoad the `gh_2022-11-07-3.json` file, which is Github Events records of an hour. One formatted row of it looks like this: \\n\\n```JSON\\nwget https://qa-build.oss-cn-beijing.aliyuncs.com/regression/variant/gh_2022-11-07-3.json\\n\\ncurl --location-trusted -u root:  -T gh_2022-11-07-3.json -H \\"read_json_by_line:true\\" -H \\"format:json\\"  http://127.0.0.1:18148/api/test_variant/github_events/_strea\\nm_load\\n\\n{\\n    \\"TxnId\\": 2,\\n    \\"Label\\": \\"086fd46a-20e6-4487-becc-9b6ca80281bf\\",\\n    \\"Comment\\": \\"\\",\\n    \\"TwoPhaseCommit\\": \\"false\\",\\n    \\"Status\\": \\"Success\\",\\n    \\"Message\\": \\"OK\\",\\n    \\"NumberTotalRows\\": 139325,\\n    \\"NumberLoadedRows\\": 139325,\\n    \\"NumberFilteredRows\\": 0,\\n    \\"NumberUnselectedRows\\": 0,\\n    \\"LoadBytes\\": 633782875,\\n    \\"LoadTimeMs\\": 7870,\\n    \\"BeginTxnTimeMs\\": 19,\\n    \\"StreamLoadPutTimeMs\\": 162,\\n    \\"ReadDataTimeMs\\": 2416,\\n    \\"WriteDataTimeMs\\": 7634,\\n    \\"CommitAndPublishTimeMs\\": 55\\n}\\n```\\n\\nCheck if the data loading succeeds:\\n\\n```sql\\n-- Check the number of rows\\nmysql> select count() from github_events;\\n+----------+\\n| count(*) |\\n+----------+\\n|   139325 |\\n+----------+\\n1 row in set (0.25 sec)\\n\\n-- View a random row\\nmysql> select * from github_events limit 1;\\n+-------------+-----------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------+---------------------+\\n| id          | type      | actor                                                                                                                                                                                                                       | repo                                                                                                                                                     | payload                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | public | created_at          |\\n+-------------+-----------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------+---------------------+\\n| 25061821748 | PushEvent | {\\"gravatar_id\\":\\"\\",\\"display_login\\":\\"jfrog-pipelie-intg\\",\\"url\\":\\"https://api.github.com/users/jfrog-pipelie-intg\\",\\"id\\":98024358,\\"login\\":\\"jfrog-pipelie-intg\\",\\"avatar_url\\":\\"https://avatars.githubusercontent.com/u/98024358?\\"} | {\\"url\\":\\"https://api.github.com/repos/jfrog-pipelie-intg/jfinte2e_1667789956723_16\\",\\"id\\":562683829,\\"name\\":\\"jfrog-pipelie-intg/jfinte2e_1667789956723_16\\"} | {\\"commits\\":[{\\"sha\\":\\"334433de436baa198024ef9f55f0647721bcd750\\",\\"author\\":{\\"email\\":\\"98024358+jfrog-pipelie-intg@users.noreply.github.com\\",\\"name\\":\\"jfrog-pipelie-intg\\"},\\"message\\":\\"commit message 10238493157623136117\\",\\"distinct\\":true,\\"url\\":\\"https://api.github.com/repos/jfrog-pipelie-intg/jfinte2e_1667789956723_16/commits/334433de436baa198024ef9f55f0647721bcd750\\"}],\\"before\\":\\"f84a26792f44d54305ddd41b7e3a79d25b1a9568\\",\\"head\\":\\"334433de436baa198024ef9f55f0647721bcd750\\",\\"size\\":1,\\"push_id\\":11572649828,\\"ref\\":\\"refs/heads/test-notification-sent-branch-10238493157623136113\\",\\"distinct_size\\":1} |      1 | 2022-11-07 11:00:00 |\\n+-------------+-----------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------+---------------------+\\n1 row in set (0.23 sec)\\n```\\n\\nView schema information via `desc`. The sub-columns will be automatically extended in the storage layer, and the data types of the sub-columns are automatically inferred.\\n\\n```sql\\n-- No display of extended columns\\nmysql> desc github_events;\\n+------------+-------------+------+-------+---------+-------+\\n| Field      | Type        | Null | Key   | Default | Extra |\\n+------------+-------------+------+-------+---------+-------+\\n| id         | BIGINT      | No   | true  | NULL    |       |\\n| type       | VARCHAR(30) | Yes  | false | NULL    | NONE  |\\n| actor      | VARIANT     | Yes  | false | NULL    | NONE  |\\n| repo       | VARIANT     | Yes  | false | NULL    | NONE  |\\n| payload    | VARIANT     | Yes  | false | NULL    | NONE  |\\n| public     | BOOLEAN     | Yes  | false | NULL    | NONE  |\\n| created_at | DATETIME    | Yes  | false | NULL    | NONE  |\\n+------------+-------------+------+-------+---------+-------+\\n7 rows in set (0.01 sec)\\n\\n-- Displaying extended columns of Variant columns\\nmysql> set describe_extend_variant_column = true;\\nQuery OK, 0 rows affected (0.01 sec)\\n\\nmysql> desc github_events;\\n+------------------------------------------------------------+------------+------+-------+---------+-------+\\n| Field                                                      | Type       | Null | Key   | Default | Extra |\\n+------------------------------------------------------------+------------+------+-------+---------+-------+\\n| id                                                         | BIGINT     | No   | true  | NULL    |       |\\n| type                                                       | VARCHAR(*) | Yes  | false | NULL    | NONE  |\\n| actor                                                      | VARIANT    | Yes  | false | NULL    | NONE  |\\n| actor.avatar_url                                           | TEXT       | Yes  | false | NULL    | NONE  |\\n| actor.display_login                                        | TEXT       | Yes  | false | NULL    | NONE  |\\n| actor.id                                                   | INT        | Yes  | false | NULL    | NONE  |\\n| actor.login                                                | TEXT       | Yes  | false | NULL    | NONE  |\\n| actor.url                                                  | TEXT       | Yes  | false | NULL    | NONE  |\\n| created_at                                                 | DATETIME   | Yes  | false | NULL    | NONE  |\\n| payload                                                    | VARIANT    | Yes  | false | NULL    | NONE  |\\n| payload.action                                             | TEXT       | Yes  | false | NULL    | NONE  |\\n| payload.before                                             | TEXT       | Yes  | false | NULL    | NONE  |\\n| payload.comment.author_association                         | TEXT       | Yes  | false | NULL    | NONE  |\\n| payload.comment.body                                       | TEXT       | Yes  | false | NULL    | NONE  |\\n....\\n+------------------------------------------------------------+------------+------+-------+---------+-------+\\n406 rows in set (0.07 sec)\\n```\\n\\nWith the `desc` statement, you can specify which partition you want to check the schema of: \\n\\n```sql\\nDESCRIBE ${table_name} PARTITION ($partition_name);\\n```\\n\\n### 03  Query\\n\\n> Note: When filtering and aggregating sub-columns, an additional CAST operation is required to ensure data type consistency. This is because the storage types may not be fixed, and the `CAST` expression in SQL can unify the data types. For example, `SELECT * FROM tbl WHERE CAST(var[\'title\'] AS TEXT) MATCH \'hello world\'`.\\n\\n**The following are simple examples of queries on Variant columns**\\n\\n1. Retrieve the Top 5 repositories with the most Stars from `github_events`.\\n\\n```sql\\nmysql> SELECT\\n    ->     cast(repo[\\"name\\"] as text) as repo_name, count() AS stars\\n    -> FROM github_events\\n    -> WHERE type = \'WatchEvent\'\\n    -> GROUP BY repo_name\\n    -> ORDER BY stars DESC LIMIT 5;\\n+--------------------------+-------+\\n| repo_name                | stars |\\n+--------------------------+-------+\\n| aplus-framework/app      |    78 |\\n| lensterxyz/lenster       |    77 |\\n| aplus-framework/database |    46 |\\n| stashapp/stash           |    42 |\\n| aplus-framework/image    |    34 |\\n+--------------------------+-------+\\n5 rows in set (0.03 sec)\\n```\\n\\n2. Count the number of events containing the keyword `doris`.\\n\\n```sql\\n-- implicit cast `payload[\'comment\'][\'body\']` to string type\\nmysql> SELECT\\n    ->     count() FROM github_events\\n    ->     WHERE payload[\'comment\'][\'body\'] MATCH \'doris\';\\n+---------+\\n| count() |\\n+---------+\\n|       3 |\\n+---------+\\n1 row in set (0.04 sec)\\n```\\n\\n3. Check the ID of the issue that has the most comments and the repository it belongs to.\\n\\n```sql\\nmysql> SELECT \\n    ->   cast(repo[\\"name\\"] as string) as repo_name, \\n    ->   cast(payload[\\"issue\\"][\\"number\\"] as int) as issue_number, \\n    ->   count() AS comments, \\n    ->   count(\\n    ->     distinct cast(actor[\\"login\\"] as string)\\n    ->   ) AS authors \\n    -> FROM  github_events \\n    -> WHERE type = \'IssueCommentEvent\' AND (cast(payload[\\"action\\"] as string) = \'created\') AND (cast(payload[\\"issue\\"][\\"number\\"] as int) > 10) \\n    -> GROUP BY repo_name, issue_number \\n    -> HAVING authors >= 4\\n    -> ORDER BY comments DESC, repo_name\\n    -> LIMIT 50;\\n+--------------------------------------+--------------+----------+---------+\\n| repo_name                            | issue_number | comments | authors |\\n+--------------------------------------+--------------+----------+---------+\\n| facebook/react-native                |        35228 |        5 |       4 |\\n| swsnu/swppfall2022-team4             |           27 |        5 |       4 |\\n| belgattitude/nextjs-monorepo-example |         2865 |        4 |       4 |\\n+--------------------------------------+--------------+----------+---------+\\n3 rows in set (0.03 sec)\\n```\\n\\n### 04  Notes\\n\\nBased on our test results, it is safe to say that there is no efficiency disparity between Variant dynamic columns and pre-defined static columns. However, in log data processing, when users need to add fields to the table, such as container labels in Kubernetes, JSON parsing and type inference during data writing incur additional overhead.\\n\\nTo strike a balance between flexibility and efficiency for the Variant data type, we recommend keeping the number of columns below 1000. A small number of columns will reduce overheads caused by data parsing and type inference and thus increase data writing performance.\\n\\nIt is also advisable to ensure field type consistency whenever possible. This is because Doris automatically performs compatible type conversions to unify fields of different data types. If it cannot find a compatible type, it will convert the data to the JSONB type, which may result in degraded performance compared to the int or text type.\\n\\n## Variant VS JSON\\n\\nTo see how the newly added Variant type impacts data storage and queries, we did comparison tests on pre-defined static columns, Variant columns, and JSON columns with ClickBench.\\n\\n**Test environment**: 16 core, 64GB, AWS EC2 instance, 1TB ESSD\\n\\n**Test result**:\\n\\n### 01 Storage space\\n\\nAs the results show, storing data as Variant columns takes up a similar storage space to storing it as pre-defined static columns. Compared with the JSON type, the Variant type requires 65% less space. **In other words, the Variant type only takes up one-third of the storage space that JSON does. The difference will be even more notable with low-cardinality data because of columnar storage.**\\n\\n\\n![Storage space](/images/storage-space.png)\\n\\n### 02 Query performance\\n\\nWe tested with 43[ Clickbench](https://github.com/ClickHouse/ClickBench/blob/main/selectdb/queries.sql) SQL queries. Queries on the Variant columns are about 10% slower than those on pre-defined static columns, and **8 times faster than those on** **JSON** **columns**. (For I/O reasons, most cold runs on JSONB data failed with OOM.) \\n\\n\\n![Query Performance](/images/query-performance.png)\\n\\n## Design & implementation of Variant\\n\\n### 01  Data writing & type inference\\n\\nIn Apache Doris, this is a normal writing process: data sorting, merging, and Segment file generation in the Memtable. Variant writing works similarly. It involves type inference and data merging of the same JSON keys within the Memtable, resulting in the creation of a prefix tree. The tree keeps the type and column information of every JSON field, and merges all type information of the same column into the least common type, generates columns, encodes them into the Doris storage formats, and appends them to the segment.\\n\\nEach Segment file not only contains data after type encoding and compaction, but also includes the metadata of dynamically generated columns. Such design ensures data integrity and queryability while also improving storage efficiency. **By type inference and merging in the memory, the Variant type largely reduces disk space usage compared to traditional raw text storage**. \\n\\n\\n![Data Writing & Type inferece](/images/data-writing-and-type-inference.png)\\n\\n### 02 Column change (column adding or column type changes)\\n\\nDuring the writing process, all metadata and data of the leaf nodes in the prefix tree will be appended to the Segment file, and the metadata of the Rowsets will be merged. Here is an example of the merging process:\\n\\n![Column change (column adding or column type changes)](/images/column-change.png)\\n\\nIn the end, the Rowset will use the `Least Common Column Schema` as the metadata after data merging. (Least common column schema is a schema with the most sub-columns and the sub-column type being the least common type.) This allows for dynamic column extension and type changes. \\n\\nBased on this mechanism, the stored schema for Variant can be considered data-driven. It offers greater flexibility compared to the Schema Change process in Doris. The diagram below illustrates the directions for type changes (type changes can only be performed in the direction indicated by the arrows, with JSONB being the common type for all types):\\n\\n\\n![Column change (column adding or column type changes)](/images/column-change2.png)\\n\\n### 03 Index & query acceleration\\n\\nIn Variant, the leaf nodes are stored in a columnar format in the Segment file, which is exactly the same as the storage format for static pre-defined columns. Thus, queries on Variant columns can also be accelerated by dictionary encoding, vectorization, and indexes (ZoneMap, inverted index, BloomFilter, etc.). Since the same column might be of different types in different files, users need to specify a type as the hint during query execution. Here is an example query: \\n\\n```sql\\n -- var[\'title\'] is to access the \'title\' sub-column of var, which is a Variant column. If there is inverted index for var, the queries will be accelerated by inverted index.\\n SELECT * FROM tbl where CAST(var[\'titile\'] as text) MATCH \\"hello world\\"\\n \\n -- If there is Bloom Filter for var, equivalence queries will be accelerated by Bloom Filter.\\n SELECT * FROM tbl where CAST(var[\'id\'] as bigint) = 1010101\\n```\\n\\nPredicates will be pushed down to the storage layer (Segment), where the storage type is checked against the target type of the CAST operation. If the types match, a more efficient predicate filtering mechanism will be utilized. This approach reduces unnecessary data reading and conversion, thus improving query performance.\\n\\n### 04  Storage optimization for sparse columns\\n\\nExamples of sparse JSON columns: \\n\\n```sql\\n{\\"a\\":[1], \\"b\\":2, \\"c\\":3, \\"x_1\\" : 1\uFF0C\\"x_2\\": \\"3\\"}\\n{\\"a\\":1, \\"b\\":2, \\"c\\":3, \\"x_1\\" : 1\uFF0C\\"x_2\\": \\"3\\"}\\n{\\"a\\":4, \\"b\\":5, \\"c\\":6, \\"x_3\\" : 1\uFF0C\\"x_4\\": \\"3\\"}\\n{\\"a\\":7, \\"b\\":8, \\"c\\":9, \\"x_5\\" : 1\uFF0C\\"x_6\\": \\"3\\"}\\n...\\n```\\n\\nThe `a, b, c` columns are dense. They are included in almost every row. While the ` x_\uFF1F` columns are sparse. Only a few of them are not null. If the system stores every column in a columnar way, it will suffer huge storage pressure and exploding meta. \\n\\nTo solve this, Doris detects the sparsity of columns based on the percentage of null values upon data ingestion. The highly sparse columns (with a high proportion of null values) will be packed into JSONB encoding and stored in a separate column. \\n\\n\\n![ Storage optimization for sparse columns](/images/storage-optimization-for-sparse-columns.png)\\n\\nSuch optimization for storing sparse columns will relieve pressure on meta and data compaction and increase flexibility. \\n\\nQueries on the sparse columns are implemented in exactly the same way as those on other columns.\\n\\n## Use case\\n\\nGuanceDB, an observability platform, used an Elasticsearch-based solution for storing logs and user behavior data. However, Elasticsearch has inadequate schemaless support, so it is inefficient in processing large amounts of user-defined fields. Under the Dynamic Mapping mechanism in Elasticsearch, frequent field type conflicts led to data losses and required lots of human intervention. Meanwhile, the writing process in Elasticsearch was resource-intensive and the performance in massive data aggregation was less than ideal.\\n\\nFor a data architectural upgrade, GuanceDB works with [VeloDB](https://www.velodb.io/) and builds an Apache Doris-based observability solution. They utilize the Variant data type to realize partition-based schema change, which is more flexible and efficient. In addition, Doris imposes no upper limit on the number of columns, meaning that it can better accommodate schema-free data. \\n\\nThe Doris-based solution also delivers lower CPU usage in data writing and higher speed in complicated data aggregation (accelerated by inverted index and query optimization techniques). After the upgrade, GuanceDB **decreased their machine costs by 70% and doubled their overall query speed**, with an over 4-time performance increase in simple queries. \\n\\n## Conclusion\\n\\nThe Variant data type has stood the test of many users before the official release of Apache Doris 2.1.0. It is production-available now. In the future, we plan to realize more lightweight changes for Variant to facilitate data modeling. \\n\\nFor more information about Variant and guides on how to build a semi-structured data analytics solution for your case, come talk to the [Apache Doris developer team](https://join.slack.com/t/apachedoriscommunity/shared_invite/zt-2unfw3a3q-MtjGX4pAd8bCGC1UV0sKcw)."},{"id":"/release-note-2.1.0","metadata":{"permalink":"/blog/release-note-2.1.0","source":"@site/blog/release-note-2.1.0.md","title":"Another big leap: Apache Doris 2.1.0 is released","description":"We appreciate the 237 contributors who made nearly 6000 commits in total to the Apache Doris project, and the nearly 100 enterprise users who provided valuable feedback.","date":"2024-03-12T00:00:00.000Z","tags":[{"inline":true,"label":"Release Notes","permalink":"/blog/tags/release-notes"}],"hasTruncateMarker":false,"authors":[{"name":"Apache Doris","key":null,"page":null}],"frontMatter":{"title":"Another big leap: Apache Doris 2.1.0 is released","description":"We appreciate the 237 contributors who made nearly 6000 commits in total to the Apache Doris project, and the nearly 100 enterprise users who provided valuable feedback.","date":"2024-03-12","author":"Apache Doris","tags":["Release Notes"],"image":"/images/2.1.0.png"},"unlisted":false,"prevItem":{"title":"Variant in Apache Doris 2.1.0: a new data type 8 times faster than JSON for semi-structured data analysis","permalink":"/blog/variant-in-apache-doris-2.1"},"nextItem":{"title":"Breaking down data silos with a unified data warehouse: an Apache Doris-based CDP","permalink":"/blog/breaking-down-data-silos-with-an-apache-doris-based-cdp"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n  http://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\nDear Apache Doris community, we are thrilled to announce the advent of Apache Doris 2.1.0. In this version, you can expect:\\n\\n- **Higher out-of-the-box query performance**: 100% faster speed proven by TPC-DS 1TB benchmark tests.\\n\\n- **Improved data lake analytics capabilities**: 4~6 times faster than Trino and Spark, compatibility with various SQL dialects for smooth migration, read/write interface based on Arrow Flight for 100 times faster data transfer.\\n\\n- **Solid support for semi-structured data analysis**: a newly-added Variant data type, support for more IP types, and a more comprehensive suite of analytic functions.\\n\\n- **Materialized view with multiple tables**: a new feature to accelerate multi-table joins, allowing transparent rewriting, auto refresh, materialized views of external tables, and direct query.\\n\\n- **Enhanced real-time writing efficiency**: faster data writing at scale powered by AUTO_INCREMENT column, AUTO PARTITION, forward placement of MemTable, and Group Commit. \\n\\n- **Better workload management**: optimizations of the Workload Group mechanism for higher performance stability and the display of SQL resource consumption in the runtime.\\n\\nWe appreciate the 237 contributors who made nearly 6000 commits in total to the Apache Doris project, and the nearly 100 enterprise users who provided valuable feedback. We will keep aiming for the stars with our agile release planning, and we appreciate your feedback in the [Apache Doris developer and user community](https://join.slack.com/t/apachedoriscommunity/shared_invite/zt-2unfw3a3q-MtjGX4pAd8bCGC1UV0sKcw). \\n\\n\\n**Download from GitHub**: https://github.com/apache/doris/releases\\n\\n**Download from website**: https://doris.apache.org/download\\n\\n\\n\\n## Higher performance\\n\\nApache Doris V2.1 makes a big leap in out-of-the-box query performance. It can deliver high performance even for complicated SQL queries without any fine-tuning. TPC-DS 1TB benchmark tests with 1 Frontend and 3 Backends (48C, 192G each) show that:\\n\\n- The total query execution time of V2.1.0 is 245.7s, **up 100%** from the 489.6s of V2.0.5;\\n\\n- V2.1 is more than twice as fast as V2.0.5 on one-third of the total 99 SQL queries, and outperforms V2.0.5 on over 80 of the SQL queries; \\n- V2.1 delivers better performance in data filtering, sorting, aggregation, multi-table joins, sub-queries, and window function computation.\\n\\n\\n![2.1-Doris-TPC-DS-higher-performance](/images/2.1-Doris-TPC-DS-best-performance.png)\\n\\nMeanwhile, we have compared Doris V2.1.0 against many other OLAP systems with the same hardware environment under various data sizes. **Recurring results show that Doris is undoubtedly far ahead**.\\n\\n### Smarter optimizer\\n\\nIn our last big release, we introduced a new query optimizer that enables fast performance for most use cases without any manual fine-tuning. Now, the V2.1 query optimizer is an upgrade on that basis. It comes with:\\n\\n- **More solid infrastructure**: We have improved the statistics-based inference and the cost model that underpin the query optimizer, so it can collect statistical information from a wider range to undertake more complicated optimization tasks.\\n\\n- **Extended optimization rules**: Absorbing feedback from our many actual use cases, we have improved many frequently used rules (operator pushdown, etc.) and introduced new rules to fit in more scenarios.\\n\\n- **Enhanced enumeration framework**: Building on Cascades and DPhyper, the V2.1 query optimizer has a clearer enumeration strategy that achieves a better balance between quality and efficiency. For example, we have dialed up the default limit of query plans in the enumeration table from 5 to 8, and then we have sharpened the DPhyper enumeration capabilities to produce better query plans.\\n\\n### Better heuristic optimization\\n\\nIn large-scale data analytics or data lake scenarios, it is always challenging and time-consuming to collect statistical information to provide references for query plans. For that, the V2.1 query optimizer, leveraging a combination of heuristic technologies, is able to generate **high-quality query plans without statistical reference**. Meanwhile, the RuntimeFilter is part of the trick. It is now more self-adaptive. It can self-adjust the predicates in expressions during execution, so it can enable higher performance without statistical information.\\n\\n### Parallel Adaptive Scan\\n\\nA complex data query will involve large sums of data scanning, during which the scan I/O can be the bottleneck for query execution speed. That\'s why we have Parallel Scan, which means one scan thread can read multiple tablets (buckets). However, that is highly dependent on the bucket number you set for data partitioning in the first place. If the user has set an inappropriate number of buckets, the scan thread will not be able to work parallelly. \\n\\nThat\'s why we have adopted Parallel Adaptive Scan in Doris V2.1. What happens is that the tablets are pooled so the scanning process can be divided into a flexible number of threads based on the total number of rows. (The upper limit is 48 threads.) In this way, users no longer have to worry that their query speed might be dragged down by unreasonable bucket numbers.\\n\\n\\n![Parallel Adaptive Scan](/images/2.1-doris-parallel-adaptive-scan.png)\\n\\nIn 2.1 and future versions, we recommend that you set **the number of buckets equal to the total number of disks in the cluster**, in order to fully utilize the I/O resources of the entire cluster.\\n\\n:::note\\nParallel Adaptive Scan is currently available for the Duplicate Key model and the Merge-on-Write tables of the Unique Key model. We plan to add it to the Aggregate Key model and the Merge-on-Read tables of the Unique Key model in version 2.1.1.\\n:::\\n\\n### Local Shuffle\\n\\nWe have introduced Local Shuffle to prevent uneven data distribution. Benchmark tests show that Local Shuffle in combination with Parallel Adaptive Scan can guarantee fast query performance despite unreasonable bucket number settings upon table creation.\\n\\nFor queries across multiple instances, uneven data distribution can prolong the query execution time. To address data skew across instances on a single backend (BE), we have introduced Local Shuffle in V2.1. It aims to shuffle and distribute data as evenly as possible, thereby accelerating queries. For example, in a typical aggregation query, a Local Shuffle node will redistribute the data evenly across different pipeline tasks, before the data is aggregated.\\n\\n\\n\\n![Local Shuffle](/images/2.1-doris-local-shuffle.png)\\n\\n\\nFor a proof of concept, we have simulated unreasonable bucket number settings. Firstly, we use the ClickBench dataset and run flat-table queries with the bucket number being 1 and 16, respectively. Then, we use the TPC-H 100G dataset and run join queries with 1 bucket and 16 buckets in each partition, respectively. Results from the runs show minimal fluctuations, which means the combination of Parallel Adaptive Scan and Local Shuffle is able to guarantee high query performance even with inappropriately sharded or unevenly distributed data.\\n\\n![Clickbench and Local Shuffle](/images/2.1-Clickbench-and-Local-shuffle.png)\\n\\n\\n:::note\\nSee the doc: https://doris.apache.org/docs/query/pipeline/pipeline-x-execution-engine\\n:::\\n\\n\\n## Increase performance on ARM\\n\\nV2.1 is specifically adapted to and optimized for ARM architecture. Compared to Doris 2.0.3, it has achieved over 100% performance improvement on multiple test datasets:\\n\\n- **ClickBench large flat-table queries**: The execution time of 43 SQL queries for V2.1 adds up to 30.73 seconds, as compared to 102.36 seconds for V2.0.3, representing a **230%** speedup.\\n\\n- **TPC-H multi-table joins**: The execution time of 22 SQL queries for V2.1 adds up to 90.4 seconds, as compared to 174.8 seconds for V2.0.3, representing a **93%** speedup.\\n\\n\\n## Improved data lake analytics capabilities\\n\\n### Data lake analytic performance\\n\\nV2.1 also reaches new heights in data lake analysis. According to TPC-DS benchmark tests (1TB) of Doris V2.1 against Trino V435,\\n\\n- Without caching, Apache Doris is **45% faster than** **Trino**, with their total execution time being 717s and 1296s, respectively. Specifically, Doris outperforms Trino in 80% of the total 99 SQL queries.\\n\\n- If you enable file cache, you can expect another 2.2-time speedup from Doris (323s). **That is 4 times the speed of Trino, with a straight win in all 99 SQL queries.**\\n\\nIn addition, TPC-DS 10TB benchmark tests show that Apache Doris 2.1 is 4.2 times as fast as Spark 3.5.0 and 6.1 times as Spark 3.3.1.\\n\\n![Data lake analytic performance](/images/2.1-doris-TPC-DS.png)\\n\\nThis is achieved by a series of optimizations in I/O for HDFS and object storage, parquet/ORC file reading, floating-point decompression, predicate pushdown, caching, and scan task scheduling. It is also built upon a more precise cost model in the optimizer and more accurate statistics collection for different data sources.\\n\\n### SQL dialects compatibility\\n\\nSQL incompatibility used to bother our users when they migrated from their existing OLAP systems (built on Clickhouse, Trino, Presto, Hive, etc.) to Doris, because they had to modify and update a significant amount of business query logic. Also, if they tried to use Doris as a unified data analysis gateway, they would also need to integrate it with their Hive or Spark systems, and incompatible SQLs could make it tough.\\n\\nTo facilitate a smooth migration or integration, we have enabled SQL dialect conversion in V2.1. Users can continue using the SQL dialect they are used to after simply setting the SQL dialect type for the current session in Doris. \\n\\nSo far, the ClickHouse, Presto, Trino, Hive, and Spark SQL dialects have been supported in this experimental feature. For example,  by `set sql_dialect = \\"trino\\"`, you can perform queries using Trino SQL syntax, without any modifications. Tests in user production environment show that Doris V2.1 is compatible with 99% of Trino SQL. \\n\\n:::note\\nSee Doc: https://doris.apache.org/docs/lakehouse/sql-dialect/\\n:::\\n\\n### High-speed data interface for 100-fold performance\\n\\nMost big data systems today adopt columnar in-memory data formats and interact with other database systems using MySQL/JDBC/ODBC protocols. That means during data transfer, there is a need to covert the data from columnar format to row-based format to fit in with the MySQL/JDBC/ODBC protocols, and then vice versa. This serialization and deserialization process slows down the data transfer speed, which becomes more noticeable when the data size is huge, like in data science scenarios.\\n\\nApache Arrow is a columnar in-memory format designed for large-scale data processing. It has efficient data structures that facilitate faster data transfer across different systems. If both the source database and target client support Arrow Flight SQL protocol, data transfer between them will entail no data serialization and deserialization. That can cut down a huge chunk of overheads. Moreover, Arrow Flight can give full play to the multi-node and multi-core architecture to parallelize operations and thus increase throughputs.\\n\\n![High-speed data interface for 100-fold performance](/images/2.1-doris-arrow-flight.png)\\n\\nReading data from Apache Doris using Python used to be a complex process. Firslty, data blocks in Doris had to be converted from its columnar format into row-based bytes. Then, in the Python client, the data had to be deserialized into a Pandas data structure. These steps largely slow down data transfer.\\n\\nNow this is revolutionized in Doris V2.1, where we provide a high-throughput data read/write interface based on Arrow Flight: HTTP Data API. Using Arrow Flight SQL, Doris converts the columnar data blocks into Arrow RecordBatch, which is also in columnar format. Then, in the Python client, Arrow RecordBatch is converted into column-oriented Pandas DataFrame. Both conversions are highly efficient and involve no serialization and deserialization. \\n\\nThis allows fast data access to Apache Doris by data science tools like Pandas and Numpy, which means Apache Doris can be seamlessly integrated with the entire AI and data science ecosystem. This unveils a future of endless possibilities. \\n\\n```C++\\nconn = flight_sql.connect(uri=\\"grpc://{FE_HOST}:{fe.conf:arrow_flight_sql_port}\\", db_kwargs={\\n            adbc_driver_manager.DatabaseOptions.USERNAME.value: \\"user\\",\\n            adbc_driver_manager.DatabaseOptions.PASSWORD.value: \\"pass\\",\\n        })\\ncursor = conn.cursor()\\ncursor.execute(\\"select * from arrow_flight_sql_test order by k0;\\")\\nprint(cursor.fetchallarrow().to_pandas())\\n```\\n\\nAccording to our comparative tests using different MySQL clients for the common data types, the Arrow Flight SQL protocol delivers almost 100 times faster performance than the MySQL protocol in data transfer.\\n\\n![MySQL protocol](/images/2.1-doris-arrow-flight-sql.png)\\n\\n### Other improvements\\n\\n- Paimon Catalog: upgrade to Paimon 0.6.0, optimized reading of Read Optimized tables, able to bring 10-fold speeds when Paimon data is fully merged\\n\\n- Iceberg Catalog: upgrade to Iceberg 1.4.3, fixed compatibility issues in AWS S3 authentication\\n\\n- Hudi Catalog: upgrade to Hudi 0.14.1, fixed compatibility issues in Hudi Flink Catalog\\n\\n## Materialized view with multiple tables\\n\\nAs a typical \\"trade disk space for time\\" strategy, materialized views pre-compute and store SQL query results so that when the same queries are requested, the materialized view table can directly provide the results. This increases query performance and reduces resource consumption by avoiding repetitive computation.\\n\\nPrevious versions of Doris offer strong consistency for single-table materialized views, ensuring atomicity between the base table and the materialized view table. They also support smart routing for query statements on materialized views, allowing for efficient query execution.\\n\\n**What\'s more exciting is that, in V2.1, we have introduced materialized views with multiple tables (also known as[asynchronous materialized view](https://doris.apache.org/docs/query/view-materialized-view/async-materialized-view)).** As the name implies, you can build a materialized view across tables. It can be based on full data or incremental data, and it can be refreshed manually or periodically. For multi-table joins or large data scale scenarios, the optimizer transparently rewrites queries based on the cost model and automatically searches for the right materialized view for **optimal query performance**. You can build asynchronous materialized views for external tables, and you can perform queries on these views directly. In other words, **this can be a game changer for** **data warehouse** **layering, data modeling, job scheduling, and data processing**.\\n\\nNow let\'s get started: \\n\\n**1. Create the tables:**\\n\\n```SQL\\nuse tpch;\\n\\nCREATE TABLE IF NOT EXISTS orders  (\\n    o_orderkey       integer not null,\\n    o_custkey        integer not null,\\n    o_orderstatus    char(1) not null,\\n    o_totalprice     decimalv3(15,2) not null,\\n    o_orderdate      date not null,\\n    o_orderpriority  char(15) not null,\\n    o_clerk          char(15) not null,\\n    o_shippriority   integer not null,\\n    o_comment        varchar(79) not null\\n    )\\n    DUPLICATE KEY(o_orderkey, o_custkey)\\n    PARTITION BY RANGE(o_orderdate)(\\n    FROM (\'2023-10-17\') TO (\'2023-10-20\') INTERVAL 1 DAY)\\n    DISTRIBUTED BY HASH(o_orderkey) BUCKETS 3\\n    PROPERTIES (\\"replication_num\\" = \\"1\\");\\n\\ninsert into orders values\\n   (1, 1, \'ok\', 99.5, \'2023-10-17\', \'a\', \'b\', 1, \'yy\'),\\n   (2, 2, \'ok\', 109.2, \'2023-10-18\', \'c\',\'d\',2, \'mm\'),\\n   (3, 3, \'ok\', 99.5, \'2023-10-19\', \'a\', \'b\', 1, \'yy\');\\n\\nCREATE TABLE IF NOT EXISTS lineitem (\\n    l_orderkey    integer not null,\\n    l_partkey     integer not null,\\n    l_suppkey     integer not null,\\n    l_linenumber  integer not null,\\n    l_quantity    decimalv3(15,2) not null,\\n    l_extendedprice  decimalv3(15,2) not null,\\n    l_discount    decimalv3(15,2) not null,\\n    l_tax         decimalv3(15,2) not null,\\n    l_returnflag  char(1) not null,\\n    l_linestatus  char(1) not null,\\n    l_shipdate    date not null,\\n    l_commitdate  date not null,\\n    l_receiptdate date not null,\\n    l_shipinstruct char(25) not null,\\n    l_shipmode     char(10) not null,\\n    l_comment      varchar(44) not null\\n    )\\n    DUPLICATE KEY(l_orderkey, l_partkey, l_suppkey, l_linenumber)\\n    PARTITION BY RANGE(l_shipdate)\\n    (FROM (\'2023-10-17\') TO (\'2023-10-20\') INTERVAL 1 DAY)\\n    DISTRIBUTED BY HASH(l_orderkey) BUCKETS 3\\n    PROPERTIES (\\"replication_num\\" = \\"1\\");\\n\\ninsert into lineitem values\\n (1, 2, 3, 4, 5.5, 6.5, 7.5, 8.5, \'o\', \'k\', \'2023-10-17\', \'2023-10-17\', \'2023-10-17\', \'a\', \'b\', \'yyyyyyyyy\'),\\n (2, 2, 3, 4, 5.5, 6.5, 7.5, 8.5, \'o\', \'k\', \'2023-10-18\', \'2023-10-18\', \'2023-10-18\', \'a\', \'b\', \'yyyyyyyyy\'),\\n (3, 2, 3, 6, 7.5, 8.5, 9.5, 10.5, \'k\', \'o\', \'2023-10-19\', \'2023-10-19\', \'2023-10-19\', \'c\', \'d\', \'xxxxxxxxx\');\\n \\n \\n CREATE TABLE IF NOT EXISTS partsupp (\\n  ps_partkey     INTEGER NOT NULL,\\n  ps_suppkey     INTEGER NOT NULL,\\n  ps_availqty    INTEGER NOT NULL,\\n  ps_supplycost  DECIMALV3(15,2)  NOT NULL,\\n  ps_comment     VARCHAR(199) NOT NULL \\n)\\nDUPLICATE KEY(ps_partkey, ps_suppkey)\\nDISTRIBUTED BY HASH(ps_partkey) BUCKETS 3\\nPROPERTIES (\\n  \\"replication_num\\" = \\"1\\"\\n)\\n```\\n\\n**2. Create materialized view:**\\n\\n```SQL\\nCREATE MATERIALIZED VIEW mv1 \\n        BUILD DEFERRED REFRESH AUTO ON MANUAL\\n        partition by(l_shipdate)\\n        DISTRIBUTED BY RANDOM BUCKETS 2\\n        PROPERTIES (\'replication_num\' = \'1\') \\n        AS \\n        select l_shipdate, o_orderdate, l_partkey, \\n            l_suppkey, sum(o_totalprice) as sum_total\\n            from lineitem\\n            left join orders on lineitem.l_orderkey = orders.o_orderkey \\n                             and l_shipdate = o_orderdate\\n            group by\\n            l_shipdate,\\n            o_orderdate,\\n            l_partkey,\\n            l_suppkey;\\n```\\n\\nTo sum up, asynchronous materialized view in V2.1 supports:\\n\\n- **Transparent rewriting**: transparently rewrites common operators including Select, Where, Join, Group By, and Aggregation, for faster query speed. For example, in BI reporting, you can create materialized views for some particularly slow queries.\\n\\n- **Auto refresh**: periodic refresh, manual refresh, full refresh, (partition-based) incremental refresh.\\n\\n- **Materialized view of external tables**: You can build materialized views based on external data such as Hive, Hudi, and Iceberg tables. You can also synchronize data from data lakes into Doris internal tables via materialized views.\\n\\n- **Direct query on materialized views**: If you regard the making of materialized views as an ETL process, then the materialized views will be the result set of ETL. In this sense, materialized views can be seen as data tables, so users can conduct queries on them directly.\\n\\n## Enhanced storage\\n\\n### AUTO_INCREMENT column\\n\\nAUTO_INCREMENT column is a common feature in OLTP databases. It provides an efficient way to automatically assign unique identifiers to newly inserted data rows. However, it is less commonly found in distributed OLAP databases because the value allocation for AUTO_INCREMENT columns involves global transactions. \\n\\nAs an MPP-based OLAP system, Apache Doris V2.1 implements AUTO_INCREMENT column with an innovative pre-allocation strategy. Leveraging the uniqueness guarantee provided by AUTO_INCREMENT, users can achieve efficient dictionary encoding and query pagination.\\n\\n**Dictionary encoding**: AUTO_INCREMENT column is helpful for queries that require accurate deduplication, such as PV/UV calculation or user segmentation. Utilizing AUTO_INCREMENT column, you can create a dictionary table for string values like UserID or OrderID. Simply writing user data in batches or in real time to the dictionary table can generate a dictionary. Then, by applying various dimensional conditions, the corresponding bitmaps can be aggregated.\\n\\n```SQL\\nCREATE TABLE `demo`.`dictionary_tbl` (\\n    `user_id` varchar(50) NOT NULL,\\n    `aid` BIGINT NOT NULL AUTO_INCREMENT\\n) ENGINE=OLAP\\nUNIQUE KEY(`user_id`)\\nDISTRIBUTED BY HASH(`user_id`) BUCKETS 32\\nPROPERTIES (\\n\\"replication_allocation\\" = \\"tag.location.default: 3\\",\\n\\"enable_unique_key_merge_on_write\\" = \\"true\\"\\n);\\n```\\n\\n**Query pagination**: Pagination is often necessary when displaying data on a webpage. Traditional pagination typically involves using `limit`, `offset` + `order by` in SQL queries. However, this can be inefficient in deep pagination queries, because even if only a small portion of the data is being requested, the database still needs to read and sort the entire dataset. This is addressed by AUTO_INCREMENT column. It generates a unique identifier for each row, remembers the maximum one from the previous page and uses it as a reference for retrieving the next page.\\n\\nThe following is an example, where `unique_value` is an AUTO INCREMENT column.\\n\\n```SQL\\nCREATE TABLE `demo`.`records_tbl2` (\\n    `key` int(11) NOT NULL COMMENT \\"\\",\\n    `name` varchar(26) NOT NULL COMMENT \\"\\",\\n    `address` varchar(41) NOT NULL COMMENT \\"\\",\\n    `city` varchar(11) NOT NULL COMMENT \\"\\",\\n    `nation` varchar(16) NOT NULL COMMENT \\"\\",\\n    `region` varchar(13) NOT NULL COMMENT \\"\\",\\n    `phone` varchar(16) NOT NULL COMMENT \\"\\",\\n    `mktsegment` varchar(11) NOT NULL COMMENT \\"\\",\\n    `unique_value` BIGINT NOT NULL AUTO_INCREMENT\\n) DUPLICATE KEY (`key`, `name`)\\nDISTRIBUTED BY HASH(`key`) BUCKETS 10\\nPROPERTIES (\\n    \\"replication_num\\" = \\"3\\"\\n);\\n```\\n\\nIn pagination display, where each page displays 100 records, this is how you can fetch the data of the first page:\\n\\n```SQL\\nselect * from records_tbl2 order by unique_value limit 100;\\n```\\n\\nThe program marks down the maximum of `unique_value` in the returned result (assuming it is 99). This is how you can fetch the data of the second page:\\n\\n```SQL\\nselect * from records_tbl2 where unique_value > 99 order by unique_value limit 100;\\n```\\n\\nIf you need data from the latter pages, for example, page 101, it will be difficult to retrieve the maximum `unique_value` of page 100, so this is how you can perform the query:\\n\\n```SQL\\nselect key, name, address, city, nation, region, phone, mktsegment\\nfrom records_tbl2, (select unique_value as max_value from records_tbl2 order by uniuqe_value limit 1 offset 9999) as previous_data\\nwhere records_tbl2.uniuqe_value > previous_data.max_value\\norder by unique_value limit 100;\\n```\\n\\n:::note\\nSee doc: https://doris.apache.org/docs/table-design/auto-increment\\n:::\\n\\n### AUTO PARTITION\\n\\nBefore V2.1, Doris requires users to manually create data partitions before data ingestion, otherwise data loading will just fail. Now, to release burden on operation and maintenance, V2.1 allows AUTO PARTITION. Upon data ingestion, it detects whether a partition exists for the data based on the partitioning column. If not, it automatically creates one and starts data ingestion.  \\n\\nTo apply AUTO PARTITION in Doris:\\n\\n```SQL\\nCREATE TABLE `DAILY_TRADE_VALUE`\\n(\\n    `TRADE_DATE`              datev2 NULL COMMENT \'Trade Date\',\\n    `TRADE_ID`                varchar(40) NULL COMMENT \'Trade ID\',\\n    ......\\n)\\nUNIQUE KEY(`TRADE_DATE`, `TRADE_ID`)\\nAUTO PARTITION BY RANGE date_trunc(`TRADE_DATE`, \'year\')\\n(\\n)\\nDISTRIBUTED BY HASH(`TRADE_DATE`) BUCKETS 10\\nPROPERTIES (\\n  \\"replication_num\\" = \\"1\\"\\n);\\n```\\n\\n:::tip\\n1. Currently, you can only specify one partitioning column for AUTO PARTITION, and it has to be NOT NULL.\\n\\n2. It supports AUTO PARTITION by Range or by List. For the former, it supports `date_trunc` as the partitioning function, and `DATE` or `DATETIME` format for the partitioning column. For the latter, it does not support function calling, it supports `BOOLEAN`, `TINYINT`, `SMALLINT`, `INT`, `BIGINT`, `LARGEINT`, `DATE`, `DATETIME`, `CHAR`, and `VARCHAR` for the partitioning column, and the values are enumeration values.\\n\\n3. For AUTO PARTITION by List, if there is no partition for a value in the partitioning column, Doris will create one for it.\\n:::\\n\\n:::note\\nSee doc: https://doris.apache.org/zh-CN/docs/table-design/data-partition#%E8%87%AA%E5%8A%A8%E5%88%86%E5%8C%BA\\n:::\\n\\n### 100% faster INSERT INTO SELECT\\n\\n`INSERT INTO\u2026SELECT` is one of the most frequently used statements in ETL. It enables fast data migration, transformation, cleaning, and aggregation. That\'s why we\'ve been optimizing its performance. In V2.0, we introduced Single Replica Load to reduce repetitive data writing and data compaction. \\n\\nFor further improvement, in V2.1, we have moved forward the execution of MemTable to reduce data ingestion overheads. Tests show that this can **double the data ingestion speed in most cases compared to V2.0**. \\n\\n![100% faster INSERT INTO SELECT](/images/2.1-INSERT-INTO-SELECT-EN.png)\\n\\nThe process comparison before and after moving forward the execution of MemTable is illustrated above. The Sink node no longer sends encoded data blocks but instead processes MemTable locally and sends the generated segments to downstream nodes. This reduces the overheads caused by multiple data encoding and improves the speed and accuracy of memory backpressure. In addition, we have replaced Ping-Pong RPC with Streaming RPC so there will be less waiting during data transfer.\\n\\nWe\'ve done tests to see how moving forward MemTable impacts data ingestion performance.\\n\\n**Test environment**: 1 Frontend + 3 Backend, 16C 64G each node, 3 high-performance cloud disks (to make sure that disk I/O is not a bottleneck)\\n\\n**Test results**: \\n\\nIn single-replica ingestion, the execution time of V2.1 is only 36% of what takes V2.0 to finish. In three-replica ingestion, that figure is 54%. This means, V2.1 has sped up the performance of `INSERT INTO\u2026SELECT` by more than 100% in general.\\n\\n![Insert into table](/images/2.1-insert-into-select.png)\\n\\n:::note\\nV2.1 moves forward the execution of MemTable by default, so you don\'t have to modify the data ingestion command. You can return to the old ingestion method by setting `enable_memtable_on_sink_node=false` in MySQL connection.\\n:::\\n\\n### High-concurrency real-time data ingestion / Group Commit\\n\\nFor data writing, V2.1 has a back pressure mechanism to avoid excessive data versions, so as to reduce resource consumption caused by data version merging. \\n\\nDuring data ingestion, data batches are written to an in-memory table and then written to disk as individual RowSet files. Each RowSet file corresponds to a specific data import version. The background compaction process automatically merges the RowSets, combining the small ones into a big one in order to increase query speed and reduce storage consumption. However, each compaction process consumes CPU, memory, and disk IO resources. The more frequently that data is written, the more RowSets are generated, and the more resources the compaction process consumes. The backpressure mechanism is a solution to this. It will throw a -235 error when there are too many data versions.\\n\\n\\n![High-concurrency real-time data ingestion ](/images/2.1-doris-group-commit.png)\\n\\nIn addition, V2.1 supports Group Commit, which means to accumulate multiple data writings in the backend and commit them as one batch. In this way, users don\'t have to keep their writing frequency at a low level because Doris will merge multiple writings into one. \\n\\n![Group Comit](/images/2.1-doris-group-commit-2.png)\\n\\nGroup Commit so far supports two modes: `sync_mode` and `async_mode`. The `sync_mode` commits multiple imports within a single transaction, and then the data becomes immediately visible. While in the `async_mode`, data is first written to the Write-Ahead Log (WAL). Then Doris, based on the system load and the value of `group_commit_interval`, asynchronously commits the data, after which the data becomes visible. When a single import is huge, the system automatically switches to the `sync_mode` to prevent the WAL from occupying too much disk space. \\n\\nBenchmark tests on Group Commit (`async_mode`) with JDBC ingestion and the Stream Load method present great results.\\n\\n- **JDBC ingestion:** \\n\\n  - A 1 Frontend + 1 Backend cluster, TPC-H SF10 Lineitem table (22GB, 180 million rows);\\n\\n  - At a concurrency level of 20, with each Insert involving less than 100 rows, Doris V2.1 reaches a writing speed of 106,900 row/s and a throughput of 11.46 MB/s. CPU usage of the Backend remains at 10%~20%.\\n\\n- **Stream Load:**\\n\\n  - A 1 Frontend + 3 Backends cluster, httplogs (31GB, 247 million rows);\\n\\n  - At a concurrency level of 10, with each writing involving less than 1MB, Doris returns a -235 error when disabling Group Commit. With Group Commit enabled, it delivers stable performance and reaches a writing speed of 810,000 row/s and a throughput of 104 MB/s.\\n\\n  - At a concurrency level of 10, with each writing involving less than 10MB, enabling Group Commit increases the writing speed by 45% and the writing throughput by 79%.\\n\\n:::note\\nSee doc and full test results: https://doris.apache.org/docs/data-operate/import/group-commit-manual\\n:::\\n\\n## Semi-structured data analysis\\n\\n### A new data type: Variant\\n\\nBefore V2.1, Doris processes semi-structured data in two ways:\\n\\n1. It requires users to pre-define table schema, make a flat table, and parse the data before it is loaded into Doris. This method ensures fast data writing and avoids parsing upon query execution. The downside is the lack of flexibility. Any change to the table schema will require a lot of maintenance efforts.\\n\\n2. It accommodates semi-structured data with JSON or stores it as JSON strings. Raw JSON data is ingested into Doris without any pre-processing and is parsed by functions upon query execution. This option requires no extra effort from the users, but you might need to put up with inefficient data parsing and reading.\\n\\nV2.1 supports a new data type named Variant. It can accommodate semi-structured data such as JSON as well as compound data structures that contain various data types such as integers, strings, and booleans. Users don\'t have to pre-define the exact data types for a Variant column in the table schema. \\n\\nThe Variant type is handy when processing nested data structures, where the structure can change dynamically. During data writing, it is capable of auto-inference for columns based on what is given, after which it merges them into the existing table schema, and stores the JSON keys and their corresponding values as dynamic sub-columns. \\n\\nYou can include both Variant columns and static columns with pre-defined data types in the same table. This provides greater flexibility in storage and queries. Additionally, the Variant type is empowered by the columnar storage, vectorized execution engine, and query optimizer for high efficiency in queries and storage.\\n\\nUse Variant in Doris:\\n\\n```SQL\\n-- No index\\nCREATE TABLE IF NOT EXISTS ${table_name} (\\n    k BIGINT,\\n    v VARIANT\\n)\\ntable_properties;\\n\\n-- Create index for the v column, specify the parser\\nCREATE TABLE IF NOT EXISTS ${table_name} (\\n    k BIGINT,\\n    v VARIANT,\\n    INDEX idx_var(v) USING INVERTED [PROPERTIES(\\"parser\\" = \\"english|unicode|chinese\\")] [COMMENT \'your comment\']\\n)\\ntable_properties;\\n\\n-- Perform queries, access sub-columns using`[]`\\nSELECT v[\\"properties\\"][\\"title\\"] from ${table_name}\\n```\\n\\n**Variant VS JSON**\\n\\nIn Apache Doris, JSON data is stored in a binary JSONB format, and the entire JSON row is stored in segments in a row-oriented way. However, with the Variant type, it automatically infers the data type upon data writing and stores the JSON data in a columnar method. Thus, no parsing is needed during queries.\\n\\nFurthermore, the Variant type is optimized for sparse JSON scenarios. It only extracts frequently occurring columns. The sparse columns are stored in a separate format.\\n\\nTests  prove that **data in Variant columns takes up the same storage space as data in static columns, which is only 35% of that in JSON format**. The Variant type should be a more cost-effective choice in low-cardinality scenarios.\\n\\n![Variant vs JSON](/images/2.1-variant-vs-json.png)\\n\\nIn terms of query performance, **the Variant type enables 8 times higher query speed than JSON** in hot runs and even more in cold runs.\\n\\n![Variant vs JSON](/images/2.1-variant-vs-json-2.png)\\n\\n:::tip\\n- Currently, the Variant type is not supported in the Aggregate Key model of Doris. It can not be the primary key or sorting key in a Unique Key model table or Duplicate model table;\\n\\n- It is recommended to go with the RANDOM mode or Group Commit for higher writing performance;\\n\\n- It is recommended to extract non-standard JSON types such as date or decimal as static fields to enable higher performance;\\n\\n- In columnar format, arrays of two or more dimensions, as well as arrays with nested objects, will be stored as JSONB encoding, resulting in lower performance than native arrays;\\n\\n- Queries involving filtering or aggregation require the use of Cast, where the storage layer will provide hints for the storage engine for predicate pushdown based on the storage type and the Cast type and thus accelerate queries.\\n:::\\n\\n:::note\\nSee doc: https://doris.apache.org/docs/sql-manual/sql-types/Data-Types/VARIANT\\n:::\\n\\n### IP types\\n\\nIP address is a widely used field in statistical analysis for network traffic monitoring. Doris V2.1 provides native support for IPv4 and IPv6. It stores IP data in binary format, which cuts down storage space usage by 60% compared to IP string in plain texts. Along with these IP types, we have added over 20 functions for IP data processing, including:\\n\\n- IPV4_NUM_TO_STRING: It converts a big-endian representation of an IPv4 address of Int16, Int32, or Int64 into its corresponding string representation;\\n\\n- IPV4_CIDR_TO_RANGE: It receives an IPv4 address and a CIDR-containing Int16 value, and returns a structure containing two IPv4 fields, representing the lower range (min) and upper range (max) of the subnet, respectively;\\n\\n- INET_ATON: It retrieves a string containing an IPv4 address in the format of A.B.C.D, where A, B, C, and D are decimal numbers separated by periods.\\n\\n:::note\\nSee [IPV6](https://doris.apache.org/docs/sql-manual/sql-types/Data-Types/IPV6) for more information.\\n:::\\n\\n### More powerful functions for compound data types\\n\\nV2.1 provides more supported data types:\\n\\n- `explode_map`: supports exploding rows into columns for the Map data type (only with the new optimizer)\\n\\nEach key-value pair in the Map field is expanded into a separate row, with the Map field replaced by two separate fields representing the key and value. The `explode_map` function should be used in conjunction with Lateral View. You can apply multiple Lateral Views. The result is a Cartesian product.\\n\\nThis is how it is used:\\n\\n```SQL\\n-- Create table\\n CREATE TABLE `sdu` (\\n  `id` INT NULL,\\n  `name` TEXT NULL,\\n  `score` MAP<TEXT,INT> NULL\\n) ENGINE=OLAP\\nDUPLICATE KEY(`id`)\\nCOMMENT \'OLAP\'\\nDISTRIBUTED BY HASH(`id`) BUCKETS 1\\nPROPERTIES (\\n\\"replication_allocation\\" = \\"tag.location.default: 1\\"\\n);\\n\\n-- Insert data\\ninsert into sdu values (0, \\"zhangsan\\", {\\"Chinese\\":\\"80\\",\\"Math\\":\\"60\\",\\"English\\":\\"90\\"});\\ninsert into sdu values (1, \\"lisi\\", {\\"null\\":null});\\ninsert into sdu values (2, \\"wangwu\\", {\\"Chinese\\":\\"88\\",\\"Math\\":\\"90\\",\\"English\\":\\"96\\"});\\ninsert into sdu values (3, \\"lisi2\\", {null:null});\\ninsert into sdu values (4, \\"amory\\", NULL);\\n\\nmysql> select name, course_0, score_0 from sdu lateral view explode_map(score) tmp as course_0,score_0;\\n+----------+----------+---------+\\n| name     | course_0 | score_0 |\\n+----------+----------+---------+\\n| zhangsan | Chinese  |      80 |\\n| zhangsan | Math     |      60 |\\n| zhangsan | English  |      90 |\\n| lisi     | null     |    NULL |\\n| wangwu   | Chinese  |      88 |\\n| wangwu   | Math     |      90 |\\n| wangwu   | English  |      96 |\\n| lisi2    | NULL     |    NULL |\\n+----------+----------+---------+\\n\\nmysql> select name, course_0, score_0, course_1, score_1 from sdu lateral view explode_map(score) tmp as course_0,score_0 lateral view explode_map(score) tmp1 as course_1,score_1;\\n+----------+----------+---------+----------+---------+\\n| name     | course_0 | score_0 | course_1 | score_1 |\\n+----------+----------+---------+----------+---------+\\n| zhangsan | Chinese  |      80 | Chinese  |      80 |\\n| zhangsan | Chinese  |      80 | Math     |      60 |\\n| zhangsan | Chinese  |      80 | English  |      90 |\\n| zhangsan | Math     |      60 | Chinese  |      80 |\\n| zhangsan | Math     |      60 | Math     |      60 |\\n| zhangsan | Math     |      60 | English  |      90 |\\n| zhangsan | English  |      90 | Chinese  |      80 |\\n| zhangsan | English  |      90 | Math     |      60 |\\n| zhangsan | English  |      90 | English  |      90 |\\n| lisi     | null     |    NULL | null     |    NULL |\\n| wangwu   | Chinese  |      88 | Chinese  |      88 |\\n| wangwu   | Chinese  |      88 | Math     |      90 |\\n| wangwu   | Chinese  |      88 | English  |      96 |\\n| wangwu   | Math     |      90 | Chinese  |      88 |\\n| wangwu   | Math     |      90 | Math     |      90 |\\n| wangwu   | Math     |      90 | English  |      96 |\\n| wangwu   | English  |      96 | Chinese  |      88 |\\n| wangwu   | English  |      96 | Math     |      90 |\\n| wangwu   | English  |      96 | English  |      96 |\\n| lisi2    | NULL     |    NULL | NULL     |    NULL |\\n+----------+----------+---------+----------+---------+\\n```\\n\\n`explode_map_outer` and `explode_outer` serve the same purpose. They display rows with NULL values in the Map-type columns.\\n\\n```SQL\\nmysql> select name, course_0, score_0 from sdu lateral view explode_map_outer(score) tmp as course_0,score_0;\\n+----------+----------+---------+\\n| name     | course_0 | score_0 |\\n+----------+----------+---------+\\n| zhangsan | Chinese  |      80 |\\n| zhangsan | Math     |      60 |\\n| zhangsan | English  |      90 |\\n| lisi     | null     |    NULL |\\n| wangwu   | Chinese  |      88 |\\n| wangwu   | Math     |      90 |\\n| wangwu   | English  |      96 |\\n| lisi2    | NULL     |    NULL |\\n| amory    | NULL     |    NULL |\\n+----------+----------+---------+\\n```\\n\\n- `IN`: supports the `STRUCT` data type (only with the new optimizer)\\n\\nThe `IN` predicate supports Struct type data constructed using the `struct()`function as the left parameter. It also allows selecting a column that contains Struct type data from a table. It supports a Struct-type array constructed using the `struct()` function as the right parameter.\\n\\nIt is an efficient alternative to WHERE clauses with many OR conditions. For example, instead of using expressions like `(a = 1 and b = \'2\') or (a = 1 and b = \'3\') or (...)`, you can use `struct(a,b) in (struct(1, \'2\'), struct(1, \'3\'), ...)`.\\n\\n```SQL\\nmysql> select struct(1,\\"2\\")  in (struct(1,3), struct(1,\\"2\\"), struct(1,1), null);\\n+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\\n| cast(struct(1, \'2\') as STRUCT<col1:TINYINT,col2:TEXT>) IN (NULL, cast(struct(1, \'2\') as STRUCT<col1:TINYINT,col2:TEXT>), cast(struct(1, 1) as STRUCT<col1:TINYINT,col2:TEXT>), cast(struct(1, 3) as STRUCT<col1:TINYINT,col2:TEXT>)) |\\n+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\\n|                                                                                                                                                                                                                                    1 |\\n+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\\nmysql> select struct(1,\\"2\\") not in (struct(1,3), struct(1,\\"2\\"), struct(1,1), null);\\n+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\\n| ( not cast(struct(1, \'2\') as STRUCT<col1:TINYINT,col2:TEXT>) IN (NULL, cast(struct(1, \'2\') as STRUCT<col1:TINYINT,col2:TEXT>), cast(struct(1, 1) as STRUCT<col1:TINYINT,col2:TEXT>), cast(struct(1, 3) as STRUCT<col1:TINYINT,col2:TEXT>))) |\\n+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\\n|                                                                                                                                                                                                                                           0 |\\n+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\\n```\\n\\n- `MAP_AGG`: It receives expr1 as the key, expr2 as the corresponding value, and returns a MAP.\\n\\n:::note\\nSee doc: https://doris.apache.org/docs/sql-manual/sql-functions/aggregate-functions/map-agg/\\n:::\\n\\n## Workload management\\n\\n### Hard isolation of resources\\n\\nOn the basis of the Workload Group mechanism, which imposes a soft limit on the resources that a workload group can use, Doris 2.1 introduces a hard limit on CPU resource consumption for workload groups as a way to ensure **higher stability in query performance**. This means that regardless of the overall CPU availability on the physical machine, workload groups configured with hard limits cannot exceed the maximum CPU usage specified in the configuration. In this way, as long as there is no significant change in user query workload, there will be stable query performance. A caveat is that, in addition to CPU usage, memory, I/O, and resource contention at the software level will all impact query execution. Thus, when the cluster switches between idle and heavy load, even with CPU hard limits configured, there might still be fluctuations in query performance. However, you can still expect better performance from the hard limits than the soft limits.\\n\\n:::tip\\n**Note**\\n\\n1. In Doris V2.0, CPU resource isolation was implemented based on a priority queue. In V2.1, this relies on the CGroup mechanism. Therefore, please note that you should configure the CGroups in advance after upgrading from V2.0 to V2.1. \\n\\n2. Currently, the Workload Group mechanism supports query-query workload isolation and ingestion-query isolation. Note that if you need to impose hard limits on import workloads, you should enable `memtable_on_sink_node`.\\n\\n3. Users need to specify either soft limits or hard limits for the current cluster using a switch. Currently, it is not supported to run both modes on the same cluster. In the future, we will consider bringing in simultaneous support for both modes based on user feedback.\\n:::\\n\\n:::note\\nSee [Workload Group](https://doris.apache.org/docs/admin-manual/resource-admin/workload-group).\\n:::\\n\\n### TopSQL\\n\\nV2.1 allows users to check the most resource-consuming SQL queries in the runtime. This can be a big help when handling cluster load spikes caused by unexpected large queries.\\n\\n```SQL\\nmysql [(none)]>desc function active_queries();\\n+------------------------+--------+------+-------+---------+-------+\\n| Field                  | Type   | Null | Key   | Default | Extra |\\n+------------------------+--------+------+-------+---------+-------+\\n| BeHost                 | TEXT   | No   | false | NULL    | NONE  |\\n| BePort                 | BIGINT | No   | false | NULL    | NONE  |\\n| QueryId                | TEXT   | No   | false | NULL    | NONE  |\\n| StartTime              | TEXT   | No   | false | NULL    | NONE  |\\n| QueryTimeMs            | BIGINT | No   | false | NULL    | NONE  |\\n| WorkloadGroupId        | BIGINT | No   | false | NULL    | NONE  |\\n| QueryCpuTimeMs         | BIGINT | No   | false | NULL    | NONE  |\\n| ScanRows               | BIGINT | No   | false | NULL    | NONE  |\\n| ScanBytes              | BIGINT | No   | false | NULL    | NONE  |\\n| BePeakMemoryBytes      | BIGINT | No   | false | NULL    | NONE  |\\n| CurrentUsedMemoryBytes | BIGINT | No   | false | NULL    | NONE  |\\n| ShuffleSendBytes       | BIGINT | No   | false | NULL    | NONE  |\\n| ShuffleSendRows        | BIGINT | No   | false | NULL    | NONE  |\\n| Database               | TEXT   | No   | false | NULL    | NONE  |\\n| FrontendInstance       | TEXT   | No   | false | NULL    | NONE  |\\n| Sql                    | TEXT   | No   | false | NULL    | NONE  |\\n+------------------------+--------+------+-------+---------+-------+\\n```\\n\\nThe `active_queries()` function records the audit information of queries running on various Backends in Doris. You can query `active_queries()` like querying a regular table. It supports operations including querying, filtering with predicates, sorting, and joining. Common metrics captured by this function include the SQL execution time, CPU time, peak memory usage on a single Backend, data volume scanned, and data volume shuffled during the query execution. It also allows rolling up to the Backend level to examine the global resource consumption of SQL queries.\\n\\nNote that only the SQL in the runtime will be displayed. The SQLs that finish execution will be written into the audit logs (fe.audit.log, mostly) instead. A few frequently used SQLs are as follows: \\n\\n```SQL\\nCheck the top N longest-running SQLs in the cluster\\nselect QueryId,max(QueryTimeMs) as query_time from active_queries() group by QueryId order by query_time desc limit 10;\\n\\nCheck the top N most CPU-consuming SQLs in the cluster\\nselect QueryId, sum(QueryCpuTimeMs) as cpu_time from active_queries() group by QueryId order by cpu_time desc limit 10\\n\\nCheck the top N SQLs with the most scan rows and their execution time\\nselect t1.QueryId,t1.scan_rows, t2.query_time from \\n    (select QueryId, sum(ScanRows) as scan_rows from active_queries()  group by QueryId order by scan_rows desc limit 10) t1 \\n    left join (select QueryId,max(QueryTimeMs) as query_time from active_queries() group by QueryId) t2 on t1.QueryId = t2.QueryId\\n    \\nCheck the load of all Backends and sort them in descending order based on CPU time/scan rows/shuffle bytes.\\nselect BeHost,sum(QueryCpuTimeMs) as query_cpu_time, sum(ScanRows) as scan_rows,sum(ShuffleSendBytes) as shuffle_bytes from active_queries() group by BeHost order by query_cpu_time desc,scan_rows desc ,shuffle_bytes desc limit 10\\n\\nCheck the top N SQL queries with the highest peak memory usage on a single Backend.\\nselect QueryId,max(BePeakMemoryBytes) as be_peak_mem from active_queries() group by QueryId order by be_peak_mem desc limit 10;\\n```\\n\\nCurrently, the main displayed workload types include `Select` and `Insert Into...Select`. The iterative versions of V2.1 are expected to support displaying the resource usage of Stream Load and Broker Load.\\n\\n:::note\\nSee doc: https://doris.apache.org/docs/sql-manual/sql-functions/table-functions/active_queries/\\n:::\\n\\n## Others\\n\\n### Decimal 256\\n\\nFor users in the financial sector or high-end manufacturing, V2.1 supports a high-precision data type: Decimal, which supports up to 76 significant digits (To enable this experimental feature, please set `enable_decimal256=true`.)\\n\\nExample:\\n\\n```SQL\\nCREATE TABLE `test_arithmetic_expressions_256` (\\n      k1 decimal(76, 30),\\n      k2 decimal(76, 30)\\n    )\\n    DISTRIBUTED BY HASH(k1)\\n    PROPERTIES (\\n    \\"replication_num\\" = \\"1\\"\\n    );\\n\\ninsert into test_arithmetic_expressions_256 values\\n  (1.000000000000000000000000000001, 9999999999999999999999999999999999999999999998.999999999999999999999999999998),\\n  (2.100000000000000000000000000001, 4999999999999999999999999999999999999999999999.899999999999999999999999999998),\\n  (3.666666666666666666666666666666, 3333333333333333333333333333333333333333333333.333333333333333333333333333333);\\n```\\n\\nQuery and result:\\n\\n```SQL\\nselect k1, k2, k1 + k2 a from test_arithmetic_expressions_256 order by 1, 2;\\n+----------------------------------+-------------------------------------------------------------------------------+-------------------------------------------------------------------------------+\\n| k1                               | k2                                                                            | a                                                                             |\\n+----------------------------------+-------------------------------------------------------------------------------+-------------------------------------------------------------------------------+\\n| 1.000000000000000000000000000001 | 9999999999999999999999999999999999999999999998.999999999999999999999999999998 | 9999999999999999999999999999999999999999999999.999999999999999999999999999999 |\\n| 2.100000000000000000000000000001 | 4999999999999999999999999999999999999999999999.899999999999999999999999999998 | 5000000000000000000000000000000000000000000001.999999999999999999999999999999 |\\n| 3.666666666666666666666666666666 | 3333333333333333333333333333333333333333333333.333333333333333333333333333333 | 3333333333333333333333333333333333333333333336.999999999999999999999999999999 |\\n+----------------------------------+-------------------------------------------------------------------------------+-------------------------------------------------------------------------------+\\n3 rows in set (0.09 sec)\\n```\\n\\n:::note\\nThe Decimal256 type consumes more CPU resources so the queries might not be as fast compared to other data types.\\n:::\\n\\n### Job scheduler\\n\\nAccording to user feedback, there is a recurring need for scheduled job execution, such as:\\n\\n- Periodic backup;\\n\\n- Scheduled data expiration;\\n\\n- Periodic import jobs: scheduling incremental or full data synchronization jobs using the Catalog feature;\\n\\n- Regular ETL: such as loading data from a flat table into a specified table on a scheduled basis, pulling data from detailed tables and storing it in aggregate tables at specific intervals, and performing scheduled denormalization for tables in the ODS layer and updates to the existing flat table.\\n\\nDespite the availability of various external scheduling systems such as Airflow and DolphinScheduler, there still exists a consistency challenge. When an external scheduling system triggers an import job in Doris and successfully executes it, but unexpectedly experiences a crash. In this case, since the external scheduling system fails to retrieve the execution result, it assumes the schedule has failed. Then it will trigger its fault tolerance mechanism. Either retries or a direct failure will result in:\\n\\n- **Waste of resources**: Since the scheduling system can mistakenly consider a job as failed, it might reschedule the execution of a job that has already succeeded, resulting in unnecessary resource consumption.\\n\\n- **Data duplication or loss**: On the one hand, retrying the import job might lead to duplicate data imports, resulting in data redundancy or inconsistency. On the other hand, if the job is marked as failed, it can result in the neglect or loss of data that has actually been successfully imported.\\n\\n- **Time delay**: After the fault tolerance mechanism is triggered, extra time is needed for job scheduling and retries, prolonging the overall data processing time.\\n\\n- **Compromised system stability**: Frequent retries or immediate failures can increase the load on both the scheduling system and Doris, thereby undermining the stability and performance of the system.\\n\\nV2.1 provides a good option for regular job scheduling: Doris Job Scheduler. It can trigger the pre-defined operations on schedule or at fixed intervals. The Doris Job Scheduler is accurate to the second. In addition to consistency guarantee for data writing, it provides:\\n\\n1. **Efficiency**: The Doris Job Scheduler can schedule jobs and events at specified time intervals to ensure efficient data processing. By employing the time wheel algorithm, it guarantees precise triggering of events at a granularity of seconds.\\n\\n2. **Flexibility**: It offers multiple scheduling options, such as scheduling at intervals of minutes, hours, days, or weeks. It supports both one-time scheduling and recurring (cyclic) event scheduling. For the latter, you can specify the start and end times for the scheduling period.\\n\\n3. **Event pool and high-performance processing queues**: It utilizes Disruptor for a high-performance producer-consumer model to minimize job overload.\\n\\n4. **Traceable scheduling records**: It stores the latest job execution records (configurable), which users can view via a simple command.\\n\\n5. **High availability**: On the basis of the Doris high availability mechanism, the jobs are easily self-recoverable.\\n\\nAn example of creating a scheduled job:\\n\\n```SQL\\n// Execute an insert statement every day from 2023-11-17 to 2038\\nCREATE\\nJOB e_daily\\n    ON SCHEDULE\\n      EVERY 1 DAY \\n      STARTS \'2023-11-17 23:59:00\'\\n      ENDS \'2038-01-19 03:14:07\'\\n    COMMENT \'Saves total number of sessions\'\\n    DO\\n        INSERT INTO site_activity.totals (time, total)\\n        SELECT CURRENT_TIMESTAMP, COUNT(*)\\n        FROM site_activity.sessions where create_time >=  days_add(now(),-1) ;\\n```\\n\\n:::note\\nDoris Job Scheduler only supports Insert operations on internal tables currently. See [CREATE-JOB](https://doris.apache.org/docs/sql-manual/sql-statements/Data-Definition-Statements/Create/CREATE-JOB).\\n:::\\n\\n## Behavior changed\\n\\n- The Unique Key model enables Merge-on-Write by default, which means `enable_unique_key_merge_on_write=true` will be included as a default setting when a table is created in the Unique Key model.\\n\\n- Since inverted index has proven to be more performant than bitmap index, V2.1 and future versions stop supporting bitmap index. Existing bitmap indexes will remain effective but new creation is not allowed. We will remove bitmap index-related code in the future.\\n\\n- `cpu_resource_limit` is no longer supported. It is to put a limit on the number of scanner threads on Doris Backend. Since the Workload Group mechanism also supports such settings, the already configured `cpu_resource_limit` will be invalid.\\n\\n- Segment compaction is enabled by default. This means Doris supports compaction of multiple segments in the same rowset, which is useful in single-batch ingestion of large datasets.\\n\\n- Audit log plug-in\\n\\n  - Since V2.1.0, Doris has a built-in audit log plug-in. Users can simply enable or disable it by setting the `enable_audit_plugin` parameter. \\n\\n  - If you have already installed your own audit log plug-in, you can either continue using it after upgrading to Doris V2.1, or uninstall it and use the one in Doris. Please note that the audit log table will be relocated after switching plug-in.\\n\\n  - For more details, please see doc: https://doris.apache.org/docs/admin-manual/audit-plugin\\n\\n\\n\\n## Credits\\n\\nSpecial thanks to the following contributors for making this happen: \\n\\nmorrySnow, Gabriel39, BiteTheDDDDt, kaijchen, starocean999, morningman, jackwener, zy-kkk, englefly, Jibing-Li, XieJiann, yujun777, Mryange, HHoflittlefish777, LiDongyangLi, HappenLee, zhangstar333, lihangyu, zclllyybb, amory, bobhan1, AKIRA, zhangdong, ZouXinyiZou, HuJerryHu, yiguolei, airborne12, wangbo, jacktengg, jacktengg, TangSiyang2001, BePPPower, Yukang-Lian, mymeiyi, liugddx, kaka11chen, AshinGau, DrogonJackDrogon, wsjz, seuhezhiqiang, zhannngchen, shuke987, KassieZ, huanghaibin, zzzxl1993, Nitin-Kashyap, AlexYue, dataroaring, seawinde, walter, xzj7019, xiaokang, SWJTU-ZhangLei, liaoxin01, dutyu, wuwenchihdu, LiBinfeng-01, daidai, qidaye, mch_ucchi, zhangguoqiang, zhengyu, plat1ko, LemonLiTree, ixzc, deardeng, yiguolei, catpineapple, LingAdonisLing, DongLiang-0, whuxingying, Tanya-W, Yulei-Yang, zzzzzzzs, caoliang-web, xueweizhang, yangshijie, Luwei, lsy3993, xy720, HowardQin, DeadlineFen, Petrichor, caiconghui, KirsCalvinKirs, SunChenyangSun, ChouGavinChou, Luzhijing, gnehil, wudi, zhiqqqq, zfr95, zxealous, kkop, yagagagaga, Chester, LuGuangmingLu, Lightman, Xiaocc, taoxutao, yuanyuan8983, KirsCalvinKirs, DuRipeng, GoGoWen, JingDas, camby, camby, Euporia, rohitrs1983, felixwluo, wudongliang, FreeOnePlus, PaiVallishPai, XuJianxu, seuhezhiqiang, luozenglin, 924060929, HB, LiuLijiaLiu, Ma1oneZhang, bingquanzhao, chunping, echo-dundun, feiniaofeiafei, walter, yongjinhou, zgxme, zhangy5, httpshirley, ChenyangSunChenyang, ZenoYang, ZhangYu0123, hechao, herry2038, jayhua, koarz, nanfeng, LiChuangLi, LiuGuangdongLiu, Jeffrey, liuJiwenliu, Stalary, DuanXujianDuan, HuZhiyuHu, jiafeng.zhang, nanfeng, py023, xiongjx, yuxuan-luo, zhaoshuo, XiaoChangmingXiao, ElvinWei, LiuHongLiu, QiHouliangQi, Hyman-zhao, HelgeLarsHelge, Uniqueyou, YangYAN, acnot, amory, feifeifeimoon, flynn, gohalo, htyoung, realize096, shee, wangqt, xyfsjq, zzwwhh, songguangfan, 467887319, BirdAmosBird, ZhuArmandoZhu, CanGuan, ChengDaqi2023, ChinaYiGuan, gitccl, colagy, DeadlineFen, Doris-Extras, HonestManXin, q763562998, guardcrystal, Dragonliu2018, ZhaoLongZhao, LuoMetaLuo, Miaohongkai, YinShaowenYin, Centurybbx, hongkun-Shao, Wanghuan, Xinxing, XueYuhai, Yoko, HeZhangJianHe, ZhongJinHacker, alan_rodriguez, allenhooo, beat4ocean, bigben0204, chen, czzmmc, dalong, deadlinefen, didiaode18, dong-shuai, feelshana, fornaix, hammer, xuke-hat, hqx871, i78086, irenesrl, julic20s, kindred77, lihuigang, wenluowen, lxliyou001, CSTGluigi, ranxiang327, shysnow, sunny, vhwzIs, wangtao, wangtianyi2004, wyx123654, xuefengze, xiangran0327, xy, yimeng, ytwp, yujian, zhangstar333, figurant, sdhzwc, LHG41278, zlw5307"},{"id":"/breaking-down-data-silos-with-an-apache-doris-based-cdp","metadata":{"permalink":"/blog/breaking-down-data-silos-with-an-apache-doris-based-cdp","source":"@site/blog/breaking-down-data-silos-with-an-apache-doris-based-cdp.md","title":"Breaking down data silos with a unified data warehouse: an Apache Doris-based CDP","description":"The insurance company uses Apache Doris, a unified data warehouse, in replacement of Spark + Impala + HBase + NebulaGraph, in their Customer Data Platform for 4 times faster customer grouping.","date":"2024-03-05T00:00:00.000Z","tags":[{"inline":true,"label":"Best Practice","permalink":"/blog/tags/best-practice"}],"hasTruncateMarker":false,"authors":[{"name":"velodb.io \xb7 VeloDB Engineering Team","key":null,"page":null}],"frontMatter":{"title":"Breaking down data silos with a unified data warehouse: an Apache Doris-based CDP","description":"The insurance company uses Apache Doris, a unified data warehouse, in replacement of Spark + Impala + HBase + NebulaGraph, in their Customer Data Platform for 4 times faster customer grouping.","date":"2024-03-05","author":"velodb.io \xb7 VeloDB Engineering Team","externalLink":"https://www.velodb.io/blog/172","tags":["Best Practice"],"image":"/images/breaking-down-data-silos-with-an-apache-doris-based-cdp.png"},"unlisted":false,"prevItem":{"title":"Another big leap: Apache Doris 2.1.0 is released","permalink":"/blog/release-note-2.1.0"},"nextItem":{"title":"Apache Doris 2.0.5 just released","permalink":"/blog/release-note-2.0.5"}},"content":"\x3c!-- \\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\nThe data silos problem is like arthritis for online business, because almost everyone gets it as they grow old. Businesses interact with customers via websites, mobile apps, H5 pages, and end devices. For one reason or another, it is tricky to integrate the data from all these sources. Data stays where it is and cannot be interrelated for further analysis. That\'s how data silos come to form. The bigger your business grows, the more diversified customer data sources you will have, and the more likely you are trapped by data silos. \\n\\nThis is exactly what happens to the insurance company I\'m going to talk about in this post. By 2023, they have already served over 500 million customers and signed 57 billion insurance contracts. When they started to build a customer data platform (CDP) to accommodate such a data size, they used multiple components. \\n\\n## Data silos in CDP\\n\\nLike most data platforms, their CDP 1.0 had a batch processing pipeline and a real-time streaming pipeline. Offline data was loaded, via Spark jobs, to Impala, where it was tagged and divided into groups. Meanwhile, Spark also sent it to NebulaGraph for OneID computation (elaborated later in this post). On the other hand, real-time data was tagged by Flink and then stored in HBase, ready to be queried.\\n\\nThat led to a component-heavy computation layer in the CDP: Impala, Spark, NebulaGraph, and HBase.\\n\\n![apache doris data silos in CDP](/images/apache-doris-data-silos-in-CDP.png)\\n\\nAs a result, offline tags, real-time tags, and graph data were scattered across multiple components. Integrating them for further data services was costly due to redundant storage and bulky data transfer. What\'s more, due to discrepancies in storage, they had to expand the size of the CDH cluster and NebulaGraph cluster, adding to the resource and maintenance costs.\\n\\n## Apache Doris-based CDP\\n\\nFor CDP 2.0, they decide to introduce a unified solution to clean up the mess. At the computation layer of CDP 2.0, [Apache Doris](https://doris.apache.org) undertakes both real-time and offline data storage and computation. \\n\\nTo ingest **offline data**, they utilize the [Stream Load](https://doris.apache.org/docs/data-operate/import/import-way/stream-load-manual) method. Their 30-thread ingestion test shows that it can perform over 300,000 upserts per second. To load **real-time data**, they use a combination of [Flink-Doris-Connector](https://doris.apache.org/docs/ecosystem/flink-doris-connector) and Stream Load. In addition, in real-time reporting where they need to extract data from multiple external data sources, they leverage the [Multi-Catalog](https://doris.apache.org/docs/lakehouse/multi-catalog/) feature for **federated queries**. \\n\\n![apache doris based-CDP](/images/apache-doris-based-CDP.png)\\n\\nThe customer analytic workflows on this CDP go like this. First, they sort out customer information, then they attach tags to each customer. Based on the tags, they divide customers into groups for more targeted analysis and operation. \\n\\nNext, I\'ll delve into these workloads and show you how Apache Doris accelerates them. \\n\\n## OneID\\n\\nHas this ever happened to you when you have different user registration systems for your products and services? You might collect the email of UserID A from one product webpage, and later the social security number of UserID B from another. Then you find out that UserID A and UserID B actually belong to the same person because they go by the same phone number.\\n\\nThat\'s why OneID arises as an idea. It is to pool the user registration information of all business lines into one large table in Apache Doris, sort it out, and make sure that one user has a unique OneID. \\n\\nThis is how they figure out which registration information belongs to the same user leveraging the functions in Apache Doris.\\n\\n![apache doris OneID](/images/apache-doris-OneID.png)\\n\\n## Tagging services\\n\\nThis CDP accommodates information of **500 million customers**, which come from over **500 source tables** and are attached to over **2000 tags** in total.\\n\\nBy timeliness, the tags can be divided into real-time tags and offline tags. The real-time tags are computed by Apache Flink and written into the flat table in Apache Doris, while the offline tags are computed by Apache Doris as they are derived from the user attribute table, business table, and user behavior table in Doris. Here is the company\'s best practice in data tagging:  \\n\\n**1. Offline tags:**\\n\\nDuring the peaks of data writing, a full update might easily cause an OOM error given their huge data scale. To avoid that, they utilize the [INSERT INTO SELECT](https://doris.apache.org/docs/data-operate/import/import-way/insert-into-manual) function of Apache Doris and enable **partial column update**. This will cut down memory consumption by a lot and maintain system stability during data loading.\\n\\n```SQL\\nset enable_unique_key_partial_update=true;\\ninsert into tb_label_result(one_id, labelxx) \\nselect one_id, label_value as labelxx\\nfrom .....\\n```\\n\\n**2. Real-time tags:**\\n\\nPartial column update is also available for real-time tags, since even real-time tags are updated at different paces. All that is needed is to set `partial_columns` to `true`.\\n\\n```SQL\\ncurl --location-trusted -u root: -H \\"partial_columns:true\\" -H \\"column_separator:,\\" -H \\"columns:id,balance,last_access_time\\" -T /tmp/test.csv http://127.0.0.1:48037/api/db1/user_profile/_stream_load\\n```\\n\\n**3. High-concurrency point queries:**\\n\\nWith its current business size, the company is receiving query requests for tags at a concurrency level of over 5000 QPS. They use a combination of strategies to guarantee high performance. Firstly, they adopt [Prepared Statement](https://doris.apache.org/docs/query-acceleration/hight-concurrent-point-query#using-preparedstatement) for pre-compilation and pre-execution of SQL. Secondly, they fine-tune the parameters for Doris Backend and the tables to optimize storage and execution. Lastly, they enable [row cache](https://doris.apache.org/docs/query-acceleration/hight-concurrent-point-query#enable-row-cache) as a complement to the column-oriented Apache Doris.\\n\\n- Fine-tune Doris Backend parameters in `be.conf`:\\n\\n```SQL\\ndisable_storage_row_cache = false                      \\nstorage_page_cache_limit=40%\\n```\\n\\n- Fine-tune table parameters upon table creation:\\n\\n```SQL\\nenable_unique_key_merge_on_write = true\\nstore_row_column = true\\nlight_schema_change = true\\n```\\n\\n**4. Tag computation (join):**\\n\\nIn practice, many tagging services are implemented by multi-table joins in the database. That often involves more than 10 tables. For optimal computation performance, they adopt the [colocation group](https://doris.apache.org/docs/query-acceleration/join-optimization/colocation-join) strategy in Doris.  \\n\\n\\n## Customer Grouping\\n\\nThe customer grouping pipeline in CDP 2.0 goes like this: Apache Doris receives SQL from customer service, executes the computation, and sends the result set to S3 object storage via SELECT INTO OUTFILE. The company has divided their customers into 1 million groups. The customer grouping task that used to take **50 seconds in Impala** to finish now only needs **10 seconds in Doris**. \\n\\n![apache doris customer grouping](/images/apache-doris-customer-grouping.png)\\n\\nApart from grouping the customers for more fine-grained analysis, sometimes they do analysis in a reverse direction. That is, to target a certain customer and find out to which groups he/she belongs. This helps analysts understand the characteristics of customers as well as how different customer groups overlap.\\n\\nIn Apache Doris, this is implemented by the BITMAP functions: `BITMAP_CONTAINS` is a fast way to check if a customer is part of a certain group, and `BITMAP_OR`, `BITMAP_INTERSECT`, and `BITMAP_XOR` are the choices for cross analysis.  \\n\\n![apache doris bitmap](/images/apache-doris-bitmap.png)\\n\\n\\n## Conclusion\\n\\nFrom CDP 1.0 to CDP 2.0, the insurance company adopts Apache Doris, a unified data warehouse, to replace Spark+Impala+HBase+NebulaGraph. That increases their data processing efficiency by breaking down the data silos and streamlining data processing pipelines. In CDP 3.0 to come, they want to group their customer by combining real-time tags and offline tags for more diversified and flexible analysis. The [Apache Doris community](https://join.slack.com/t/apachedoriscommunity/shared_invite/zt-2unfw3a3q-MtjGX4pAd8bCGC1UV0sKcw) and the [VeloDB](https://www.velodb.io) team will continue to be a supporting partner during this upgrade."},{"id":"/release-note-2.0.5","metadata":{"permalink":"/blog/release-note-2.0.5","source":"@site/blog/release-note-2.0.5.md","title":"Apache Doris 2.0.5 just released","description":"Thanks to our community users and developers, about 217 improvements and bug fixes have been made in Doris 2.0.5 version.","date":"2024-02-28T00:00:00.000Z","tags":[{"inline":true,"label":"Release Notes","permalink":"/blog/tags/release-notes"}],"hasTruncateMarker":false,"authors":[{"name":"Apache Doris","key":null,"page":null}],"frontMatter":{"title":"Apache Doris 2.0.5 just released","description":"Thanks to our community users and developers, about 217 improvements and bug fixes have been made in Doris 2.0.5 version.","date":"2024-02-28","author":"Apache Doris","tags":["Release Notes"],"image":"/images/2.0.5.png"},"unlisted":false,"prevItem":{"title":"Breaking down data silos with a unified data warehouse: an Apache Doris-based CDP","permalink":"/blog/breaking-down-data-silos-with-an-apache-doris-based-cdp"},"nextItem":{"title":"A financial anti-fraud solution based on the Apache Doris data warehouse","permalink":"/blog/a-financial-anti-fraud-solution-based-on-the-apache-doris-data-warehouse"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n  http://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\nThanks to our community users and developers, about 217 improvements and bug fixes have been made in Doris 2.0.5 version.\\n\\n**Quick Download:** [https://doris.apache.org/download/](https://doris.apache.org/download/)\\n\\n**GitHub\uFF1A** [https://github.com/apache/doris/releases](https://github.com/apache/doris/releases)\\n\\n\\n## Behavior changes\\n- change char function behaviour: select char(0) = \'\\\\0\' return true as MySQL\\n  - https://github.com/apache/doris/pull/30034\\n- Allow exporting empty data\\n  - https://github.com/apache/doris/pull/30703\\n\\n## New features\\n- Eliminate left outer join with is null condition\\n- Add show-tablets-belong stmt for analyzing a batch of tablet-ids\\n- InferPredicates support In, such as a = b & a in [1, 2] -> b in [1, 2]\\n- Optimize plan when column stats are unavailable\\n- Optimize plan using rollup column stats\\n- Support analyze materialized view\\n- Support ShowProcessStmt Show all Fe connection\\n\\n## Improvements\\n- Optimize query plan when column stats are unaviable\\n- Optimize query plan using rollup column stats\\n- Stop analyze quickly after user close auto analyze\\n- Catch load column stats exception, avoid print too much stack info to fe.out\\n- Select materialized view by specify the view name in sql\\n- Change auto analyze max table width default value to 100\\n- Escape characters for columns in recovery predicate pushdown in jdbc catalog\\n- Fix jdbc mysql catalog to_date fun pushdown\\n- Optimize the close logic of JDBC client\\n- Optimize jdbc connection pool parameter settings\\n- Obtain hudi partition information through HMS\'s API\\n- Optimize routine load job error msg and memory\\n- Skip all backup/restore jobs if max allowd option is set to 0\\n\\nSee the complete list of improvements and bug fixes on [github](https://github.com/apache/doris/compare/2.0.4-rc06...2.0.5-rc02).\\n\\n\\n## Credits\\nThanks all who contribute to this release:\\n\\nairborne12, alexxing662, amorynan, AshinGau, BePPPower, bingquanzhao, BiteTheDDDDt, ByteYue, caiconghui, cambyzju, catpineapple, dataroaring, eldenmoon, Emor-nj, englefly, felixwluo, GoGoWen, HappenLee, hello-stephen, HHoflittlefish777, HowardQin, JackDrogon, jacktengg, jackwener, Jibing-Li, KassieZ, LemonLiTree, liaoxin01, liugddx, LuGuangming, morningman, morrySnow, mrhhsg, Mryange, mymeiyi, nextdreamblue, qidaye, ryanzryu, seawinde,starocean999, TangSiyang2001, vinlee19, w41ter, wangbo, wsjz, wuwenchi, xiaokang, XieJiann, xingyingone, xy720,xzj7019, yujun777, zclllyybb, zhangstar333, zhannngchen, zhiqiang-hhhh, zxealous, zy-kkk, zzzxl1993"},{"id":"/a-financial-anti-fraud-solution-based-on-the-apache-doris-data-warehouse","metadata":{"permalink":"/blog/a-financial-anti-fraud-solution-based-on-the-apache-doris-data-warehouse","source":"@site/blog/a-financial-anti-fraud-solution-based-on-the-apache-doris-data-warehouse.md","title":"A financial anti-fraud solution based on the Apache Doris data warehouse","description":"Financial fraud prevention is a race against time. This post will get into details about how a retail bank builds their fraud risk management platform based on Apache Doris and how it performs. ","date":"2024-02-22T00:00:00.000Z","tags":[{"inline":true,"label":"Best Practice","permalink":"/blog/tags/best-practice"}],"hasTruncateMarker":false,"authors":[{"name":"velodb.io \xb7VeloDB Engineering Team","key":null,"page":null}],"frontMatter":{"title":"A financial anti-fraud solution based on the Apache Doris data warehouse","description":"Financial fraud prevention is a race against time. This post will get into details about how a retail bank builds their fraud risk management platform based on Apache Doris and how it performs. ","date":"2024-02-22","author":"velodb.io \xb7VeloDB Engineering Team","externalLink":"https://www.velodb.io/blog/165","tags":["Best Practice"],"image":"/images/a-financial-anti-fraud-solution-based-on-the-apache-doris-data-warehouse.png"},"unlisted":false,"prevItem":{"title":"Apache Doris 2.0.5 just released","permalink":"/blog/release-note-2.0.5"},"nextItem":{"title":"A deep dive into inverted index: how it speeds up text searches by 40 times","permalink":"/blog/inverted-index-accelerates-text-searches-by-40-time-apache-doris"}},"content":"\x3c!-- \\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n\\nFinancial fraud prevention is a race against time. Implementation-wise, it relies heavily on the data processing power, especially under large datasets. Today I\'m going to share with you the use case of a retail bank with over 650 million individual customers. They have compared analytics components including [Apache Doris](https://doris.apache.org), ClickHouse, Greenplum, Cassandra, and Kylin. After 5 rounds of deployment and comparison based on 89 custom test cases, they settled on Apache Doris, because they witnessed a six-fold writing speed and faster multi-table joins in Apache Doris as compared to the mighty ClickHouse.\\n\\nI will get into details about how the bank builds their fraud risk management platform based on Apache Doris and how it performs. \\n\\n\\n\\n## Fraud Risk Management Platform\\n\\nIn this platform, **80% of ad-hoc queries** return results in less than **2 seconds,** and **95%** of them are finished in under **5 seconds.** On average, the solution **intercepts tens of thousands of suspicious transactions** every day and **avoids losses of millions of dollars** for bank customers. \\n\\nThis is an overview of the entire platform from an architectural perspective. \\n\\n![Fraud Risk Management Platform](/images/fraud-risk-management-platform.png)\\n\\nThe **source data** can be roughly categorized as:\\n\\n- Dimension data: mostly stored in PostgreSQL\\n- Real-time transaction data: decoupled from various external systems via Kafka message queues\\n- Offline data: directly ingested from external systems to Hive, making data reconciliation easy\\n\\n\\n\\nFor **data ingestion**, this is how they collect the three types of source data. First of all, they leverage the [JDBC Catalog](https://doris.apache.org/docs/lakehouse/multi-catalog/jdbc) to to synchronize metadata and user data from PostgreSQL.  \\n\\nThe transaction data needs to be combined with dimension data for further analysis. Thus, they employ a Flink SQL API to read dimension data from PostgreSQL, and real-time transaction data from Kafka. Then, in Flink, they do multi-stream joins and generate wide tables. For real-time refreshing of dimension tables, they use a Lookup Join mechanism, which dynamically looks up and refreshes dimension data when processing data streams. They also utilize Java UDFs to serve their specific needs in ETL. After that, they write the data into Apache Doris via the[ Flink-Doris-Connector](https://doris.apache.org/docs/ecosystem/flink-doris-connector/). \\n\\nThe offline data is cleaned, transformed, and written into Hive, Kafka, and PostgreSQL, for which Doris creates catalogs as mappings, based on its [Multi-Catalog](https://doris.apache.org/docs/lakehouse/multi-catalog/) capability, to facilitate federated analysis. In this process, Hive Metastore is in place to access and refresh data from Hive automatically.\\n\\nIn terms of **data modeling**, they use Apache Doris as a data warehouse and apply different [data models](https://doris.apache.org/docs/data-table/data-model) for different layers. Each layer aggregates or rolls up data from the previous layer at a coarser granularity. Eventually, it produces a highly aggregated Rollup or Materialized View. \\n\\nNow let me show you what analytics tasks are running on this platform. Based on the scale of monitoring and human involvement, these tasks can be divided into real-time risk reporting, multi-dimensional analysis, federated queries, and auto alerting. \\n\\n\\n\\n## Real-time risk report\\n\\nWhen it comes to fraud prevention, what is diminishing the effectiveness of your anti-fraud efforts? It is incomplete exposure of potential risks and untimely risk identification. That\'s why people always want real-time, full-scale monitoring and reporting.\\n\\nThe bank\'s solution to that is built on Apache Flink and Apache Doris. First of all, they put together the 17 dimensions. After cleaning, aggregation, and other computations, they visualize the data on a real-time dashboard. \\n\\nAs for **scale**, it analyzes the workflows of **over 10 million customers, 30,000 clerks, 10,000 branches, and 1000 products**. \\n\\nAs for **speed**, the bank now has evolved from next-day data refreshing to near real-time data processing. Targeted analysis can be done within minutes instead of hours. The solution also supports complicated ad-hoc queries to capture underlying risks by monitoring how the data models and rules run. \\n\\n\\n\\n## Multi-dimensional analysis to identify risks\\n\\nCase tracing is another common anti-fraud practice. The bank has a fraud model library. Based on the fraud models, they analyze the risks of each transaction and visualize the results in near real time, so their staff can take prompt measures if needed. \\n\\nFor that purpose, they use Apache Doris for **multi-dimensional analysis** of cases. They check the patterns of transactions, including sources, types, and time, for a comprehensive overview. During this process, they often need to combine **over 10 filtering conditions** of different dimensions. This is empowered by the **ad-hoc query** capabilities of Apache Doris. Both rule-based matching and list-based matching of cases can be done **within seconds** without manual efforts.\\n\\n\\n\\n## Federated queries to locate risk details\\n\\nApart from identifying risks from each transaction, the bank also receives risk reports from customers. In these cases, the corresponding transaction will be labeled as \\"risky\\", and it will be categorized and recorded in the ticketing system. The labels make sure that the high-risk transactions are promptly attended to. \\n\\nOne problem is that, the ticketing system is overloaded with such data, so it is not able to directly present all the details of the risky transactions. What needs to be done is to relate the tickets to the transaction details so the bank staff can locate the actual risks. \\n\\nHow is that implemented? Every day, Apache Doris traverses the incremental tickets and the basic information table to get the ticket IDs, and then it relates the ticket IDs to the dimension data stored in itself. At the end, the ticket details are presented at the frontend of Doris. This entire process takes **only a few minutes**. This is a big game change compared to the old time when they had to manually look up the suspicious transaction.\\n\\n\\n\\n## Auto alerting\\n\\nBased on Apache Doris, the bank designs their own alerting rules, models, and strategies. The system monitors how everything runs. Once it detects a situation that matches the alert rules, it will trigger an alarm. They have also established a real-time feedback mechanism for the alerting rules, so if a newly added rule causes any negative effects, it will be adjusted or removed rapidly. \\n\\nSo far, the bank has added nearly 100 alerting rules for various risk types to the system. During the past two months, **over 100 alarms** were issued with **over 95% accuracy** in less than **5 seconds** after the risk situation arises.  \\n\\n\\n\\n## Conclusion\\n\\nFor a comprehensive anti-fraud solution, the bank conducts full-scale real-time monitoring and reporting for all their data workflows. Then, for each transaction, they look into the multiple dimensions of it to identify risks. For the suspicious transactions reported by the bank customers, they perform federated queries to retrieve the full details of them. Also, an auto alerting mechanism is always on patrol to safeguard the whole system. These are the various types of analytic workloads in this solution. The implementation of them rely on the capabilities of Apache Doris, which is a data warehouse designed to be an all-in-one platform for various workloads. If you try to build your own anti-fraud solution, the [Apache Doris open source developers](https://join.slack.com/t/apachedoriscommunity/shared_invite/zt-2unfw3a3q-MtjGX4pAd8bCGC1UV0sKcw) are happy to exchange ideas with you."},{"id":"/inverted-index-accelerates-text-searches-by-40-time-apache-doris","metadata":{"permalink":"/blog/inverted-index-accelerates-text-searches-by-40-time-apache-doris","source":"@site/blog/inverted-index-accelerates-text-searches-by-40-time-apache-doris.md","title":"A deep dive into inverted index: how it speeds up text searches by 40 times","description":"As an open-source real-time data warehouse, Apache Doris provides a rich choice of indexes to speed up data scanning and filtering.This post is a deep dive into inverted index and NGram BloomFilter index, providing a hands-on guide to applying them for various queries.","date":"2024-02-01T00:00:00.000Z","tags":[{"inline":true,"label":"Tech Sharing","permalink":"/blog/tags/tech-sharing"}],"hasTruncateMarker":false,"authors":[{"name":"velodb.io \xb7 VeloDB Engineering Team","key":null,"page":null}],"frontMatter":{"title":"A deep dive into inverted index: how it speeds up text searches by 40 times","description":"As an open-source real-time data warehouse, Apache Doris provides a rich choice of indexes to speed up data scanning and filtering.This post is a deep dive into inverted index and NGram BloomFilter index, providing a hands-on guide to applying them for various queries.","date":"2024-02-01","author":"velodb.io \xb7 VeloDB Engineering Team","externalLink":"https://www.velodb.io/blog/163","tags":["Tech Sharing"],"image":"/images/how-inverted-index-accelerates-text-searches-by-40-times.png"},"unlisted":false,"prevItem":{"title":"A financial anti-fraud solution based on the Apache Doris data warehouse","permalink":"/blog/a-financial-anti-fraud-solution-based-on-the-apache-doris-data-warehouse"},"nextItem":{"title":"Apache Doris 2.0.4 just released","permalink":"/blog/release-note-2.0.4"}},"content":"\x3c!-- \\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\nAs an open-source real-time data warehouse, [Apache Doris](https://doris.apache.org) provides a rich choice of indexes to speed up data scanning and filtering. Based on user involvement, they can be divided into built-in smart indexes and user-created indexes. The former is automatically generated by Apache Doris on data ingestion, such as ZoneMap index and prefix index, while the latter is the index users choose for various use cases, including inverted index and NGram BloomFilter index.\\n\\nThis post is a deep dive into inverted index and NGram BloomFilter index, providing a hands-on guide to applying them for various queries.\\n\\n## Sample dataset\\n\\nThe test dataset comprises about 130 million Amazon customer reviews. It is a few Snappy-compressed Parquet files with a total size of 37GB. These are a few samples:\\n\\n![img](/images/sample-dataset.png)\\n\\nEach row includes 15 columns including `customer_id`, `review_id`, `product_id`, `product_category`, `star_rating`, `review_headline`, and `review_body`. \\n\\nA lot of these columns can be accelerated by indexes based on their structures. For example, `customer_id` is a high-cardinality numerical field while `product_id` is a low-cardinality fixed-length text field, and `product_title` and `review_body` are short and long text fields, respectively.\\n\\nQueries on these columns can be roughly divided into two types:\\n\\n- **Text searches**: searches for certain contents in the `review_body` field.\\n- **Non-primary key column queries**: query reviews about certain `product_id` or from certain `customer_id`.\\n\\nThese are also the main threads of this article. I will present to you how indexes can speed up these queries.\\n\\n## Prerequisites\\n\\nFor a quick run, here we use a single-node cluster (1 frontend, 1 backend).\\n\\n1. Deploy Apache Doris: refer to [Quick Start](https://doris.apache.org/docs/get-starting/quick-start/)\\n2. Create a table using the following statements: \\n\\n```SQL\\nCREATE TABLE `amazon_reviews` (  \\n  `review_date` int(11) NULL,  \\n  `marketplace` varchar(20) NULL,  \\n  `customer_id` bigint(20) NULL,  \\n  `review_id` varchar(40) NULL,\\n  `product_id` varchar(10) NULL,\\n  `product_parent` bigint(20) NULL,\\n  `product_title` varchar(500) NULL,\\n  `product_category` varchar(50) NULL,\\n  `star_rating` smallint(6) NULL,\\n  `helpful_votes` int(11) NULL,\\n  `total_votes` int(11) NULL,\\n  `vine` boolean NULL,\\n  `verified_purchase` boolean NULL,\\n  `review_headline` varchar(500) NULL,\\n  `review_body` string NULL\\n) ENGINE=OLAP\\nDUPLICATE KEY(`review_date`)\\nCOMMENT \'OLAP\'\\nDISTRIBUTED BY HASH(`review_date`) BUCKETS 16\\nPROPERTIES (\\n\\"replication_allocation\\" = \\"tag.location.default: 1\\",\\n\\"compression\\" = \\"ZSTD\\"\\n);\\n```\\n\\n3.  Download datasets: Snappy-compressed Parquet files with a total size of 37GB\\n   - [amazon_reviews_2010](https://datasets-documentation.s3.eu-west-3.amazonaws.com/amazon_reviews/amazon_reviews_2010.snappy.parquet)\\n   - [amazon_reviews_2011](https://datasets-documentation.s3.eu-west-3.amazonaws.com/amazon_reviews/amazon_reviews_2011.snappy.parquet)\\n   - [amazon_reviews_2012](https://datasets-documentation.s3.eu-west-3.amazonaws.com/amazon_reviews/amazon_reviews_2012.snappy.parquet)\\n   - [amazon_reviews_2013](https://datasets-documentation.s3.eu-west-3.amazonaws.com/amazon_reviews/amazon_reviews_2013.snappy.parquet)\\n   - [amazon_reviews_2014](https://datasets-documentation.s3.eu-west-3.amazonaws.com/amazon_reviews/amazon_reviews_2014.snappy.parquet) \\n   - [amazon_reviews_2015](https://datasets-documentation.s3.eu-west-3.amazonaws.com/amazon_reviews/amazon_reviews_2015.snappy.parquet)\\n4. Execute the following commands to load the datasets\\n\\n```shell\\ncurl --location-trusted -u root: -T amazon_reviews_2010.snappy.parquet -H \\"format:parquet\\" http://${BE_IP}:${BE_PORT}/api/${DB}/amazon_reviews/_stream_load\\ncurl --location-trusted -u root: -T amazon_reviews_2011.snappy.parquet -H \\"format:parquet\\" http://${BE_IP}:${BE_PORT}/api/${DB}/amazon_reviews/_stream_load\\ncurl --location-trusted -u root: -T amazon_reviews_2012.snappy.parquet -H \\"format:parquet\\" http://${BE_IP}:${BE_PORT}/api/${DB}/amazon_reviews/_stream_load\\ncurl --location-trusted -u root: -T amazon_reviews_2013.snappy.parquet -H \\"format:parquet\\" http://${BE_IP}:${BE_PORT}/api/${DB}/amazon_reviews/_stream_load\\ncurl --location-trusted -u root: -T amazon_reviews_2014.snappy.parquet -H \\"format:parquet\\" http://${BE_IP}:${BE_PORT}/api/${DB}/amazon_reviews/_stream_load\\ncurl --location-trusted -u root: -T amazon_reviews_2015.snappy.parquet -H \\"format:parquet\\" http://${BE_IP}:${BE_PORT}/api/${DB}/amazon_reviews/_stream_load\\n```\\n\\n5. Check and verify: After the above steps, execute the following statements in the MySQL client to check and see the size of the dataset. It can be seen from below that 135589433 rows are loaded and they take up 25.873GB in Apache Doris, which is 30% smaller than the original Parquet files. \\n\\n```SQL\\nmysql> SELECT COUNT() FROM amazon_reviews;\\n+-----------+\\n| count(*)  |\\n+-----------+\\n| 135589433 |\\n+-----------+\\n1 row in set (0.02 sec)\\nmysql> SHOW DATA FROM amazon_reviews;\\n+----------------+----------------+-----------+--------------+-----------+------------+\\n| TableName      | IndexName      | Size      | ReplicaCount | RowCount  | RemoteSize |\\n+----------------+----------------+-----------+--------------+-----------+------------+\\n| amazon_reviews | amazon_reviews | 25.873 GB | 16           | 135589433 | 0.000      |\\n|                | Total          | 25.873 GB | 16           |           | 0.000      |\\n+----------------+----------------+-----------+--------------+-----------+------------+\\n2 rows in set (0.00 sec)\\n```\\n\\n## Accelerate text searches\\n\\n### No index\\n\\nNow let\'s try running text searches on the `review_body` field. Specifically, we\'re trying to retrieve the top 5 products whose reviews include the keywords \\"is super awesome\\". The results should be sorted in descending order based on the number of reviews. Each result should include the product ID, a randomly selected product title, the average star rating, and the total number of reviews. \\n\\nThis is the query statement:\\n\\n```SQL\\nSELECT\\n    product_id,\\n    any(product_title),\\n    AVG(star_rating) AS rating,\\n    COUNT() AS count\\nFROM\\n    amazon_reviews\\nWHERE\\n    review_body LIKE \'%is super awesome%\'\\nGROUP BY\\n    product_id\\nORDER BY\\n    count DESC,\\n    rating DESC,\\n    product_id\\nLIMIT 5;\\n```\\n\\nSince the `review_body` field contains lengthy reviews, such text searches can be time-consuming. Without enabling any indexes, it took **7.6 seconds** to return the results: \\n\\n```SQL\\n+------------+------------------------------------------+--------------------+-------+\\n| product_id | any_value(product_title)                 | rating             | count |\\n+------------+------------------------------------------+--------------------+-------+\\n| B00992CF6W | Minecraft                                | 4.8235294117647056 |    17 |\\n| B009UX2YAC | Subway Surfers                           | 4.7777777777777777 |     9 |\\n| B00DJFIMW6 | Minion Rush: Despicable Me Official Game |              4.875 |     8 |\\n| B0086700CM | Temple Run                               |                  5 |     6 |\\n| B00KWVZ750 | Angry Birds Epic RPG                     |                  5 |     6 |\\n+------------+------------------------------------------+--------------------+-------+\\n5 rows in set (7.60 sec)\\n```\\n\\n### Ngram BloomFilter index\\n\\nNow, let\'s try accelerating such text searches using the Ngram BloomFilter index.\\n\\n- `gram_size`: the value of \\"N\\" in \\"Ngram\\", representing the length of consecutive characters. In the snippet below, `\\"gram_size\\"=\\"10\\"` means that the texts will be divided into a number of 10-character strings, which are the basis of the Ngram BloomFilter index.\\n- `bf_size`: the size of the BloomFilter in bytes. `\\"bf_size\\"=\\"10240\\"` indicates that the BloomFilter occupies 10240 bytes of space.\\n\\n```SQL\\nALTER TABLE amazon_reviews ADD INDEX review_body_ngram_idx(review_body) USING NGRAM_BF PROPERTIES(\\"gram_size\\"=\\"10\\", \\"bf_size\\"=\\"10240\\");\\n```\\n\\nThis time, the query is finished within **0.93 seconds**. That means Ngram BloomFilter brings a speedup of **8 times**.\\n\\n```SQL\\n+------------+------------------------------------------+--------------------+-------+\\n| product_id | any_value(product_title)                 | rating             | count |\\n+------------+------------------------------------------+--------------------+-------+\\n| B00992CF6W | Minecraft                                | 4.8235294117647056 |    17 |\\n| B009UX2YAC | Subway Surfers                           | 4.7777777777777777 |     9 |\\n| B00DJFIMW6 | Minion Rush: Despicable Me Official Game |              4.875 |     8 |\\n| B0086700CM | Temple Run                               |                  5 |     6 |\\n| B00KWVZ750 | Angry Birds Epic RPG                     |                  5 |     6 |\\n+------------+------------------------------------------+--------------------+-------+\\n5 rows in set (0.93 sec)\\n```\\n\\n**So how does Ngram BloomFilter do the magic?** The way it works can be explained in two parts.\\n\\n- **Ngram tokenization**: When `gram_size=5`, the phrase \\"hello world\\" is split into [\\"hello\\", \\"ello \\", \\"llo w\\", \\"lo wo\\", \\"o wor\\", \\" worl\\", \\"world\\"]. These sub-strings are then hashed and added to a BloomFilter of the `bf_size`. Since data in Apache Doris is stored by page, the BloomFilters are generated also by page. \\n- **Query acceleration**: For example, to query the word \\"hello\\" in the texts, \\"hello\\" is tokenized and compared with the BloomFilters of each page. If the BloomFilter detects a potential match (there might be false positives) in a page, that page is loaded for further matching. Otherwise, that page is skipped. \\n\\nBy skipping the irrelevant pages, the BloomFilter index reduces unnecessary data scanning and thus greatly reduces query latency.\\n\\n![img](/images/data-storage-structure-in-apache-doris.png)\\n\\n<div style={{textAlign:\'center\'}}> Data storage structure in Apache Doris </div >\\n\\n![img](/images/illustration-of-ngram-bloomfilter.png)\\n\\n<div style={{textAlign:\'center\'}}> Illustration of Ngram BloomFilter </div >\\n\\n**How to find the optimal parameter configurations for Ngram BloomFilter?**\\n\\n`gram_size` determines the matching efficiency, while `bf_size` impacts the false positive rate. Typically, a large `bf_size` reduces the false positive rate but also requires more storage space. Thus, we suggest that you configure these two parameters based on these two factors: \\n\\n1. Text length:\\n\\n   - For short texts (words or phrases), a small `gram_size` (2~4) and a small `bf_size` are recommended.\\n   - For long texts (sentences or paragraphs), a large `gram_size` (5~10) and a large `bf_size` work better.\\n\\n2. Query pattern: \\n\\n   - If the queries often involve phrases or complete words, a large `gram_size` will be more efficient.\\n   - For fuzzy matching or diverse queries, a small `gram_size` allows more flexible matching.\\n\\n### Inverted index\\n\\n[Inverted index](https://doris.apache.org/blog/Building-A-Log-Analytics-Solution-10-Times-More-Cost-Effective-Than-Elasticsearch) is another way to accelerate text searches. Creating inverted index is simple:\\n\\n1.  **Add inverted index**: Refer to the snippet below to create inverted index for the `review_body` column of the `amazon_reviews` table. Inverted index supports phrase searching, in which the order of the tokenized words will affect the search results.\\n\\n2. **Add inverted index for historical data**: You can also create inverted index for historical data.\\n\\n```SQL\\nALTER TABLE amazon_reviews ADD INDEX review_body_inverted_idx(`review_body`) \\n    USING INVERTED PROPERTIES(\\"parser\\" = \\"english\\",\\"support_phrase\\" = \\"true\\"); \\nBUILD INDEX review_body_inverted_idx ON amazon_reviews;\\n```\\n\\n3. **Check and verify**: You can check and see the created indexes using the following statement:\\n\\n```SQL\\nmysql> show BUILD INDEX WHERE TableName=\\"amazon_reviews\\";\\n+-------+----------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------+-------------------------+-------------------------+---------------+----------+------+----------+\\n| JobId | TableName      | PartitionName  | AlterInvertedIndexes                                                                                                              | CreateTime              | FinishTime              | TransactionId | State    | Msg  | Progress |\\n+-------+----------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------+-------------------------+-------------------------+---------------+----------+------+----------+\\n| 10152 | amazon_reviews | amazon_reviews | [ADD INDEX review_body_inverted_idx (\\nreview_body\\n) USING INVERTED PROPERTIES(\\"parser\\" = \\"english\\", \\"support_phrase\\" = \\"true\\")],  | 2024-01-23 15:42:28.658 | 2024-01-23 15:48:42.990 | 11            | FINISHED |      | NULL     |\\n+-------+----------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------+-------------------------+-------------------------+---------------+----------+------+----------+\\n1 row in set (0.00 sec)\\n```\\n\\nIf you want to see how tokenization works, you can test with the `TOKENIZE` function. Just input the text that needs to be tokenized and the parameters: \\n\\n```SQL\\nmysql> SELECT TOKENIZE(\'I can honestly give the shipment and package 100%, it came in time that it was supposed to with no hasels, and the book was in PERFECT condition.\\nsuper awesome buy, and excellent for my college classs\', \'\\"parser\\" = \\"english\\",\\"support_phrase\\" = \\"true\\"\');\\n+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\\n| tokenize(\'I can honestly give the shipment and package 100%, it came in time that it was supposed to with no hasels, and the book was in PERFECT condition. super awesome buy, and excellent for my college classs\', \'\\"parser\\" = \\"english\\",\\"support_phrase\\" = \\"true\\"\')                                              |\\n+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\\n| [\\"i\\", \\"can\\", \\"honestly\\", \\"give\\", \\"the\\", \\"shipment\\", \\"and\\", \\"package\\", \\"100\\", \\"it\\", \\"came\\", \\"in\\", \\"time\\", \\"that\\", \\"it\\", \\"was\\", \\"supposed\\", \\"to\\", \\"with\\", \\"no\\", \\"hasels\\", \\"and\\", \\"the\\", \\"book\\", \\"was\\", \\"in\\", \\"perfect\\", \\"condition\\", \\"super\\", \\"awesome\\", \\"buy\\", \\"and\\", \\"excellent\\", \\"for\\", \\"my\\", \\"college\\", \\"classs\\"] |\\n+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\\n1 row in set (0.05 sec)\\n```\\n\\nWith inverted index, now we retrieve customer reviews containing \\"is super awesome\\" using `MATCH_PHRASE`. \\n\\n```SQL\\nSELECT\\n    product_id,\\n    any(product_title),\\n    AVG(star_rating) AS rating,\\n    COUNT() AS count\\nFROM\\n    amazon_reviews\\nWHERE\\n    review_body MATCH_PHRASE \'is super awesome\'\\nGROUP BY\\n    product_id\\n\\nORDER BY\\n    count DESC,\\n    rating DESC,\\n    product_id\\nLIMIT 5;\\n```\\n\\nThe clause `review_body MATCH_PHRASE \'is super awesome\'` searches for text fragments in the `review_body` column that contains all three keywords \\"is\\", \\"super\\", and \\"awesome\\" in that exact order, with no other words in between. \\n\\nThe `MATCH` query is case-insensitive, which is also what sets it apart from the `LIKE` query. The `MATCH` query is more efficient in large datasets.\\n\\nResults show that inverted index has decreased the query latency to **0.19 seconds**, bringing a **4-time performance increase** compared to the Ngram BloomFilter index, and **a nearly 40-time increase** compared to having no indexes at all.\\n\\n```SQL\\n+------------+------------------------------------------+-------------------+-------+\\n| product_id | any_value(product_title)                 | rating            | count |\\n+------------+------------------------------------------+-------------------+-------+\\n| B00992CF6W | Minecraft                                | 4.833333333333333 |    18 |\\n| B009UX2YAC | Subway Surfers                           |               4.7 |    10 |\\n| B00DJFIMW6 | Minion Rush: Despicable Me Official Game |                 5 |     7 |\\n| B0086700CM | Temple Run                               |                 5 |     6 |\\n| B00KWVZ750 | Angry Birds Epic RPG                     |                 5 |     6 |\\n+------------+------------------------------------------+-------------------+-------+\\n5 rows in set (0.19 sec)\\n```\\n\\n\\n\\n**How does inverted index make it possible?**\\n\\nInverted index splits the texts into words and maps each word to a row number. Then the tokenized words are sorted alphabetically and and a skip list index is created. When executing queries of specific words, the system locates the row numbers in this orderly mapping using the skip list index and binary search methods. Based on the row numbers, the system retrieves the entire data record. \\n\\nThis approach avoids line-by-line matching and reduces computational complexity from O(n) to O(logn). That\'s how inverted index speeds up queries on large datasets. \\n\\n![img](/images/illustration-of-inverted-index.png)\\n\\n\\n<div style={{textAlign:\'center\'}}> Illustration of Inverted Index </div >\\n\\nTo provide a deeper understanding of inverted index, I will start from its read/write logic. In Doris, logically, inverted index is applied at the column level of a table. However, from a physical storage and implementation perspective, it is actually built on data files. \\n\\n- **Writing**: When data is written to a data file, it is also synchronously written to the inverted index file, and the row numbers are matched.\\n- **Query**: In a query, if the `WHERE` condition involves a column for which an inverted index has been built, Doris will go directly to the index file and returns the corresponding row numbers. Then, based on the row numbers, it skips the irrelevant pages and rows and only reads the target rows. \\n\\nIn short, inverted index enables high-speed text searches by mapping, and its implementation relies on the coordination of data files and index files.\\n\\n## Accelerate non-primary key column queries\\n\\nTo showcase the impact of inverted index on non-primary key column queries, let\'s try some multi-dimensional queries.\\n\\n### No index\\n\\nRetrieve the review from Customer ID 13916588 about Product ID B002DMK1R0. Without indexes, the system has to scan the entire table. The query is finished within **1.81 seconds**.\\n\\n```SQL\\nmysql> SELECT product_title,review_headline,review_body,star_rating \\nFROM amazon_reviews \\nWHERE product_id=\'B002DMK1R0\' AND customer_id=13916588;\\n+-----------------------------------------------------------------+----------------------+-----------------------------------------------------------------------------------------------------------------------------+-------------+\\n| product_title                                                   | review_headline      | review_body                                                                                                                 | star_rating |\\n+-----------------------------------------------------------------+----------------------+-----------------------------------------------------------------------------------------------------------------------------+-------------+\\n| Magellan Maestro 4700 4.7-Inch Bluetooth Portable GPS Navigator | Nice Features But... | This is a great GPS. Gets you where you are going. Don\'t forget to buy the separate (grr!) cord for the traffic kit though! |           4 |\\n+-----------------------------------------------------------------+----------------------+-----------------------------------------------------------------------------------------------------------------------------+-------------+\\n1 row in set (1.81 sec)\\n```\\n\\n### Inverted index\\n\\nThis query is executed in a different way than what is said above, because the system does not have to tokenize the `product_id` and `customer_id`, but creates a Value\u2192RowID inverted index table. \\n\\nFirst of all, create inverted index via the following statement: \\n\\n```SQL\\nALTER TABLE amazon_reviews ADD INDEX product_id_inverted_idx(product_id) USING INVERTED ;\\nALTER TABLE amazon_reviews ADD INDEX customer_id_inverted_idx(customer_id) USING INVERTED ;\\nBUILD INDEX product_id_inverted_idx ON amazon_reviews;\\nBUILD INDEX customer_id_inverted_idx ON amazon_reviews;\\n```\\n\\nWith inverted index, the same query is finished within **0.06 seconds**. That represents a **30-time** higher speed compared to the previous 1.81 seconds.\\n\\n```SQL\\nmysql> SELECT product_title,review_headline,review_body,star_rating FROM amazon_reviews WHERE product_id=\'B002DMK1R0\' AND customer_id=\'13916588\';\\n+-----------------------------------------------------------------+----------------------+-----------------------------------------------------------------------------------------------------------------------------+-------------+\\n| product_title                                                   | review_headline      | review_body                                                                                                                 | star_rating |\\n+-----------------------------------------------------------------+----------------------+-----------------------------------------------------------------------------------------------------------------------------+-------------+\\n| Magellan Maestro 4700 4.7-Inch Bluetooth Portable GPS Navigator | Nice Features But... | This is a great GPS. Gets you where you are going. Don\'t forget to buy the separate (grr!) cord for the traffic kit though! |           4 |\\n+-----------------------------------------------------------------+----------------------+-----------------------------------------------------------------------------------------------------------------------------+-------------+\\n1 row in set (0.06 sec)\\n```\\n\\n### Profile\\n\\nThis is an excerpt of the SegmentIterator Profile, from which you can tell why inverted index accelerates query execution.\\n\\n(Note that if you need to check the Profile of a query, make sure you have executed `SET enable_profile=true;` in the MySQL client before you execute the query. Then you can check the Profile at *http://FE_IP:FE_HTTP_PORT/QueryProfile*)\\n\\n```YAML\\nSegmentIterator:\\n  - FirstReadSeekCount: 0\\n  - FirstReadSeekTime: 0ns\\n  - FirstReadTime: 13.119ms\\n  - IOTimer: 19.537ms\\n  - InvertedIndexQueryTime: 11.583ms\\n  - RawRowsRead: 1\\n  - RowsConditionsFiltered: 0\\n  - RowsInvertedIndexFiltered: 16.907403M (16907403)\\n  - RowsShortCircuitPredInput: 0\\n  - RowsVectorPredFiltered: 0\\n  - RowsVectorPredInput: 0\\n  - ShortPredEvalTime: 0ns\\n  - TotalPagesNum: 27\\n  - UncompressedBytesRead: 3.71 MB\\n  - VectorPredEvalTime: 0ns\\n```\\n\\n`RowsInvertedIndexFiltered: 16.907403M (16907403)` and `RawRowsRead: 1` means that the inverted index has filtered out 16907403 rows and only reads 1 row (the target row). `FirstReadTime: 13.119ms` means that it takes 13.119 ms to read the page where the target row is located, and `InvertedIndexQueryTime: 11.583ms` means that the system **filters out 16907403 rows within only 11.58 ms**. \\n\\nFor comparision, this is the SegmentIterator Profile when no index is used:\\n\\n```YAML\\nSegmentIterator:\\n  - FirstReadSeekCount: 9.374K (9374)\\n  - FirstReadSeekTime: 400.522ms\\n  - FirstReadTime: 3s144ms\\n  - IOTimer: 2s564ms\\n  - InvertedIndexQueryTime: 0ns\\n  - RawRowsRead: 16.680706M (16680706)\\n  - RowsConditionsFiltered: 226.698K (226698)\\n  - RowsInvertedIndexFiltered: 0\\n  - RowsShortCircuitPredInput: 1\\n  - RowsVectorPredFiltered: 16.680705M (16680705)\\n  - RowsVectorPredInput: 16.680706M (16680706)\\n  - RowsZonemapFiltered: 226.698K (226698)\\n  - ShortPredEvalTime: 2.723ms\\n  - TotalPagesNum: 5.421K (5421)\\n  - UncompressedBytesRead: 277.05 MB\\n  - VectorPredEvalTime: 8.114ms\\n```\\n\\nWithout inverted index, it takes 3.14s to load 16680706 rows (`FirstReadTime: 3s144ms`). Then, the system conducts filtering by Predicate Evaluate and screens out 16680705 rows. The conditional filtering process only takes less than 10ms, making original data loading the most time-consuming task.\\n\\nTo sum up, inverted index increases query execution efficiency by enabling quick retrieval of the target rows and thus reducing unnecessary data loading.\\n\\n## Accelerate low-cardinality text column queries\\n\\nSo inverted index is a big accelerator for queries on high-cardinality text columns, but that might raise a concern: For low-cardinality columns, will too many indexes bring excessive overheads and undermine query performance? \\n\\nThe answer is: no. Let me show you why and how. The following example uses `product_category` as the predicate column for filtering. \\n\\n```SQL\\nmysql> SELECT COUNT(DISTINCT product_category) FROM amazon_reviews ;\\n+----------------------------------+\\n| count(DISTINCT product_category) |\\n+----------------------------------+\\n|                               43 |\\n+----------------------------------+\\n1 row in set (0.57 sec)\\n```\\n\\nAs is shown, the `product_category` column has only 43 distinct categories, making it a typical low-cardinality text column. Now, let\'s add inverted index to it.\\n\\n```SQL\\nALTER TABLE amazon_reviews ADD INDEX product_category_inverted_idx(`product_category`) USING INVERTED;\\nBUILD INDEX product_category_inverted_idx ON amazon_reviews;\\n```\\n\\nAfter adding inverted index, run the following SQL query to retrieve the top 3 products with the most reviews in the \\"Mobile_Electronics\\" product category. \\n\\n```SQL\\nSELECT \\n    product_id,\\n    product_title,\\n    AVG(star_rating) AS rating,\\n    any(review_body),\\n    any(review_headline),\\n    COUNT(*) AS count \\nFROM \\n    amazon_reviews \\nWHERE \\n    product_category = \'Mobile_Electronics\' \\nGROUP BY \\n    product_title, product_id \\nORDER BY \\n    count DESC \\nLIMIT 10;\\n```\\n\\nWith inverted index, the query takes 1.54s to finish.\\n\\n```SQL\\n+------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------+-------+\\n| product_id | product_title                                                                                                                                                                                          | rating             | any_value(review_body)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | any_value(review_headline)      | count |\\n+------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------+-------+\\n| B00J46XO9U | iXCC Lightning Cable 3ft, iPhone charger, for iPhone X, 8, 8 Plus, 7, 7 Plus, 6s, 6s Plus, 6, 6 Plus, SE 5s 5c 5, iPad Air 2 Pro, iPad mini 2 3 4, iPad 4th Gen Apple MFi Certified(Black and White) | 4.3766233766233764 | Great cable and works well. Exact fit as Apple cable. I would recommend this to anyone who is looking to save money and for a quality cable.                                                                                                                                                                                                                                                                                                                                                             | Apple certified lightning cable |  1078 |\\n| B004911E9M | Wall AC Charger USB Sync Data Cable for iPhone 4, 3GS, and iPod                                                                                                                                        | 2.4281805745554035 | A total waste of money for me because I needed it for a iPhone 4.  The plug will only go in upside down and thus won\'t work at all.                                                                                                                                                                                                                                                                                                                                                                      | Won\'t work with a iPhone 4!     |   731 |\\n| B002D4IHYM | New Trent Easypak 7000mAh Portable Triple USB Port External Battery Charger/Power Pack for Smartphones, Tablets and more (w/built-in USB cable)                                                        | 4.5216095380029806 | I bought this product based on the reviews that i read and i am very glad that i did. I did have a problem with the product charging my itouch after i received it but i emailed the company and they corrected the problem immediately. VERY GOOD customer service, very prompt. The product itself is very good. It charges my power hungry itouch very quickly and the imax battery power lasts for a long time. All in all a very good purchase that i would recommend to anyone who owns an itouch. | Great product & company         |   671 |\\n+------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------+-------+\\n3 rows in set (1.54 sec)\\n```\\n\\nNow let\'s try again without enabling inverted index. The same query takes 1.8s to finish. (You can simply disable inverted index by executing `set enable_inverted_index_query=false;` in the MySQL client.)\\n\\n```SQL\\n+------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------+-------+\\n| product_id | product_title                                                                                                                                                                                          | rating             | any_value(review_body)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | any_value(review_headline)            | count |\\n+------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------+-------+\\n| B00J46XO9U | iXCC Lightning Cable 3ft, iPhone charger, for iPhone X, 8, 8 Plus, 7, 7 Plus, 6s, 6s Plus, 6, 6 Plus, SE 5s 5c 5, iPad Air 2 Pro, iPad mini 2 3 4, iPad 4th Gen Apple MFi Certified(Black and White) | 4.3766233766233764 | These cables are great. They feel quality, and best of all, they work as they should. I have no issues with them whatsoever and will be buying more when needed.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | Just like the original from Apple     |  1078 |\\n| B004911E9M | Wall AC Charger USB Sync Data Cable for iPhone 4, 3GS, and iPod                                                                                                                                        | 2.4281805745554035 | I ordered two of these chargers for an Iphone 4. Then I started experiencing weird behavior from the touch screen. It would select the wrong area of the screen, or it would refuse to scroll beyond a certain point and jump back up to the top of the page. This behavior occurs whenever either of the two that I bought are attached and charging. When I remove them, it works fine once again. Needless to say, these items are being returned.                                                                                                                                                                                                                                                                                                                                                                              | Beware - these chargers are defective |   731 |\\n| B002D4IHYM | New Trent Easypak 7000mAh Portable Triple USB Port External Battery Charger/Power Pack for Smartphones, Tablets and more (w/built-in USB cable)                                                        | 4.5216095380029806 | I received this in the mail 4 days ago, and after charging it for 6 hours, I\'ve been using it as the sole source for recharging my 3Gs to see how long it would work.  I use my Iphone A LOT every day and usually by the time I get home it\'s down to 50% or less.  After 4 days of using the IMAX to recharge my Iphone, it finally went from 3 bars to 4 this afternoon when I plugged my iphone in.  It charges the iphone very quickly, and I\'ve been topping my phone off (stopping around 95% or so) twice a day.  This is a great product and the size is very similar to a deck of cards (not like an iphone that someone else posted) and is very easy to carry in a jacket pocket or back pack.  I bought this for a 4 day music festival I\'m going to, and I have no worries at all of my iphone running out of juice! | FANTASTIC product!                    |   671 |\\n+------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------+-------+\\n3 rows in set (1.80 sec)\\n```\\n\\nTo sum up, inverted index can bring a 15% speedup for queries on low-cardinality columns. So it is not only harmless but also beneficial to low-cardinality data filtering.\\n\\nIn addition, Apache Doris adopts effective dictionary encoding and compression for low-cardinality columns. It also utilizes built-in indexes like ZoneMap for filtering. Thus, it can deliver ideal query performance even without inverted indexes.\\n\\n## Conclusion\\n\\nInverted index in Apache Doris optimizes data filtering based on the predicate column (the `WHERE` clause in SQL queries). It reduces unnecessary data scanning and significantly increases query speed on high-cardinality columns and guarantees no negative effects on low-cardinality columns. It supports lightweight index management, including ADD/DROP INDEX and BUILD INDEX. It can be easily enabled or disabled via `enable_inverted_index_query=true/false`. \\n\\nInverted index and NGram BloomFilter index apply to different scenarios. This is how you decide which one is the optimal choice: \\n\\n- **Non-primary key column queries**: These cases often involve widely scattered values and a low hit rate. **Inverted index** can work in conjunction with the built-in smart indexes in Doris to accelerate these queries. It has well-established support for scalar data types including characters, numerics, and datetime.\\n- **Text searches on short texts**: If the dataset includes short texts that are highly diverse, **NGram BloomFilter** will be an effective choice for fuzzy matching (`LIKE`) on short texts. If the short texts are very similar (with lots of identical content), **inverted index** will be more efficient because it ensures a smaller dictionary and faster retrieval of the row numbers. \\n- **Text searches on long texts**: Inverted index is a better choice for long texts. Compared to brutal-force string matching, it largely reduces CPU resource consumption. \\n\\nInverted index has been available in Apache Doris for almost a year and stood the test of many users in their production environment with massive data. In future versions of Apache Doris regarding inverted index, we plan to add support for:\\n\\n- **Self-defined tokenization**: provides a user-defined tokenizer to fit in different use cases.\\n- **More data types**: Users will be able to create inverted index for complex data types including Array and Map.\\n\\nIf you encounter any issues while trying it out in Apache Doris or would like to know more details, join our [Slack](https://join.slack.com/t/apachedoriscommunity/shared_invite/zt-2unfw3a3q-MtjGX4pAd8bCGC1UV0sKcw) community and talk to us!"},{"id":"/release-note-2.0.4","metadata":{"permalink":"/blog/release-note-2.0.4","source":"@site/blog/release-note-2.0.4.md","title":"Apache Doris 2.0.4 just released","description":"Thanks to our community users and developers, 333 improvements and bug fixes have been made in Doris 2.0.4.","date":"2024-01-26T00:00:00.000Z","tags":[{"inline":true,"label":"Release Notes","permalink":"/blog/tags/release-notes"}],"hasTruncateMarker":false,"authors":[{"name":"Apache Doris","key":null,"page":null}],"frontMatter":{"title":"Apache Doris 2.0.4 just released","description":"Thanks to our community users and developers, 333 improvements and bug fixes have been made in Doris 2.0.4.","date":"2024-01-26","author":"Apache Doris","tags":["Release Notes"],"image":"/images/2.0.4.png"},"unlisted":false,"prevItem":{"title":"A deep dive into inverted index: how it speeds up text searches by 40 times","permalink":"/blog/inverted-index-accelerates-text-searches-by-40-time-apache-doris"},"nextItem":{"title":"Financial data warehousing: fast, secure, and highly available with Apache Doris","permalink":"/blog/a-fast-secure-high-available-real-time-data-warehouse-based-on-apache-doris"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n  http://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\nThanks to our community users and developers, about 333 improvements and bug fixes have been made in Doris 2.0.4 version.\\n\\n**Quick Download** : [https://doris.apache.org/download/](https://doris.apache.org/download/)\\n\\n**GitHub** : [https://github.com/apache/doris/releases](https://github.com/apache/doris/releases)\\n\\n\\n## Behavior changes\\n- More reasonable and accurate precision and scale inference for decimal data type\\n  - [[improvement](decimal) use new way for decimal arithmetic precision promotion](https://github.com/apache/doris/pull/28034)\\n\\n- Support drop policy for user or role\\n  - [[fix](polixy)support drop policy for user or role](https://github.com/apache/doris/pull/29488)\\n\\n## New features\\n\\n- Support datev1, datetimev1 and decimalv2 datatypes in new optimizer Nereids.\\n- Support ODBC table for new optimizer Nereids.\\n- Add `lower_case` and `ignore_above` option for inverted index\\n- Support `match_regexp` and `match_phrase_prefix` optimization by inverted index\\n- Support paimon native reader in datalake\\n- Support audit-log for `insert into` SQL\\n- Support reading parquet file in lzo compressed format\\n\\n## Improvements\\n\\n- Improve storage management including balance, migration, publish and others.\\n- Improve storage cooldown policy to use save disk space.\\n- Performance optimization for substr with ascii string.\\n- Improve partition prune when date function is used.\\n- Improve auto analyze visibility and performance.\\n\\nSee the complete list of improvements and bug fixes on github [dev/2.0.4-merged](https://github.com/apache/doris/issues?q=label%3Adev%2F2.0.4-merged+is%3Aclosed)\\n\\n\\n\\n## Credits\\nLast but not least, this release would not have been possible without the following contributors: \\n\\nairborne12, amorynan, AshinGau, BePPPower, bingquanzhao, BiteTheDDDDt, bobhan1, ByteYue, caiconghui,CalvinKirs, cambyzju, caoliang-web, catpineapple, csun5285, dataroaring, deardeng, dutyu, eldenmoon, englefly, feifeifeimoon, fornaix, Gabriel39, gnehil, HappenLee, hello-stephen, HHoflittlefish777,hubgeter, hust-hhb, ixzc, jacktengg, jackwener, Jibing-Li, kaka11chen, KassieZ, LemonLiTree,liaoxin01, LiBinfeng-01, lihuigang, liugddx, luwei16, morningman, morrySnow, mrhhsg, Mryange, nextdreamblue, Nitin-Kashyap, platoneko, py023, qidaye, shuke987, starocean999, SWJTU-ZhangLei, w41ter, wangbo, wsjz, wuwenchi, Xiaoccer, xiaokang, XieJiann, xingyingone, xinyiZzz, xuwei0912, xy720, xzj7019, yujun777, zclllyybb, zddr, zhangguoqiang666, zhangstar333, zhannngchen, zhiqiang-hhhh, zy-kkk, zzzxl1993"},{"id":"/a-fast-secure-high-available-real-time-data-warehouse-based-on-apache-doris","metadata":{"permalink":"/blog/a-fast-secure-high-available-real-time-data-warehouse-based-on-apache-doris","source":"@site/blog/a-fast-secure-high-available-real-time-data-warehouse-based-on-apache-doris.md","title":"Financial data warehousing: fast, secure, and highly available with Apache Doris","description":"A whole-journey guide for financial users looking for fast data processing performance, data security, and high service availability.","date":"2024-01-08T00:00:00.000Z","tags":[{"inline":true,"label":"Best Practice","permalink":"/blog/tags/best-practice"}],"hasTruncateMarker":false,"authors":[{"name":"velodb.io \xb7 VeloDB Engineering Team","key":null,"page":null}],"frontMatter":{"title":"Financial data warehousing: fast, secure, and highly available with Apache Doris","description":"A whole-journey guide for financial users looking for fast data processing performance, data security, and high service availability.","date":"2024-01-08","author":"velodb.io \xb7 VeloDB Engineering Team","externalLink":"https://www.velodb.io/blog/155","tags":["Best Practice"],"image":"/images/apache-doris-a-fast-secure-and-highly-available-real-time-data-warehouse.png"},"unlisted":false,"prevItem":{"title":"Apache Doris 2.0.4 just released","permalink":"/blog/release-note-2.0.4"},"nextItem":{"title":"Apache Doris speeds up data reporting, tagging, and data lake analytics","permalink":"/blog/apache-doris-speeds-up-data-reporting-tagging-and-data-lake-analytics"}},"content":"\x3c!-- \\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n\\nThis is a whole-journey guide for Apache Doris users, especially those from the financial sector which requires a high level of data security and availability. If you don\'t know how to build a real-time data pipeline and make the most of the [Apache Doris](https://doris.apache.org/) functionalities, start with this post and you will be loaded with inspiration after reading.\\n\\nThis is the best practice of a non-banking payment service provider that serves over 25 million retailers and processes data from 40 million end devices. Data sources include MySQL, Oracle, and MongoDB. They were using Apache Hive as an offline data warehouse but feeling the need to add a real-time data processing pipeline. **After introducing Apache Doris, they increase their data ingestion speed by 2~5 times, ETL performance by 3~12 times, and query execution speed by 10~15 times.**\\n\\nIn this post, you will learn how to integrate Apache Doris into your data architecture, including how to arrange data inside Doris, how to ingest data into it, and how to enable efficient data updates. Plus, you will learn about the enterprise features that Apache Doris provides to guarantee data security, system stability, and service availability.\\n\\n![offline-vs-real-time-data-warehouse](https://cdn.selectdb.com/static/offline_vs_real_time_data_warehouse_6b3fd0d1bc.png)\\n\\n## Building a real-time data warehouse with Apache Doris\\n### Choice of data models\\n\\nApache Doris arranges data with three data models. The main difference between these models lies in whether or how they aggregate data.\\n\\n- **[Duplicate Key model](https://doris.apache.org/docs/data-table/data-model#duplicate-model)**: for detailed data queries. It supports ad-hoc queries of any dimension.\\n- **[Unique Key model](https://doris.apache.org/docs/data-table/data-model#unique-model)**: for use cases with data uniqueness constraints. It supports precise deduplication, multi-stream upserts, and partial column updates.\\n- **[Aggregate Key model](https://doris.apache.org/docs/data-table/data-model#aggregate-model)**: for data reporting. It accelerates data reporting by pre-aggregating data.\\n\\nThe financial user adopts different data models in different data warehouse layers:\\n\\n- **ODS - Duplicate Key model**: As a payment service provider, the user receives a million settlement data every day. Since the settlement cycle can span a whole year, the relevant data needs to be kept intact for a year. Thus, the proper way is to put it in the Duplicate Key model, which does not perform any data aggregations. An exception is that some data is prone to constant changes, like order status from retailers. Such data should be put into the Unique Key model so that the newly updated record of the same retailer ID or order ID will always replace the old one.\\n- **DWD & DWS - Unique Key model**: Data in the DWD and DWS layers are further abstracted, but it is all put in the Unique Key model so that the settlement data can be automatically updated.\\n- **ADS - Aggregate Key model**: Data is highly abstracted in this layer. It is pre-aggregated to mitigate the computation load of downstream analytics.\\n\\n\\n### Partitioning and bucketing strategies\\n\\nThe idea of partitioning and bucketing is to \\"cut\\" data into smaller pieces to increase data processing speed. The key is to set an appropriate number of data partitions and buckets. Based on their use case, the user tailors the bucketing field and bucket number to each table. For example, they often need to query the dimensional data of different retailers from the retailer flat table, so they specify the retailer ID column as the bucketing field, and list the recommended bucket number for various data sizes.\\n\\n![partitioning-and-bucketing-strategies](https://cdn.selectdb.com/static/partitioning_and_bucketing_strategies_c91ad6a340.png)\\n\\n### Multi-source data migration\\n\\nIn the adoption of Apache Doris, the user had to migrate all local data from their branches into Doris, which was when they found out their branches were using **different databases** and had **data files of very different formats**, so the migration could be a mess.\\n\\n![multi-source-data-migration](https://cdn.selectdb.com/static/multi_source_data_migration_2b4f54e005.png)\\n\\nLuckily, Apache Doris supports a rich collection of data integration methods for both real-time data streaming and offline data import.\\n\\n- **Real-time data streaming**: Apache Doris fetches MySQL Binlogs in real time. Part of them is written into Doris directly via Flink CDC, while the high-volume ones are synchronized into Kafka for peak shaving, and then written into Doris via the Flink-Doris-Connector.\\n- **Offline data import**: This includes more diversified data sources and data formats. Historical data and incremental data from S3 and HDFS will be ingested into Doris via the [Broker Load](https://doris.apache.org/docs/data-operate/import/import-way/broker-load-manual) method, data from Hive or JDBC will be synchronized to Doris via the [Insert Into](https://doris.apache.org/docs/data-operate/import/import-way/insert-into-manual) method, and files will be loaded to Doris via the Flink-Doris-Connector and Flink FTP Connector. (FTP is how the user transfers files across systems internally, so they developed the Flink-FTP-Connector to support the complicated data formats and multiple newline characters in data.)\\n\\n### Full data ingestion and incremental data ingestion\\n\\nTo ensure business continuity and data accuracy, the user figures out the following ways to ingest full data and incremental data:\\n\\n- **Full data ingestion**: Create a temporary table of the target schema in Doris, ingest full data into the temporary table, and then use the `ALTER TABLE t1 REPLACE WITH TABLE t2` statement for atomic replacement of the regular table with the temporary table. This method prevents interruptions to queries on the frontend.\\n\\n```SQL\\nalter table ${DB_NAME}.${TBL_NAME} drop partition IF EXISTS p${P_DOWN_DATE};\\nALTER TABLE ${DB_NAME}.${TBL_NAME} ADD PARTITION IF NOT EXISTS  p${P_DOWN_DATE} VALUES [(\'${P_DOWN_DATE}\'), (\'${P_UP_DATE}\'));\\n\\nLOAD LABEL ${TBL_NAME}_${load_timestamp} ...\\n```\\n\\n- **Incremental data ingestion**: Create a new data partition to accommodate incremental data.\\n\\n### Offline data processing\\n\\nThe user has moved their offline data processing workload to Apache Doris and thus **increased execution speed by 5 times**. \\n\\n![offline-data-processing](https://cdn.selectdb.com/static/offline_data_processing_82e20fc59a.png)\\n\\n- **Before**: The old Hive-based offline data warehouse used the TEZ execution engine to process 30 million new data records every day. With 2TB computation resources, the whole pipeline took 2.5 hours. \\n- **After**: Apache Doris finishes the same tasks within only 30 minutes and consumes only 1TB. Script execution takes only 10 seconds instead of 8 minutes.\\n\\n## Enterprise features for financial players\\n\\n### Multi-tenant resource isolation\\n\\nThis is required because it often happens that the same piece of data is requested by multiple teams or business systems. These tasks can lead to resource preemption and thus performance decrease and system instability.\\n\\n**Resource limit for different workloads**\\n\\nThe user classifies their analytics workloads into four types and sets a resource limit for each of them. In particular, they have four different types of Doris accounts and set a limit on the CPU and memory resources for each type of account.\\n\\n![multi-tenant-resource-isolation](https://cdn.selectdb.com/static/multi_tenant_resource_isolation_772a57a4f1.png)\\n\\nIn this way, when one tenant requires excessive resources, it will only compromise its own efficiency but not affect other tenants.\\n\\n**Resource tag-based isolation**\\n\\nFor data security under the parent-subsidiary company hierarchy, the user has set isolated resource groups for the subsidiaries. Data of each subsidiary is stored in its own resource group with three replicas, while data of the parent company is stored with four replicas: three in the parent company resource group, and the other one in the subsidiary resource group. Thus, when an employee from a subsidiary requests data from the parent company, the query will only executed in the subsidiary resource group. Specifically, they take these steps:\\n\\n![ resource-tag-based-isolation](https://cdn.selectdb.com/static/resource_tag_based_isolation_442e20f09c.png)\\n\\n**Workload group**\\n\\nThe resource tag-based isolation plan ensures isolation on a physical level, but as Apache Doris developers, we want to further optimize resource utilization and pursue more fine-grained resource isolation. For these purposes, we released the [Workload Group](https://doris.apache.org/docs/admin-manual/workload-group) feature in [Apache Doris 2.0](https://doris.apache.org/blog/release-note-2.0.0). \\n\\nThe Workload Group mechanism relates queries to workload groups, which limit the share of CPU and memory resources of the backend nodes that a query can use. When cluster resources are in short supply, the biggest queries will stop execution. On the contrary, when there are plenty of available cluster resources and a workload group requires more resources than the limit, it will get assigned with the idle resources proportionately. \\n\\nThe user is actively planning their transition to the Workload Group plan and utilizing the task prioritizing mechanism and query queue feature to organize the execution order.\\n\\n**Fine-grained user privilege management**\\n\\nFor regulation and compliance reasons, this payment service provider implements strict privilege control to make sure that everyone only has access to what they are supposed to access. This is how they do it:\\n\\n- **User privilege setting**: System users of different subsidiaries or with different business needs are granted different data access privileges.\\n- **Privilege control over databases, tables, and rows**: The `ROW POLICY` mechanism of Apache Doris makes these operations easy.\\n- **Privilege control over columns**: This is done by creating views.\\n\\n![fine-grained-user-privilege-management.png](https://cdn.selectdb.com/static/fine_grained_user_privilege_management_f0cd060011.png)\\n\\n### Cluster stability guarantee\\n\\n- **Circuit Breaking**: From time to time, system users might input faulty SQL, causing excessive resource consumption. A circuit-breaking mechanism is in place for that. It will promptly stop these resource-intensive queries and prevent interruption to the system.\\n- **Data ingestion concurrency control**: The user has a frequent need to integrate historical data into their data platform. That involves a lot of data modification tasks and might stress the cluster. To solve that, they turn on the [Merge-on-Write](https://doris.apache.org/docs/data-table/data-model#merge-on-write-of-unique-model) mode in the Unique Key model, enable [Vertical Compaction](https://doris.apache.org/docs/advanced/best-practice/compaction#vertical-compaction) and [Segment Compaction](https://doris.apache.org/docs/advanced/best-practice/compaction#segment-compaction), and tune the data compaction parameters to control data ingestion concurrency.\\n- **Network traffic control**: Considering their two clusters in different cities, they employ Quality of Service (QoS) strategies tailored to different scenarios for precise network isolation and ensuring network quality and stability.\\n- **Monitoring and alerting**: The user has integrated Doris with their internal monitoring and alerting platform so any detected issues will be notified via their messaging software and email in real time.\\n\\n### Cross-cluster replication\\n\\nDisaster recovery is crucial for the financial industry. The user leverages the Cross-Cluster Replication (CCR) capability and builds a dual-cluster solution. As the primary cluster undertakes all the queries, the major business data is also synchronized into the backup cluster and updated in real time, so that in the case of service downtime in the primary cluster, the backup cluster will take over swiftly and ensure business continuity.\\n\\n## Conclusion\\n\\nWe appreciate the user for their active [communication](https://join.slack.com/t/apachedoriscommunity/shared_invite/zt-2unfw3a3q-MtjGX4pAd8bCGC1UV0sKcw) with us along the way and are glad to see so many Apache Doris features fit in their needs. They are also planning on exploring federated query, compute-storage separation, and auto maintenance with Apache Doris. We look forward to more best practice and feedback from them."},{"id":"/apache-doris-speeds-up-data-reporting-tagging-and-data-lake-analytics","metadata":{"permalink":"/blog/apache-doris-speeds-up-data-reporting-tagging-and-data-lake-analytics","source":"@site/blog/apache-doris-speeds-up-data-reporting-tagging-and-data-lake-analytics.md","title":"Apache Doris speeds up data reporting, tagging, and data lake analytics","description":"The user leverages the capabilities of Apache Doris in reporting, customer tagging, and data lake analytics and achieves high performance.","date":"2023-12-27T00:00:00.000Z","tags":[{"inline":true,"label":"Best Practice","permalink":"/blog/tags/best-practice"}],"hasTruncateMarker":false,"authors":[{"name":"velodb.io \xb7 VeloDB Engineering Team","key":null,"page":null}],"frontMatter":{"title":"Apache Doris speeds up data reporting, tagging, and data lake analytics","description":"The user leverages the capabilities of Apache Doris in reporting, customer tagging, and data lake analytics and achieves high performance.","date":"2023-12-27","author":"velodb.io \xb7 VeloDB Engineering Team","externalLink":"https://www.velodb.io/blog/152","tags":["Best Practice"],"image":"/images/apache-doris-speeds-up-data-reporting-data-lake-analytics.jpg"},"unlisted":false,"prevItem":{"title":"Financial data warehousing: fast, secure, and highly available with Apache Doris","permalink":"/blog/a-fast-secure-high-available-real-time-data-warehouse-based-on-apache-doris"},"nextItem":{"title":"From Elasticsearch to Apache Doris: upgrading an observability platform","permalink":"/blog/from-elasticsearch-to-apache-doris-upgrading-an-observability-platform"}},"content":"\x3c!-- \\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\nAs much as we say [Apache Doris](https://doris.apache.org/) is an all-in-one data platform that is capable of various analytics workloads, it is always compelling to demonstrate that by real use cases. That\'s why I would like to share this user story with you. It is about how they leverage the capabilities of Apache Doris in reporting, customer tagging, and data lake analytics and achieve high performance.\\n\\nThis fintech service provider is a long-term user of Apache Doris. They have almost 10 clusters for production, hundreds of Doris backend nodes, and thousands of CPU Cores. The total data size is near 1 PB. Every day, they have hundreds of workflows running simultaneously, receive almost 10 billion new data records, and respond to millions of data queries.\\n\\nBefore migrating to Apache Doris, they used ClickHouse, MySQL, and Elasticsearch. Then frictions arise from their ever-enlarging data size. They found it hard to scale out the ClickHouse clusters because there were too many dependencies. As for MySQL, they had to switch between various MySQL instances because one MySQL instance had its limits and cross-instance queries were not supported.\\n\\n## Reporting\\n\\n### From ClickHouse + MySQL to Apache Doris\\n\\nData reporting is one of the major services they provide to their customers and they are bound by an SLA. They used to support such service with a combination of ClickHouse and MySQL, but they found significant fluctuations in their data synchronization duration, making it hard for them to meet the service levels outlined in their SLA. Diagnosis showed that it was because the multiple components add to the complexity and instability of data synchronization tasks. To fix that, they have used Apache Doris as a unified analytic engine to support data reporting. \\n\\n![From ClickHouse + MySQL to Apache Doris](/images/blogs/speed-up-lakehouse/from-clickHouse-mysql-to-apache-doris.png)\\n\\n### Performance improvements\\n\\nWith Apache Doris, they ingest data via the [Broker Load](https://doris.apache.org/docs/1.2/data-operate/import/import-way/broker-load-manual) method and reach an SLA compliance rate of over 99% in terms of data synchronization performance.\\n\\n![data-synchronization-size-and-duration](/images/blogs/speed-up-lakehouse/data-synchronization-size-and-duration.png)\\n\\nAs for data queries, the Doris-based architecture maintains an **average query response time** of less than **10s** and a **P90 response time** of less than **30s**. This is a 50% speedup compared to the old architecture. \\n\\n![average-query-response-time](/images/blogs/speed-up-lakehouse/average-query-response-time.png)\\n\\n![query-response-time-percentile](/images/blogs/speed-up-lakehouse/query-response-time-percentile.png)\\n\\n\\n## Tagging\\n\\nTagging is a common operation in customer analytics. You assign labels to customers based on their behaviors and characteristics, so that you can divide them into groups and figure out targeted marketing strategies for each group of them. \\n\\nIn the old processing architecture where Elasticsearch was the processing engine, raw data was ingested and tagged properly. Then, it will be merged into JSON files and imported into Elasticsearch, which provides data services for analysts and marketers. In this process, the merging step was to reduce updates and relieve load for Elasticsearch, but it turned out to be a troublemaker:\\n\\n- Any problematic data in any of the tags could spoil the entire merging operation and thus interrupt the data services.\\n- The merging operation was implemented based on Spark and MapReduce and took up to 4 hours. Such a long time frame could encroach on marketing opportunities and lead to unseen losses.\\n\\n![tagging-services](/images/blogs/speed-up-lakehouse/tagging-services.png)\\n\\nThen Apache Doris takes this over. Apache Doris arranges tag data with its data models, which process data fast and smoothly. The aforementioned merging step can be done by the [Aggregate Key model](https://doris.apache.org/docs/data-table/data-model#aggregate-model), which aggregates tag data based on the specified Aggregate Key upon data ingestion. The [Unique Key model](https://doris.apache.org/docs/data-table/data-model#unique-model) is handy for partial column updates. Again, all you need is to specify the Unique Key. This enables swift and flexible data updating and saves you from the trouble of replacing the entire flat table. You can also put your detailed data into a [Duplicate model](https://doris.apache.org/docs/data-table/data-model#duplicate-model) to speed up certain queries. **In practice, it took the user 1 hour to finish the data ingestion, compared to 4 hours with the old architecture.**\\n\\nIn terms of query performance, Doris is equipped with well-developed bitmap indexes and techniques tailored to high-concurrency queries, so in this case, it can finish **customer segmentation within seconds** and reach over **700 QPS in user-facing queries**.\\n\\n## Data lake analytics\\n\\nIn data lake scenarios, the data size you need to handle tends to be huge, but the data processing volume in each query tends to vary. To ensure fast data ingestion and high query performance of huge data sets, you need more resources. On the other hand, during non-peak time, you want to scale down your cluster for more efficient resource management. How do you handle this dilemma?\\n\\nApache Doris has a few features that are designed for data lake analytics, including Multi-Catalog and Compute Node. The former shields you from the headache of data ingestion in data lake analytics while the latter enables elastic cluster scaling.\\n\\nThe [Multi-Catalog](https://doris.apache.org/docs/lakehouse/multi-catalog/?_highlight=multi&_highlight=catalog) mechanism allows you to connect Doris to a variety of external data sources so you can use Doris as a unified query gateway without worrying about bulky data ingestion into Doris.\\n\\nThe [Compute Node](https://doris.apache.org/docs/advanced/compute-node/) of Apache Doris is a backend role that is designed for remote federated query workloads, like those in data lake analytics. Normal Doris backend nodes are responsible for both SQL query execution and data management, while the Compute Nodes in Doris, as the name implies, only perform computation. Compute Nodes are stateless, making them elastic enough for cluster scaling.\\n\\nThe user introduces Compute Nodes into their cluster and deploys them with other components in a hybrid configuration. As a result, the cluster automatically scales down during the night, when there are fewer query requests, and scales out during the daytime to handle the massive query workload. This is more resource-efficient.\\n\\nFor easier deployment, they have also optimized their Deploy on Yarn process via Skein. As is shown below, they define the number of Compute nodes and the required resources in the YAML file, and then pack the installation file, configuration file, and startup script into the distributed file system. In this way, they can start or stop the entire cluster of over 100 nodes within minutes using one simple line of code.\\n\\n![skein](/images/blogs/speed-up-lakehouse/skein.png)\\n\\n## Conclusion\\n\\nFor data reporting and customer tagging, Apache Doris smoothens data ingestion and merging steps, and delivers high query performance based on its own design and functionality. For data lake analytics, the user improves resource efficiency by elastic scaling of clusters using the Compute Node. Along their journey with Apache Doris, they have also developed a data ingestion task prioritizing mechanism and contributed it to the Doris project. A gesture to facilitate their use case ends up benefiting the whole [open source community](https://join.slack.com/t/apachedoriscommunity/shared_invite/zt-2unfw3a3q-MtjGX4pAd8bCGC1UV0sKcw). This is a great example of open-source products thriving on user involvement.\\n\\nCheck Apache Doris [repo](https://github.com/apache/doris) on GitHub"},{"id":"/from-elasticsearch-to-apache-doris-upgrading-an-observability-platform","metadata":{"permalink":"/blog/from-elasticsearch-to-apache-doris-upgrading-an-observability-platform","source":"@site/blog/from-elasticsearch-to-apache-doris-upgrading-an-observability-platform.md","title":"From Elasticsearch to Apache Doris: upgrading an observability platform","description":"GuanceDB, an observability platform, replaces Elasticsearch with Apache Doris as its query and storage engine and realizes 70% less storage costs and 200%~400% data query performance.","date":"2023-12-14T00:00:00.000Z","tags":[{"inline":true,"label":"Best Practice","permalink":"/blog/tags/best-practice"}],"hasTruncateMarker":false,"authors":[{"name":"Apache Doris","key":null,"page":null}],"frontMatter":{"title":"From Elasticsearch to Apache Doris: upgrading an observability platform","description":"GuanceDB, an observability platform, replaces Elasticsearch with Apache Doris as its query and storage engine and realizes 70% less storage costs and 200%~400% data query performance.","date":"2023-12-14","author":"Apache Doris","tags":["Best Practice"],"image":"/images/from-elasticsearch-to-apache-doris-upgrading-an-observability-platform.jpg"},"unlisted":false,"prevItem":{"title":"Apache Doris speeds up data reporting, tagging, and data lake analytics","permalink":"/blog/apache-doris-speeds-up-data-reporting-tagging-and-data-lake-analytics"},"nextItem":{"title":"Apache Doris 2.0.3 just released","permalink":"/blog/release-note-2.0.3"}},"content":"\x3c!-- \\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\nObservability platforms are akin to the immune system. Just like immune cells are everywhere in human bodies, an observability platform patrols every corner of your devices, components, and architectures, identifying any potential threats and proactively mitigating them. However, I might have gone too far with that metaphor, because till these days, we have never invented a system as sophisticated as the human body, but we can always make advancements.\\n\\nThe key to upgrading an observability platform is to increase data processing speed and reduce costs. This is based on two reasons:\\n\\n1. The faster you can identify abnormalities from your data, the more you can contain the potential damage.\\n2. An observability platform needs to store a sea of data, and low storage cost is the only way to make that sustainable.\\n\\nThis post is about how GuanceDB, an observability platform, makes progress in these two aspects by replacing Elasticsearch with Apache Doris as its query and storage engine. **The result is 70% less storage costs and 200%\uFF5E400% data query performance.**\\n\\n## GuanceDB\\n\\nGuanceDB is an all-around observability solution. It provides services including data analytics, data visualization, monitoring and alerting, and security inspection. From GuanceDB, users can have an understanding of their objects, network performance, applications, user experience, system availability, etc.\\n\\nFrom the standpoint of a data pipeline, GuanceDB can be divided into two parts: data ingestion and data analysis. I will get to them one by one.\\n\\n### Data integration\\n\\nFor data integration, GuanceDB uses its self-made tool called DataKit. It is an all-in-one data collector that extracts from different end devices, business systems, middleware, and data infrastructure. It can also preprocess data and relate it with metadata. It provides extensive support for data, from logs, and time series metrics, to data of distributed tracing, security events, and user behaviors from mobile APPs and web browsers. To cater to diverse needs across multiple scenarios, it ensures compatibility with various open-source probes and collectors as well as data sources of custom formats.\\n\\n![observability-platform-architecture](/images/observability-platform-architecture.png)\\n\\n### Query & storage engine\\n\\nData collected by DataKit, goes through the core computation layer and arrive in GuanceDB, which is a multil-model database that combines various database technologies. It consists of the query engine layer and the storage engine layer. By decoupling the query engine and the storage engine, it enables pluggable and interchangeable architecture. \\n\\n![observability-platform-query-engine-storage-engine](/images/observability-platform-query-engine-storage-engine.png)\\n\\nFor time series data, they built Metric Store, which is a self-developed storage engine based on VictoriaMetrics. For logs, they integrate Elasticsearch and OpenSearch. GuanceDB is performant in this architecture, while Elasticsearch demonstrates room for improvement:\\n\\n- **Data writing**: Elasticsearch consumes a big share of CPU and memory resources. It is not only costly but also disruptive to query execution.\\n- **Schemaless support**: Elasticsearch provides schemaless support by Dynamic Mapping, but that\'s not enough to handle large amounts of user-defined fields. In this case, it can lead to field type conflict and thus data loss.\\n- **Data aggregation**: Large aggregation tasks often trigger a timeout error in Elasticsearch. \\n\\nSo this is where the upgrade happens. GuanceDB tried and replaced Elasticsearch with [Apache Doris](https://doris.apache.org/). \\n\\n## DQL\\n\\nIn the GuanceDB observability platform, almost all queries involve timestamp filtering. Meanwhile, most data aggregations need to be performed within specified time windows. Additionally, there is a need to perform rollups of time series data on individual sequences within a time window. Expressing these semantics using SQL often requires nested subqueries, resulting in complex and cumbersome statements.\\n\\nThat\'s why GuanceDB developed their own Data Query Language (DQL). With simplified syntax elements and computing functions optimized for observability use cases, this DQL can query metrics, logs, object data, and data from distributed tracing.\\n\\n![observability-platform-query-engine-storage-engine-apache-doris](/images/observability-platform-query-engine-storage-engine-apache-doris.png)\\n\\nThis is how DQL works together with Apache Doris. GuanceDB has found a way to make full use of the analytic power of Doris, while complementing its SQL functionalities.\\n\\nAs is shown below, Guance-Insert is the data writing component, while Guance-Select is the DQL query engine.\\n\\n- **Guance-Insert**: It allows data of different tenants to be accumulated in different batches, and strikes a balance between writing throughput and writing latency. When logs are generated in large volumes, it can maintain a low data latency of 2~3 seconds.\\n- **Guance-Select**: For query execution, if the query SQL semantics or function is supported in Doris, Guance-Select will push the query down to the Doris Frontend for computation; if not, it will go for a fallback option: acquire columnar data in Arrow format via the Thrift RPC interface, and then finish computation in Guance-Select. The catch is that it cannot push the computation logic down to Doris Backend, so it can be slightly slower than executing queries in Doris Frontend.\\n\\n![DQL-GranceDB-apache-doris](/images/DQL-GranceDB-apache-doris.png)\\n\\n## Observations\\n\\n### Storage cost 70% down, query speed 300% up\\n\\nPreviously, with Elasticsearch clusters, they used 20 cloud virtual machines (16vCPU 64GB) and had independent index writing services (that\'s another 20 cloud virtual machines). Now with Apache Doris, they only need 13 cloud virtual machines of the same configuration in total, representing **a 67% cost reduction**. This is contributed by three capabilities of Apache Doris:\\n\\n- **High writing throughput**: Under a consistent writing throughput of 1GB/s, Doris maintains a CPU usage of less than 20%. That equals 2.6 cloud virtual machines. With low CPU usage, the system is more stable and better prepared for sudden writing peaks.\\n\\n![writing-throughput-cpu-usage-apache-doris](/images/writing-throughput-cpu-usage-apache-doris.png)\\n\\n- **High data compression ratio**: Doris utilizes the ZSTD compression algorithm on top of columnar storage. It can realize a compression ratio of 8:1. Compared to 1.5:1 in Elasticsearch, Doris can reduce storage costs by around 80%.\\n- **[Tiered storage](https://doris.apache.org/blog/Tiered-Storage-for-Hot-and-Cold-Data-What-Why-and-How)**: Doris allows a more cost-effective way to store data: to put hot data in local disks and cold data object storage. Once the storage policy is set, Doris can automatically manage the \\"cooldown\\" process of hot data and move cold data to object storage. Such data lifecycle is transparent to the data application layer so it is user-friendly. Also, Doris speeds up cold data queries by local cache.\\n\\nWith lower storage costs, Doris does not compromise query performance. It doubles the execution speed of queries that return a single row and those that return a result set. For aggregation queries without sampling, Doris runs at 4 times the speed of Elasticsearch.\\n\\n**To sum up, Apache Doris achieves 2~4 times the query performance of Elasticsearch with only 1/3 of the storage cost it consumes.**\\n\\n### Inverted index for full-text search\\n\\nInverted index is the magic potion for log analytics because it can considerably increase full-text search performance and reduce query overheads. \\n\\nIt is especially useful in these scenarios:\\n\\n- Full-text search by `MATCH_ALL`, `MATCH_ANY`, and `MATCH_PHRASE`. `MATCH_PHRASE` in combination with inverted index is the alternative to the Elasticsearch full-text search functionality.\\n- Equivalence queries (=, ! =, IN), range queries (>, >=, <, <=), and support for numerics, datetime, and strings.\\n\\n```SQL\\nCREATE TABLE httplog\\n(\\n  `ts` DATETIME,\\n  `clientip` VARCHAR(20),\\n  `request` TEXT,\\n  INDEX idx_ip (`clientip`) USING INVERTED,\\n  INDEX idx_req (`request`) USING INVERTED PROPERTIES(\\"parser\\" = \\"english\\") \\n)\\nDUPLICATE KEY(`ts`)\\n...\\n\\n-- Retrieve the latest 10 records of Client IP \\"8.8.8.8\\"\\nSELECT * FROM httplog WHERE clientip = \'8.8.8.8\' ORDER BY ts DESC LIMIT 10;\\n-- Retrieve the latest 10 records with \\"error\\" or \\"404\\" in the \\"request\\" field\\nSELECT * FROM httplog WHERE request MATCH_ANY \'error 404\' ORDER BY ts DESC LIMIT 10;\\n-- Retrieve the latest 10 records with \\"image\\" and \\"faq\\" in the \\"request\\" field\\nSELECT * FROM httplog WHERE request MATCH_ALL \'image faq\' ORDER BY ts DESC LIMIT 10;\\n-- Retrieve the latest 10 records with \\"query error\\" in the \\"request\\" field\\nSELECT * FROM httplog WHERE request MATCH_PHRASE \'query error\' ORDER BY ts DESC LIMIT 10;\\n```\\n\\nAs a powerful accelerator for full-text searches, inverted index in Doris is flexible because we witness the need for on-demand adjustments. In Elasticsearch, indexes are fixed upon creation, so there needs to be good planning of which fields need to be indexed, otherwise, any changes to the index will require a complete rewrite.\\n\\nIn contrast, Doris allows for dynamic indexing. You can add inverted index to a field during runtime and it will take effect immediately. You can also decide which data partitions to create indexes on.\\n\\n### A new data type for dynamic schema change\\n\\nBy nature, an observability platform requires support for dynamic schema, because the data it collects is prone to changes. Every click by a user on the webpage might add a new metric to the database. \\n\\nLooking around the database landscape, you will find that static schema is the norm. Some databases take a step further. For example, Elasticsearch realizes dynamic schema by mapping. However, this functionality can be easily interrupted by field type conflicts or unexpired historical fields.\\n\\nThe Doris solution for dynamic schema is a newly-introduced data type: Variant, and GuanceDB is among the first to try it out. (It will officially be available in Apache Doris V2.1.)\\n\\nThe Variant data type is the move of Doris to embrace semi-structured data analytics. It can solve a lot of the problems that often harass database users:\\n\\n- **JSON** **data storage**: A Variant column in Doris can accommodate any legal JSON data, and can automatically recognize the subfields and data types.\\n- **Schema explosion due to too many fields**: The frequently occurring subfields will be stored in a column-oriented manner to facilitate analysis, while the less frequently seen subfields will be merged into the same column to streamline the data schema.\\n- **Write failure due to data type conflicts**: A Variant column allows different types of data in the same field, and applies different storage for different data types.\\n\\n**Difference** **between Variant and Dynamic Mapping**\\n\\nFrom a functional perspective, the biggest difference between Variant in Doris and Dynamic Mapping in Elasticsearch is that the scope of Dynamic Mapping extends throughout the entire lifecycle of the current table, while that of Variant can be limited to the current data partition. \\n\\nFor example, if a user has changed the business logic and renamed some Variant fields today, the old field name will remain on the partitions before today, but will not appear on the new partitions since tomorrow. **So there is a lower risk of data type conflict.**\\n\\nIn the case of field type conflicts in the same partition, the two fields will be changed to JSON type to avoid data error or data loss. For example, there are two `status` fields in the user\'s business system: One is strings, and the other is numerics, so in queries, the user can decide whether to query the string field, or the nuemric field, or both. (E.g. If you specify `status = \\"ok\\"` in the filters, the query will only be executed on the string field.)\\n\\nFrom the users\' perspective, they can use the Variant type as simply as other data types. They can add or remove Variant fields based on their business needs, and no extra syntax or annotation is required.\\n\\nCurrently, the Variant type requires extra type assertion, we plan to automate this process in future versions of Doris. GuanceDB is one step faster in this aspect. They have realized auto type assertion for their DQL queries. In most cases, type assertion is based on the actual data type of Variant fields. In some rare cases when there is a type conflict, the Variant fields will be upgraded to JSON fields, and then type assertion will be based on the semantics of operators in DQL queries.\\n\\n## Conclusion\\n\\nGuanceDB\'s transition from Elasticsearch to Apache Doris showcases a big stride in improving data processing speed and reducing costs. For these purposes, Apache Doris has optimized itself in the two major aspects of data processing: data integration and data analysis. It has expanded its schemaless support to flexibly accommodate more data types, introduced features like inverted index and tiered storage to enable faster and more cost-effective queries. Evolution is an ongoing process. Apache Doris has never stopped improving itself. We have a lot of new features under development and the Doris [community](https://join.slack.com/t/apachedoriscommunity/shared_invite/zt-2unfw3a3q-MtjGX4pAd8bCGC1UV0sKcw) embrace any input and feedback.\\n\\nCheck Apache Doris GitHub [repo](https://github.com/apache/doris)\\n\\nFind Apache Doris makers on [Slack](https://join.slack.com/t/apachedoriscommunity/shared_invite/zt-2unfw3a3q-MtjGX4pAd8bCGC1UV0sKcw)"},{"id":"/release-note-2.0.3","metadata":{"permalink":"/blog/release-note-2.0.3","source":"@site/blog/release-note-2.0.3.md","title":"Apache Doris 2.0.3 just released","description":"Thanks to our community users and developers, 1000 improvements and bug fixes have been made in Doris 2.0.3.","date":"2023-12-14T00:00:00.000Z","tags":[{"inline":true,"label":"Release Notes","permalink":"/blog/tags/release-notes"}],"hasTruncateMarker":false,"authors":[{"name":"Apache Doris","key":null,"page":null}],"frontMatter":{"title":"Apache Doris 2.0.3 just released","description":"Thanks to our community users and developers, 1000 improvements and bug fixes have been made in Doris 2.0.3.","date":"2023-12-14","author":"Apache Doris","tags":["Release Notes"],"image":"/images/2.0.3.png"},"unlisted":false,"prevItem":{"title":"From Elasticsearch to Apache Doris: upgrading an observability platform","permalink":"/blog/from-elasticsearch-to-apache-doris-upgrading-an-observability-platform"},"nextItem":{"title":"Empowering cyber security by enabling 7 times faster log analysis","permalink":"/blog/empowering-cyber-security-by-enabling-seven-times-faster-log-analysis"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n  http://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\nThanks to our community users and developers, about 1000 improvements and bug fixes have been made in Doris 2.0.3 version, including optimizer statistics, inverted index, complex datatypes, data lake, replica management.\\n\\n\\n\\n## Behavior changes\\n\\n- The output format of the complex data type array/map/struct has been changed to be consistent to the input format and JSON specification. The main changes from the previous version are that DATE/DATETIME and STRING/VARCHAR are enclosed in double quotes and null values inside ARRAY/MAP are displayed as `null` instead of `NULL`.\\n  - https://github.com/apache/doris/pull/25946\\n- SHOW_VIEW permission is supported. Users with SELECT or LOAD permission will no longer be able to execute the \'SHOW CREATE VIEW\' statement and must be granted the SHOW_VIEW permission separately.\\n  - https://github.com/apache/doris/pull/25370\\n\\n\\n## New features\\n\\n### 1. Support collecting statistics for optimizer automatically\\n\\nCollecting statistics helps the optimizer understand the data distribution characteristics and choose a better plan to greatly improve query performance. It is officially supported starting from version 2.0.3 and is enabled all day by default.\\n\\nsee more: https://doris.apache.org/docs/2.0/query/nereids/statistics\\n\\n\\n### 2. Support complex datatypes for more datalake source\\n- Support complex datatypes for JAVA UDF, JDBC and Hudi MOR\\n  - https://github.com/apache/doris/pull/24810\\n  - https://github.com/apache/doris/pull/26236\\n- Support complex datatypes for Paimon\\n  - https://github.com/apache/doris/pull/25364\\n- Suport Paimon version 0.5\\n  - https://github.com/apache/doris/pull/24985\\n\\n\\n### 3. Add more builtin functions\\n- Support the BitmapAgg function in new optimizer\\n  - https://github.com/apache/doris/pull/25508\\n- Supports SHA series digest functions\\n  - https://github.com/apache/doris/pull/24342 \\n- Support the BITMAP datatype in the aggregate functions min_by and max_by \\n  - https://github.com/apache/doris/pull/25430 \\n- Add milliseconds/microseconds_add/sub/diff functions\\n  - https://github.com/apache/doris/pull/24114\\n- Add some json functions: json_insert, json_replace, json_set\\n  - https://github.com/apache/doris/pull/24384\\n\\n\\n## Improvements\\n\\n### Performance optimizations\\n\\n- When the inverted index MATCH WHERE condition with a high filter rate is combined with the common WHERE condition with a low filter rate, the I/O of the index column is greatly reduced. \\n- Optimize the efficiency of random data access after the where filter.\\n- Optimizes the performance of the old get_json_xx function on JSON data types by 2~4x.\\n- Supports the configuration to reduce the priority of the data read thread, ensuring the CPU resources for real-time writing.\\n- Adds `uuid-numeric` function that returns largeint, which is 20 times faster than `uuid` function that returns string.\\n- Optimized the performance of case when by 3x.\\n- Cut out unnecessary predicate calculations in storage engine execution.\\n- Accelerate count performance by pushing down count operator to storage tier.\\n- Optimizes the computation performance of the nullable type in and or expressions.\\n- Supports rewriting the limit operator before `join` in more scenarios to improve query performance. \\n- Eliminate useless `order by` operators from inline view to improve query performance.\\n- Optimizes the accuracy of cardinality estimates and cost models in some cases. \\n- Optimized jdbc catalog predicate pushdown logic.\\n- Optimized the read efficiency of the file cache when it\'s enable for the first time.\\n- Optimizes the hive table sql cache policy and uses the partition update time stored in HMS to improve the cache hit ratio. \\n- Optimize mow compaction efficiency.\\n- Optimized thread allocation logic for external table query to reduce memory usage \\n- Optimize memory usage for column reader.\\n\\n\\n\\n### Distributed replica management improvements\\n\\nDistributed replica management improvements include skipping partition deletion, colocate group deletion, balance failure due to continuous write, and hot and cold seperation table balance.\\n\\n\\n### Security enhancement\\n- The audit log plug-in uses a token instead of a plaintext password to enhance security\\n  - https://github.com/apache/doris/pull/26278\\n- log4j configures security enhancement\\n  - https://github.com/apache/doris/pull/24861  \\n- Sensitive user information is not displayed in logs\\n  - https://github.com/apache/doris/pull/26912\\n\\n\\n## Bug fixes\\n\\n### Complex datatypes\\n- Fix issues that fixed-length CHAR(n) was not truncated correctly in map/struct.\\n  - https://github.com/apache/doris/pull/25725\\n- Fix write failure for struct datatype nested for map/array\\n  - https://github.com/apache/doris/pull/26973\\n- Fix the issue that count distinct did not support array/map/struct \\n  - https://github.com/apache/doris/pull/25483\\n- Fix be crash in updating to 2.0.3 after the delete complex type appeared in query \\n  - https://github.com/apache/doris/pull/26006\\n- Fix be crash when JSON datatype is in WHERE clause.\\n  - https://github.com/apache/doris/pull/27325\\n- Fix be crash when ARRAY datatype is in OUTER JOIN clause.\\n  - https://github.com/apache/doris/pull/25669\\n- Fix reading incorrect result for DECIMAL datatype in ORC format.\\n  - https://github.com/apache/doris/pull/26548\\n  - https://github.com/apache/doris/pull/25977\\n  - https://github.com/apache/doris/pull/26633\\n\\n### Inverted index\\n- Fix incorrect result for OR NOT combination in WHERE clause were incorrect when disable inverted index query. \\n  - https://github.com/apache/doris/pull/26327\\n- Fix be crash when write a empty with inverted index\\n  - https://github.com/apache/doris/pull/25984\\n- Fix be crash in index compaction when the output of compaction is empty.\\n  - https://github.com/apache/doris/pull/25486\\n- Fixed the problem of adding an inverted index to be crashed when no data is written to the newly added column.\\n- Fix be crash when BUILD INDEX after ADD COLUMN without new data written.\\n  - https://github.com/apache/doris/pull/27276\\n- Fix missing and leak problem of hardlink for inverted index file.\\n  - https://github.com/apache/doris/pull/26903\\n- Fix index file corrupt when disk is full temporarily\\n  - https://github.com/apache/doris/pull/28191\\n- Fix incorrect result due to optimization for skip reading index column\\n  - https://github.com/apache/doris/pull/28104\\n\\n### Materialized View\\n- Fix the problem of BE crash caused by repeated expressions in the group by statement\\n- Fix be crash when there are duplicate expressions in `group by` statements.\\n  - https://github.com/apache/doris/pull/27523\\n- Disables the float/doubld type in the `group by` clause when a view is created. \\n  - https://github.com/apache/doris/pull/25823\\n- Improve the function of select query matching materialized view \\n  - https://github.com/apache/doris/pull/24691 \\n- Fix an issue that materialized views could not be matched when a table alias was used \\n  - https://github.com/apache/doris/pull/25321\\n- Fix the problem using percentile_approx when creating materialized views \\n  - https://github.com/apache/doris/pull/26528\\n\\n### Table sample\\n- Fix the problem that table sample query can not work on table with partitions.\\n  - https://github.com/apache/doris/pull/25912  \\n- Fix the problem that table sample query can not work when specify tablet.\\n  - https://github.com/apache/doris/pull/25378 \\n\\n\\n### Unique with merge on write\\n- Fix null pointer exception in conditional update based on primary key  \\n  - https://github.com/apache/doris/pull/26881    \\n- Fix field name capitalization issues in partial update  \\n  - https://github.com/apache/doris/pull/27223 \\n- Fix duplicate keys occur in mow during schema change repairement. \\n  - https://github.com/apache/doris/pull/25705\\n\\n\\n### Load and compaction\\n- Fix unknown slot descriptor error in routineload for running multiple tables \\n  - https://github.com/apache/doris/pull/25762\\n- Fix be crash due to concurrent memory access when caculating memory \\n  - https://github.com/apache/doris/pull/27101 \\n- Fix be crash on duplicate cancel for load.\\n  - https://github.com/apache/doris/pull/27111\\n- Fix broker connection error during broker load\\n  - https://github.com/apache/doris/pull/26050\\n- Fix incorrect result delete predicates in concurrent case of compation and scan.\\n  - https://github.com/apache/doris/pull/24638\\n- Fix the problem tha compaction task would print too many stacktrace logs \\n  - https://github.com/apache/doris/pull/25597\\n\\n\\n### Data Lake compatibility\\n- Solve the problem that the iceberg table contains special characters that cause query failure \\n  - https://github.com/apache/doris/pull/27108 \\n- Fix compatibility issues of different hive metastore versions \\n  - https://github.com/apache/doris/pull/27327 \\n- Fix an error reading MaxCompute partition table \\n  - https://github.com/apache/doris/pull/24911 \\n- Fix the issue that backup to object storage failed \\n  - https://github.com/apache/doris/pull/25496 \\n  - https://github.com/apache/doris/pull/25803\\n\\n\\n### JDBC external table compatibility \\n\\n- Fix Oracle date type format error in jdbc catalog  \\n  - https://github.com/apache/doris/pull/25487 \\n- Fix MySQL 0000-00-00 date exception in jdbc catalog  \\n  - https://github.com/apache/doris/pull/26569 \\n- Fix an exception in reading data from Mariadb where the default value of the time type is current_timestamp \\n  - https://github.com/apache/doris/pull/25016 \\n- Fix be crash when processing BITMAP datatype in jdbc catalog\\n  - https://github.com/apache/doris/pull/25034 \\n  - https://github.com/apache/doris/pull/26933\\n\\n\\n### SQL Planner and Optimizer\\n\\n- Fix partition prune error in some scenes\\n  - https://github.com/apache/doris/pull/27047\\n  - https://github.com/apache/doris/pull/26873\\n  - https://github.com/apache/doris/pull/25769\\n  - https://github.com/apache/doris/pull/27636\\n\\n- Fix incorrect sub-query processing in some scenarios\\n  - https://github.com/apache/doris/pull/26034\\n  - https://github.com/apache/doris/pull/25492\\n  - https://github.com/apache/doris/pull/25955\\n  - https://github.com/apache/doris/pull/27177\\n\\n- Fix some semantic parsing errors\\n  - https://github.com/apache/doris/pull/24928\\n  - https://github.com/apache/doris/pull/25627\\n\\n- Fix data loss during right outer/anti join\\n  - https://github.com/apache/doris/pull/26529\\n\\n- Fix incorrect pushing down of predicate pass aggregation operators.\\n  - https://github.com/apache/doris/pull/25525\\n\\n- Fix incorrect result header in some cases\\n  - https://github.com/apache/doris/pull/25372\\n\\n- Fix incorrect plan when the nullsafeEquals expression (<=>) is used as the join condition\\n  - https://github.com/apache/doris/pull/27127\\n\\n- Fix correct column prune in set operation operator.\\n  - https://github.com/apache/doris/pull/26884\\n\\n\\n### Others\\n\\n- Fix BE crash when the order of columns in a table is changed and then upgraded to 2.0.3.\\n  - https://github.com/apache/doris/pull/28205\\n\\n\\nSee the complete list of improvements and bug fixes on [github dev/2.0.3-merged](https://github.com/apache/doris/issues?q=label%3Adev%2F2.0.3-merged+is%3Aclosed) ."},{"id":"/empowering-cyber-security-by-enabling-seven-times-faster-log-analysis","metadata":{"permalink":"/blog/empowering-cyber-security-by-enabling-seven-times-faster-log-analysis","source":"@site/blog/empowering-cyber-security-by-enabling-seven-times-faster-log-analysis.md","title":"Empowering cyber security by enabling 7 times faster log analysis","description":"This is about how a cyber security service provider built its log storage and analysis system (LSAS) and realized 3X data writing speed, 7X query execution speed, and visualized management.","date":"2023-12-07T00:00:00.000Z","tags":[{"inline":true,"label":"Best Practice","permalink":"/blog/tags/best-practice"}],"hasTruncateMarker":false,"authors":[{"name":"Apache Doris","key":null,"page":null}],"frontMatter":{"title":"Empowering cyber security by enabling 7 times faster log analysis","description":"This is about how a cyber security service provider built its log storage and analysis system (LSAS) and realized 3X data writing speed, 7X query execution speed, and visualized management.","date":"2023-12-07","author":"Apache Doris","tags":["Best Practice"],"image":"/images/cyber-security.png"},"unlisted":false,"prevItem":{"title":"Apache Doris 2.0.3 just released","permalink":"/blog/release-note-2.0.3"},"nextItem":{"title":"How big data is saving lives in real time: IoV data analytics helps prevent accidents","permalink":"/blog/how-big-data-is-saving-lives-in-real-time-iov-data-analytics-helps-prevent-accidents"}},"content":"\x3c!-- \\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\nThis is about how a cyber security service provider built its log storage and analysis system (LSAS) and realized 3X data writing speed, 7X query execution speed, and visualized management. \\n\\n## Log storage & analysis platform\\n\\nIn this use case, the LSAS collects system logs from its enterprise users, scans them, and detects viruses. It also provides data management and file tracking services. \\n\\nWithin the LSAS, it scans local files and uploads the file information as MD5 values to its cloud engine and identifies suspicious viruses. The cloud engine returns a log entry to tell the risk level of the files. The log entry includes messages like `file_name`, `file_size`, `file_level`, and `event_time`. Such information goes into a Topic in Apache Kafka, and then the real-time data warehouse normalizes the log messages. After that, all log data will be backed up to the offline data warehouse. Some log data requires further security analysis, so it will be pulled into the analytic engine and the self-developed Extended Detection and Response system (XDR) for more comprehensive detection. \\n\\n![cyber-security-log-storage-and-analysis-platform](/images/cyber-security-log-storage-and-analysis-platform.png)\\n\\nThe above process comes down to log writing and analysis, and the company faced some issues in both processes with their old system, which used StarRocks as the analytic engine.\\n\\n### Slow data writing\\n\\nThe cloud engine interacts with tens of millions of terminal software and digests over 100 billion logs every day. The enormous data size poses a big challenge. The LSAS used to rely on StarRocks for log storage. With the ever-increasing daily log influx, data writing gradually slows down. The severe backlogs during peak times undermines system stability. They tried scaling the cluster from 3 nodes to 13 nodes, but the writing speed wasn\'t substantially improved.\\n\\n### Slow query execution\\n\\nFrom an execution standpoint, extracting security information from logs involves a lot of keyword matching in the text fields (URL, payload, etc.). The StarRocks-based system does that by the SQL LIKE operator, which implements full scanning and brutal-force matching. In that way, queries on a 100-billion-row table often take one or several minutes. After screening out irrelevant data based on time range, the query response time still ranges from seconds to dozens of seconds, and it gets worse with concurrent queries.\\n\\n## Architectural upgrade\\n\\nIn the search for a new database tool, the cyber security company set their eye on [Apache Doris](https://doris.apache.org/zh-CN/), which happened to have sharpened itself up in [version 2.0](https://doris.apache.org/zh-CN/blog/release-note-2.0.0) for log analysis. It supports [inverted index](https://doris.apache.org/docs/dev/data-table/index/inverted-index/) to empower text search, and [NGram BloomFilter](https://doris.apache.org/docs/dev/data-table/index/ngram-bloomfilter-index?_highlight=ngram) to speed up the LIKE operator. \\n\\nAlthough StarRocks was a fork of Apache Doris, it has rewritten part of the code and is now very different from Apache Doris in terms of features. The foregoing inverted index and NGram BloomFilter are a fragment of the current advancements that Apache Doris has made.\\n\\nThey tried Apache Doris out to evaluate its writing speed, query performance, and the associated storage and maintenance costs. \\n\\n### 300% data writing speed\\n\\nTo test the peak performance of Apache Doris, they only used 3 servers and connected it to Apache Kafka to receive their daily data input, and this is the test result compared to the old StarRocks-based LSAS.\\n\\n![apache-doris-vs-starrocks-writing-throughput](/images/apache-doris-vs-starrocks-writing-throughput.png)\\n\\nBased on the peak performance of Apache Doris, it\'s estimated that a 3-server cluster with 30% of CPU usage will be able to handle the writing workload. That can save them over 70% of hardware resources. Notably, in this test, they enabled inverted index for half of the fields. If it were disabled, the writing speed could be increased by another 50%.\\n\\n### 60% storage cost\\n\\nWith inverted index enabled, Apache Doris used even smaller storage space than the old system without inverted indexes. The data compression ratio was 1: 5.7 compared to the previous 1: 4.3.\\n\\nIn most databases and similar tools, the index file is often 2~4 times the size of the data file it belongs to, but in Apache Doris, the index-data size is basically one to one. That means Apache Doris can save a lot of storage space for users. This is because it has adopted columnar storage and the ZStandard compression. With data and indexes being stored column by column, it is easier to compress them, and the ZStandard algorithm is faster with higher compression ratio so it is perfect for log processing. \\n\\n### 690% query speed\\n\\nTo compare the query performance before and after upgrading, they tested the old and the new systems with 79 of their frequently executed SQL statements on the same 100 billion rows of log data with the same cluster size of 10 backend nodes.\\n\\nThey jotted down the query response time as follows:\\n\\nThe new Apache Doris-based system is faster in all 79 queries. On average, it reduces the query execution time by a factor of 7.\\n\\n![apache-doris-vs-starrocks-query-performance](/images/apache-doris-vs-starrocks-query-performance.png)\\n\\nAmong these queries, the greatest increases in speed were enabled by a few features and optimizations of Apache Doris for log analysis.\\n\\n**1. Inverted index accelerating keyword searches: Q23, Q24, Q30, Q31, Q42, Q43, Q50**\\n\\nExample: Q43 was sped up 88.2 times.\\n\\n```SQL\\nSELECT count() from table2 \\nWHERE ( event_time >= 1693065600000 and event_time < 1693152000000) \\n  AND (rule_hit_big MATCH \'xxxx\');\\n```\\n\\nHow is [inverted index](https://doris.apache.org/docs/dev/data-table/index/inverted-index/) implemented? Upon data writing, Apache Doris tokenizes the texts into words, and takes notes of which word exists in which rows. For example, the word \\"machine\\" is in Row 127 and Row 201. In keyword searches, the system can quickly locate the relevant data by tracking the row numbers in the indexes.\\n\\nInverted index is much more efficient than brutal-force scanning in text searches. For one thing, it doesn\'t have to read that much data. For another, it doesn\'t require text matching. So it is able to increase execution speed by orders of magnitudes.\\n\\n![cyber-security-inverted-index](/images/cyber-security-inverted-index.png)\\n\\n**2. NGram BloomFilter accelerating the LIKE operator: Q75, Q76, Q77, Q78**\\n\\nExample: Q75 was sped up 44.4 times.\\n\\n```SQL\\nSELECT * FROM table1\\nWHERE  ent_id = \'xxxxx\'   \\n   AND event_date = \'2023-08-27\'   \\n   AND file_level = 70     \\n   AND rule_group_id LIKE \'adid:%\'     \\nORDER BY event_time LIMIT 100\uFF1B\\n```\\n\\nFor non-verbatim searches, the LIKE operator is an important implementation method, so Apache Doris 2.0 introduces the [NGram BloomFilter](https://doris.apache.org/docs/dev/data-table/index/ngram-bloomfilter-index) to empower that. \\n\\nDifferent from regular BloomFilter, the NGram BloomFilter does not put the entire text into the filter, but splits it into continuous sub-strings of length N, and then puts the sub-strings into the filter. For a query like `cola LIKE \'%pattern%\'`, it splits `\'pattern\'` into several strings of length N, and sees if each of these sub-strings exists in the dataset. The absence of any sub-string in the dataset will indicate that the dataset does not contain the word `\'pattern\'`, so it will be skipped in data scanning, and that\'s how the NGram BloomFilter accelerates queries.\\n\\n**3. Optimizations for Top-N queries: Q19~Q29**\\n\\nExample: Q22 was sped up 50.3 times.\\n\\n```SQL\\nSELECT * FROM table1\\nwhere event_date = \'2023-08-27\' and file_level = 70 \\n  and ent_id = \'nnnnnnn\' and file_name = \'xxx.exe\'\\norder by event_time limit 100;\\n```\\n\\nTop-N queries are to find the N logs that fit into the specified conditions. It is a common type of query in log analysis, with the SQl being like `SELECT * FROM t WHERE xxx ORDER BY xx LIMIT n`. Apache Doris has optimized itself for that. Based on the intermediate status of queries, it figures out the dynamic range of the ranking field and implements automatic predicate pushdown to reduce data scanning. In some cases, this can decrease the scanned data volume by an order of magnitude.\\n\\n### Visualized operation & maintenance\\n\\nFor more efficient cluster maintenance, VeloDB, a commercial company with products based on Apache Doris , has contributed a visualized cluster management tool called [Doris Manager](https://github.com/apache/doris-manager) to the Apache Doris project. Everyday management and maintenance operations can be done via the Doris Manager, including cluster monitoring, inspection, configuration modification, scaling, and upgrading. The visualized tool can save a lot of manual efforts and avoid the risks of maloperations on Doris.\\n\\n![doris-manager-for-visualized-operation-and-maintenance](/images/doris-manager-for-visualized-operation-and-maintenance.png)\\n\\nApart from cluster management, Doris Manager provides a visualized WebUI for log analysis (think of Kibana), so it\'s very friendly to users who are familiar with the ELK Stack. It supports keyword searches, trend charts, field filtering, and detailed data listing and collapsed display, so it enables interactive analysis and easy drilling down of logs.\\n\\n![doris-manager-webui-showcase](/images/doris-manager-webui-showcase.png)\\n\\nAfter a month-long trial run, they officially replaced their old LSAS with the Apache Doris-based system for production, and achieved great results as they expected. Now, they ingest their 100s of billions of new logs every day via the [Routine Load](https://doris.apache.org/docs/dev/data-operate/import/import-way/routine-load-manual/) method at a speed 3 times as fast as before. Among the 7-time overall query performance increase, they benefit from a speedup of over 20 times in full-text searches. And they enjoy easier maintenance and interactive analysis. Their next step is to expand the coverage of JSON data type and delve into semi-structured data analysis. Luckily, the upcoming Apache Doris 2.1 will provide more schema-free support. It will have a new Variant data type, support JSON data of any structures, and allow for flexible changes in field numbers and field types. Relevant updates will be released on the [Apache Doris website](https://doris.apache.org/) and the [Apache Doris community](https://join.slack.com/t/apachedoriscommunity/shared_invite/zt-2unfw3a3q-MtjGX4pAd8bCGC1UV0sKcw)."},{"id":"/how-big-data-is-saving-lives-in-real-time-iov-data-analytics-helps-prevent-accidents","metadata":{"permalink":"/blog/how-big-data-is-saving-lives-in-real-time-iov-data-analytics-helps-prevent-accidents","source":"@site/blog/how-big-data-is-saving-lives-in-real-time-iov-data-analytics-helps-prevent-accidents.md","title":"How big data is saving lives in real time: IoV data analytics helps prevent accidents","description":"What needs to be taken care of in IoV data analysis? What\'s the difference between a near real-time analytic data platform and an actual real-time analytic data platform?","date":"2023-11-29T00:00:00.000Z","tags":[{"inline":true,"label":"Best Practice","permalink":"/blog/tags/best-practice"}],"hasTruncateMarker":false,"authors":[{"name":"velodb.io \xb7 VeloDB Engineering Team","key":null,"page":null}],"frontMatter":{"title":"How big data is saving lives in real time: IoV data analytics helps prevent accidents","description":"What needs to be taken care of in IoV data analysis? What\'s the difference between a near real-time analytic data platform and an actual real-time analytic data platform?","date":"2023-11-29","author":"velodb.io \xb7 VeloDB Engineering Team","tags":["Best Practice"],"externalLink":"https://www.velodb.io/blog/141","image":"/images/Iov.png"},"unlisted":false,"prevItem":{"title":"Empowering cyber security by enabling 7 times faster log analysis","permalink":"/blog/empowering-cyber-security-by-enabling-seven-times-faster-log-analysis"},"nextItem":{"title":"Less components, higher performance: Apache Doris instead of ClickHouse, MySQL, Presto, and HBase","permalink":"/blog/less-components-higher-performance-apache-doris-instead-of-clickhouse-mysql-presto-and-hbase"}},"content":"\x3c!-- \\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\nInternet of Vehicles, or IoV, is the product of the marriage between the automotive industry and IoT. IoV data is expected to get larger and larger, especially with electric vehicles being the new growth engine of the auto market. The question is: Is your data platform ready for that? This post shows you what an OLAP solution for IoV looks like.\\n\\n## What is special about IoV data?\\n\\nThe idea of IoV is intuitive: to create a network so vehicles can share information with each other or with urban infrastructure. What\u2018s often under-explained is the network within each vehicle itself. On each car, there is something called Controller Area Network (CAN) that works as the communication center for the electronic control systems. For a car traveling on the road, the CAN is the guarantee of its safety and functionality, because it is responsible for:\\n\\n- **Vehicle system monitoring**: The CAN is the pulse of the vehicle system. For example, sensors send the temperature, pressure, or position they detect to the CAN; controllers issue commands (like adjusting the valve or the drive motor) to the executor via the CAN. \\n- **Real-time feedback**: Via the CAN, sensors send the speed, steering angle, and brake status to the controllers, which make timely adjustments to the car to ensure safety. \\n- **Data sharing and coordination**: The CAN allows for data exchange (such as status and commands) between various devices, so the whole system can be more performant and efficient.\\n- **Network management and troubleshooting**: The CAN keeps an eye on devices and components in the system. It recognizes, configures, and monitors the devices for maintenance and troubleshooting.\\n\\nWith the CAN being that busy, you can imagine the data size that is traveling through the CAN every day. In the case of this post, we are talking about a car manufacturer who connects 4 million cars together and has to process 100 billion pieces of CAN data every day. \\n\\n## IoV data processing\\n\\nTo turn this huge data size into valuable information that guides product development, production, and sales is the juicy part. Like most data analytic workloads, this comes down to data writing and computation, which are also where challenges exist:\\n\\n- **Data writing at scale**: Sensors are everywhere in a car: doors, seats, brake lights... Plus, many sensors collect more than one signal. The 4 million cars add up to a data throughput of millions of TPS, which means dozens of terabytes every day. With increasing car sales, that number is still growing. \\n- **Real-time analysis**: This is perhaps the best manifestation of \\"time is life\\". Car manufacturers collect the real-time data from their vehicles to identify potential malfunctions, and fix them before any damage happens.\\n- **Low-cost computation and storage**: It\'s hard to talk about huge data size without mentioning its costs. Low cost makes big data processing sustainable.\\n\\n## From Apache Hive to Apache Doris: a transition to real-time analysis\\n\\nLike Rome, a real-time data processing platform is not built in a day. The car manufacturer used to rely on the combination of a batch analytic engine (Apache Hive) and some streaming frameworks and engines (Apache Flink, Apache Kafka) to gain near real-time data analysis performance. They didn\'t realize they needed real-time that bad until real-time was a problem.\\n\\n**Near Real-Time Data Analysis Platform**\\n\\nThis is what used to work for them:\\n\\n![IoV-Hive-based-data-warehouse](/images/IoV-Hive-based-data-warehouse.png)\\n\\nData from the CAN and vehicle sensors are uploaded via 4G network to the cloud gateway, which writes the data into Kafka. Then, Flink processes this data and forwards it to Hive. Going through several data warehousing layers in Hive, the aggregated data is exported to MySQL. At the end, Hive and MySQL provide data to the application layer for data analysis, dashboarding, etc.\\n\\nSince Hive is primarily designed for batch processing rather than real-time analytics, you can tell the mismatch of it in this use case.\\n\\n- **Data writing**: With such a huge data size, the data ingestion time from Flink into Hive was noticeably long. In addition, Hive only supports data updating at the granularity of partitions, which is not enough for some cases.\\n- **Data analysis**: The Hive-based analytic solution delivers high query latency, which is a multi-factor issue. Firstly, Hive was slower than expected when handling large tables with 1 billion rows. Secondly, within Hive, data is extracted from one layer to another by the execution of Spark SQL, which could take a while. Thirdly, as Hive needs to work with MySQL to serve all needs from the application side, data transfer between Hive and MySQL also adds to the query latency. \\n\\n**Real-Time Data Analysis Platform**\\n\\nThis is what happens when they add a real-time analytic engine to the picture:\\n\\n![IoV-Doris-based-data-warehouse](/images/IoV-Doris-based-data-warehouse.png)\\n\\nCompared to the old Hive-based platform, this new one is more efficient in three ways:\\n\\n- **Data writing**: Data ingestion into [Apache Doris](https://doris.apache.org/) is quick and easy, without complicated configurations and the introduction of extra components. It supports a variety of data ingestion methods. For example, in this case, data is written from Kafka into Doris via [Stream Load](https://doris.apache.org/docs/data-operate/import/import-way/stream-load-manual), and from Hive into Doris via [Broker Load](https://doris.apache.org/docs/data-operate/import/import-way/broker-load-manual). \\n- **Data analysis**: To showcase the query speed of Apache Doris by example, it can return a 10-million-row result set within seconds in a cross-table join query. Also, it can work as a [unified query gateway](https://doris.apache.org/docs/lakehouse/multi-catalog/) with its quick access to external data (Hive, MySQL, Iceberg, etc.), so analysts don\'t have to juggle between multiple components.\\n- **Computation and storage costs**: Apache Doris provides the Z-Standard algorithm that can bring a 3~5 times higher data compression ratio. That\'s how it helps reduce costs in data computation and storage. Moreover, the compression can be done solely in Doris so it won\'t consume resources from Flink.\\n\\nA good real-time analytic solution not only stresses data processing speed, it also considers all the way along your data pipeline and smoothens every step of it. Here are two examples:\\n\\n### 1. The arrangement of CAN data\\n\\nIn Kafka, CAN data was arranged by the dimension of CAN ID. However, for the sake of data analysis, analysts had to compare signals from various vehicles, which meant to concatenate data of different CAN ID into a flat table and align it by timestamp. From that flat table, they could derive different tables for different analytic purposes. Such transformation was implemented using Spark SQL, which was time-consuming in the old Hive-based architecture, and the SQL statements are high-maintenance. Moreover, the data was updated by batch on a daily basis, which meant they could only get data from a day ago. \\n\\nIn Apache Doris, all they need is to build the tables with the [Aggregate Key model](https://doris.apache.org/docs/data-table/data-model#aggregate-model), specify VIN (Vehicle Identification Number) and timestamp as the Aggregate Key, and define other data fields by `REPLACE_IF_NOT_NULL`. With Doris, they don\'t have to take care of the SQL statements or the flat table, but are able to extract real-time insights from real-time data.\\n\\n![IoV-CAN-data](/images/IoV-CAN-data.jpeg)\\n\\n### 2. DTC data query\\n\\nOf all CAN data, DTC (Diagnostic Trouble Code) deserves high attention and separate storage, because it tells you what\'s wrong with a car. Each day, the manufacturer receives around 1 billion pieces of DTC. To capture life-saving information from the DTC, data engineers need to relate the DTC data to a DTC configuration table in MySQL.\\n\\nWhat they used to do was to write the DTC data into Kafka every day, process it in Flink, and store the results in Hive. In this way, the DTC data and the DTC configuration table were stored in two different components. That caused a dilemma: a 1-billion-row DTC table was hard to write into MySQL, while querying from Hive was slow. As the DTC configuration table was also constantly updated, engineers could only import a version of it into Hive on a regular basis. That meant they didn\'t always get to relate the DTC data to the latest DTC configurations. \\n\\nAs is mentioned, Apache Doris can work as a unified query gateway. This is supported by its [Multi-Catalog](https://doris.apache.org/docs/lakehouse/multi-catalog/) feature. They import their DTC data from Hive into Doris, and then they create a MySQL Catalog in Doris to map to the DTC configuration table in MySQL. When all this is done, they can simply join the two tables within Doris and get real-time query response.\\n\\n![IoV-DTC-data-query](/images/IoV-DTC-data-query.png)\\n\\n## Conclusion\\n\\nThis is an actual real-time analytic solution for IoV. It is designed for data at really large scale, and it is now supporting a car manufacturer who receives 10 billion rows of new data every day in improving driving safety and experience.\\n\\nBuilding a data platform to suit your use case is not easy, I hope this post helps you in building your own analytic solution.\\n\\n\\n\\nApache Doris [GitHub repo](https://github.com/apache/doris)\\n\\nFind Apache Doris makers on [Slack](https://join.slack.com/t/apachedoriscommunity/shared_invite/zt-2unfw3a3q-MtjGX4pAd8bCGC1UV0sKcw)"},{"id":"/less-components-higher-performance-apache-doris-instead-of-clickhouse-mysql-presto-and-hbase","metadata":{"permalink":"/blog/less-components-higher-performance-apache-doris-instead-of-clickhouse-mysql-presto-and-hbase","source":"@site/blog/less-components-higher-performance-apache-doris-instead-of-clickhouse-mysql-presto-and-hbase.md","title":"Less components, higher performance: Apache Doris instead of ClickHouse, MySQL, Presto, and HBase","description":"This post is about building a unified OLAP platform. An insurance company tries to build a data warehouse that can undertake all their customer-facing, analyst-facing, and management-facing data analysis workloads.","date":"2023-11-22T00:00:00.000Z","tags":[{"inline":true,"label":"Best Practice","permalink":"/blog/tags/best-practice"}],"hasTruncateMarker":false,"authors":[{"name":"velodb.io \xb7 CIGNA & CMB","key":null,"page":null}],"frontMatter":{"title":"Less components, higher performance: Apache Doris instead of ClickHouse, MySQL, Presto, and HBase","description":"This post is about building a unified OLAP platform. An insurance company tries to build a data warehouse that can undertake all their customer-facing, analyst-facing, and management-facing data analysis workloads.","date":"2023-11-22","author":"velodb.io \xb7 CIGNA & CMB","tags":["Best Practice"],"externalLink":"https://www.velodb.io/blog/140","image":"/images/cigna-cmb.png"},"unlisted":false,"prevItem":{"title":"How big data is saving lives in real time: IoV data analytics helps prevent accidents","permalink":"/blog/how-big-data-is-saving-lives-in-real-time-iov-data-analytics-helps-prevent-accidents"},"nextItem":{"title":"Apache Doris Summit Asia 2023: what can you expect from apache doris as a data warehouse?","permalink":"/blog/apache-doris-summit-asia-2023-what-can-you-expect-from-apache-doris-as-a-data-warehouse"}},"content":"\x3c!-- \\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\nThis post is about building a unified OLAP platform. An insurance company tries to build a data warehouse that can undertake all their customer-facing, analyst-facing, and management-facing data analysis workloads. The main tasks include: \\n\\n- **Self-service insurance contract query**: This is for insurance customers to check their contract details by their contract ID. It should also support filters such as coverage period, insurance types, and claim amount. \\n- **Multi-dimensional analysis**: Analysts develop their reports based on different data dimensions as they need, so they can extract insights to facilitate product innovation and their anti-fraud efforts. \\n- **Dashboarding**: This is to create visual overview of the insurance sales trends and the horizontal and vertical comparison of different metrics.\\n\\n## Component-Heavy Data Architecture\\n\\nThe user started with Lambda architecture, splitting their data pipeline into a batch processing link and a stream processing link. For real-time data streaming, they apply Flink CDC; for batch import, they incorporate Sqoop, Python, and DataX to build their own data integration tool named Hisen.  \\n\\n![multi-component-data-warehouse-mysql-clickhouse-hbase-hive-presto](/images/multi-component-data-warehouse-mysql-clickhouse-hbase-hive-presto.png)\\n\\nThen, the real-time and offline data meets in the data warehousing layer, which is made up of five components.\\n\\n**ClickHouse**\\n\\nThe data warehouse is of flat table design and ClickHouse is superb in flat table reading. But as business evolves, things become challenging in two ways:\\n\\n- To support cross-table joins and point queries, the user requires the star schema, but that\'s difficult to implement in ClickHouse.\\n- Changes in insurance contracts need to be updated in the data warehouse in real time. In ClickHouse, that is done by recreating a flat table to overwrite the old one, which is not fast enough.\\n\\n**MySQL**\\n\\nAfter calculation, data metrics are stored in MySQL, but as the data size grows, MySQL starts to struggle, with emerging problems like prolonged execution time and errors thrown.\\n\\n**Apache** **Hive** **+ Presto**\\n\\nHive is the main executor in the batch processing link. It can transform, aggregate, query offline data. Presto is a complement to Hive for interactive analysis.\\n\\n**Apache HBase**\\n\\nHBase undertakes primary key queries. It reads customer status from MySQL and Hive, including customer credits, coverage period, and sum insured. However, since HBase does not support secondary indexes, it has limited capability in reading non-primary key columns. Plus, as a NoSQL database, HBase does not support SQL statements.\\n\\nThe components have to work in conjunction to serve all needs, making the data warehouse too much to take care of. It is not easy to get started with because engineers must be trained on all these components. Also, the complexity of architecture adds to the risks of latency. \\n\\nSo the user tried to look for a tool that ticks more boxes in fulfilling their requirements. The first thing they need is real-time capabilities, including real-time writing, real-time updating, and real-time response to data queries. Secondly, they need more flexibility in data analysis to support customer-facing self-service queries, like multi-dimensional analysis, join queries of large tables, primary key indexes, roll-ups, and drill-downs. Then, for batch processing, they also want high throughput in data writing.\\n\\nThey eventually made up their mind with [Apache Doris](https://doris.apache.org/). \\n\\n## Replacing Four Components with Apache Doris\\n\\n Apache Doris is capable of both real-time and offline data analysis, and it supports both high-throughput interactive analysis and high-concurrency point queries. That\'s why it can replace ClickHouse, MySQL, Presto, and Apache HBase and work as the unified query gateway for the entire data system. \\n\\n![unified-data-warehouse-kafka-apache-doris-hive](/images/unified-data-warehouse-kafka-apache-doris-hive.png)\\n\\nThe improved data pipeline is a much cleaner Lambda architecture. \\n\\nApache Doris provides a wide range of data ingestion methods. It\'s quick in data writing. On top of this, it also implements Merge-on-Write to improve its performance on concurrent point queries. \\n\\n**Reduced Cost**\\n\\nThe new architecture has reduced the user\'s cost in human efforts. For one thing, the much simpler data architecture leads to much easier maintenance; for another, developers no longer need to join the real-time and offline data in the data serving API.\\n\\nThe user can also save money with Doris because it supports tiered storage. It allows the user to put their huge amount of rarely accessed historical data in object storage, which is much cheaper to hoard data.\\n\\n**Higher Efficiency**\\n\\nApache Doris can reach a QPS of 10,000s and respond to billions of point queries within milliseconds, so the customer-facing queries are easy for it to handle. Tiered storage that separates hot data from cold data also increases their query efficiency.\\n\\n**Service Availability**\\n\\nAs a unified data warehouse for storage, computation, and data services, Apache Doris allows for easy disaster recovery. With less components, they don\'t have to worry about data loss or duplication. \\n\\nAn important guarantee of service availability for the user is the Cross-Cluster Replication (CCR) capability of Apache Doris. It can synchronize data from cluster to cluster within minutes or even seconds, and it implements two mechanisms to ensure data reliability:\\n\\n- **Binlog**: This mechanism can automatically log the data changes and generate a LogID for each data modification operation. The incremental LogIDs make sure that data changes are traceable and orderly.\\n- **Data persistence**: In the case of system meltdown or emergencies, data will be put into disks.\\n\\n## A Deeper Look into Apache Doris\\n\\nApache Doris can replace the ClickHouse, MySQL, Presto, and HBase because it has a comprehensive collection of capabilities all along the data processing pipeline. In data ingestion, it enables low-latency real-time writing based on its support for Flink CDC and Merge-on-Write. It guarantees Exactly-Once writing by its Label mechanism and transactional loading. In data queries, it supports both Star Schema and flat table aggregation, so it can provide high performance in bother multi-table joins and large single table queries. It also provides various ways to speed up different queries, like [inverted index](https://doris.apache.org/docs/dev/data-table/index/inverted-index/) for full-text search and range queries, short-circuit plan and prepared statements for point queries."},{"id":"/apache-doris-summit-asia-2023-what-can-you-expect-from-apache-doris-as-a-data-warehouse","metadata":{"permalink":"/blog/apache-doris-summit-asia-2023-what-can-you-expect-from-apache-doris-as-a-data-warehouse","source":"@site/blog/apache-doris-summit-asia-2023-what-can-you-expect-from-apache-doris-as-a-data-warehouse.md","title":"Apache Doris Summit Asia 2023: what can you expect from apache doris as a data warehouse?","description":"The past year marks a breakthrough of Apache Doris, an open-source real-time data warehouse that has just undergone an overall upgrade after long consistent incremental optimizations.","date":"2023-11-10T00:00:00.000Z","tags":[{"inline":true,"label":"Top News","permalink":"/blog/tags/top-news"}],"hasTruncateMarker":false,"authors":[{"name":"Apache Doris","key":null,"page":null}],"frontMatter":{"title":"Apache Doris Summit Asia 2023: what can you expect from apache doris as a data warehouse?","description":"The past year marks a breakthrough of Apache Doris, an open-source real-time data warehouse that has just undergone an overall upgrade after long consistent incremental optimizations.","date":"2023-11-10","author":"Apache Doris","tags":["Top News"],"image":"/images/doris-summit-asia.png"},"unlisted":false,"prevItem":{"title":"Less components, higher performance: Apache Doris instead of ClickHouse, MySQL, Presto, and HBase","permalink":"/blog/less-components-higher-performance-apache-doris-instead-of-clickhouse-mysql-presto-and-hbase"},"nextItem":{"title":"Data analysis for live streaming: what happens in real time is analyzed in real time","permalink":"/blog/data-analysis-for-live-streaming-what-happens-in-real-time-is-analyzed-in-real-time"}},"content":"\x3c!-- \\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\nWhen it is cranberry and pumpkin season, we had the unforgettable Apache Doris Summit Asia 2023 with our remarkable committers, users, and community partners, to honor what we have achieved in the past year, and provide a preview of where we are going next.\\n\\nThe past year marks a breakthrough of [Apache Doris](https://doris.apache.org/), an open-source real-time data warehouse that has just undergone an overall upgrade after long consistent incremental optimizations:\\n\\n**More**\\n\\nThanks to the hard work of 275 committers, the [Apache Doris 2.0](https://doris.apache.org/blog/release-note-2.0.0) milestone has merged over 4100 pull requests, representing a 70% increase from version 1.2 last year and a 10-fold increase from 1.1. \\n\\n**Faster** \\n\\nThis year, Apache Doris has attained a 10-fold performance increase in blind benchmarking and single-table queries, a 13-fold increase in multi-table joins, and a 20-fold increase in concurrent point queries. The high query performance is supported by the smart design of Apache Doris, including a vectorized execution engine, Merge-on-Write mechanism, the Light Schema Change feature, a self-adaptive parallel execution model, and a [new query optimizer](https://doris.apache.org/docs/query/nereids/nereids-new).\\n\\n**Wider**\\n\\nWe have built Apache Doris into more than just a powerful OLAP engine but also a data warehouse for a wider range of use cases, including log analysis and high-concurrency data services. To expand the data warehousing capabilities of Apache Doris, we have introduced [Multi-Catalog](https://doris.apache.org/docs/lakehouse/multi-catalog/) to connect Doris to a wide array of data sources.\\n\\n## One of the most active open source big data projects\\n\\nApache Doris has become one of the world\'s most active open-source big data projects in all aspects:\\n\\n- It has hit **10K stars** on [GitHub](https://github.com/apache/doris/), a year-on-year growth of 70%, and the momentum keeps going.\\n- The community has included almost 600 contributors and welcomes new faces every week.\\n- With **120 monthly active contributors**, Apache Doris has become a more active project than Apache Spark, Elasticsearch, Trino, and Apache Druid.\\n- Over **160 pull requests** are created every week. Meanwhile, we have established a mature code review pipeline, making sure that every pull request stands the test of 3000 use cases. This is how we guarantee stability in the midst of agile iteration.\\n\\n![Apache-Doris-monthly-active-contributors](/images/summit2023/Apache-Doris-monthly-active-contributors.png)\\n\\nAlong with such growth, we\'ve also witnessed higher diversity among contributors. They are engineers from tech giants and database unicorns, like VeloDB, which is a commercial company with products based on Apache Doris. Many cloud service providers, including Alibaba Cloud, Tencent Cloud, Huawei Cloud, AWS and GCP (coming soon), have also jumped on the bandwagon and provided Doris-based data warehouse cloud hosting services.\\n\\n## Fast-expanding user base\\n\\nApache Doris now has a user base of over 30,000 data engineers from more than **4000 enterprises**, including those from the tech sector, finance, telecom, manufacturing, logistics, and retail. The great majority of them keep in close touch with the Apache Doris developers, committing code, getting involved in tests, and sharing experience and feedback with the community. \\n\\n## Fruit that have been reaped\\n\\nWe aim to make Apache Doris the first choice for people in real-time data analysis. What we have done in the past year can be concluded in three keywords:\\n\\n- **Real-time**: We have realized high-throughput real-time data writing and updates, as well as low query latency.\\n- **Unified**: As we\'ve been trying to make Doris an all-in-one platform that can undertake most of the analytic workloads for users, we have expanded and enhanced the data lakehousing capabilities of Doris, enabled faster log analysis, faster ELT/ETL, and faster response to point queries.\\n- **Cloud-native**: This is a leap towards cloud infrastructure. Apache Doris can now be deployed and run on Kubernetes to reduce storage and computation costs.\\n\\n### Real-time response to queries\\n\\nAs is said, Apache Doris 2.0 delivers 10 times faster query speed than the previous versions, but what is the key accelerator behind such high performance? It is the [cost-based query optimizer](https://doris.apache.org/docs/query/nereids/nereids-new) and the self-adaptive [pipeline parallel execution model](https://doris.apache.org/docs/query-acceleration/pipeline-execution-engine/) of Apache Doris. \\n\\nIn traditional data reporting, data is often arranged in flat tables. The idea of flat tables and pre-aggregated tables is to trade storage space for query speed. In these cases, the key to high performance is to accelerate data scanning and aggregation. However, since nowadays data analytic workloads involve more complex computations with more and larger batch processing, data engineers often have to fine-tune the database and rewrite the SQL before they can enjoy satisfactory query speeds. That\'s why we have refactored the the query optimizer in Apache Doris. The new query optimizer can figure out the most efficient query execution plan for a thousand-line SQL or a join query that relates dozens of tables, saving engineers lots of efforts.\\n\\nSimilarly, the new version of Doris has automated another engineering-intensive process: adjusting the compute instance execution concurrency in the backend. What bothered our users was that when queries of different sizes happened concurrently, these queries tended to fight for resources and thus required human intervention. To solve that, we have introduced a pipeline execution model. It automatically decides the execution concurrency for the current situation to make sure queries of all sizes are executed smoothly. As a result, Doris now has more efficient CPU usage and higher system stability during query execution.\\n\\nFor **[high concurrency point queries](https://doris.apache.org/blog/How-We-Increased-Database-Query-Concurrency-by-20-Times)**, Apache Doris 2.0 reached a throughput of 30,000 QPS. It is a 20-fold improvement driven by optimizations in data storage, reading, and query execution. As a column-oriented DBMS, Apache Doris has relatively low row reading efficiency, so we have introduced ow/column hybrid storage and [row cache](https://doris.apache.org/docs/query-acceleration/hight-concurrent-point-query/) to make up for that. We have also enabled the short circuit plan and prepared statements in Apache Doris. The former allows simple queries to skip the query planner for faster execution, and the latter allows users to reuse SQL for similar queries and thus reduce frontend overhead.\\n\\n![hybrid-column-row-storage](/images/summit2023/hybrid-column-row-storage.png)\\n\\nFor **multi-dimensional data analysis**, we introduced [inverted index](https://doris.apache.org/blog/Building-A-Log-Analytics-Solution-10-Times-More-Cost-Effective-Than-Elasticsearch) to accelerate fuzzy keyword queries, equivalence queries, and range queries.\\n\\n### Real-time data writing and update\\n\\nData writing is another side of the real-time story, so we also spent great efforts improving the data ingestion speed of Apache Doris. After optimizations like Memtable parallel flushing and single-copy ingestion, Apache Doris is now 2~8 times faster in data writing. \\n\\n![data-writing-efficiency](/images/summit2023/data-writing-efficiency.png)\\n\\nThe **[Merge-on-Write](https://doris.apache.org/docs/data-table/data-model#merge-on-write)** mechanism has been upgraded in version 2.0. It enables an upsert throughput of nearly 1 million rows per second, and it now supports a wider range of updating operations, including partial column updates.\\n\\n![merge-on-write](/images/summit2023/merge-on-write.png)\\n\\n### Support for more use cases\\n\\nFor **[data lakehousing](https://doris.apache.org/blog/Building-the-Next-Generation-Data-Lakehouse-10X-Performance)**, our last big move was to introduce [Multi-Catalog](https://doris.apache.org/docs/lakehouse/multi-catalog/) for auto-mapping and auto-synchronization of heterogeneous data sources. In 2.0, we have further enhanced that. It now supports even more data sources, and it is also much faster in various production environments. With multi-catalog, users can ingest their multi-source data into Doris using the simple `insert into select` operation. \\n\\nFor **[log analysis](https://doris.apache.org/blog/Building-A-Log-Analytics-Solution-10-Times-More-Cost-Effective-Than-Elasticsearch)**, Doris 2.0 provides native support for semi-structured data, which can be arranged in data types like Json, Array, and Map. On the basis of Light Schema Change, it allows Schema Evolution. In addition to the foregoing inverted index, Doris 2.0 comes with a high-performance text analysis algorithm. Built on its large-size data writing and low-cost storage capabilities, Apache Doris is 10 times more cost-effective than the common log analytic solutions on the market.\\n\\nFor different analytic workloads in one single cluster, the Doris solution to **resource isolation** is [Workload Group](https://doris.apache.org/docs/admin-manual/workload-group). As the name implies, it is to divide various workloads into groups and thus allow more flexible use of memory and CPU resources. Users can limit the number of queries that a workload group can handle concurrently, so when there are too many query requests, the excessive ones will wait in a queue. This is a way to release system burden. \\n\\n![resource-isolation-workload-group](/images/summit2023/resource-isolation-workload-group.png)\\n\\n### Low cost and high availability\\n\\nApache Doris provides **[tiered storage](https://doris.apache.org/blog/Tiered-Storage-for-Hot-and-Cold-Data-What-Why-and-How)**. The less frequently accessed data, namely, cold data, will be put into object storage to reduce costs. Moreover, since object storage only requires a single copy of data, the storage costs will be further cut by 2/3 compared to 3-replica storage. Calculation based on AWS pricing shows that tiered storage can save you 70% of your cloud disk expenditure.\\n\\n![tiered-storage](/images/summit2023/tiered-storage.png)\\n\\nTo facilitate Kubernetes deployment, we have built a **Kubernetes Operator**. With it, users can easily deploy, scale, inspect, and maintain all Apache Doris nodes (frontends, backends, compute nodes, brokers) on Kubernetes. Compute node is a variant of backend nodes but it does not store any data, which is why it is a good fit for auto-scaling of clusters. During computation peaks, compute nodes can flexibly join the cluster and share the burden. Auto-scaling has been under active testing and will soon be released in upcoming versions of Apache Doris.\\n\\n![kubernetes-operator-for-apache-doris](/images/summit2023/kubernetes-operator-for-apache-doris.png)\\n\\nFor service availability guarantee, Apache Doris 2.0 supports **Cross-Cluster Replication (CCR)**. As a disaster recovery solution, it supports read-write separation and multi-data center backup. \\n\\n## Reach for the stars\\n\\nIn the foreseeable future, Apache Doris will go further on the aforementioned three directions: real-time, unified, and cloud-native. \\n\\n### Get even faster\\n\\nIn the upcoming Apache Doris 2.1, the **cost-based query optimizer (CBO)** will be able to automatically collect execution statistics and provide support for hint syntax. It will also allow users to adjust the optimizing rules. To fully demonstrate the performance of our CBO, we will release a TPC-DS benchmark results. \\n\\nIn addition, Doris 2.1 will support **multi-table materialized views** and **writing intermediate results to disks**. Meanwhile, a Union All operator will be added to accelerate the ETL process in Apache Doris. That means users will experience higher performance and stability when processing large batches of data. You can also expect a new Join algorithm that can double the execution speed of multi-table join queries.\\n\\nIn terms of **data writing**, we try to make it simpler and more intuitive for you, and efforts will be made in three aspects. \\n\\n1. In future versions, data streams, local files, and those from relational databases or data lakes will all be put into relational tables, and they can all be written into Doris using the simple `insert into` statement. \\n2. We will simplify the data writing pipeline. Data writing will be implemented by the built-in job scheduling mechanism, so users won\'t need an extra data synchronization component. \\n3. When there is frequent data writing, Doris will wait until the data accumulates into a sizable batch at the server end, so as to reduce the pressure caused by small file merging.\\n\\nIn terms of **data updating**, as the Merge-on-Write mechanism advances towards maturity, it will be enabled in Doris by default. Users will be able to flexibly update or modify any columns in tables as they want. Also, based on Merge-on-Write, we will build a one-size-fits-all data model, so users don\'t have to rack their brains choosing the right data model for various use cases.\\n\\nApache Doris 2.1 will have enhanced **observability**. It will provide a brand new Profile for users to monitor operator execution, and visualize the query execution status with the aid of [Doris Manager](https://github.com/apache/doris-manager).\\n\\n![doris-manager](/images/summit2023/doris-manager.png)\\n\\n### Support more analytic scenarios\\n\\nThe above-mentioned multi-table materialized view and built-in job scheduling mechanism will also benefit the **data lakehousing** capability of Doris. From heterogeneous data sources to the data warehouse, users won\'t need a second component to do ETL and data warehouse layering. \\n\\nIn version 2.0, we support data writeback to JDBC sources, and we are going to expand that functionality to more data sources, including Apache Iceberg, Apache Hudi, Delta Lake and Apache Paimon.\\n\\n![apache-doris-data-warehouse-layers](/images/summit2023/apache-doris-data-warehouse-layers.png)\\n\\nFor data ingestion from data lakes, Apache Doris currently adopts the MySQL protocol. In large-scale data reading or data science use cases (like those involving Pandas), this might be a throughput bottleneck. Thus, what we are doing is introducing an Arrow Flight-based high-speed reading interface, which transfers data via the Doris backends directly. **In our tests, the new interface delivers a writing throughput that is 100 times higher.**\\n\\n![writing-throughput](/images/summit2023/writing-throughput.png)\\n\\nFor **log analysis**, the inverted index will support more complicated data types, such as Array, Map, and GEO. We will also introduce a new data type named Variant to provide **schema-free support**. This means users can not only put Json data of any shapes and types in the table fields, but also easily handle schema changes without any DDL operations.\\n\\n![schemaless-variant-data-type](/images/summit2023/schemaless-variant-data-type.png)\\n\\nFor **workload management**, we will enable higher flexibility. Users will be able to use SQL to create, manage, and allocate resources for their Workload Groups. We will continue to maximize resource utilization while ensuring resource isolation between workload groups.\\n\\n### Cloud-nativeness and storage-compute separation\\n\\nWhen Apache Doris 2.0 was released, we previewed the merging of the SelectDB Cloud storage-compute separation solution into the Apache Doris project. After some intense code refactoring and compatibility building, this functionality will be good and ready in Apache Doris 2.2, and users will be able to experience the elastic computation capability. \\n\\n![storage-compute-separation](/images/summit2023/storage-compute-separation.png)\\n\\n## Stick to Innovation\\n\\nAs Apache Doris is on the ramp, we look back on its ten-year development and ask ourselves: **what injects vitality to this great project and keep it vibrant for this long?** The answer is, we have been working with innovators.\\n\\nBack in the time when SQL on Hadoop gained currency, Apache Doris chose to stay outside the Hadoop ecosystem. It does not rely on HDFS for data storage, nor Zookeeper for distributed monitoring, but insists on providing high availability by its scalable processes. When the major databases on the market goes by their own syntaxes, Apache Doris adopts stand SQL and the MySQL protocol, in order to lower the threshold for users. \\n\\nFrom the self-developed pre-aggregation storage engine, materialized views, and the MPP framework, to inverted index, row/column hybrid storage, Light Schema Change, Merge-on-Write, and the Variant data type, Apache Doris never stops breaking new ground to provide better performance and user experience, which is also what we are going to do next:\\n\\n- We want to work with more open-source enthusiasts to make a difference to the world.\\n- We want to keep inspiring the data world by presenting more use cases.\\n- We want to provide more and better choices for users by collaborating with partners along the data pipeline and cloud service providers.\\n\\nBy choosing Apache Doris, you choose to stay in the heartbeat of innovation. The [Apache Doris community](https://join.slack.com/t/apachedoriscommunity/shared_invite/zt-2unfw3a3q-MtjGX4pAd8bCGC1UV0sKcw) awaits newcomers."},{"id":"/data-analysis-for-live-streaming-what-happens-in-real-time-is-analyzed-in-real-time","metadata":{"permalink":"/blog/data-analysis-for-live-streaming-what-happens-in-real-time-is-analyzed-in-real-time","source":"@site/blog/data-analysis-for-live-streaming-what-happens-in-real-time-is-analyzed-in-real-time.md","title":"Data analysis for live streaming: what happens in real time is analyzed in real time","description":"As live streaming emerges as a way of doing business, the need for data analysis follows up. This post is about how a live streaming service provider with 800 million end users found the right database to support its analytic solution.","date":"2023-10-30T00:00:00.000Z","tags":[{"inline":true,"label":"Best Practice","permalink":"/blog/tags/best-practice"}],"hasTruncateMarker":false,"authors":[{"name":"velodb.io \xb7 He Gong","key":null,"page":null}],"frontMatter":{"title":"Data analysis for live streaming: what happens in real time is analyzed in real time","description":"As live streaming emerges as a way of doing business, the need for data analysis follows up. This post is about how a live streaming service provider with 800 million end users found the right database to support its analytic solution.","date":"2023-10-30","author":"velodb.io \xb7 He Gong","tags":["Best Practice"],"externalLink":"https://www.velodb.io/blog/139","image":"/images/live-streaming.png"},"unlisted":false,"prevItem":{"title":"Apache Doris Summit Asia 2023: what can you expect from apache doris as a data warehouse?","permalink":"/blog/apache-doris-summit-asia-2023-what-can-you-expect-from-apache-doris-as-a-data-warehouse"},"nextItem":{"title":"Apache Doris announced the official release of version 2.0.2","permalink":"/blog/release-2.0.2"}},"content":"\x3c!-- \\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n\\n\\n## What\'s different about data analytics in live streaming?\\n\\nLive streaming is one typical use case for real-time data analysis, because it stresses speed. Livestream organizers need to keep abreast of the latest data to see what is happening and maximize effectiveness. To realize that requires high efficiency in every step of data processing:\\n\\n- **Data writing**: A live event churns out huge amounts of data every second, so the database should be able to ingest such high throughput stably.\\n- **Data update**: As life itself, live streaming entails a lot of data changes, so there should be a quick and reliable data updating mechanism to absorb the changes.\\n- **Data queries**: Data should be ready and accessible as soon as analysts want it. Mostly that means real-time visibility.\\n- **Maintenance**: What\'s special about live streaming is that the data stream has prominent highs and lows. The analytic system should be able to ensure stability during peak times, and allow scaling down in off-peak times in order to improve resource utilization. If possible, it should also provide disaster recovery services to guarantee system availability, since the worst case in live streaming is interruption. \\n\\nThe rest of this post is about how a live streaming service provider with 800 million end users found the right database to support its analytic solution.\\n\\n## Simplify the Components\\n\\nIn this case, the live streaming data analytic platform adopts the Lambda architecture, which consists of a batch processing pipeline and a streaming pipeline, the former for user profile information and the latter for real-time generated data, including metrics like real-time subscription, visitor count, comments and responses. \\n\\n- **Batching processing**: The user basic information stored in HDFS is written into HBase to form a table.\\n- **Streaming**: Real-time generated data from MySQL, collected via Flink CDC, goes into Apache Kafka. Flink works as the computation engine and then the data is stored in Redis.\\n\\n![database-for-live-shopping-Elasticsearch-HBase](/images/xiaoe-tech-1.png)\\n\\nThe real-time metrics will be combined with the user profile information to form a flat table, and Elasticsearch will work as the query engine.\\n\\nAs their business burgeons, the expanding data size becomes unbearable for this platform, with problems like:\\n\\n- **Delayed data writing**: The multiple components result in multiple steps in data writing, and inevitably lead to prolonged data writing, especially during peak times. \\n- **Complicated updating mechanism**: Every time there is a data change, such as that in user subscription information, it must be updated into the main tables and dimensional tables, and then the tables are correlated to generate a new flat table. And don\'t forget that this long process has to be executed across multiple components. So just imagine the complexity.\\n- **Slow queries**: As the query engine, Elasticsearch struggles with concurrent query requests and data accesses. It is also not flexible enough to deal with the join queries.\\n- **Time-consuming maintenance**: All engineers developing or maintaining this platform need to master all the components. That\'s a lot of training. And adding new metrics to the data pool is labor-intensive.\\n\\nSo to sum up, the main problem for this architecture is its complexity. To reduce the components means to find a database that is not only capable of most workloads, but also performant in data writing and queries. After 6 months of testing, they finally upgraded their live streaming analytic platform with [Apache Doris](https://doris.apache.org/). \\n\\nThey converge the streaming and the batch processing pipelines at Apache Doris. It can undertake analytic workloads and also provides a storage layer so data doesn\'t have to shuffle back to Elasticsearch and HBase as it did in the old architecture.\\n\\nWith Apache Doris as the data warehouse, the platform architecture becomes neater.\\n\\n![database-for-live-shopping-Apache-Doris](/images/xiaoe-tech-2.png)\\n\\n- **Smooth data writing**: Raw data is processed by Flink and written into Apache Doris in real time. The Doris community provides a [Flink-Doris-Connector](https://github.com/apache/doris-flink-connector) with built-in Flink CDC.\\n- **Flexible data update**: For data changes, Apache Doris implements [Merge-on-Write](https://doris.apache.org/docs/data-table/data-model/#merge-on-write). This is especially useful in small-batch real-time writing because you don\'t have to renew the entire flat table. It also supports partial update of columns, which is another way to make data updates more lightweight. In this case, Apache Doris is able to finish Upsert or Insert Overwrite operations for **200,000 rows per second**, and these are all done in large tables with the biggest ones reaching billions of rows. \\n- **Faster queries**: For join queries, Apache Doris can easily join multiple large tables (10 billion rows). It can respond to a rich variety of queries within seconds or even milliseconds, including tag retrievals, fuzzy queries, ranking, and paginated queries.\\n- **Easier maintenance**: As for Apache Doris itself, the frontend and backend nodes are both flexibly scalable. It is compatible with MySQL protocol. What took the developers a month now can be finished within a week, which allows for more agile iteration of metrics. \\n\\nThe above shows how Apache Doris speeds up the entire data processing pipeline with its all-in-one capabilities. Beyond that, it has some delightful features that can increase query efficiency and ensure service reliability in the case of live streaming.  \\n\\n## Disaster Recovery\\n\\nThe last thing you want in live streaming is service breakdown, so disaster recovery is necessary.\\n\\nBefore the live streaming platform had Apache Doris in place, they only backed up their data to object storage. It took an hour from when a failure was reported to when it was fixed. That one-hour window is fatal for live commerce because viewers will leave immediately. Thus, disaster recovery must be quick.\\n\\nNow, with Apache Doris, they have a dual-cluster solution: a primary cluster and a backup cluster. This is for hot backup. Besides that, they have a cold backup plan, which is the same as what they did: backing up their everyday data to object storage via Backup and Freeze policies.\\n\\nThis is how they do hot backup before [Apache Doris 2.0](https://doris.apache.org/zh-CN/blog/release-note-2.0.0): \\n\\n- **Data dual-write**: Write data to both the primary cluster and backup cluster. \\n- **Load balancing**: In case there is something wrong with one cluster, query requests can be directed to the other cluster via reverse proxy.\\n- **Monitoring**: Regularly check the data consistency between the two clusters. \\n\\nApache Doris 2.0 supports [Cross Cluster Replication (CCR)](https://doris.apache.org/zh-CN/blog/release-note-2.0.0#support-for-cross-cluster-replication-ccr), which can automate the above processes to reduce maintenance costs and inconsistency risks due to human factors.\\n\\n## Data Visualization\\n\\nIn addition to reporting, dashboarding, and ad-hoc queries, the platform also allows analysts to configure various data sources to produce their own visualized data lists. \\n\\nApache Doris is compatible with most BI tools on the market, so the platform developers can tap on that and provide a broader set of functionalities for live streamers.\\n\\nAlso, built on the real-time capabilities and quick computation of Apache Doris, live streams can view data and see what happens in real time, instead of waiting for a day for data analysis.\\n\\n## Bitmap Index to Accelerate Tag Queries\\n\\nA big part of data analysis in live streaming is viewer profiling. Viewers are divided into groups based on their online footprint. They are given tags like \\"watched for over one minute\\" and \\"visited during the past minute\\". As the show goes on, viewers are constantly tagged and untagged. In the data warehouse, it means frequent data insertion and deletion. Plus, one viewer is given multiple tags. To gain an overall understanding of users entail join queries, which is why the join performance of the data warehouse is important. \\n\\nThe following snippets give you a general idea of how to tag users and conduct tag queries in Apache Doris.\\n\\n**Create a Tag Table**\\n\\nA tag table lists all the tags that are given to the viewers, and maps the tags to the corresponding viewer ID.\\n\\n```SQL\\ncreate table db.tags (  \\nu_id string,  \\nversion string,  \\ntags string\\n) with (  \\n\'connector\' = \'doris\',  \\n\'fenodes\' = \'\',  \\n\'table.identifier\' = \'tags\',  \\n\'username\' = \'\',  \\n\'password\' = \'\',  \\n\'sink.properties.format\' = \'json\',  \\n\'sink.properties.strip_outer_array\' = \'true\',  \\n\'sink.properties.fuzzy_parse\' = \'true\',  \\n\'sink.properties.columns\' = \'id,u_id,version,a_tags,m_tags,a_tags=bitmap_from_string(a_tags),m_tags=bitmap_from_string(m_tags)\',  \\n\'sink.batch.interval\' = \'10s\',  \\n\'sink.batch.size\' = \'100000\' \\n);\\n```\\n\\n**Create a Tag Version Table**\\n\\nThe tag table is constantly changing, so there are different versions of it as time goes by.\\n\\n```SQL\\ncreate table db.tags_version (  \\nid string,  \\nu_id string,  \\nversion string  \\n) with (  \\n\'connector\' = \'doris\',  \\n\'fenodes\' = \'\',  \\n\'table.identifier\' = \'db.tags_version\',  \\n\'username\' = \'\',  \\n\'password\' = \'\',  \\n\'sink.properties.format\' = \'json\',  \\n\'sink.properties.strip_outer_array\' = \'true\',  \\n\'sink.properties.fuzzy_parse\' = \'true\',  \\n\'sink.properties.columns\' = \'id,u_id,version\',  \\n\'sink.batch.interval\' = \'10s\',  \\n\'sink.batch.size\' = \'100000\'  \\n);\\n```\\n\\n**Write Data into Tag Table and Tag Version Table**\\n\\n```SQL\\ninsert into db.tags\\nselect\\nu_id,  \\nlast_timestamp as version,\\ntags\\nfrom db.source;  \\n  \\ninsert into rtime_db.tags_version\\nselect \\nu_id,  \\nlast_timestamp as version\\nfrom db.source;\\n```\\n\\n**Tag Queries Accelerated by Bitmap Index**\\n\\nFor example, analysts need to find out the latest tags related to a certain viewer with the last name Thomas. Apache Doris will run the LIKE operator in the user information table to find all \\"Thomas\\". Then it creates bitmap indexes for the tags. Lastly, it relates all user information table, tag table, and tag version table to return the result.\\n\\n**Of almost a billion viewers and each of them has over a thousand tags, the bitmap index can help reduce the query response time to less than one second.**\\n\\n```SQL\\nwith t_user as (\\n    select \\n           u_id,\\n           name\\n    from db.user\\n    where partition_id = 1\\n    and name like \'%Thomas%\'\\n),\\n\\n t_tags as (\\n         select \\n                 u_id, \\n                 version\\n         from db.tags\\n         where (\\n                   bitmap_and_count(a_tags, bitmap_from_string(\\"123,124,125,126,333\\")) > 0 \\n           )\\n ),\\n \\n t_tag_version as (\\n         select id, u_id, version\\n         from db.tags_version\\n )\\n\\nselect \\n    t1.u_id\\n    t1.name\\nfrom t_user t1\\njoin t_tags t2 on t1.u_id = t2.u_id\\njoin t_tag_version t3 on t2.u_id = t3.u_id and t2.version = t3.version\\norder by t1.u_id desc\\nlimit 1,10;\\n```\\n\\n## Conclusion\\n\\nData analysis in live streaming is challenging for the underlying database, but it is also where the key competitiveness of Apache Doris comes to play. First of all, Apache Doris can handle most data processing workloads, so platform builders don\'t have to worry about putting many components together and consequential maintenance issues. Secondly, it has a lot of query-accelerating features, including but not limited to indexes. After tackling the speed issues, the [Apache Doris developer community](https://join.slack.com/t/apachedoriscommunity/shared_invite/zt-2unfw3a3q-MtjGX4pAd8bCGC1UV0sKcw) has been exploring its boundaries, such as introducing a more efficient cost-based query optimizer in version 2.0 and inverted index for text searches, fuzzy queries, and range queries. These features are embraced by the live streaming service provider as they are actively testing them and planning to transfer their log analytic workloads to Apache Doris, too."},{"id":"/release-2.0.2","metadata":{"permalink":"/blog/release-2.0.2","source":"@site/blog/release-2.0.2.md","title":"Apache Doris announced the official release of version 2.0.2","description":"Thanks to our community users and developers, 489 improvements and bug fixes have been made in Doris 2.0.2.","date":"2023-10-13T00:00:00.000Z","tags":[{"inline":true,"label":"Release Notes","permalink":"/blog/tags/release-notes"}],"hasTruncateMarker":false,"authors":[{"name":"Apache Doris","key":null,"page":null}],"frontMatter":{"title":"Apache Doris announced the official release of version 2.0.2","description":"Thanks to our community users and developers, 489 improvements and bug fixes have been made in Doris 2.0.2.","date":"2023-10-13","author":"Apache Doris","tags":["Release Notes"],"image":"/images/2.0.2.png"},"unlisted":false,"prevItem":{"title":"Data analysis for live streaming: what happens in real time is analyzed in real time","permalink":"/blog/data-analysis-for-live-streaming-what-happens-in-real-time-is-analyzed-in-real-time"},"nextItem":{"title":"Migrating from ClickHouse to Apache Doris: what happened?","permalink":"/blog/migrating-from-clickhouse-to-apache-doris-what-happened"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n# Release 2.0.2\\n\\nThanks to our community users and developers, 489 improvements and bug fixes have been made in Doris 2.0.2.\\n\\n## Behavior Changes\\n\\n- [Remove json -> operator convert to json_extract #24679](https://github.com/apache/doris/pull/24679)\\n\\n  Remove json \'->\' operator since it is conflicted with lambda function syntax. It\'s a syntax sugar for function json_extract and can be replaced with the former.\\n- [Start the script to set metadata_failure_recovery #24308](https://github.com/apache/doris/pull/24308)\\n\\n  Move metadata_failure_recovery from fe.conf to start_fe.sh argument to prevent being used unexpectedly.\\n- [Change ordinary type null value is \\\\N,complex type null value is null #24207](https://github.com/apache/doris/pull/24207)\\n- [Optimize priority_ network matching logic for be #23795](https://github.com/apache/doris/pull/23795)\\n- [Fix cancel load failed because Job could not be cancelled\u2026 #17730](https://github.com/apache/doris/pull/17730)\\n  \\n  Allow cancel a retrying load job.\\n\\n## Improvements\\n\\n### Easier to use\\n\\n- [Support custom lib dir to save custom libs #23887](https://github.com/apache/doris/pull/23887)\\n  \\n  Add a custom_lib dir to allow users place custom lib files and custom_lib will not be replaced.\\n- [Optimize priority_ network matching logic #23784](https://github.com/apache/doris/pull/23784) \\n\\n  Optimize priority_network logic to avoid error when this config is wrong or not configured.\\n- [Row policy support role #23022](https://github.com/apache/doris/pull/23022) \\n\\n  Support role based auth for row policy.\\n\\n### New optimizer Nereids statistics collection improvement\\n\\n- [Disable file cache while running analysis tasks. #23663](https://github.com/apache/doris/pull/23663)\\n- [Show column stats even when error occurred. #23703](https://github.com/apache/doris/pull/23703)\\n- [Support basic jdbc external table stats collection. #23965](https://github.com/apache/doris/pull/23965)\\n- [Skip unknown col stats check on __internal_scheam and information_schema #24625](https://github.com/apache/doris/pull/24625)\\n\\n### Better support for JDBC, HDFS, Hive, MySQL, MaxCompute, Multi-Catalog\\n\\n- [Support hadoop viewfs. #24168](https://github.com/apache/doris/pull/24168)\\n- [Avoid calling checksum when replaying creating jdbc catalog and fix ranger issue #22369](https://github.com/apache/doris/pull/22369)\\n- [Optimize the JDBC Catalog connection error message #23868](https://github.com/apache/doris/pull/23868) \\n\\n  Improve property check and error message for JDBC catalog\\n- [Fix mc decimal type parse, fix wrong obj location #24242](https://github.com/apache/doris/pull/24242) \\n\\n  Fix some issues for MaxCompute catalog\\n- [Support sql cache for hms catalog #23391](https://github.com/apache/doris/pull/23391) \\n\\n  SQL cache for Hive catalog\\n- [Merge hms partition events. #22869](https://github.com/apache/doris/pull/22869) \\n\\n  Improve performance for Hive metadata sync\\n- [Add metadata_name_ids for quickly get catlogs,db,table and add profiling table in order to Compatible with mysql #22702](https://github.com/apache/doris/pull/22702)\\n\\n### Performance for inverted index query\\n\\n- [Add bkd index query cache to improve perf #23952](https://github.com/apache/doris/pull/23952)\\n- [Improve performance for count on index other than match #24678](https://github.com/apache/doris/pull/24678)\\n- [Improve match performance without index #24751](https://github.com/apache/doris/pull/24751)\\n- [Optimize multiple terms conjunction query #23871](https://github.com/apache/doris/pull/23871) \\nImprove performance of MATCH_ALL\\n- [Optimize unnecessary conversions #24389](https://github.com/apache/doris/pull/24389) \\nImprove performance of MATCH\\n\\n### Improve Array functions\\n\\n- [[Fix old optimizer with some array literal functions #23630](https://github.com/apache/doris/pull/23630)\\n- [Improve array union support multi params #24327](https://github.com/apache/doris/pull/24327)\\n- [Improve explode func with array nested complex type #24455](https://github.com/apache/doris/pull/24455)\\n\\n## Important Bug fixes\\n\\n- [The parameter positions of timestamp diff function to sql are reversed #23601](https://github.com/apache/doris/pull/23601)\\n- [Fix old optimizer with some array literal functions #23630](https://github.com/apache/doris/pull/23630)\\n- [Fix query cache returns wrong result after deleting partitions. #23555](https://github.com/apache/doris/pull/23555)\\n- [Fix potential data loss when clone task\'s dst tablet is cooldown replica #17644](https://github.com/apache/doris/pull/17644)\\n- [Fix array map batch append data with right next_array_item_rowid #23779](https://github.com/apache/doris/pull/23779)\\n- [Fix or to in rule #23940](https://github.com/apache/doris/pull/23940)\\n- [Fix \'char\' function\'s toSql implementation is wrong #23860](https://github.com/apache/doris/pull/23860)\\n- [Record wrong best plan properties #23973](https://github.com/apache/doris/pull/23973)\\n- [Make TVF\'s distribution spec always be RANDOM #24020](https://github.com/apache/doris/pull/24020)\\n- [External scan use STORAGE_ANY instead of ANY as distibution #24039](https://github.com/apache/doris/pull/24039)\\n- [Runtimefilter target is not SlotReference #23958](https://github.com/apache/doris/pull/23958)\\n- [mv in select materialized_view should disable show table #24104](https://github.com/apache/doris/pull/24104)\\n- [Fail over to remote file reader if local cache failed #24097](https://github.com/apache/doris/pull/24097)\\n- [Fix revoke role operation cause fe down #23852](https://github.com/apache/doris/pull/23852)\\n- [Handle status code correctly and add a new error code `ENTRY_NOT_FOUND` #24139](https://github.com/apache/doris/pull/24139)\\n- [Fix leaky abstraction and shield the status code `END_OF_FILE` from upper layers #24165](https://github.com/apache/doris/pull/24165)\\n- [Fix bug that Read garbled files caused be crash. #24164](https://github.com/apache/doris/pull/24164)\\n- [Fix be core when user sepcified empty `column_separator` using hdfs tvf #24369](https://github.com/apache/doris/pull/24369)\\n- [Fix need to restart BE after replacing the jar package in java-udf #24372](https://github.com/apache/doris/pull/24372)\\n- [Need to call \'set_version\' in nested functions #24381](https://github.com/apache/doris/pull/24381)\\n- [windown_funnel compatibility issue with multi backends #24385](https://github.com/apache/doris/pull/24385)\\n- [correlated anti join shouldn\'t be translated to null aware anti join #24290](https://github.com/apache/doris/pull/24290)\\n- [Change ordinary type null value is \\\\N,complex type null value is null #24207](https://github.com/apache/doris/pull/24207)\\n- [Fix analyze failed when there are thousands of partitions. #24521](https://github.com/apache/doris/pull/24521)\\n- [Do not use enum as the data type for JavaUdfDataType. #24460](https://github.com/apache/doris/pull/24460)\\n- [Fix multi window projection issue temporarily #24568](https://github.com/apache/doris/pull/24568)\\n- [Make metadata compatible with 2.0.3 #24610](https://github.com/apache/doris/pull/24610)\\n- [Select outfile column order is wrong #24595](https://github.com/apache/doris/pull/24595)\\n- [Incorrect result of semi/anti mark join #24616](https://github.com/apache/doris/pull/24616)\\n- [Fix broker read issue #24635](https://github.com/apache/doris/pull/24635)\\n- [Skip unknown col stats check on __internal_scheam and information_schema #24625](https://github.com/apache/doris/pull/24625)\\n- [Fixed bug when parsing multi-character delimiters. #24572](https://github.com/apache/doris/pull/24572)\\n- [Fix timezone parse when there is no tzfile #24578](https://github.com/apache/doris/pull/24578)\\n- [We need to issue an error when starting FE without setting the Java home environment #23943](https://github.com/apache/doris/pull/23943)\\n- [Enable_unique_key_partial_update should be forwarded to master #24697](https://github.com/apache/doris/pull/24697)\\n- [Fix paimon file catalog meta issue and replication num analysis issue #24681](https://github.com/apache/doris/pull/24681)\\n- [Add more log for ingest_binlog && Fix ingest_binlog not rewrite rowset_meta tablet_uid #24617](https://github.com/apache/doris/pull/24617)\\n- [Do not abort when a disk is broken #24692](https://github.com/apache/doris/pull/24692)\\n- [colocate join could not work well on full outer join #24700](https://github.com/apache/doris/pull/24700)\\n- [Optimize unnecessary conversions #24389](https://github.com/apache/doris/pull/24389)\\n- [Optimize the reading efficiency of nullable (string) columns. #24698](https://github.com/apache/doris/pull/24698)\\n- [Fix segment cache core when output rowset is nullptr #24778](https://github.com/apache/doris/pull/24778)\\n- [Fix duplicate key in schema change #24782](https://github.com/apache/doris/pull/24782)\\n- [Make metadata compatible for future version after 2.0.2 #24800](https://github.com/apache/doris/pull/24800)\\n- [Fix map/array deserialize string with quote pair #24808](https://github.com/apache/doris/pull/24808)\\n- [Failed on arm platform, with clang compiler and pch on, close #24633 #24636](https://github.com/apache/doris/pull/24636)\\n- [Table column order is changed if add a column and do truncate #24981](https://github.com/apache/doris/pull/24981)\\n- [Make parser mode coarse grained by default #24949](https://github.com/apache/doris/pull/24949)\\n\\nSee the complete list of improvements and bug fixes on [github](https://github.com/apache/doris/issues?q=label%3Adev%2F2.0.2-merged+is%3Aclosed) .\\n\\n## Big Thanks\\n\\nThanks all who contribute to this release:\\n\\n[@adonis0147](https://github.com/adonis0147) [@airborne12](https://github.com/airborne12) [@amorynan](https://github.com/amorynan) [@AshinGau](https://github.com/AshinGau) [@BePPPower](https://github.com/BePPPower) [@BiteTheDDDDt](https://github.com/BiteTheDDDDt) [@bobhan1](https://github.com/bobhan1) [@ByteYue](https://github.com/ByteYue) [@caiconghui](https://github.com/caiconghui) [@CalvinKirs](https://github.com/CalvinKirs) [@cambyzju](https://github.com/cambyzju) [@ChengDaqi2023](https://github.com/ChengDaqi2023) [@ChinaYiGuan](https://github.com/ChinaYiGuan) [@CodeCooker17](https://github.com/CodeCooker17) [@csun5285](https://github.com/csun5285) [@dataroaring](https://github.com/dataroaring) [@deadlinefen](https://github.com/deadlinefen) [@DongLiang-0](https://github.com/DongLiang-0) [@Doris-Extras](https://github.com/Doris-Extras) [@dutyu](https://github.com/dutyu) [@eldenmoon](https://github.com/eldenmoon) [@englefly](https://github.com/englefly) [@freemandealer](https://github.com/freemandealer) [@Gabriel39](https://github.com/Gabriel39) [@gnehil](https://github.com/gnehil) [@GoGoWen](https://github.com/GoGoWen) [@gohalo](https://github.com/gohalo) [@HappenLee](https://github.com/HappenLee) [@hello-stephen](https://github.com/hello-stephen) [@HHoflittlefish777](https://github.com/HHoflittlefish777) [@hubgeter](https://github.com/hubgeter) [@hust-hhb](https://github.com/hust-hhb) [@ixzc](https://github.com/ixzc) [@JackDrogon](https://github.com/JackDrogon) [@jacktengg](https://github.com/jacktengg) [@jackwener](https://github.com/jackwener) [@Jibing-Li](https://github.com/Jibing-Li) [@JNSimba](https://github.com/JNSimba) [@kaijchen](https://github.com/kaijchen) [@kaka11chen](https://github.com/kaka11chen) [@Kikyou1997](https://github.com/Kikyou1997) [@Lchangliang](https://github.com/Lchangliang) [@LemonLiTree](https://github.com/LemonLiTree) [@liaoxin01](https://github.com/liaoxin01) [@LiBinfeng-01](https://github.com/LiBinfeng-01) [@liugddx](https://github.com/liugddx) [@luwei16](https://github.com/luwei16) [@mongo360](https://github.com/mongo360) [@morningman](https://github.com/morningman) [@morrySnow](https://github.com/morrySnow) @mrhhsg @Mryange @mymeiyi @neuyilan @pingchunzhang @platoneko @qidaye @realize096 @RYH61 @shuke987 @sohardforaname @starocean999 @SWJTU-ZhangLei @TangSiyang2001 @Tech-Circle-48 @w41ter @wangbo @wsjz @wuwenchi @wyx123654 @xiaokang @XieJiann @xinyiZzz @XuJianxu @xutaoustc @xy720 @xyfsjq @xzj7019 @yiguolei @yujun777 @Yukang-Lian @Yulei-Yang @zclllyybb @zddr @zhangguoqiang666 @zhangstar333 @ZhangYu0123 @zhannngchen @zxealous @zy-kkk @zzzxl1993 @zzzzzzzs"},{"id":"/migrating-from-clickhouse-to-apache-doris-what-happened","metadata":{"permalink":"/blog/migrating-from-clickhouse-to-apache-doris-what-happened","source":"@site/blog/migrating-from-clickhouse-to-apache-doris-what-happened.md","title":"Migrating from ClickHouse to Apache Doris: what happened?","description":"A user of Apache Doris has written down their migration process from ClickHouse to Doris, including why they need the change, what needs to be taken care of, and how they compare the performance of the two databases in their environment. ","date":"2023-10-11T00:00:00.000Z","tags":[{"inline":true,"label":"Best Practice","permalink":"/blog/tags/best-practice"}],"hasTruncateMarker":false,"authors":[{"name":"velodb.io \xb7 Chuang Li","key":null,"page":null}],"frontMatter":{"title":"Migrating from ClickHouse to Apache Doris: what happened?","description":"A user of Apache Doris has written down their migration process from ClickHouse to Doris, including why they need the change, what needs to be taken care of, and how they compare the performance of the two databases in their environment. ","date":"2023-10-11","author":"velodb.io \xb7 Chuang Li","tags":["Best Practice"],"externalLink":"https://www.velodb.io/blog/138","image":"/images/e-commerce.png"},"unlisted":false,"prevItem":{"title":"Apache Doris announced the official release of version 2.0.2","permalink":"/blog/release-2.0.2"},"nextItem":{"title":"Introduction to Apache Doris: a next-generation real-time data warehouse","permalink":"/blog/introduction-to-apache-doris-a-next-generation-real-time-data-warehouse"}},"content":"\x3c!-- \\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\nMigrating from one OLAP database to another is huge. Even if you\'re unhappy with your current data tool and have found some promising candidate, you might still hesitate to do the big surgery on your data architecture, because you\'re uncertain about how things are going to work. So you need experience shared by someone who has walked the path. \\n\\nLuckily, a user of Apache Doris has written down their migration process from ClickHouse to Doris, including why they need the change, what needs to be taken care of, and how they compare the performance of the two databases in their environment. \\n\\nTo decide whether you want to continue reading, check if you tick one of the following boxes:\\n\\n- You need your join queries to be executed faster.\\n- You need flexible data updates.\\n- You need real-time data analysis.\\n- You need to minimize your components.\\n\\nIf you do, this post might be of some help to you.\\n\\n## Replacing Kylin, ClickHouse, and Druid with Apache Doris\\n\\nThe user undergoing this change is an e-commerce SaaS provider. Its data system serves realtime and offline reporting, customer segmentation, and log analysis. Initially, they used different OLAP engines for these various purposes:\\n\\n- **Apache Kylin for offline reporting**: The system provides offline reporting services for over 5 million sellers. The big ones among them have more than 10 million registered members and 100,000 SKU, and the detailed information is put into over 400 data cubes on the platform. \\n- **ClickHouse for customer segmentation and Top-N log queries**: This entails high-frequency updates, high QPS, and complicated SQL.\\n- **Apache Druid for real-time reporting**: Sellers extract data they need by combining different dimensions, and such real-time reporting requires quick data updates, quick query response, and strong stability of the system. \\n\\n![ClickHouse-Druid-Apache-Kylin](/images/youzan-1.png)\\n\\nThe three components have their own sore spots.\\n\\n- **Apache Kylin** runs well with a fixed table schema, but every time you want to add a dimension, you need to create a new data cube and refill the historical data in it.\\n- **ClickHouse** is not designed for multi-table processing, so you might need an extra solution for federated queries and multi-table join queries. And in this case, it was below expectation in high-concurrency scenarios.\\n- **Apache Druid** implements idempotent writing so it does not support data updating or deletion itself. That means when there is something wrong at the upstream, you will need a full data replacement. And such data fixing is a multi-step process if you think it all the way through, because of all the data backups and movements. Plus, newly ingested data will not be accessible for queries until it is put in segments in Druid. That means a longer window such that data inconsistency between upstream and downstream.\\n\\nAs they work together, this architecture might be too demanding to navigate because it requires knowledge of all these components in terms of development, monitoring, and maintenance. Also, every time the user scales a cluster, they must stop the current cluster and migrate all databases and tables, which is not only a big undertaking but also a huge interruption to business.\\n\\n![Replace-ClickHouse-Druid-Apache-Kylin-with-Apache-Doris](/images/youzan-2.png)\\n\\nApache Doris fills these gaps.\\n\\n- **Query performance**: Doris is good at high-concurrency queries and join queries, and it is now equipped with inverted index to speed up searches in logs.\\n- **Data update**: The Unique Key model of Doris supports both large-volume update and high-freqency real-time writing, and the Duplicate Key model and Unique Key model supports partial column update. It also provides exactly-once guarantee in data writing and ensures consistency between base tables, materialized views, and replicas.\\n- **Maintenance**: Doris is MySQL-compatible. It supports easy scaling and light schema change. It comes with its own integration tools such as Flink-Doris-Connector and Spark-Doris-Connector. \\n\\nSo they plan on the migration.\\n\\n## The Replacement Surgery\\n\\nClickHouse was the main performance bottleneck in the old data architecture and why the user wanted the change in the first place, so they started with ClickHouse.\\n\\n### Changes in SQL statements\\n\\n**Table creation statements**\\n\\n![table-creation-statements-in-ClickHouse-and-Apache-Doris](/images/youzan-3.png)\\n\\nThe user built their own SQL rewriting tool that can convert a ClickHouse table creation statement into a Doris table creation statement. The tool can automate the following changes:\\n\\n- **Mapping the field types**: It converts ClickHouse field types into the corresponding ones in Doris. For example, it converts String as a Key into Varchar, and String as a partitioning field into Date V2.\\n- **Setting the number of historical partitions in dynamic partitioning tables**: Some tables have historical partitions and the number of partitions should be specified upon table creation in Doris, otherwise a \\"No Partition\\" error will be thrown.\\n- **Determining the number of buckets**: It decides the number of buckets based on the data volume of historical partitions; for non-partitioned tables, it decides the bucketing configurations based on the historical data volume.\\n- **Determining TTL**: It decides the time to live of partitions in dynamic partitioning tables.\\n- **Setting the import sequence**: For the Unique Key model of Doris, it can specify the data import order based on the Sequence column to ensure orderliness in data ingestion.\\n\\n![changes-in-table-creation-statements-from-ClickHouse-to-Apache-Doris](/images/youzan-4.png)\\n\\n**Query statements**\\n\\nSimilarly, they have their own tool to transform the ClickHouse query statements into Doris query statements. This is to prepare for the comparison test between ClickHouse and Doris. The key considerations in the conversions include:\\n\\n- **Conversion of table names**: This is simple given the mapping rules in table creation statements.\\n- **Conversion of functions**: For example, the `COUNTIF` function in ClickHouse is equivalent to `SUM(CASE WHEN_THEN 1 ELSE 0)`, `Array Join` is equivalent to `Explode` and `Lateral View`, and `ORDER BY` and `GROUP BY` should be converted to window functions.\\n- **Difference** **in semantics**: ClickHouse goes by its own protocol while Doris is MySQL-compatible, so there needs to be alias for subqueries. In this use case, subqueries are common in customer segmentation, so they use `sqlparse` \\n\\n### Changes in data ingestion methods\\n\\n![changes-in-data-ingestion-methods-from-ClickHouse-to-Apache-Doris](/images/youzan-5.png)\\n\\nApache Doris provides broad options of data writing methods. For the real-time link, the user adopts Stream Load to ingest data from NSQ and Kafka. \\n\\nFor the sizable offline data, the user tested different methods and here are the takeouts:\\n\\n1. **Insert Into**\\n\\nUsing Multi-Catalog to read external data sources and ingesting with Insert Into can serve most needs in this use case.\\n\\n2. **Stream Load**\\n\\nThe Spark-Doris-Connector is a more general method. It can handle large data volumes and ensure writing stability. The key is to find the right writing pace and parallelism.\\n\\nThe Spark-Doris-Connector also supports Bitmap. It allows you to move the computation workload of Bitmap data in Spark clusters. \\n\\nBoth the Spark-Doris-Connector and the Flink-Doris-Connector rely on Stream Load. CSV is the recommended format choice. Tests on the user\'s billions of rows showed that CSV was 40% faster than JSON.  \\n\\n3. **Spark Load**\\n\\nThe Spark Load method utilizes Spark resources for data shuffling and ranking. The computation results are put in HDFS, and then Doris reads the files from HDFS directly (via Broker Load). This approach is ideal for huge data ingestion. The more data there is, the faster and more resource-efficient the ingestion is.  \\n\\n## Pressure Test\\n\\nThe user compared performance of the two components on their SQL and join query scenarios, and calculated the CPU and memory consumption of Apache Doris.\\n\\n### SQL query performance\\n\\nApache Doris outperformed ClickHouse in 10 of the 16 SQL queries, and the biggest performance gap was a ratio of almost 30. Overall, Apache Doris was 2~3 times faster than ClickHouse. \\n\\n![SQL-query-performance-ClickHouse-VS-Apache-Doris](/images/youzan-6.png)\\n\\n### Join query performance\\n\\nFor join query tests, the user used different sizes of main tables and dimension tables.\\n\\n- **Primary tables**: user activity table (4 billion rows), user attribute table (25 billion rows), and user attribute table (96 billion rows)\\n- **Dimension tables**: 1 million rows, 10 million rows, 50 million rows, 100 million rows, 500 million rows, 1 billion rows, and 2.5 billion rows.\\n\\nThe tests include **full join queries** and **filtering join queries**. Full join queries join all rows of the primary table and dimension tables, while filtering join queries retrieve data of a certain seller ID with a `WHERE` filter. The results are concluded as follows:\\n\\n**Primary table (4 billion rows):**\\n\\n- Full join queries: Doris outperforms ClickHouse in full join queries with all dimension tables. The performance gap widens as the dimension tables get larger. The largest difference is a ratio of 5.\\n- Filtering join queries: Based on the seller ID, the filter screened out 41 million rows from the primary table. With small dimension tables, Doris was 2~3 times faster than ClickHouse; with large dimension tables, Doris was over 10 times faster; with dimension tables larger than 100 million rows, ClickHouse threw an OOM error and Doris functions normally. \\n\\n**Primary table (25 billion rows):**\\n\\n- Full join queries: Doris outperforms ClickHouse in full join queries with all dimension tables. ClickHouse produced an OOM error with dimension tables larger than 50 million rows.\\n- Filtering join queries: The filter screened out 570 million rows from the primary table. Doris responded within seconds and ClickHouse finished within minutes and broke down when joining large dimension tables.\\n\\n**Primary table (96 billion rows):**\\n\\nDoris delivered relatively quick performance in all queries and ClickHouse was unable to execute all of them.\\n\\nIn terms of CPU and memory consumption, Apache Doris maintained stable cluster loads in all sizes of join queries.\\n\\n## Future Directions\\n\\nAs the migration goes on, the user works closely with the [Doris community](https://join.slack.com/t/apachedoriscommunity/shared_invite/zt-2unfw3a3q-MtjGX4pAd8bCGC1UV0sKcw), and their feedback has contributed to the making of [Apache Doris 2.0.0](https://doris.apache.org/docs/dev/releasenotes/release-2.0.0/). We will continue assisting them in their migration from Kylin and Druid to Doris, and we look forward to see their Doris-based unified data platform come into being."},{"id":"/introduction-to-apache-doris-a-next-generation-real-time-data-warehouse","metadata":{"permalink":"/blog/introduction-to-apache-doris-a-next-generation-real-time-data-warehouse","source":"@site/blog/introduction-to-apache-doris-a-next-generation-real-time-data-warehouse.md","title":"Introduction to Apache Doris: a next-generation real-time data warehouse","description":"This is a technical overview of Apache Doris, introducing how it enables fast query performance with its architectural design, features, and mechanisms.","date":"2023-10-03T00:00:00.000Z","tags":[{"inline":true,"label":"Tech Sharing","permalink":"/blog/tags/tech-sharing"}],"hasTruncateMarker":false,"authors":[{"name":"velodb.io \xb7 VeloDB Engineering Team","key":null,"page":null}],"frontMatter":{"title":"Introduction to Apache Doris: a next-generation real-time data warehouse","description":"This is a technical overview of Apache Doris, introducing how it enables fast query performance with its architectural design, features, and mechanisms.","date":"2023-10-03","author":"velodb.io \xb7 VeloDB Engineering Team","externalLink":"https://www.velodb.io/blog/127","tags":["Tech Sharing"],"image":"/images/doris-intro.png"},"unlisted":false,"prevItem":{"title":"Migrating from ClickHouse to Apache Doris: what happened?","permalink":"/blog/migrating-from-clickhouse-to-apache-doris-what-happened"},"nextItem":{"title":"Log analysis: Elasticsearch vs Apache Doris","permalink":"/blog/log-analysis-elasticsearch-vs-apache-doris"}},"content":"\x3c!-- \\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n## What is Apache Doris?\\n\\n[Apache Doris](https://doris.apache.org/) is an open-source real-time data warehouse. It can collect data from various data sources, including relational databases (MySQL, PostgreSQL, SQL Server, Oracle, etc.), logs, and time series data from IoT devices. It is capable of reporting, ad-hoc analysis, federated queries, and log analysis, so it can be used to support dashboarding, self-service BI, A/B testing, user behavior analysis and the like.\\n\\nApache Doris supports both batch import and stream writing. It can be well integrated with Apache Spark, Apache Hive, Apache Flink, Airbyte, DBT, and Fivetran. It can also connect to data lakes such as Apache Hive, Apache Hudi, Apache Iceberg, Delta Lake, and Apache Paimon.\\n\\n![What-Is-Apache-Doris](/images/introduction_1.png)\\n\\n## Performance\\n\\nAs a real-time OLAP engine, Apache Doris hasn a competitive edge in query speed. According to the TPC-H and SSB-Flat benchmarking results, Doris can deliver much faster performance than Presto, Greenplum, and ClickHouse.\\n\\nAs for its self-volution, it has increased its query speed by over 10 times in the past two years, both in complex queries and flat table analysis.\\n\\n![Apache-Doris-VS-Presto-Greenplum-ClickHouse](/images/introduction_2.png)\\n\\n## Architectural Design\\n\\nBehind the fast speed of Apache Doris is the architectural design, features, and mechanisms that contribute to the performance of Doris. \\n\\nFirst of all, Apache Doris has a cost-based optimizer (CBO) that can figure out the most efficient execution plan for complicated big queries. It has a fully vectorized execution engine so it can reduce virtual function calls and cache misses. It is MPP-based (Massively Parallel Processing) so it can give full play to the user\'s machines and cores. In Doris, query execution is data-driven, which means whether a query gets executed is determined by whether its relevant data is ready, and this enables more efficient use of CPUs. \\n\\n## Fast Point Queries for A Column-Oriented Database\\n\\nApache Doris is a column-oriented database so it can make data compression and data sharding easier and faster. But this might not be suitable for cases such as customer-facing services. In these cases, a data platform will have to handle requests from a large number of users concurrently (these are called \\"high-concurrency point queries\\"), and having a columnar storage engine will amplify I/O operations per second, especially when data is arranged in flat tables. \\n\\nTo fix that, Apache Doris enables hybrid storage, which means to have row storage and columnar storage at the same time. \\n\\n![Hybrid-Columnar-Row-Storage](/images/Introduction_3.png)\\n\\nIn addition, since point queries are all simple queries, it will be unnecessary and wasteful to call out the query planner, so Doris executes a short circuit plan for them to reduce overhead. \\n\\nAnother big source of overheads in high-concurrency point queries is SQL parsing. For that, Doris has prepared statements. The idea is to pre-compute the SQL statement and cache them, so they can be reused for similar queries.\\n\\n![prepared-statement-and-short-circuit-plan](/images/Introduction_4.png)\\n\\n## Data Ingestion\\n\\nApache Doris provides a range of methods for data ingestion.\\n\\n**Real-Time stream writing**:\\n\\n- **[Stream Load](https://doris.apache.org/docs/dev/data-operate/import/import-way/stream-load-manual?_highlight=stream&_highlight=loa)**: You can apply this method to write local files or data streams via HTTP. It is linearly scalable and can reach a throughput of 10 million records per second in some use cases.\\n- **[Flink-Doris-Connector](https://doris.apache.org/docs/1.2/ecosystem/flink-doris-connector/)**: With built-in Flink CDC, this Connector ingests data from OLTP databases to Doris. So far, we have realized auto-synchronization of data from MySQL and Oracle to Doris.\\n- **[Routine Load](https://doris.apache.org/docs/dev/data-operate/import/import-way/routine-load-manual)**: This is to subscribe data from Kafka message queues. \\n- **[Insert Into](https://doris.apache.org/docs/dev/data-operate/import/import-way/insert-into-manual)**: This is especially useful when you try to do ETL in Doris internally, like writing data from one Doris table to another.\\n\\n**Batch writing**:\\n\\n- **[Spark Load](https://doris.apache.org/docs/dev/data-operate/import/import-way/spark-load-manual)**: With this method, you can leverage Spark resources to pre-process data from HDFS and object storage before writing to Doris.\\n- **[Broker Load](https://doris.apache.org/docs/dev/data-operate/import/import-way/broker-load-manual)**: This supports HDFS and S3 protocol.\\n- `insert into <internal table> select from <external table>`: This simple statement allows you to connect Doris to various storage systems, data lakes, and databases.\\n\\n## Data Update\\n\\nFor data updates, what Apache Doris has to offer is that, it supports both Merge on Read and Merge on Write, the former for low-frequency batch updates and the latter for real-time writing. With Merge on Write, the latest data will be ready by the time you execute queries, and that\'s why it can improve your query speed by 5 to 10 times compared to Merge on Read. \\n\\nFrom an implementation perspective, these are a few common data update operations, and Doris supports them all: \\n\\n- **Upsert**: to replace or update a whole row\\n- **Partial column update**: to update just a few columns in a row\\n- **Conditional updating**: to filter out some data by combining a few conditions in order to replace or delete it\\n- **Insert Overwrite**: to rewrite a table or partition\\n\\nIn some cases, data updates happen concurrently, which means there is numerous new data coming in and trying to modify the existing data record, so the updating order matters a lot. That\'s why Doris allows you to decide the order, either by the order of transaction commit or that of the sequence column (something that you specify in the table in advance). Doris also supports data deletion based on the specified predicate, and that\'s how conditional updating is done.\\n\\n## Service Availability & Data Reliability\\n\\nApart from fast performance in queries and data ingestion, Apache Doris also provides service availability guarantee, and this is how: \\n\\nArchitecturally, Doris has two processes: frontend and backend. Both of them are easily scalable. The frontend nodes manage the clusters, metadata and handle user requests; the backend nodes execute the queries and are capable of auto data balancing and auto-restoration. It supports cluster upgrading and scaling to avoid interruption to services.\\n\\n![architecture-design-of-Apache-Doris](/images/introduction_5.png)\\n\\n## Cross Cluster Replication\\n\\nEnterprise users, especially those in finance or e-commerce, will need to backup their clusters or their entire data center, just in case of force majeure. So Doris 2.0 provides Cross Cluster Replication (CCR). With CCR, users can do a lot:\\n\\n- **Disaster recovery**: for quick restoration of data services\\n- **Read-write separation**: master cluster + slave cluster; one for reading, one for writing\\n- **Isolated upgrade of clusters**: For cluster scaling, CCR allows users to pre-create a backup cluster for a trial run so they can clear out the possible incompatibility issues and bugs.\\n\\nTests show that Doris CCR can reach a data latency of minutes. In the best case, it can reach the upper speed limit of the hardware environment.\\n\\n![Cross-Cluster-Replication-in-Apache-Doris](/images/introduction_6.png)\\n\\n## Multi-Tenant Management\\n\\nApache Doris has sophisticated Role-Based Access Control, and it allows fine-grained privilege control on the level of databases, tables, rows, and columns. \\n\\n![multi-tenant-management-in-Apache-Doris](/images/introduction_7.png)\\n\\nFor resource isolation, Doris used to implement a hard isolation plan, which is to divide the backend nodes into resource groups, and assign the Resource Groups to different workloads. This is a hard isolation plan. It was simple and neat. But sometimes users can make the most out of their computing resource because some Resource Groups are idle.\\n\\n![resource-group-in-Apache-Doris](/images/introduction_8.png)\\n\\nThus, instead of Resource Groups, Doris 2.0 introduces Workload Group. A soft limit is set for a Workload Group about how many resources it can use. When that soft limit is hit, and meanwhile there are some idle resources available. The idle resources will be shared across the workload groups. Users can also prioritize the workload groups in terms of their access to idle resources.\\n\\n![workload-group-in-Apache-Doris](/images/introduction_9.png)\\n\\n## Easy to Use\\n\\nAs many capabilities as Apache Doris provides, it is also easy to use. It supports standard SQL and is compatible with MySQL protocol and most BI tools on the market.\\n\\nAnother effort that we\'ve made to improve usability is a feature called Light Schema Change. This means if users need to add or delete some columns in a table, they just need to update the metadata in the frontend but don\'t have to modify all the data files. Light Schema Change can be done within milliseconds. It also allows changes to indexes and data type of columns. The combination of Light Schema Change and Flink-Doris-Connector means synchronization of upstream tables within milliseconds.\\n\\n## Semi-Structured Data Analysis\\n\\nCommon examples of semi-structure data include logs, observability data, and time series data. These cases require schema-free support, lower cost, and capabilities in multi-dimensional analysis and full-text search.\\n\\nIn text analysis, mostly, people use the LIKE operator, so we put a lot of effort into improving the performance of it, including pushing down the LIKE operator down to the storage layer (to reduce data scanning), and introducing the NGram Bloomfilter, the Hyperscan regex matching library, and the Volnitsky algorithm (for sub-string matching).\\n\\n![LIKE-operator](/images/introduction_10.png)\\n\\nWe have also introduced inverted index for text tokenization. It is a power tool for fuzzy keyword search, full-text search, equivalence queries, and range queries.\\n\\n## Data Lakehouse\\n\\nFor users to build a high-performing data lakehouse and a unified query gateway, Doris can map, cache, and auto-refresh the meta data from external sources. It supports Hive Metastore and almost all open data lakehouse formats. You can connect it to relational databases, Elasticsearch, and many other sources. And it allows you to reuse your own authentication systems, like Kerberos and Apache Ranger, on the external tables.\\n\\nBenchmark results show that Apache Doris is 3~5 times faster than Trino in queries on Hive tables. It is the joint result of a few features: \\n\\n1. Efficient query engine\\n2. Hot data caching mechanism\\n3. Compute nodes\\n4. Views in Doris\\n\\nThe [Compute Nodes](https://doris.apache.org/docs/dev/advanced/compute-node) is a newly introduced solution in version 2.0 for data lakehousing. Unlike normal backend nodes, Compute Nodes are stateless and do not store any data. Neither are they involved in data balancing during cluster scaling. Thus, they can join the cluster flexibly and easily during computation peak times. \\n\\nAlso, Doris allows you to write the computation results of external tables into Doris to form a view. This is a similar thinking to Materialized Views: to trade space for speed. After a query on external tables is executed, the results can be put in Doris internally. When there are similar queries following up, the system can directly read the results of previous queries from Doris, and that speeds things up.\\n\\n## Tiered Storage\\n\\nThe main purpose of tiered storage is to save money. [Tiered storage ](https://doris.apache.org/docs/dev/advanced/cold-hot-separation?_highlight=cold)means to separate hot data and cold data into different storage, with hot data being the data that is frequently accessed and cold data that isn\'t. It allows users to put hot data in the quick but expensive disks (such as SSD and HDD), and cold data in object storage.\\n\\n![tiered-storage-in-Apache-Doris](/images/introduction_11.png)\\n\\nRoughly speaking, for a data asset consisting of 80% cold data, tiered storage will reduce your storage cost by 70%.\\n\\n## The Apache Doris Community\\n\\nThis is an overview of Apache Doris, an open-source real-time data warehouse. It is actively evolving with an agile release schedule, and the [community](https://join.slack.com/t/apachedoriscommunity/shared_invite/zt-2unfw3a3q-MtjGX4pAd8bCGC1UV0sKcw) embraces any questions, ideas, and feedback."},{"id":"/log-analysis-elasticsearch-vs-apache-doris","metadata":{"permalink":"/blog/log-analysis-elasticsearch-vs-apache-doris","source":"@site/blog/log-analysis-elasticsearch-vs-apache-doris.md","title":"Log analysis: Elasticsearch vs Apache Doris","description":"As a major part of a company\'s data asset, logs brings values to businesses in three aspects: system observability, cyber security, and data analysis. They are your first resort for troubleshooting, your reference for improving system security, and your data mine where you can extract information that points to business growth.","date":"2023-09-28T00:00:00.000Z","tags":[{"inline":true,"label":"Tech Sharing","permalink":"/blog/tags/tech-sharing"}],"hasTruncateMarker":false,"authors":[{"name":"velodb.io \xb7 VeloDB Engineering Team","key":null,"page":null}],"frontMatter":{"title":"Log analysis: Elasticsearch vs Apache Doris","description":"As a major part of a company\'s data asset, logs brings values to businesses in three aspects: system observability, cyber security, and data analysis. They are your first resort for troubleshooting, your reference for improving system security, and your data mine where you can extract information that points to business growth.","date":"2023-09-28","author":"velodb.io \xb7 VeloDB Engineering Team","externalLink":"https://www.velodb.io/blog/132","tags":["Tech Sharing"],"image":"/images/es-vs-doris.png"},"unlisted":false,"prevItem":{"title":"Introduction to Apache Doris: a next-generation real-time data warehouse","permalink":"/blog/introduction-to-apache-doris-a-next-generation-real-time-data-warehouse"},"nextItem":{"title":"Log analysis: how to digest 15 billion logs per day and keep big queries within 1 second","permalink":"/blog/Log-Analysis-How-to-Digest-15-Billion-Logs-Per-Day-and-Keep-Big-Queries-Within-1-Second"}},"content":"\x3c!-- \\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\nAs a major part of a company\'s data asset, logs brings values to businesses in three aspects: system observability, cyber security, and data analysis. They are your first resort for troubleshooting, your reference for improving system security, and your data mine where you can extract information that points to business growth.\\n\\nLogs are the sequential records of events in the computer system. If you think about how logs are generated and used, you will know what an ideal log analysis system should look like:\\n\\n- **It should have schema-free support.** Raw logs are unstructured free texts and basically impossible for aggregation and calculation, so you needed to turn them into structured tables (the process is called \\"ETL\\") before putting them into a database or data warehouse for analysis. If there was a schema change, lots of complicated adjustments needed to put into ETL and the structured tables. Therefore, semi-structured logs, mostly in JSON format, emerged. You can add or delete fields in these logs and the log storage system will adjust the schema accordingly. \\n- **It should be low-cost.** Logs are huge and they are generated continuously. A fairly big company produces 10~100 TBs of log data. For business or compliance reasons, it should keep the logs around for half a year or longer. That means to store a log size measured in PB, so the cost is considerable.\\n- **It should be capable of real-time processing.** Logs should be written in real time, otherwise engineers won\'t be able to catch the latest events in troubleshooting and security tracking. Plus, a good log system should provide full-text searching capabilities and respond to interactive queries quickly.\\n\\n## The Elasticsearch-Based Log Analysis Solution\\n\\nA popular log processing solution within the data industry is the **ELK stack: Elasticsearch, Logstash, and Kibana**. The pipeline can be split into five modules:\\n\\n- **Log collection**: Filebeat collects local log files and writes them to a Kafka message queue.\\n- **Log transmission**: Kafka message queue gathers and caches logs.\\n- **Log transfer**: Logstash filters and transfers log data in Kafka.\\n- **Log storage**: Logstash writes logs in JSON format into Elasticsearch for storage.\\n- **Log query**: Users search for logs via Kibana visualization or send a query request via Elasticsearch DSL API.\\n\\n![ELK-Stack](/images/LAS_1.png)\\n\\nThe ELK stack has outstanding real-time processing capabilities, but frictions exist.\\n\\n### Inadequate Schema-Free Support\\n\\nThe Index Mapping in Elasticsearch defines the table scheme, which includes the field names, data types, and whether to enable index creation.\\n\\n![index-mapping-in-Elasticsearch](/images/LAS_2.png)\\n\\nElasticsearch also boasts a Dynamic Mapping mechanism that automatically adds fields to the Mapping according to the input JSON data. This provides some sort of schema-free support, but it\'s not enough because:\\n\\n- Dynamic Mapping often creates too many fields when processing dirty data, which interrupts the whole system.\\n- The data type of fields is immutable. To ensure compatibility, users often configure \\"text\\" as the data type, but that results in much slower query performance than binary data types such as integer.\\n- The index of fields is immutable, too. Users cannot add or delete indexes for a certain field, so they often create indexes for all fields to facilitate data filtering in queries. But too many indexes require extra storage space and slow down data ingestion.\\n\\n### Inadequate Analytic Capability\\n\\nElasticsearch has its unique Domain Specific Language (DSL), which is very different from the tech stack that most data engineers and analysts are familiar with, so there is a steep learning curve. Moreover, Elasticsearch has a relatively closed ecosystem so there might be strong resistance in integration with BI tools. Most importantly, Elastisearch only supports single-table analysis and is lagging behind the modern OLAP demands for multi-table join, sub-query, and views.\\n\\n![Elasticsearch-DSL](/images/LAS_3.png)\\n\\n### High Cost & Low Stability\\n\\nElasticsearch users have been complaining about the computation and storage costs. The root reason lies in the way Elasticsearch works.\\n\\n- **Computation cost**: In data writing, Elasticsearch also executes compute-intensive operations including inverted index creation, tokenization, and inverted index ranking. Under these circumstances, data is written into Elasticsearch at a speed of around 2MB/s per core. When CPU resources are tight, data writing requirements often get rejected during peak times, which further leads to higher latency. \\n- **Storage cost**: To speed up retrieval, Elasticsearch stores the forward indexes, inverted indexes, and docvalues of the original data, consuming a lot more storage space. The compression ratio of a single data copy is only 1.5:1, compared to the 5:1 in most log solutions.\\n\\nAs data and cluster size grows, maintaining stability can be another issue:\\n\\n- **During data writing peaks**: Clusters are prone to overload during data writing peaks.\\n\\n- **During queries**: Since all queries are processed in the memory, big queries can easily lead to JVM OOM.\\n\\n- **Slow recovery**: For a cluster failure, Elasticsearch should reload indexes, which is resource-intensive, so it will take many minutes to recover. That challenges service availability guarantee.\\n\\n  \\n\\n## A More Cost-Effective Option\\n\\nReflecting on the strengths and limitations of the Elasticsearch-based solution, the Apache Doris developers have optimized Apache Doris for log processing. \\n\\n- **Increase writing throughout**: The performance of Elasticsearch is bottlenecked by data parsing and inverted index creation, so we improved Apache Doris in these factors: we quickened data parsing and index creation by SIMD instructions and CPU vector instructions; then we removed those data structures unnecessary for log analysis scenarios, such as forward indexes, to simplify index creation.\\n- **Reduce storage costs**: We removed forward indexes, which represented 30% of index data. We adopted columnar storage and the ZSTD compression algorithm, and thus achieved a compression ratio of 5:1 to 10:1. Given that a large part of the historical logs are rarely accessed, we introduced tiered storage to separate hot and cold data. Logs that are older than a specified time period will be moved to object storage, which is much less expensive. This can reduce storage costs by around 70%. \\n\\nBenchmark tests with ES Rally, the official testing tool for Elasticsearch, showed that Apache Doris was around 5 times as fast as Elasticsearch in data writing, 2.3 times as fast in queries, and it consumed only 1/5 of the storage space that Elasticsearch used. On the test dataset of HTTP logs, it achieved a writing speed of 550 MB/s and a compression ratio of 10:1.\\n\\n![Elasticsearch-VS-Apache-Doris](/images/LAS_4.png)\\n\\nThe below figure show what a typical Doris-based log processing system looks like. It is more inclusive and allows for more flexible usage from data ingestion, analysis, and application:\\n\\n- **Ingestion**: Apache Doris supports various ingestion methods for log data. You can push logs to Doris via HTTP Output using Logstash, you can use Flink to pre-process the logs before you write them into Doris, or you can load logs from Flink or object storage to Doris via Routine Load and S3 Load. \\n- **Analysis**: You can put log data in Doris and conduct join queries across logs and other data in the data warehouse.\\n- **Application**: Apache Doris is compatible with MySQL protocol, so you can integrate a wide variety of data analytic tools and clients to Doris, such as Grafana and Tableau. You can also connect applications to Doris via JDBC and ODBC APIs. We are planning to build a Kibana-like system to visualize logs.\\n\\n![Apache-Doris-log-analysis-stack](/images/LAS_5.png)\\n\\nMoreover, Apache Doris has better scheme-free support and a more user-friendly analytic engine.\\n\\n### Native Support for Semi-Structured Data\\n\\n**Firstly, we worked on the data types.** We optimized the string search and regular expression matching for \\"text\\" through vectorization and brought a performance increase of 2~10 times. For JSON strings, Apache Doris will parse and store them as a more compacted and efficient binary format, which can speed up queries by 4 times. We also added a new data type for complicated data: Array Map. It can structuralize concatenated strings to allow for higher compression rate and faster queries.\\n\\n**Secondly, Apache Doris supports schema evolution.** This means you can adjust the schema as your business changes. You can add or delete fields and indexes, and change the data types for fields.\\n\\nApache Doris provides Light Schema Change capabilities, so you can add or delete fields within milliseconds:\\n\\n```SQL\\n-- Add a column. Result will be returned in milliseconds.\\nALTER TABLE lineitem ADD COLUMN l_new_column INT;\\n```\\n\\n \\n\\nYou can also add index only for your target fields, so you can avoid overheads from unnecessary index creation. After you add an index, by default, the system will generate the index for all incremental data, and you can specify which historical data partitions that need the index.\\n\\n```SQL\\n-- Add inverted index. Doris will generate inverted index for all new data afterward.\\nALTER TABLE table_name ADD INDEX index_name(column_name) USING INVERTED;\\n\\n-- Build index for the specified historical data partitions.\\nBUILD INDEX index_name ON table_name PARTITIONS(partition_name1, partition_name2);\\n```\\n\\n### SQL-Based Analytic Engine\\n\\nThe SQL-based analytic engine makes sure that data engineers and analysts can smoothly grasp Apache Doris in a short time and bring their experience with SQL to this OLAP engine. Building on the rich features of SQL, users can execute data retrieval, aggregation, multi-table join, sub-query, UDF, logic views, and materialized views to serve their own needs. \\n\\nWith MySQL compatibility, Apache Doris can be integrated with most GUI and BI tools in the big data ecosystem, so users can realize more complex and diversified data analysis.\\n\\n### Performance in Use Case\\n\\nA gaming company has transitioned from the ELK stack to the Apache Doris solution. Their Doris-based log system used 1/6 of the storage space that they previously needed. \\n\\nIn a cybersecurity company who built their log analysis system utilizing inverted index in Apache Doris, they supported a data writing speed of 300,000 rows per second with 1/5 of the server resources that they formerly used. \\n\\n## Hands-On Guide\\n\\nNow let\'s go through the three steps of building a log analysis system with Apache Doris. \\n\\nBefore you start, [download](https://doris.apache.org/download/) Apache Doris 2.0 or newer versions from the website and [deploy](https://doris.apache.org/docs/dev/install/standard-deployment/) clusters.\\n\\n### Step 1: Create Tables\\n\\nThis is an example of table creation.\\n\\nExplanations for the configurations:\\n\\n- The DATETIMEV2 time field is specified as the Key in order to speed up queries for the latest N log records.\\n- Indexes are created for the frequently accessed fields, and fields that require full-text search are specified with Parser parameters.\\n- \\"PARTITION BY RANGE\\" means to partition the data by RANGE based on time fields, [Dynamic Partition](https://doris.apache.org/docs/dev/advanced/partition/dynamic-partition/) is enabled for auto-management.\\n- \\"DISTRIBUTED BY RANDOM BUCKETS AUTO\\" means to distribute the data into buckets randomly and the system will automatically decide the number of buckets based on the cluster size and data volume.\\n- \\"log_policy_1day\\" and \\"log_s3\\" means to move logs older than 1 day to S3 storage.\\n\\n```Go\\nCREATE DATABASE log_db;\\nUSE log_db;\\n\\nCREATE RESOURCE \\"log_s3\\"\\nPROPERTIES\\n(\\n    \\"type\\" = \\"s3\\",\\n    \\"s3.endpoint\\" = \\"your_endpoint_url\\",\\n    \\"s3.region\\" = \\"your_region\\",\\n    \\"s3.bucket\\" = \\"your_bucket\\",\\n    \\"s3.root.path\\" = \\"your_path\\",\\n    \\"s3.access_key\\" = \\"your_ak\\",\\n    \\"s3.secret_key\\" = \\"your_sk\\"\\n);\\n\\nCREATE STORAGE POLICY log_policy_1day\\nPROPERTIES(\\n    \\"storage_resource\\" = \\"log_s3\\",\\n    \\"cooldown_ttl\\" = \\"86400\\"\\n);\\n\\nCREATE TABLE log_table\\n(\\n  `ts` DATETIMEV2,\\n  `clientip` VARCHAR(20),\\n  `request` TEXT,\\n  `status` INT,\\n  `size` INT,\\n  INDEX idx_size (`size`) USING INVERTED,\\n  INDEX idx_status (`status`) USING INVERTED,\\n  INDEX idx_clientip (`clientip`) USING INVERTED,\\n  INDEX idx_request (`request`) USING INVERTED PROPERTIES(\\"parser\\" = \\"english\\")\\n)\\nENGINE = OLAP\\nDUPLICATE KEY(`ts`)\\nPARTITION BY RANGE(`ts`) ()\\nDISTRIBUTED BY RANDOM BUCKETS AUTO\\nPROPERTIES (\\n\\"replication_num\\" = \\"1\\",\\n\\"storage_policy\\" = \\"log_policy_1day\\",\\n\\"deprecated_dynamic_schema\\" = \\"true\\",\\n\\"dynamic_partition.enable\\" = \\"true\\",\\n\\"dynamic_partition.time_unit\\" = \\"DAY\\",\\n\\"dynamic_partition.start\\" = \\"-3\\",\\n\\"dynamic_partition.end\\" = \\"7\\",\\n\\"dynamic_partition.prefix\\" = \\"p\\",\\n\\"dynamic_partition.buckets\\" = \\"AUTO\\",\\n\\"dynamic_partition.replication_num\\" = \\"1\\"\\n);\\n```\\n\\n### Step 2: Ingest the Logs\\n\\nApache Doris supports various ingestion methods. For real-time logs, we recommend the following three methods:\\n\\n- Pull logs from Kafka message queue: Routine Load \\n- Logstash: write logs into Doris via HTTP API\\n- Self-defined writing program: write logs into Doris via HTTP API\\n\\n**Ingest from Kafka**\\n\\nFor JSON logs that are written into Kafka message queues, create [Routine Load](https://doris.apache.org/docs/dev/data-operate/import/import-way/routine-load-manual/) so Doris will pull data from Kafka. The following is an example. The `property.*` configurations are optional:\\n\\n```SQL\\n-- Prepare the Kafka cluster and topic (\\"log_topic\\")\\n\\n-- Create Routine Load, load data from Kafka log_topic to \\"log_table\\"\\nCREATE ROUTINE LOAD load_log_kafka ON log_db.log_table\\nCOLUMNS(ts, clientip, request, status, size)\\nPROPERTIES (\\n    \\"max_batch_interval\\" = \\"60\\",\\n    \\"max_batch_rows\\" = \\"20000000\\",\\n    \\"max_batch_size\\" = \\"1073741824\\", \\n    \\"load_to_single_tablet\\" = \\"true\\",\\n    \\"format\\" = \\"json\\"\\n)\\nFROM KAFKA (\\n    \\"kafka_broker_list\\" = \\"host:port\\",\\n    \\"kafka_topic\\" = \\"log_topic\\",\\n    \\"property.group.id\\" = \\"your_group_id\\",\\n    \\"property.security.protocol\\"=\\"SASL_PLAINTEXT\\",     \\n    \\"property.sasl.mechanism\\"=\\"GSSAPI\\",     \\n    \\"property.sasl.kerberos.service.name\\"=\\"kafka\\",     \\n    \\"property.sasl.kerberos.keytab\\"=\\"/path/to/xxx.keytab\\",     \\n    \\"property.sasl.kerberos.principal\\"=\\"xxx@yyy.com\\"\\n);\\n```\\n\\nYou can check how the Routine Load runs via the `SHOW ROUTINE LOAD` command. \\n\\n**Ingest via Logstash**\\n\\nConfigure HTTP Output for Logstash, and then data will be sent to Doris via HTTP Stream Load.\\n\\n1. Specify the batch size and batch delay in `logstash.yml` to improve data writing performance.\\n\\n```Plain\\npipeline.batch.size: 100000\\npipeline.batch.delay: 10000\\n```\\n\\n2. Add HTTP Output to the log collection configuration file `testlog.conf`, URL => the Stream Load address in Doris.\\n\\n- Since Logstash does not support HTTP redirection, you should use a backend address instead of a FE address.\\n- Authorization in the headers is `http basic auth`. It is computed with `echo -n \'username:password\' | base64`.\\n- The `load_to_single_tablet` in the headers can reduce the number of small files in data ingestion.\\n\\n```Plain\\noutput {\\n    http {\\n       follow_redirects => true\\n       keepalive => false\\n       http_method => \\"put\\"\\n       url => \\"http://172.21.0.5:8640/api/logdb/logtable/_stream_load\\"\\n       headers => [\\n           \\"format\\", \\"json\\",\\n           \\"strip_outer_array\\", \\"true\\",\\n           \\"load_to_single_tablet\\", \\"true\\",\\n           \\"Authorization\\", \\"Basic cm9vdDo=\\",\\n           \\"Expect\\", \\"100-continue\\"\\n       ]\\n       format => \\"json_batch\\"\\n    }\\n}\\n```\\n\\n**Ingest via self-defined program**\\n\\nThis is an example of ingesting data to Doris via HTTP Stream Load.\\n\\nNotes:\\n\\n- Use `basic auth` for HTTP authorization, use `echo -n \'username:password\' | base64` in computation\\n- `http header \\"format:json\\"`: the data type is specified as JSON\\n- `http header \\"read_json_by_line:true\\"`: each line is a JSON record\\n- `http header \\"load_to_single_tablet:true\\"`: write to one tablet each time\\n- For the data writing clients, we recommend a batch size of 100MB~1GB. Future versions will enable Group Commit at the server end and reduce batch size from clients.\\n\\n```shell\\ncurl \\\\\\n--location-trusted \\\\\\n-u username:password \\\\\\n-H \\"format:json\\" \\\\\\n-H \\"read_json_by_line:true\\" \\\\\\n-H \\"load_to_single_tablet:true\\" \\\\\\n-T logfile.json \\\\\\nhttp://fe_host:fe_http_port/api/log_db/log_table/_stream_load\\n```\\n\\n### Step 3: Execute Queries\\n\\nApache Doris supports standard SQL, so you can connect to Doris via MySQL client or JDBC and then execute SQL queries.\\n\\n```SQL\\nmysql -h fe_host -P fe_mysql_port -u root -Dlog_db\\n```\\n\\n**A few common queries in log analysis:**\\n\\n- Check the latest 10 records.\\n\\n```SQL\\nSELECT * FROM log_table ORDER BY ts DESC LIMIT 10;\\n```\\n\\n- Check the latest 10 records of Client IP \\"8.8.8.8\\".\\n\\n```SQL\\nSELECT * FROM log_table WHERE clientip = \'8.8.8.8\' ORDER BY ts DESC LIMIT 10;\\n```\\n\\n- Retrieve the latest 10 records with \\"error\\" or \\"404\\" in the \\"request\\" field. **MATCH_ANY** is a SQL syntax keyword for full-text search in Doris. It means to find the records that include any one of the specified keywords.\\n\\n```SQL\\nSELECT * FROM log_table WHERE request MATCH_ANY \'error 404\' ORDER BY ts DESC LIMIT 10;\\n```\\n\\n- Retrieve the latest 10 records with \\"image\\" and \\"faq\\" in the \\"request\\" field. **MATCH_ALL** is also a SQL syntax keyword for full-text search in Doris. It means to find the records that include all of the specified keywords.\\n\\n```SQL\\nSELECT * FROM log_table WHERE request MATCH_ALL \'image faq\' ORDER BY ts DESC LIMIT 10;\\n```\\n\\n## Conclusion\\n\\nIf you are looking for an efficient log analytic solution, Apache Doris is friendly to anyone equipped with SQL knowledge; if you find friction with the ELK stack, try Apache Doris provides better schema-free support, enables faster data writing and queries, and brings much less storage burden.\\n\\nBut we won\'t stop here. We are going to provide more features to facilitate log analysis. We plan to add more complicated data types to inverted index, and support BKD index to make Apache Doris a fit for geo data analysis. We also plan to expand capabilities in semi-structured data analysis, such as working on the complex data types (Array, Map, Struct, JSON) and high-performance string matching algorithm. And we welcome any [user feedback and development advice](https://join.slack.com/t/apachedoriscommunity/shared_invite/zt-2unfw3a3q-MtjGX4pAd8bCGC1UV0sKcw)."},{"id":"/Log-Analysis-How-to-Digest-15-Billion-Logs-Per-Day-and-Keep-Big-Queries-Within-1-Second","metadata":{"permalink":"/blog/Log-Analysis-How-to-Digest-15-Billion-Logs-Per-Day-and-Keep-Big-Queries-Within-1-Second","source":"@site/blog/Log-Analysis-How-to-Digest-15-Billion-Logs-Per-Day-and-Keep-Big-Queries-Within-1-Second.md","title":"Log analysis: how to digest 15 billion logs per day and keep big queries within 1 second","description":"This article describes a large-scale data warehousing use case to provide reference for data engineers who are looking for log analytic solutions. It introduces the log processing architecture and real case practice in data ingestion, storage, and queries.","date":"2023-09-16T00:00:00.000Z","tags":[{"inline":true,"label":"Best Practice","permalink":"/blog/tags/best-practice"}],"hasTruncateMarker":false,"authors":[{"name":"velodb.io \xb7 Yuqi Liu","key":null,"page":null}],"frontMatter":{"title":"Log analysis: how to digest 15 billion logs per day and keep big queries within 1 second","description":"This article describes a large-scale data warehousing use case to provide reference for data engineers who are looking for log analytic solutions. It introduces the log processing architecture and real case practice in data ingestion, storage, and queries.","date":"2023-09-16","author":"velodb.io \xb7 Yuqi Liu","externalLink":"https://www.velodb.io/blog/137","tags":["Best Practice"],"image":"/images/telegram.jpeg"},"unlisted":false,"prevItem":{"title":"Log analysis: Elasticsearch vs Apache Doris","permalink":"/blog/log-analysis-elasticsearch-vs-apache-doris"},"nextItem":{"title":"Apache Doris announced the official release of version 1.2.7","permalink":"/blog/release-note-1.2.7"}},"content":"\x3c!-- \\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n\\n\\nThis data warehousing use case is about **scale**. The user is one of the world\'s biggest telecommunication service providers. Using Apache Doris, they deploy multiple petabyte-scale clusters on dozens of machines to support their 15 billion daily log additions from their over 30 business lines. Such a gigantic log analysis system is part of their cybersecurity management. For the need of real-time monitoring, threat tracing, and alerting, they require a log analytic system that can automatically collect, store, analyze, and visualize logs and event records.\\n\\nFrom an architectural perspective, the system should be able to undertake real-time analysis of various formats of logs, and of course, be scalable to support the huge and ever-enlarging data size. The rest of this post is about what their log processing architecture looks like, and how they realize stable data ingestion, low-cost storage, and quick queries with it.\\n\\n## System Architecture\\n\\nThis is an overview of their data pipeline. The logs are collected into the data warehouse, and go through several layers of processing.\\n\\n![real-time-data-warehouse-2.0](/images/Unicom-1.png)\\n\\n- **ODS**: Original logs and alerts from all sources are gathered into Apache Kafka. Meanwhile, a copy of them will be stored in HDFS for data verification or replay.\\n- **DWD**: This is where the fact tables are. Apache Flink cleans, standardizes, backfills, and de-identifies the data, and write it back to Kafka. These fact tables will also be put into Apache Doris, so that Doris can trace a certain item or use them for dashboarding and reporting. As logs are not averse to duplication, the fact tables will be arranged in the [Duplicate Key model](https://doris.apache.org/docs/table-design/data-model/duplicate) of Apache Doris.  \\n- **DWS**: This layer aggregates data from DWD and lays the foundation for queries and analysis.\\n- **ADS**: In this layer, Apache Doris auto-aggregates data with its Aggregate Key model, and auto-updates data with its Unique Key model. \\n\\nArchitecture 2.0 evolves from Architecture 1.0, which is supported by ClickHouse and Apache Hive. The transition arised from the user\'s needs for real-time data processing and multi-table join queries. In their experience with ClickHouse, they found inadequate support for concurrency and multi-table joins, manifested by frequent timeouts in dashboarding and OOM errors in distributed joins.\\n\\n![real-time-data-warehouse-1.0](/images/Unicom-2.png)\\n\\nNow let\'s take a look at their practice in data ingestion, storage, and queries with Architecture 2.0.\\n\\n## Real-Case Practice\\n\\n### Stable ingestion of 15 billion logs per day\\n\\nIn the user\'s case, their business churns out 15 billion logs every day. Ingesting such data volume quickly and stably is a real problem. With Apache Doris, the recommended way is to use the Flink-Doris-Connector. It is developed by the Apache Doris community for large-scale data writing. The component requires simple configuration. It implements Stream Load and can reach a writing speed of 200,000~300,000 logs per second, without interrupting the data analytic workloads.\\n\\nA lesson learned is that when using Flink for high-frequency writing, you need to find the right parameter configuration for your case to avoid data version accumulation. In this case, the user made the following optimizations:\\n\\n- **Flink Checkpoint**: They increase the checkpoint interval from 15s to 60s to reduce writing frequency and the number of transactions processed by Doris per unit of time. This can relieve data writing pressure and avoid generating too many data versions.\\n- **Data Pre-Aggregation**: For data of the same ID but comes from various tables, Flink will pre-aggregate it based on the primary key ID and create a flat table, in order to avoid excessive resource consumption caused by multi-source data writing.\\n- **Doris Compaction**: The trick here includes finding the right Doris backend (BE) parameters to allocate the right amount of CPU resources for data compaction, setting the appropriate number of data partitions, buckets, and replicas (too much data tablets will bring huge overheads), and dialing up `max_tablet_version_num` to avoid version accumulation.\\n\\nThese measures together ensure daily ingestion stability. The user has witnessed stable performance and low compaction score in Doris backend. In addition, the combination of data pre-processing in Flink and the [Unique Key model](https://doris.apache.org/docs/table-design/data-model/unique) in Doris can ensure quicker data updates.\\n\\n### Storage strategies to reduce costs by 50%\\n\\nThe size and generation rate of logs also impose pressure on storage. Among the immense log data, only a part of it is of high informational value, so storage should be differentiated. The user has three storage strategies to reduce costs. \\n\\n- **ZSTD (ZStandard) compression algorithm**: For tables larger than 1TB, specify the compression method as \\"ZSTD\\" upon table creation, it will realize a compression ratio of 10:1. \\n- **Tiered storage of hot and cold data**: This is supported by the [new feature](https://blog.devgenius.io/hot-cold-data-separation-what-why-and-how-5f7c73e7a3cf) of Doris. The user sets a data \\"cooldown\\" period of 7 days. That means data from the past 7 days (namely, hot data) will be stored in SSD. As time goes by, hot data \\"cools down\\" (getting older than 7 days), it will be automatically moved to HDD, which is less expensive. As data gets even \\"colder\\", it will be moved to object storage for much lower storage costs. Plus, in object storage, data will be stored with only one copy instead of three. This further cuts down costs and the overheads brought by redundant storage. \\n- **Differentiated replica numbers for different data partitions**: The user has partitioned their data by time range. The principle is to have more replicas for newer data partitions and less for the older ones. In their case, data from the past 3 months is frequently accessed, so they have 2 replicas for this partition. Data that is 3~6 months old has two replicas, and data from 6 months ago has one single copy. \\n\\nWith these three strategies, the user has reduced their storage costs by 50%.\\n\\n### Differentiated query strategies based on data size\\n\\nSome logs must be immediately traced and located, such as those of abnormal events or failures. To ensure real-time response to these queries, the user has different query strategies for different data sizes:\\n\\n- **Less than 100G**: The user utilizes the dynamic partitioning feature of Doris. Small tables will be partitioned by date and large tables will be partitioned by hour. This can avoid data skew. To further ensure balance of data within a partition, they use the snowflake ID as the bucketing field. They also set a starting offset of 20 days, which means data of the recent 20 days will be kept. In this way, they find the balance point between data backlog and analytic needs.\\n- **100G~1T**: These tables have their materialized views, which are the pre-computed result sets stored in Doris. Thus, queries on these tables will be much faster and less resource-consuming. The DDL syntax of materialized views in Doris is the same as those in PostgreSQL and Oracle.\\n- **More than 100T**: These tables are put into the Aggregate Key model of Apache Doris and pre-aggregate them. **In this way, we enable queries of 2 billion log records to be done in 1~2s.** \\n\\nThese strategies have shortened the response time of queries. For example, a query of a specific data item used to take minutes, but now it can be finished in milliseconds. In addition, for big tables that contain 10 billion data records, queries on different dimensions can all be done in a few seconds.\\n\\n## Ongoing Plans\\n\\nThe user is now testing with the newly added [inverted index](https://doris.apache.org/docs/table-design/index/inverted-index) in Apache Doris. It is designed to speed up full-text search of strings as well as equivalence and range queries of numerics and datetime. They have also provided their valuable feedback about the auto-bucketing logic in Doris: Currently, Doris decides the number of buckets for a partition  based on the data size of the previous partition. The problem for the user is, most of their new data comes in during daytime, but little at nights. So in their case, Doris creates too many buckets for night data but too few in daylight, which is the opposite of what they need. They hope to add a new auto-bucketing logic, where the reference for Doris to decide the number of buckets is the data size and distribution of the previous day. They\'ve come to the [Apache Doris community](https://join.slack.com/t/apachedoriscommunity/shared_invite/zt-2unfw3a3q-MtjGX4pAd8bCGC1UV0sKcw) and we are now working on this optimization."},{"id":"/release-note-1.2.7","metadata":{"permalink":"/blog/release-note-1.2.7","source":"@site/blog/release-note-1.2.7.md","title":"Apache Doris announced the official release of version 1.2.7","description":"Dear community, Apache Doris 1.2.7 is now available, with several enhancements and bug fixes based on 1.2.0\uFF0Cenabling smoother user experience.","date":"2023-09-04T00:00:00.000Z","tags":[{"inline":true,"label":"Release Notes","permalink":"/blog/tags/release-notes"}],"hasTruncateMarker":false,"authors":[{"name":"Apache Doris","key":null,"page":null}],"frontMatter":{"title":"Apache Doris announced the official release of version 1.2.7","description":"Dear community, Apache Doris 1.2.7 is now available, with several enhancements and bug fixes based on 1.2.0\uFF0Cenabling smoother user experience.","date":"2023-09-04","author":"Apache Doris","tags":["Release Notes"],"image":"/images/1.2.7.png"},"unlisted":false,"prevItem":{"title":"Log analysis: how to digest 15 billion logs per day and keep big queries within 1 second","permalink":"/blog/Log-Analysis-How-to-Digest-15-Billion-Logs-Per-Day-and-Keep-Big-Queries-Within-1-Second"},"nextItem":{"title":"Apache Doris announced the official release of version 2.0.1","permalink":"/blog/release-note-2.0.1"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n## Bug Fixes\\n\\n- Fixed some query issues.\\n- Fix some storage issues.\\n- Fix some decimal precision issues.\\n- Fix query error caused by invalid `sql_select_limit` session variable\'s value.\\n- Fix the problem that hdfs short-circuit read cannot be used.\\n- Fix the problem that Tencent Cloud cosn cannot be accessed.\\n- Fix several issues with hive catalog kerberos access.\\n- Fix the problem that stream load profile cannot be used.\\n- Fix promethus monitoring parameter format problem.\\n- Fix the table creation timeout issue when creating a large number of tablets.\\n\\n## New Features\\n\\n- Unique Key model supports array type as value column\\n- Added `have_query_cache` variable for compatibility with MySQL ecosystem.\\n- Added `enable_strong_consistency_read` to support strong consistent read between sessions\\n- FE metrics supports user-level query counter"},{"id":"/release-note-2.0.1","metadata":{"permalink":"/blog/release-note-2.0.1","source":"@site/blog/release-note-2.0.1.md","title":"Apache Doris announced the official release of version 2.0.1","description":"Dear community, Apache Doris has fixed 383 issues or performance improvements in version 2.0.1 based on 2.0.0, enabling smoother user experience.","date":"2023-09-04T00:00:00.000Z","tags":[{"inline":true,"label":"Release Notes","permalink":"/blog/tags/release-notes"}],"hasTruncateMarker":false,"authors":[{"name":"Apache Doris","key":null,"page":null}],"frontMatter":{"title":"Apache Doris announced the official release of version 2.0.1","description":"Dear community, Apache Doris has fixed 383 issues or performance improvements in version 2.0.1 based on 2.0.0, enabling smoother user experience.","date":"2023-09-04","author":"Apache Doris","tags":["Release Notes"],"image":"/images/2.0.1.png"},"unlisted":false,"prevItem":{"title":"Apache Doris announced the official release of version 1.2.7","permalink":"/blog/release-note-1.2.7"},"nextItem":{"title":"LLM-powered OLAP: the Tencent application with Apache Doris","permalink":"/blog/Tencent-LLM"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n\\nThanks to our community users and developers, 383 improvements and bug fixes have been made in Doris 2.0.1.\\n\\n## Behavior changes\\n\\n- [https://github.com/apache/doris/pull/21302](https://github.com/apache/doris/pull/21302)\\n\\n## Improvements\\n\\n### Functionality and stability of array and map datatypes\\n- [https://github.com/apache/doris/pull/22793](https://github.com/apache/doris/pull/22793)\\n- [https://github.com/apache/doris/pull/22927](https://github.com/apache/doris/pull/22927)\\n- https://github.com/apache/doris/pull/22738\\n- https://github.com/apache/doris/pull/22347\\n- https://github.com/apache/doris/pull/23250\\n- https://github.com/apache/doris/pull/22300\\n\\n### Performance for inverted index query\\n- https://github.com/apache/doris/pull/22836\\n- https://github.com/apache/doris/pull/23381\\n- https://github.com/apache/doris/pull/23389\\n- https://github.com/apache/doris/pull/22570\\n\\n### Performance for bitmap, like, scan, agg functions\\n- https://github.com/apache/doris/pull/23172\\n- https://github.com/apache/doris/pull/23495\\n- https://github.com/apache/doris/pull/23476\\n- https://github.com/apache/doris/pull/23396\\n- https://github.com/apache/doris/pull/23182\\n- https://github.com/apache/doris/pull/22216\\n\\n### Functionality and stability of CCR\\n- https://github.com/apache/doris/pull/22447\\n- https://github.com/apache/doris/pull/22559\\n- https://github.com/apache/doris/pull/22173\\n- https://github.com/apache/doris/pull/22678\\n\\n### Merge on write unique table\\n\\n- https://github.com/apache/doris/pull/22282\\n- https://github.com/apache/doris/pull/22984\\n- https://github.com/apache/doris/pull/21933\\n- https://github.com/apache/doris/pull/22874\\n\\n### Optimizer table stats and analyze\\n\\n- https://github.com/apache/doris/pull/22658\\n- https://github.com/apache/doris/pull/22211\\n- https://github.com/apache/doris/pull/22775\\n- https://github.com/apache/doris/pull/22896\\n- https://github.com/apache/doris/pull/22788\\n- https://github.com/apache/doris/pull/22882\\n- \\n\\n### Functionality and performance of multi catalog\\n\\n- https://github.com/apache/doris/pull/22949\\n- https://github.com/apache/doris/pull/22923\\n- https://github.com/apache/doris/pull/22336\\n- https://github.com/apache/doris/pull/22915\\n- https://github.com/apache/doris/pull/23056\\n- https://github.com/apache/doris/pull/23297\\n- https://github.com/apache/doris/pull/23279\\n\\n\\n## Bug fixes\\n\\n- https://github.com/apache/doris/pull/22673\\n- https://github.com/apache/doris/pull/22656\\n- https://github.com/apache/doris/pull/22892\\n- https://github.com/apache/doris/pull/22959\\n- https://github.com/apache/doris/pull/22902\\n- https://github.com/apache/doris/pull/22976\\n- https://github.com/apache/doris/pull/22734\\n- https://github.com/apache/doris/pull/22840\\n- https://github.com/apache/doris/pull/23008\\n- https://github.com/apache/doris/pull/23003\\n- https://github.com/apache/doris/pull/22966\\n- https://github.com/apache/doris/pull/22965\\n- https://github.com/apache/doris/pull/22784\\n- https://github.com/apache/doris/pull/23049\\n- https://github.com/apache/doris/pull/23084\\n- https://github.com/apache/doris/pull/22947\\n- https://github.com/apache/doris/pull/22919\\n- https://github.com/apache/doris/pull/22979\\n- https://github.com/apache/doris/pull/23096\\n- https://github.com/apache/doris/pull/23113\\n- https://github.com/apache/doris/pull/23062\\n- https://github.com/apache/doris/pull/22918\\n- https://github.com/apache/doris/pull/23026\\n- https://github.com/apache/doris/pull/23175\\n- https://github.com/apache/doris/pull/23167\\n- https://github.com/apache/doris/pull/23015\\n- https://github.com/apache/doris/pull/23165\\n- https://github.com/apache/doris/pull/23264\\n- https://github.com/apache/doris/pull/23246\\n- https://github.com/apache/doris/pull/23198\\n- https://github.com/apache/doris/pull/23221\\n- https://github.com/apache/doris/pull/23277\\n- https://github.com/apache/doris/pull/23249\\n- https://github.com/apache/doris/pull/23272\\n- https://github.com/apache/doris/pull/23383\\n- https://github.com/apache/doris/pull/23372\\n- https://github.com/apache/doris/pull/23399\\n- https://github.com/apache/doris/pull/23295\\n- https://github.com/apache/doris/pull/23446\\n- https://github.com/apache/doris/pull/23406\\n- https://github.com/apache/doris/pull/23387\\n- https://github.com/apache/doris/pull/23421\\n- https://github.com/apache/doris/pull/23456\\n- https://github.com/apache/doris/pull/23361\\n- https://github.com/apache/doris/pull/23402\\n- https://github.com/apache/doris/pull/23369\\n- https://github.com/apache/doris/pull/23245\\n- https://github.com/apache/doris/pull/23532\\n- https://github.com/apache/doris/pull/23529\\n- https://github.com/apache/doris/pull/23601\\n\\n\\nSee the complete list of improvements and bug fixes on [github](https://github.com/apache/doris/issues?q=label%3Adev%2F2.0.1-merged+is%3Aclosed) .\\n\\n\\n## Credits\\n\\nThanks all who contribute to this release:\\n\\n@adonis0147\\n@airborne12\\n@amorynan\\n@AshinGau\\n@BePPPower\\n@BiteTheDDDDt\\n@bobhan1\\n@ByteYue\\n@caiconghui\\n@CalvinKirs\\n@csun5285\\n@DarvenDuan\\n@deadlinefen\\n@DongLiang-0\\n@Doris-Extras\\n@dutyu\\n@englefly\\n@freemandealer\\n@Gabriel39\\n@GoGoWen\\n@HappenLee\\n@hello-stephen\\n@HHoflittlefish777\\n@hubgeter\\n@hust-hhb\\n@JackDrogon\\n@jacktengg\\n@jackwener\\n@Jibing-Li\\n@kaijchen\\n@kaka11chen\\n@Kikyou1997\\n@Lchangliang\\n@LemonLiTree\\n@liaoxin01\\n@LiBinfeng-01\\n@lsy3993\\n@luozenglin\\n@morningman\\n@morrySnow\\n@mrhhsg\\n@Mryange\\n@mymeiyi\\n@shuke987\\n@sohardforaname\\n@starocean999\\n@TangSiyang2001\\n@Tanya-W\\n@ucasfl\\n@vinlee19\\n@wangbo\\n@wsjz\\n@wuwenchi\\n@xiaokang\\n@XieJiann\\n@xinyiZzz\\n@yujun777\\n@Yukang-Lian\\n@Yulei-Yang\\n@zclllyybb\\n@zddr\\n@zenoyang\\n@zgxme\\n@zhangguoqiang666\\n@zhangstar333\\n@zhannngchen\\n@zhiqiang-hhhh\\n@zxealous\\n@zy-kkk\\n@zzzxl1993\\n@zzzzzzzs"},{"id":"/Tencent-LLM","metadata":{"permalink":"/blog/Tencent-LLM","source":"@site/blog/Tencent-LLM.md","title":"LLM-powered OLAP: the Tencent application with Apache Doris","description":"The exploration of a LLM+OLAP solution is a bumpy journey, but phew, it now works well for the Tencent case, and they\'re writing down every lesson learned to share with you.","date":"2023-08-29T00:00:00.000Z","tags":[{"inline":true,"label":"Best Practice","permalink":"/blog/tags/best-practice"}],"hasTruncateMarker":false,"authors":[{"name":"velodb.io \xb7 Jun Zhang & Lei Luo","key":null,"page":null}],"frontMatter":{"title":"LLM-powered OLAP: the Tencent application with Apache Doris","description":"The exploration of a LLM+OLAP solution is a bumpy journey, but phew, it now works well for the Tencent case, and they\'re writing down every lesson learned to share with you.","date":"2023-08-29","author":"velodb.io \xb7 Jun Zhang & Lei Luo","tags":["Best Practice"],"externalLink":"https://www.velodb.io/blog/131","image":"/images/tme.png"},"unlisted":false,"prevItem":{"title":"Apache Doris announced the official release of version 2.0.1","permalink":"/blog/release-note-2.0.1"},"nextItem":{"title":"Choosing an OLAP engine for financial risk management: what to consider?","permalink":"/blog/Choosing-an-OLAP-Engine-for-Financial-Risk-Management-What-to-Consider"}},"content":"\x3c!-- \\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\nSix months ago, I wrote about [why we replaced ClickHouse with Apache Doris as an OLAP engine](https://doris.apache.org/blog/Tencent-Data-Engineers-Why-We-Went-from-ClickHouse-to-Apache-Doris) for our data management system. Back then, we were struggling with the auto-generation of SQL statements. As days pass, we have made progresses big enough to be references for you (I think), so here I am again. \\n\\nWe have adopted Large Language Models (LLM) to empower our Doris-based OLAP services.\\n\\n## LLM + OLAP\\n\\nOur incentive was to save our internal staff from the steep learning curve of SQL writing. Thus, we used LLM as an intermediate. It transforms natural language questions into SQL statements and sends the SQLs to the OLAP engine for execution.\\n\\n![LLM-OLAP-solution](/images/Tencent_LLM_1.png)\\n\\nLike every AI-related experience, we came across some friction:\\n\\n1. LLM does not understand data jargons, like \\"fields\\", \\"rows\\", \\"columns\\" and \\"tables\\". Instead, they can perfectly translate business terms like \\"corporate income\\" and \\"DAU\\", which are basically what the fields/rows/columns are about. That means it can work well only if the analysts use the exact right word to refer to the metric they need when typing their questions.\\n2. The LLM we are using is slow in inference. It takes over 10 seconds to respond. As it charges fees by token, cost-effectiveness becomes a problem.\\n3. Although the LLM is trained on a large collection of public datasets, it is under-informed of niche knowledge. In our case, the LLM is super unfamiliar with indie songs, so even if the songs are included in our database, the LLM will not able to identify them properly. \\n4. Sometimes our input questions require adequate and latest legal, political, financial, and regulatory information, which is hard to be included in a training dataset or knowledge base. We need to connect the LLM to wider info bases in order to perform more diversified tasks.\\n\\nWe knock these problems down one by one.\\n\\n### 1. A semantic layer\\n\\nFor problem No.1, we introduce a semantic layer between the LLM and the OLAP engine. This layer translates business terms into the corresponding data fields. It can identify data filtering conditions from the various natural language wordings, relate them to the metrics involved, and then generate SQL statements. \\n\\nBesides that, the semantic layer can optimize the computation logic. When analysts input a question that involves a complicated query, let\'s say, a multi-table join, the semantic layer can split that into multiple single-table queries to reduce semantic distortion.\\n\\n![LLM-OLAP-semantic-layer](/images/Tencent_LLM_2.png)\\n\\n### 2. LLM parsing rules\\n\\nTo increase cost-effectiveness in using LLM, we evaluate the computation complexity of all scenarios, such as metric computation, detailed record retrieval, and user segmentation. Then, we create rules and dedicate the LLM-parsing step to only complicated tasks. That means for the simple computation tasks, it will skip the parsing. \\n\\nFor example, when an analyst inputs \\"tell me the earnings of the major musical platforms\\", the LLM identifies that this question only entails several metrics or dimensions, so it will not further parse it but send it straight for SQL generation and execution. This can largely shorten query response time and reduce API expenses. \\n\\n![LLM-OLAP-parsing-rules](/images/Tencent_LLM_3.png)\\n\\n### 3. Schema Mapper and external knowledge base\\n\\nTo empower the LLM with niche knowledge, we added a Schema Mapper upstream from the LLM. The Schema Mapper maps the input question to an external knowledge base, and then the LLM will do parsing.\\n\\nWe are constantly testing and optimizing the Schema Mapper. We categorize and rate content in the external knowledge base, and do various levels of mapping (full-text mapping and fuzzy mapping) to enable better semantic parsing.\\n\\n![LLM-OLAP-schema-mapper](/images/Tencent_LLM_4.png)\\n\\n### 4. Plugins\\n\\nWe used plugins to connect the LLM to more fields of information, and we have different integration methods for different types of plugins:\\n\\n- **Embedding local files**: This is especially useful when we need to \\"teach\\" the LLM the latest regulatory policies, which are often text files. Firstly, the system vectorizes the local text file, executes semantic searches to find matching or similar terms in the local file, extracts the relevant contents and puts them into the LLM parsing window to generate output. \\n- **Third-party plugins**: The marketplace is full of third-party plugins that are designed for all kinds of sectors. With them, the LLM is able to deal with wide-ranging topics. Each plugin has its own prompts and calling function. Once the input question hits a prompt, the relevant plugin will be called.\\n\\n![LLM-OLAP-plugins](/images/Tencent_LLM_5.png)\\n\\nAfter we are done with above four optimizations, the SuperSonic framework comes into being.\\n\\n## The SuperSonic framework\\n\\nNow let me walk you through this [framework](https://github.com/tencentmusic/supersonic):\\n\\n![LLM-OLAP-supersonic-framework](/images/Tencent_LLM_6.png)\\n\\n- An analyst inputs a question.\\n- The Schema Mapper maps the question to an external knowledge base.\\n- If there are matching fields in the external knowledge base, the question will not be parsed by the LLM. Instead, a metric computation formula will trigger the OLAP engine to start querying. If there is no matching field, the question will enter the LLM.\\n- Based on the pre-defined rules, the LLM rates the complexity level of the question. If it is a simple query, it will go directly to the OLAP engine; if it is a complicated query, it will be semantically parsed and converted to a DSL statement.\\n- At the Semantic Layer, the DSL statement will be split based on its query scenario. For example, if it is a multi-table join query, this layer will generate multiple single-table query SQL statements.\\n- If the question involves external knowledge, the LLM will call a third-party plugin.\\n\\n**Example**\\n\\n![LLM-OLAP-chatbot-query-interface](/images/Tencent_LLM_7.png)\\n\\nTo answer whether a certain song can be performed on variety shows, the system retrieves the OLAP data warehouse for details about the song, and presents it with results from the Commercial Use Query third-party plugin.\\n\\n## OLAP Architecture\\n\\nAs for the OLAP part of this framework, after several rounds of architectural evolution, this is what our current OLAP pipeline looks like. \\n\\nRaw data is sorted into tags and metrics, which are custom-defined by the analysts. The tags and metrics are under unified management in order to avoid inconsistent definitions. Then, they are combined into various tagsets and metricsets for various queries. \\n\\n![LLM-OLAP-architecture](/images/Tencent_LLM_8.png)\\n\\nWe have drawn two main takeaways for you from our architectural optimization experience.\\n\\n**1. Streamline the links**\\n\\nBefore we adopted Apache Doris, we used to have ClickHouse to accelerate the computation of tags and metrics, and Elasticsearch to process dimensional data. That\'s two analytic engines and requires us to adapt the query statements to both of them. It was high-maintenance.\\n\\nThus, we replaced ClickHouse with Apache Doris, and utilized the [Elasticsearch Catalog](https://doris.apache.org/docs/lakehouse/database/es) functionality to connect Elasticsearch data to Doris. In this way, we make Doris our unified query gateway. \\n\\n**2. Split the flat tables**\\n\\nIn early versions of our OLAP architecture, we used to put data into flat tables, which made things tricky. For one thing, flat tables absorbed all the writing latency from upstreams, and that added up to considerable loss in data realtimeliness. For another, 50% of data in a flat table was dimensional data, which was rarely updated. With every new flat table came some bulky dimensional data that consumed lots of storage space. \\n\\nTherefore, we split the flat tables into metric tables and dimension tables. As they are updated in different paces, we put them into different data models.\\n\\n- **Metric tables**: We arrange metric data in the Aggregate Key model of Apache Doris, which means new data will be merged with the old data by way of SUM, MAX, MIN, etc.\\n- **Dimension tables**: These tables are in the Unique Key model of Apache Doris, which means new data record will replace the old. This can greatly increase performance in our query scenarios.\\n\\nYou might ask, does this cause trouble in queries, since most queries require data from both types of tables? Don\'t worry, we address that with the Rollup feature of Doris. On the basis of the base tables, we can select the dimensions we need to create Rollup views, which will automatically execute `GROUP BY`. This relieves us of the need to define tags for each Rollup view and largely speed up queries.\\n\\n## Other Tricks\\n\\nIn our experience with Apache Doris, we also find some other functionalities handy, so I list them here for you, too:\\n\\n**1. Materialized View**\\n\\nA Materialized View is a pre-computed dataset. It is a way to accelerate queries when you frequently need to access data of certain dimensions. In these scenarios, we define derived tags and metrics based on the original ones. For example, we create a derived metric by combining Metric 1, Metric 2, and Metric 3: `sum(m1+m2+m3)`. Then, we can create a Materialized View for it. According to the Doris release schedule, version 2.1 will support multi-table Materialized Views, and we look forward to that.\\n\\n**2. Flink-Doris-Connector**\\n\\nThis is for Exactly-Once guarantee in data ingestion. The Flink-Doris-Connector implements a checkpoint mechanism and two-stage commit, and allows for auto data synchronization from relational databases to Doris.\\n\\n**3. Compaction**\\n\\nWhen the number of aggregation tasks or data volume becomes overwhelming for Flink, there might be huge latency in data compaction. We solve that with Vertical Compaction and Segment Compaction. Vertical Compaction supports loading of only part of the columns, so it can reduce storage consumption when compacting flat tables. Segment Compaction can avoid generating too much segments during data writing, and allows for compaction while writing simultaneously.   \\n\\n## What\'s Next\\n\\nWith an aim to reduce costs and increase service availability, we plan to test the newly released Storage-Compute Separation and Cross-Cluster Replication of Doris, and we embrace any ideas and inputs about the SuperSonic framework and the Apache Doris project."},{"id":"/Choosing-an-OLAP-Engine-for-Financial-Risk-Management-What-to-Consider","metadata":{"permalink":"/blog/Choosing-an-OLAP-Engine-for-Financial-Risk-Management-What-to-Consider","source":"@site/blog/Choosing-an-OLAP-Engine-for-Financial-Risk-Management-What-to-Consider.md","title":"Choosing an OLAP engine for financial risk management: what to consider?","description":"This post provides reference for what you should take into account when choosing an OLAP engine in a financial scenario.","date":"2023-08-17T00:00:00.000Z","tags":[{"inline":true,"label":"Best Practice","permalink":"/blog/tags/best-practice"}],"hasTruncateMarker":false,"authors":[{"name":"velodb.io \xb7 Jianbo Liu","key":null,"page":null}],"frontMatter":{"title":"Choosing an OLAP engine for financial risk management: what to consider?","description":"This post provides reference for what you should take into account when choosing an OLAP engine in a financial scenario.","date":"2023-08-17","author":"velodb.io \xb7 Jianbo Liu","tags":["Best Practice"],"externalLink":"https://www.velodb.io/blog/136","image":"/images/financial-risk.png"},"unlisted":false,"prevItem":{"title":"LLM-powered OLAP: the Tencent application with Apache Doris","permalink":"/blog/Tencent-LLM"},"nextItem":{"title":"Auto synchronization of an entire MySQL database for data analysis","permalink":"/blog/Auto-Synchronization-of-an-Entire-MySQL-Database-for-Data-Analysis"}},"content":"\x3c!-- \\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n\\n\\nFrom a data engineer\'s point of view, financial risk management is a series of data analysis activities on financial data. The financial sector imposes its unique requirements on data engineering. This post explains them with a use case of Apache Doris, and provides reference for what you should take into account when choosing an OLAP engine in a financial scenario. \\n\\n## Data Must Be Combined\\n\\nThe financial data landscape is evolving from standalone to distributed, heterogeneous systems. For example, in this use case scenario, the fintech service provider needs to connect the various transaction processing (TP) systems (MySQL, Oracle, and PostgreSQL) of its partnering banks. Before they adopted an OLAP engine, they were using Kettle to collect data. The ETL tool did not support join queries across different data sources and it could not store data. The ever-enlarging data size at the source end was pushing the system towards latency and instability. That\'s when they decided to introduce an OLAP engine.\\n\\nThe financial user\'s main pursuit is quick queries on large data volume with as least engineering and maintenance efforts as possible, so when it comes to the choice of OLAP engines, SQL on Hadoop should be crossed off the list due to its huge ecosystem and complicated components. One reason that they landed on Apache Doris was the metadata management capability. Apache Doris collects metadata of various data sources via API so it is a fit for the case which requires combination of different TP systems. \\n\\n## High Concurrency & High Throughput\\n\\nFinancial risk control is based on analysis of large amounts of transaction data. Sometimes analysts identify abnormalities by combining data from different large tables, and often times they need to check a certain data record, which comes in the form of concurrent point queries in the data system. Thus, the OLAP engine should be able to handle both high-throughput queries and high-concurrency queries. \\n\\nTo speed up the highly concurrent point queries, you can create [Materialized Views](https://doris.apache.org/docs/dev/query-acceleration/materialized-view/) in Apache Doris. A Materialized View is a pre-computed data set stored in Apache Doris so that the system can respond much faster to queries that are frequently conducted. \\n\\nTo facilitate queries on large tables, you can leverage the [Colocation Join](https://doris.apache.org/docs/dev/query-acceleration/join-optimization/colocation-join/) mechanism. Colocation Join minimizes data transfer between computation nodes to reduce overheads brought by data movement. Thus, it can largely improve query speed when joining large tables.\\n\\n![colocation-join](/images/Xingyun_1.png)\\n\\n## Log Analysis\\n\\nLog analysis is important in financial data processing. Real-time processing and monitoring of logs can expose risks in time. Apache Doris provides data storage and analytics capabilities to make log analysis easier and more efficient. As logs are bulky, Apache Doris can deliver a high data compression rate to lower storage costs. \\n\\nRetrieval is a major part of log analysis, so [Apache Doris 2.0](https://doris.apache.org/docs/dev/releasenotes/release-2.0.0) supports inverted index, which is a way to accelerate text searching and equivalence/range queries on numerics and datetime. It allows users to quickly locate the log record that they need among the massive data. The JSON storage feature in Apache Doris is reported to reduce storage costs of user activity logs by 70%, and the variety of parse functions provided can save data engineers from developing their own SQl functions. \\n\\n![log-analysis](/images/Xingyun_2.png)\\n\\n## Easy Maintenance\\n\\nIn addition to the easy deployment, Apache Doris has a few mechanisms that are designed to save maintenance efforts. For example, it ensures high availability of cluster nodes with Systemd, and high availability of data with multi-replica and auto-balancing of replicas, so all maintenance required is to backup metadata on a regular basis. Apache Doris also supports [dynamic partitioning of data](https://doris.apache.org/docs/dev/advanced/partition/dynamic-partition/), which means it will automatically create or delete data partitions according to the rules specified by the user. This saves efforts in partition management and eliminates possible efforts caused by manual management.\\n\\n## Architecture Overview\\n\\nThis is overall data architecture in the case. The user utilizes Apache Flume for log data collection, and DataX for data update. Data from multiple sources will be collected into Apache Doris to form a data mart, from which analysts extract information to generate reports and dashboards for reference in risk control and business decisions. As for stability of the data mart itself, Grafana and Prometheus are used to monitor memory usage, compaction score and query response time of Apache Doris to make sure it is running well.\\n\\n![data-architecture](/images/Xingyun_3.png)"},{"id":"/Auto-Synchronization-of-an-Entire-MySQL-Database-for-Data-Analysis","metadata":{"permalink":"/blog/Auto-Synchronization-of-an-Entire-MySQL-Database-for-Data-Analysis","source":"@site/blog/Auto-Synchronization-of-an-Entire-MySQL-Database-for-Data-Analysis.md","title":"Auto synchronization of an entire MySQL database for data analysis","description":"Flink-Doris-Connector 1.4.0 allows users to ingest a whole database (MySQL or Oracle) that contains thousands of tables into Apache Doris, in one step.","date":"2023-08-16T00:00:00.000Z","tags":[{"inline":true,"label":"Tech Sharing","permalink":"/blog/tags/tech-sharing"}],"hasTruncateMarker":false,"authors":[{"name":"velodb.io \xb7 VeloDB Engineering Team","key":null,"page":null}],"frontMatter":{"title":"Auto synchronization of an entire MySQL database for data analysis","description":"Flink-Doris-Connector 1.4.0 allows users to ingest a whole database (MySQL or Oracle) that contains thousands of tables into Apache Doris, in one step.","date":"2023-08-16","author":"velodb.io \xb7 VeloDB Engineering Team","tags":["Tech Sharing"],"externalLink":"https://www.velodb.io/blog/135","image":"/images/auto-synchronize.png"},"unlisted":false,"prevItem":{"title":"Choosing an OLAP engine for financial risk management: what to consider?","permalink":"/blog/Choosing-an-OLAP-Engine-for-Financial-Risk-Management-What-to-Consider"},"nextItem":{"title":"New milestone: Apache Doris 2.0.0 just released","permalink":"/blog/release-note-2.0.0"}},"content":"\x3c!-- \\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n\\n\\nFlink-Doris-Connector 1.4.0 allows users to ingest a whole database (**MySQL** or **Oracle**) that contains thousands of tables into [Apache Doris](https://doris.apache.org/zh-CN/), a real-time analytic database, **in one step**.\\n\\nWith built-in Flink CDC, the Connector can directly synchronize the table schema and data from the upstream source to Apache Doris, which means users no longer have to write a DataStream program or pre-create mapping tables in Doris. \\n\\nWhen a Flink job starts, the Connector automatically checks for data equivalence between the source database and Apache Doris. If the data source contains tables which do not exist in Doris, the Connector will automatically create those same tables in Doris, and utilizes the side outputs of Flink to facilitate the ingestion of multiple tables at once; if there is a schema change in the source, it will automatically obtain the DDL statement and make the same schema change in Doris. \\n\\n## Quick Start\\n\\nDownload Flink Doris Connector: https://doris.apache.org/download/\\n\\n## How to Use It\\n\\nFor example, to ingest a whole MySQL database `mysql_db` into Doris (the MySQL table names start with `tbl` or `test`), simply execute the following command (no need to create the tables in Doris in advance):\\n\\n```Shell\\n<FLINK_HOME>/bin/flink run \\\\\\n    -Dexecution.checkpointing.interval=10s \\\\\\n    -Dparallelism.default=1 \\\\\\n    -c org.apache.doris.flink.tools.cdc.CdcTools \\\\\\n    lib/flink-doris-connector-1.16-1.4.0.jar \\\\\\n    mysql-sync-database \\\\\\n    --database test_db \\\\\\n    --mysql-conf hostname=127.0.0.1 \\\\\\n    --mysql-conf username=root \\\\\\n    --mysql-conf password=123456 \\\\\\n    --mysql-conf database-name=mysql_db \\\\\\n    --including-tables \\"tbl|test.*\\" \\\\\\n    --sink-conf fenodes=127.0.0.1:8030 \\\\\\n    --sink-conf username=root \\\\\\n    --sink-conf password=123456 \\\\\\n    --sink-conf jdbc-url=jdbc:mysql://127.0.0.1:9030 \\\\\\n    --sink-conf sink.label-prefix=label1 \\\\\\n    --table-conf replication_num=1 \\n```\\n\\nTo ingest an Oracle database: please refer to the [example code](https://github.com/apache/doris-flink-connector/pull/156).\\n\\n## How It Performs\\n\\nWhen it comes to synchronizing a whole database (containing hundreds or even thousands of tables, active or inactive), most users want it to be done within seconds. So we tested the Connector to see if it came up to scratch:\\n\\n- 1000 MySQL tables, each having 100 fields. All tables were active (which meant they were continuously updated and each data writing involved over a hundred rows)\\n- Flink job checkpoint: 10s\\n\\nUnder pressure test, the system showed high stability, with key metrics as follows:\\n\\n![Flink-Doris-Connector](/images/FDC_1.png)\\n\\n![Flink-CDC](/images/FDC_2.png)\\n\\n![Doris-Cluster-Compaction-Score](/images/FDC_3.png)\\n\\nAccording to feedback from early adopters, the Connector has also delivered high performance and system stability in 10,000-table database synchronization in their production environment. This proves that the combination of Apache Doris and Flink CDC is capable of large-scale data synchronization with high efficiency and reliability.\\n\\n## How It Benefits Data Engineers\\n\\nEngineers no longer have to worry about table creation or table schema maintenance, saving them days of tedious and error-prone work. Previously in Flink CDC, you need to create a Flink job for each table and build a log parsing link at the source end, but now with whole-database ingestion, resource consumption in the source database is largely reduced. It is also a unified solution for incremental update and full update.\\n\\n## Other Features\\n\\n**1. Joining dimension table and fact table**\\n\\nThe common practice is to put dimension tables in Doris and run join queries via the real-time stream of Flink. Based on the [Async I/O of Flink](https://nightlies.apache.org/flink/flink-docs-release-1.14/docs/dev/datastream/operators/asyncio/), Flink-Doris-Connector 1.4.0 implements asynchronous Lookup Join, so the Flink real-time stream won\'t be blocked due to queries. Also, the Connector allows you to combine multiple queries into one big query, and send it to Doris at once for processing. This improves the efficiency and throughput of such join queries.\\n\\n**2. Thrift** **SDK**\\n\\nWe introduced Thrift-Service SDK into the Connector so users no longer have to use Thrift plug-ins or configure a Thrift environment in compilation. This makes the compilation process much simpler.\\n\\n**3. On-Demand Stream Load**\\n\\nDuring data synchronization, when there is no new data ingestion, no Stream Load requests will be issued. This avoids unnecessary consumption of cluster resources.\\n\\n**4. Polling of Backend Nodes**\\n\\nFor data ingestion, Doris calls a frontend node to obtain a list of the backend nodes, and randomly chooses one to launch an ingestion request. That backend node will be the Coordinator. Flink-Doris-Connector 1.4.0 allows users to enable the polling mechanism, which is to have a different backend node to be the Coordinator at each Flink checkpoint to avoid too much pressure on a single backend node for a long time.\\n\\n**5. Support for More Data Types**\\n\\nIn addition to the common data types, Flink-Doris-Connector 1.4.0 supports DecimalV3/DateV2/DateTimev2/Array/JSON in Doris.\\n\\n## Example Usage\\n\\n**Read from Apache Doris:** \\n\\nYou can read data from Doris via DataStream or FlinkSQL (bounded stream). Predicate pushdown is supported.\\n\\n```Java\\nCREATE TABLE flink_doris_source (\\n    name STRING,\\n    age INT,\\n    score DECIMAL(5,2)\\n    ) \\n    WITH (\\n      \'connector\' = \'doris\',\\n      \'fenodes\' = \'127.0.0.1:8030\',\\n      \'table.identifier\' = \'database.table\',\\n      \'username\' = \'root\',\\n      \'password\' = \'password\',\\n      \'doris.filter.query\' = \'age=18\'\\n);\\n\\nSELECT * FROM flink_doris_source;\\n```\\n\\n**Join dimension table and fact table**:\\n\\n```Java\\nCREATE TABLE fact_table (\\n  `id` BIGINT,\\n  `name` STRING,\\n  `city` STRING,\\n  `process_time` as proctime()\\n) WITH (\\n  \'connector\' = \'kafka\',\\n  ...\\n);\\n\\ncreate table dim_city(\\n  `city` STRING,\\n  `level` INT ,\\n  `province` STRING,\\n  `country` STRING\\n) WITH (\\n  \'connector\' = \'doris\',\\n  \'fenodes\' = \'127.0.0.1:8030\',\\n  \'jdbc-url\' = \'jdbc:mysql://127.0.0.1:9030\',\\n  \'lookup.jdbc.async\' = \'true\',\\n  \'table.identifier\' = \'dim.dim_city\',\\n  \'username\' = \'root\',\\n  \'password\' = \'\'\\n);\\n\\nSELECT a.id, a.name, a.city, c.province, c.country,c.level \\nFROM fact_table a\\nLEFT JOIN dim_city FOR SYSTEM_TIME AS OF a.process_time AS c\\nON a.city = c.city\\n```\\n\\n**Write to Apache Doris**: \\n\\n```Java\\nCREATE TABLE doris_sink (\\n    name STRING,\\n    age INT,\\n    score DECIMAL(5,2)\\n    ) \\n    WITH (\\n      \'connector\' = \'doris\',\\n      \'fenodes\' = \'127.0.0.1:8030\',\\n      \'table.identifier\' = \'database.table\',\\n      \'username\' = \'root\',\\n      \'password\' = \'\',\\n      \'sink.label-prefix\' = \'doris_label\',\\n      //json write in\\n      \'sink.properties.format\' = \'json\',\\n      \'sink.properties.read_json_by_line\' = \'true\'\\n);\\n```\\n\\nIf you\'ve got any questions, find Apache Doris developers on [Slack](https://join.slack.com/t/apachedoriscommunity/shared_invite/zt-2unfw3a3q-MtjGX4pAd8bCGC1UV0sKcw)."},{"id":"/release-note-2.0.0","metadata":{"permalink":"/blog/release-note-2.0.0","source":"@site/blog/release-note-2.0.0.md","title":"New milestone: Apache Doris 2.0.0 just released","description":"Dear community, we are excited to announce that Apache Doris 2.0.0 is now production-ready on August 16, 2023","date":"2023-08-16T00:00:00.000Z","tags":[{"inline":true,"label":"Release Notes","permalink":"/blog/tags/release-notes"}],"hasTruncateMarker":false,"authors":[{"name":"Apache Doris","key":null,"page":null}],"frontMatter":{"title":"New milestone: Apache Doris 2.0.0 just released","description":"Dear community, we are excited to announce that Apache Doris 2.0.0 is now production-ready on August 16, 2023","date":"2023-08-16","author":"Apache Doris","tags":["Release Notes"],"image":"/images/2.0.0.png"},"unlisted":false,"prevItem":{"title":"Auto synchronization of an entire MySQL database for data analysis","permalink":"/blog/Auto-Synchronization-of-an-Entire-MySQL-Database-for-Data-Analysis"},"nextItem":{"title":"Database in fintech: how to support 10,000 dashboards without causing a mess","permalink":"/blog/Database-in-Fintech-How-to-Support-ten-thousand-Dashboards-Without-Causing-a-Mess"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n\\nWe are more than excited to announce that, after six months of coding, testing, and fine-tuning, Apache Doris 2.0.0 is now production-ready. Special thanks to the 275 committers who altogether contributed over 4100 optimizations and fixes to the project. \\n\\nThis new version highlights:\\n\\n- 10 times faster data queries\\n- Enhanced log analytic and federated query capabilities\\n- More efficient data writing and updates\\n- Improved multi-tenant and resource isolation mechanisms\\n- Progresses in elastic scaling of resources and storage-compute separation\\n- Enterprise-facing features for higher usability\\n\\n> Download: https://doris.apache.org/download\\n>\\n> GitHub source code: https://github.com/apache/doris/releases/tag/2.0.0-rc04\\n\\n## **A 10 Times Performance Increase**\\n\\nIn SSB-Flat and TPC-H benchmarking, Apache Doris 2.0.0 delivered **over 10-time faster query performance** compared to an early version of Apache Doris.\\n\\n![TPCH-benchmark-SSB-Flat-benchmark](/images/release-note-2.0.0-1.png)\\n\\nThis is realized by the introduction of a smarter query optimizer, inverted index, a parallel execution model, and a series of new functionalities to support high-concurrency point queries.\\n\\n### A smarter query optimizer\\n\\nThe brand new query optimizer, Nereids, has a richer statistical base and adopts the Cascades framework. It is capable of self-tuning in most query scenarios and supports all 99 SQLs in TPC-DS, so users can expect high performance without any fine-tuning or SQL rewriting.\\n\\nTPC-H tests showed that Nereids, with no human intervention, outperformed the old query optimizer by a wide margin. Over 100 users have tried Apache Doris 2.0.0 in their production environment and the vast majority of them reported huge speedups in query execution.\\n\\n![Nereids-optimizer-TPCH](/images/release-note-2.0.0-2.png)\\n\\n**Doc**: https://doris.apache.org/docs/dev/query-acceleration/nereids/\\n\\nNereids is enabled by default in Apache Doris 2.0.0: `SET enable_nereids_planner=true`. Nereids collects statistical data by calling the Analyze command.\\n\\n### Inverted Index\\n\\nIn Apache Doris 2.0.0, we introduced [inverted index](https://doris.apache.org/docs/dev/data-table/index/inverted-index?_highlight=inverted) to better support fuzzy keyword search, equivalence queries, and range queries.\\n\\nA smartphone manufacturer tested Apache Doris 2.0.0 in their user behavior analysis scenarios. With inverted index enabled, v2.0.0 was able to finish the queries within milliseconds and maintain stable performance as the query concurrency level went up. In this case, it is 5 to 90 times faster than its old version. \\n\\n![inverted-index-performance](/images/release-note-2.0.0-3.png)\\n\\n### 20 times higher concurrency capability\\n\\nIn scenarios like e-commerce order queries and express tracking, a huge number of end data users search for a certain data record simultaneously. These are what we call high-concurrency point queries, which can bring huge pressure on the system. A traditional solution is to introduce Key-Value stores like Apache HBase for such queries, and Redis as a cache layer to ease the burden, but that means redundant storage and higher maintenance costs.\\n\\nFor a column-oriented DBMS like Apache Doris, the I/O usage of point queries will be multiplied. We need neater execution. Thus, on the basis of columnar storage, we added row storage format and row cache to increase row reading efficiency, short-circuit plans to speed up data retrieval, and prepared statements to reduce frontend overheads.\\n\\nAfter these optimizations, Apache Doris 2.0 reached a concurrency level of **30,000 QPS per node** on YCSB on a 16 Core 64G cloud server with 4\xd71T hard drives, representing an improvement of **20 times** compared to its older version. This makes Apache Doris a good alternative to HBase in high-concurrency scenarios, so that users don\'t need to endure extra maintenance costs and redundant storage brought by complicated tech stacks.\\n\\nRead more: https://doris.apache.org/blog/How-We-Increased-Database-Query-Concurrency-by-20-Times\\n\\n### A self-adaptive parallel execution model\\n\\nApache 2.0 brought in a Pipeline execution model for higher efficiency and stability in hybrid analytic workloads. In this model, the execution of queries is driven by data. The blocking operators in all query execution processes are split into pipelines. Whether a pipeline gets an execution thread depends on whether its relevant data is ready. This enables asynchronous blocking operations and more flexible system resource management. Also, this improves CPU efficiency as the system doesn\'t have to create and destroy threads that much.\\n\\nDoc: https://doris.apache.org/docs/dev/query-acceleration/pipeline-execution-engine/\\n\\n**How to enable the Pipeline execution model**\\n\\n- The Pipeline execution engine is enabled by default in Apache Doris 2.0: `Set enable_pipeline_engine = true`.\\n- `parallel_pipeline_task_num` represents the number of pipeline tasks that are parallelly executed in SQL queries. The default value of it is `0`, which means Apache Doris will automatically set the concurrency level to half the number of CPUs in each backend node. Users can change this value as they need it. \\n- For those who are upgrading to Apache Doris 2.0 from an older version, it is recommended to set the value of `parallel_pipeline_task_num` to that of `parallel_fragment_exec_instance_num` in the old version.\\n\\n## A Unified Platform for Multiple Analytic Workloads\\n\\nApache Doris has been pushing its boundaries. Starting as an OLAP engine for reporting, it is now a data warehouse capable of ETL/ELT and more. Version 2.0 is making advancements in its log analysis and data lakehousing capabilities.  \\n\\n### A 10 times more cost-effective log analysis solution\\n\\nApache Doris 2.0.0 provides native support for semi-structured data. In addition to JSON and Array, it now supports a complex data type: Map. Based on Light Schema Change, it also supports Schema Evolution, which means you can adjust the schema as your business changes. You can add or delete fields and indexes, and change the data types for fields. As we introduced inverted index and a high-performance text analysis algorithm into it, it can execute full-text search and dimensional analysis of logs more efficiently. With faster data writing and query speed and lower storage cost, it is 10 times more cost-effective than the common log analytic solution within the industry.\\n\\n![Apache-Doris-VS-Elasticsearch](/images/release-note-2.0.0-4.png)\\n\\n### Enhanced data lakehousing capabilities\\n\\nIn Apache Doris 1.2, we introduced Multi-Catalog to allow for auto-mapping and auto-synchronization of data from heterogeneous sources. In version 2.0.0, we extended the list of data sources supported and optimized Doris for based on users\' needs in production environment.\\n\\n![Apache-Doris-data-lakehouse](/images/release-note-2.0.0-5.png)\\n\\nApache Doris 2.0.0 supports dozens of data sources including Hive, Hudi, Iceberg, Paimon, MaxCompute, Elasticsearch, Trino, ClickHouse, and almost all open lakehouse formats. It also supports snapshot queries on Hudi Copy-on-Write tables and read optimized queries on Hudi Merge-on-Read tables. It allows for authorization of Hive Catalog using Apache Ranger, so users can reuse their existing privilege control system. Besides, it supports extensible authorization plug-ins to enable user-defined authorization methods for any catalog. \\n\\nTPC-H benchmark tests showed that Apache Doris 2.0.0 is 3~5 times faster than Presto/Trino in queries on Hive tables. This is realized by all-around optimizations (in small file reading, flat table reading, local file cache, ORC/Parquet file reading, Compute Nodes, and information collection of external tables) finished in this development cycle and the distributed execution framework, vectorized execution engine, and query optimizer of Apache Doris. \\n\\n![Apache-Doris-VS-Trino](/images/release-note-2.0.0-6.png)\\n\\nAll this gives Apache Doris 2.0.0 an edge in data lakehousing scenarios. With Doris, you can do incremental or overall synchronization of multiple upstream data sources in one place, and expect much higher data query performance than other query engines. The processed data can be written back to the sources or provided for downstream systems. In this way, you can make Apache Doris your unified data analytic gateway.\\n\\n## Efficient Data Update\\n\\nData update is important in real-time analysis, since users want to always be accessible to the latest data, and be able to update data flexibly, such as updating a row or just a few columns, batching updating or deleting their specified data, or even overwriting a whole data partition.\\n\\nEfficient data updating has been another hill to climb in data analysis. Apache Hive only supports updates on the partition level, while Hudi and Iceberg do better in low-frequency batch updates instead of real-time updates due to their Merge-on-Read and Copy-on-Write implementations.\\n\\nAs for data updating, Apache Doris 2.0.0 is capable of:\\n\\n- **Faster data writing**: In the pressure tests with an online payment platform, under 20 concurrent data writing tasks, Doris reached a writing throughput of 300,000 records per second and maintained stability throughout the over 10-hour continuous writing process.\\n- **Partial column update**: Older versions of Doris implements partial column update by `replace_if_not_null` in the Aggregate Key model. In 2.0.0, we enable partial column updates in the Unique Key model. That means you can directly write data from multiple source tables into a flat table, without having to concatenate them into one output stream using Flink before writing. This method avoids a complicated processing pipeline and the extra resource consumption. You can simply specify the columns you need to update.\\n- **Conditional update and deletion**: In addition to the simple Update and Delete operations, we realize complicated conditional updates and deletes operations on the basis of Merge-on-Write. \\n\\n## Faster, Stabler, and Smarter Data Writing\\n\\n### Higher speed in data writing\\n\\nAs part of our continuing effort to strengthen the real-time analytic capability of Apache Doris, we have improved the end-to-end real-time data writing capability of version 2.0.0. Benchmark tests reported higher throughput in various writing methods:\\n\\n- Stream Load, TPC-H 144G lineitem table, 48-bucket Duplicate table, triple-replica writing: throughput increased by 100%\\n- Stream Load, TPC-H 144G lineitem table, 48-bucket Unique Key table, triple-replica writing: throughput increased by 200%\\n- Insert Into Select, TPC-H 144G lineitem table, 48-bucket Duplicate table: throughput increased by 50%\\n- Insert Into Select, TPC-H 144G lineitem table, 48-bucket Unique Key table: throughput increased by 150%\\n\\n### Greater stability in high-concurrency data writing \\n\\nThe sources of system instability often includes small file merging, write amplification, and the consequential disk I/O and CPU overheads. Hence, we introduced Vertical Compaction and Segment Compaction in version 2.0.0 to eliminate OOM errors in compaction and avoid the generation of too many segment files during data writing. After such improvements, Apache Doris can write data 50% faster while **using only 10% of the memory that it previously used**.\\n\\nRead more: https://doris.apache.org/blog/Understanding-Data-Compaction-in-3-Minutes/\\n\\n### Auto-synchronization of table schema\\n\\nThe latest Flink-Doris-Connector allows users to synchronize an entire database (such as MySQL and Oracle) to Apache Doris by one simple step. According to our test results, one single synchronization task can support the real-time concurrent writing of thousands of tables. Users no longer need to go through a complicated synchronization procedure because Apache Doris has automated the process. Changes in the upstream data schema will be automatically captured and dynamically updated to Apache Doris in a seamless manner.\\n\\nRead more: https://doris.apache.org/blog/Auto-Synchronization-of-an-Entire-MySQL-Database-for-Data-Analysis\\n\\n## A New Multi-Tenant Resource Isolation Solution\\n\\nThe purpose of multi-tenant resource isolation is to avoid resource preemption in the case of heavy loads. For that sake, older versions of Apache Doris adopted a hard isolation plan featured by Resource Group: Backend nodes of the same Doris cluster would be tagged, and those of the same tag formed a Resource Group. As data was ingested into the database, different data replicas would be written into different Resource Groups, which will be responsible for different workloads. For example, data reading and writing will be conducted on different data tablets, so as to realize read-write separation. Similarly, you can also put online and offline business on different Resource Groups. \\n\\n![resource-isolation](/images/release-note-2.0.0-7.png)\\n\\nThis is an effective solution, but in practice, it happens that some Resource Groups are heavily occupied while others are idle. We want a more flexible way to reduce vacancy rate of resources. Thus, in 2.0.0, we introduce Workload Group resource soft limit.\\n\\n![workload-group](/images/release-note-2.0.0-8.png)\\n\\nThe idea is to divide workloads into groups to allow for flexible management of CPU and memory resources. Apache Doris associates a query with a Workload Group, and limits the percentage of CPU and memory that a single query can use on a backend node. The memory soft limit can be configured and enabled by the user. \\n\\nWhen there is a cluster resource shortage, the system will kill the largest memory-consuming query tasks; when there are sufficient cluster resources, once a Workload Group uses more resources than expected, the idle cluster resources will be shared among all the Workload Groups to give full play to the system memory and ensure stable execution of queries. You can also prioritize the Workload Groups in terms of resource allocation. In other words, you can decide which tasks can be assigned with adequate resources and which not.\\n\\nMeanwhile, we introduced Query Queue in 2.0.0. Upon Workload Group creation, you can set a maximum query number for a query queue. Queries beyond that limit will wait for execution in the queue. This is to reduce system burden under heavy workloads.\\n\\n## Elastic Scaling and Storage-Compute Separation\\n\\nWhen it comes to computation and storage resources, what do users want?\\n\\n- **Elastic scaling of computation resources**: Scale up resources quickly in peak times to increase efficiency and scale down in valley times to reduce costs.\\n- **Lower storage costs**: Use low-cost storage media and separate storage from computation.\\n- **Separation of workloads**: Isolate the computation resources of different workloads to avoid preemption.\\n- **Unified management of data**: Simply manage catalogs and data in one place.\\n\\nTo separate storage and computation is a way to realize elastic scaling of resources, but it demands more efforts in maintaining storage stability, which determines the stability and continuity of OLAP services. To ensure storage stability, we introduced mechanisms including cache management, computation resource management, and garbage collection.\\n\\n In this respect, we divide our users into three groups after investigation:\\n\\n1. Users with no need for resource scaling\\n2. Users requiring resource scaling, low storage costs, and workload separation from Apache Doris\\n3. Users who already have a stable large-scale storage system and thus require an advanced compute-storage-separated architecture for efficient resource scaling\\n\\nApache Doris 2.0 provides two solutions to address the needs of the first two types of users.\\n\\n1. **Compute nodes**. We introduced stateless compute nodes in version 2.0. Unlike the mix nodes, the compute nodes do not save any data and are not involved in workload balancing of data tablets during cluster scaling. Thus, they are able to quickly join the cluster and share the computing pressure during peak times. In addition, in data lakehouse analysis, these nodes will be the first ones to execute queries on remote storage (HDFS/S3) so there will be no resource competition between internal tables and external tables.\\n   1.  [Read more in Docs](https://doris.apache.org/docs/2.0/admin-manual/resource-admin/compute-node/)\\n2. **Hot-cold data separation**. Hot/cold data refers to data that is frequently/seldom accessed, respectively. Generally, it makes more sense to store cold data in low-cost storage. Older versions of Apache Doris support lifecycle management of table partitions: As hot data cooled down, it would be moved from SSD to HDD. However, data was stored with multiple replicas on HDD, which was still a waste. Now, in Apache Doris 2.0, cold data can be stored in object storage, which is even cheaper and allows single-copy storage. That reduces the storage costs by 70% and cuts down the computation and network overheads that come with storage.\\n   1. [Read more in Docs](https://doris.apache.org/blog/Tiered-Storage-for-Hot-and-Cold-Data-What-Why-and-How)\\n\\nFor neater separate of computation and storage, the VeloDB team is going to contribute the Cloud Compute-Storage-Separation solution to the Apache Doris project. The performance and stability of it has stood the test of hundreds of companies in their production environment. The merging of code will be finished by October this year, and all Apache Doris users will be able to get an early taste of it in September.\\n\\n## Enhanced Usability\\n\\nApache Doris 2.0.0 also highlights some enterprise-facing functionalities.\\n\\n### Support for Kubernetes Deployment\\n\\nOlder versions of Apache Doris communicate based on IP, so any host failure in Kubernetes deployment that causes a POD IP drift will lead to cluster unavailability. Now, version 2.0 supports FQDN. That means the failed Doris nodes can recover automatically without human intervention, which lays the foundation for Kubernetes deployment and elastic scaling.  \\n\\n### Support for Cross-Cluster Replication (CCR)\\n\\nApache Doris 2.0.0 supports cross-cluster replication (CCR). Data changes at the database/table level in the source cluster will be synchronized to the target cluster. You can choose to replicate the incremental data or the overall data. \\n\\nIt also supports synchronization of DDL, which means DDL statements executed by the source cluster can also by automatically replicated to the target cluster. \\n\\nIt is simple to configure and use CCR in Doris. Leveraging this functionality, you can implement read-write separation and multi-datacenter replication \\n\\nThis feature allows for higher availability of data, read/write workload separation, and cross-data-center replication more efficiently.\\n\\n## Behavior Change\\n\\n- Use rolling upgrade from 1.2-ITS to 2.0.0, and restart upgrade from preview versions of 2.0 to 2.0.0;\\n- The new query optimizer (Nereids) is enabled by default: `enable_nereids_planner=true`;\\n- All non-vectorized code has been removed from the system, so the `enable_vectorized_engine` parameter no long works;\\n- A new parameter `enable_single_replica_compaction` has been added;\\n- datev2, datetimev2, and decimalv3 are the default data types in table creation; datav1, datetimev1, and decimalv2 are not supported in table creation;\\n- decimalv3 is the default data type for JDBC and Iceberg Catalog;\\n- A new data type `AGG_STATE` has been added;\\n- The cluster column has been removed from backend tables;\\n- For better compatibility with BI tools, datev2 and datetimev2 are displayed as date and datetime when `show create table`;\\n- max_openfiles and swaps checks are added to the backend startup script so inappropriate system configuration might lead to backend failure;\\n- Password-free login is not allowed when accessing frontend on localhost;\\n- If there is a Multi-Catalog in the system, by default, only data of the internal catalog will be displayed when querying information schema;\\n- A limit has been imposed on the depth of the expression tree. The default value is 200;\\n- The single quote in the return value of array string has been changed to double quote;\\n- The Doris processes are renamed to DorisFE and DorisBE.\\n\\n## Embarking on the 2.0.0 Journey\\n\\nTo make Apache Doris 2.0.0 production-ready, we invited hundreds of enterprise users to engage in the testing and optimized it for better performance, stability, and usability. In the next phase, we will continue responding to user needs with agile release planning. We plan to launch 2.0.1 in late August and 2.0.2 in September, as we keep fixing bugs and adding new features. We also plan to release an early version of 2.1 in September to bring a few long-requested capabilities to you. For example, in Doris 2.1, the Variant data type will better serve the schema-free analytic needs of semi-structured data; the multi-table materialized views will be able to simplify the data scheduling and processing link while speeding up queries; more and neater data ingestion methods will be added and nested composite data types will be realized.\\n\\nIf you have any questions or ideas when investigating, testing, and deploying Apache Doris, please find us on [Slack](https://join.slack.com/t/apachedoriscommunity/shared_invite/zt-2unfw3a3q-MtjGX4pAd8bCGC1UV0sKcw). Our developers will be happy to hear them and provide targeted support."},{"id":"/Database-in-Fintech-How-to-Support-ten-thousand-Dashboards-Without-Causing-a-Mess","metadata":{"permalink":"/blog/Database-in-Fintech-How-to-Support-ten-thousand-Dashboards-Without-Causing-a-Mess","source":"@site/blog/Database-in-Fintech-How-to-Support-ten-thousand-Dashboards-Without-Causing-a-Mess.md","title":"Database in fintech: how to support 10,000 dashboards without causing a mess","description":"This article introduces the lifecycle of financial metrics in a database, from how they\'re produced to how they\'re efficiently presented in data reports.","date":"2023-08-05T00:00:00.000Z","tags":[{"inline":true,"label":"Best Practice","permalink":"/blog/tags/best-practice"}],"hasTruncateMarker":false,"authors":[{"name":"velodb.io \xb7 Hou Lan","key":null,"page":null}],"frontMatter":{"title":"Database in fintech: how to support 10,000 dashboards without causing a mess","description":"This article introduces the lifecycle of financial metrics in a database, from how they\'re produced to how they\'re efficiently presented in data reports.","date":"2023-08-05","author":"velodb.io \xb7 Hou Lan","tags":["Best Practice"],"externalLink":"https://www.velodb.io/blog/133","image":"/images/fintech-service.png"},"unlisted":false,"prevItem":{"title":"New milestone: Apache Doris 2.0.0 just released","permalink":"/blog/release-note-2.0.0"},"nextItem":{"title":"For entry-level data engineers: how to build a simple but solid data architecture","permalink":"/blog/For-Entry-Level-Data-Engineers-How-to-Build-a-Simple-but-Solid-Data-Architecture"}},"content":"\x3c!-- \\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\nIn a data-intensive industry like finance, data comes from numerous entries and goes to numerous exits. Such status quo can easily, and almost inevitably, lead to chaos in data analysis and management. For example, analysts from different business lines define their own financial metrics in data reports. When you pool these countless reports together in your data architecture, you will find that many metrics overlap or even contradict each other in definition. The consequence is, developing a simple data report will require lots of clarification back and forth, making the process more complicated and time-consuming than it should be.\\n\\nAs your business grows, your data management will arrive at a point when \\"standardization\\" is needed. In terms of data engineering, that means you need a data platform where you can produce and manage all metrics. That\'s your architectural prerequisite to provide efficient financial services. \\n\\nThis article introduces the lifecycle of financial metrics in a database (in this case, [Apache Doris](https://doris.apache.org/)), from how they\'re produced to how they\'re efficiently presented in data reports. You will get an inside view of what\'s behind those fancy financial dashboards. \\n\\n## Define New Metrics & Add Them to Your Database\\n\\nFundamentally, metrics are fields in a table. To provide a more concrete idea of them, I will explain with an example in the banking industry. \\n\\nBanks measure the assets of customers by AUM (Assets Under Management). In this scenario, AUM is an **atomic metric**, which is often a field in the source data table. On the basis of AUM, analysts derive a series of **derivative metrics**, such as \\"year-on-year AUM growth\\", \\"month-on-month AUM growth\\", and \\"AUM per customer\\".\\n\\nOnce you define the new metrics, you add them to your data reports, which involves a few simple configurations in Apache Doris:\\n\\nDevelopers update the metadata accordingly, register the base table where the metrics are derived, configure the data granularity and update frequency of intermediate tables, and input the metric name and definition. Some engineers will also monitor the metrics to identify abnormalities and remove redundant metrics based on a metric evaluation system.\\n\\nWhen the metrics are soundly put in place, you can ingest new data into your database to get your data reports. For example, if you ingest CSV files, we recommend the Stream Load method of Apache Doris and a file size of 1~10G per batch. Eventually, these metrics will be visualized in data charts. \\n\\n## Calculate Your Metrics\\n\\nAs is mentioned, some metrics are produced by combining multiple fields in the source table. In data engineering, that is a multi-table join query. Based on the optimization experience of an Apache Doris user, we recommend flat tables instead of Star/Snowflake Schema. The user reduced the query response time on tables of 100 million rows **from 5s to 63ms** after such a change.\\n\\n![join-queries](/images/Pingan_1.png)\\n\\nThe flat table solution also eliminates jitter.\\n\\n![reduced-jitter](/images/Pingan_2.png)\\n\\n## Enable SQL Caching to Reduce Resource Consumption\\n\\nAnalysts often check data reports of the same metrics on a regular basis. These reports are produced by the same SQL, so one way to further improve query speed is SQL caching. Here is how it turns out in a use case with SQL caching enabled.\\n\\n- All queries are responded within 10ms;\\n- When computing 30 metrics simultaneously (over 120 SQL commands), results can be returned within 600ms;\\n- A TPS (Transactions Per Second) of 300 is reached, with CPU, memory, disk, and I/O usage under 80%;\\n- Under the recommended cluster size, over 10,000 metrics can be cached, which means you can save a lot of computation resources.\\n\\n![reduced-computation-resources](/images/Pingan_3.png)\\n\\n## Conclusion\\n\\nThe complexity of data analysis in the financial industry lies in the data itself other than the engineering side. Thus, the underlying data architecture should focus on facilitating the unified and efficient management of data. Apache Doris provides the flexibility of simple metric registration and the ability of fast and resource-efficient metric computation. In this case, the user is able to handle 10,000 active financial metrics in 10,000 dashboards with 30% less ETL efforts.\\n\\nFind Apache Doris developers on [Slack](https://join.slack.com/t/apachedoriscommunity/shared_invite/zt-2unfw3a3q-MtjGX4pAd8bCGC1UV0sKcw)."},{"id":"/For-Entry-Level-Data-Engineers-How-to-Build-a-Simple-but-Solid-Data-Architecture","metadata":{"permalink":"/blog/For-Entry-Level-Data-Engineers-How-to-Build-a-Simple-but-Solid-Data-Architecture","source":"@site/blog/For-Entry-Level-Data-Engineers-How-to-Build-a-Simple-but-Solid-Data-Architecture.md","title":"For entry-level data engineers: how to build a simple but solid data architecture","description":"This article aims to provide reference for non-tech companies who are seeking to empower your business with data analytics.","date":"2023-07-31T00:00:00.000Z","tags":[{"inline":true,"label":"Best Practice","permalink":"/blog/tags/best-practice"}],"hasTruncateMarker":false,"authors":[{"name":"Zhenwei Liu","key":null,"page":null}],"frontMatter":{"title":"For entry-level data engineers: how to build a simple but solid data architecture","description":"This article aims to provide reference for non-tech companies who are seeking to empower your business with data analytics.","date":"2023-07-31","author":"Zhenwei Liu","tags":["Best Practice"],"image":"/images/how-to-build-a-simple-but-solid-data-architecture.png"},"unlisted":false,"prevItem":{"title":"Database in fintech: how to support 10,000 dashboards without causing a mess","permalink":"/blog/Database-in-Fintech-How-to-Support-ten-thousand-Dashboards-Without-Causing-a-Mess"},"nextItem":{"title":"Is your latest data really the latest? check the data update mechanism of your database","permalink":"/blog/Is-Your-Latest-Data-Really-the-Latest-Check-the-Data-Update-Mechanism-of-Your-Database"}},"content":"\x3c!-- \\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n\\nThis article aims to provide reference for non-tech companies who are seeking to empower your business with data analytics. You will learn the basics about how to build an efficient and easy-to-use data system, and I will walk you through every aspect of it with a use case of Apache Doris, an MPP-based analytic data warehouse. \\n\\n## What You Need\\n\\nThis case is about a ticketing service provider who want a data platform that boasts quick processing, low maintenance costs, and ease of use, and I think they speak for the majority of entry-level database users.\\n\\nA prominent feature of ticketing services is the periodic spikes in ticket orders, you know, before the shows go on. So from time to time, the company has a huge amount of new data rushing in and requires real-time processing of it, so they can make timely adjustments during the short sales window. But in other time, they don\'t want to spend too much energy and funds on maintaining the data system. Furthermore, for a beginner of digital operation who only require basic analytic functions, it is better to have a data architecture that is easy-to-grasp and user-friendly. After research and comparison, they came to the Apache Doris community and we help them build a Doris-based data architecture.\\n\\n## Simple Architecture\\n\\nThe building blocks of this architecture are simple. You only need Apache Flink and Apache Kafka for data ingestion, and Apache Doris as an analytic data warehouse. \\n\\n![simple-data-architecture-with-Apache-Doris](/images/Poly_1.png)\\n\\nConnecting data sources to the data warehouse is simple, too. The key component, Apache Doris, supports various data loading methods to fit with different data sources. You can perform column mapping, transforming, and filtering during data loading to avoid duplicate collection of data. To ingest a table, users only need to add the table name to the configurations, instead of writing a script themselves. \\n\\n## Data Update\\n\\nFlink CDC was found to be the optimal choice if you are looking for higher stability in data ingestion. It also allows you to update the dynamically changing tables in real time. The process includes the following steps:\\n\\n- Configure Flink CDC for the source MySQL database, so that it allows dynamic updating of the table management configurations (which you can think of as the \\"metadata\\").\\n- Create two CDC jobs in Flink, one to capture the changed data (the Forward stream), the other to update the table management configurations (the Broadcast stream).\\n- Configure all tables of the source database at the Sink end (the output end of Flink CDC). When there is newly added table in the source database, the Broadcast stream will be triggered to update the table management configurations. (You just need to configure the tables, instead of \\"creating\\" the tables.)\\n\\n![configure-Flink-CDC](/images/Poly_2.png)\\n\\n## Layering of Data Warehouse\\n\\nData flows from various sources into the data warehouse, where it is cleaned and organized before it is ready for queries and analysis. The data processing here is divided into five typical layers. Such layering simplifies the data cleaning process because it provides a clear division of labor and makes things easier to locate and comprehend. \\n\\n- **ODS**: This is the prep zone of the data warehouse. The unprocessed original data is put in the [Unique Key Model](https://doris.apache.org/docs/dev/data-table/data-model/#unique-model) of Apache Doris, which can avoid duplication of data. \\n- **DWD**: This layer cleans, formats, and de-identifies data to produce fact tables. Every detailed data record is preserved. Data in this layer is also put into the Unique Key Model.\\n- **DWS**: This layer produces flat tables of a certain theme (order, user, etc.) based on data from the DWD layer. \\n- **ADS**: This layer auto-aggregates data, which is implemented by the [Aggregate Key Model](https://doris.apache.org/docs/dev/data-table/data-model/#aggregate-model) of Apache Doris.\\n- **DIM**: The DIM layer accommodates dimension data (in this case, data about the theaters, projects, and show sessions, etc.), which is used in combination with the order details.\\n\\nAfter the original data goes through these layers, it is available for queries via one data export interface.\\n\\n## Reporting\\n\\nLike many non-tech business, the ticketing service provider needs a data warehouse mainly for reporting. They derive trends and patterns from all kinds of data reports, and then figure out ways towards efficient management and sales increase. Specifically, this is the information they are observing in their reports:\\n\\n- **Statistical Reporting**: These are the most frequently used reports, including sales reports by theater, distribution channel, sales representative, and show.\\n- **Agile Reporting**: These are reports developed for specific purposes, such as daily and weekly project data reports, sales summary reports, GMV reports, and settlement reports.\\n- **Data Analysis**: This involves data such as membership orders, attendance rates, and user portraits.\\n- **Dashboarding**: This is to visually display sales data.\\n\\n![Real-Time-Data-Warehouse-and-Reporting](/images/Poly_3.png)\\n\\nThese are all entry-level tasks in data analytics. One of the biggest burdens for the data engineers was to quickly develop new reports as the internal analysts required. The [Aggregate Key Model](https://doris.apache.org/docs/dev/data-table/data-model#aggregate-model) of Apache Doris is designed for this. \\n\\n### Quick aggregation to produce reports on demand\\n\\nFor example, supposing that analysts want a sales report by sales representatives, data engineers can produce that by simple configuration:\\n\\n1. Put the original data in the Aggregate Key Model\\n2. Specify the sales representative ID column and the payment date column as the Key columns, and the order amount column as the Value column\\n\\nThen, order amounts of the same sale representative within the specified period of time will be auto-aggregated. Bam! That\'s the report you need! \\n\\nAccording to the user, this whole process only takes them 10~30 minutes, depending on the complexity of the report required. So the Aggregate Key Model largely releases data engineers from the pressure of report development.\\n\\n### Quick response to data queries\\n\\nMost data analysts would just want their target data to be returned the second they need it. In this case, the user often leverages two capabilities of Apache Doris to realize quick query response.\\n\\nFirstly, Apache Doris is famously fast in Join queries. So if you need to extract information across multiple tables, you are in good hands. Secondly, in data analysis, it often happens that analysts frequently input the same request. For example, they frequently want to check the sales data of different theaters. In this scenario, Apache Doris allows you to create a [Materialized View](https://doris.apache.org/docs/dev/query-acceleration/materialized-view/), which means you pre-aggregate the sales data of each theater, and store this table in isolation from the original tables. In this way, every time you need to check the sales data by theater, the system directly goes to the Materialized View and reads data from there, instead of scanning the original table all over again. This can increase query speed by orders of magnitudes.\\n\\n## Conclusion\\n\\nThis is the overview of a simple data architecture and how it can provide the data services you need. It ensures data ingestion stability and quality with Flink CDC, and quick data analysis with Apache Doris. The deployment of this architecture is simple, too. If you plan for a data analytic upgrade for your business, you might refer to this case. If you need advice and help, you may join our [community here](https://join.slack.com/t/apachedoriscommunity/shared_invite/zt-2unfw3a3q-MtjGX4pAd8bCGC1UV0sKcw)."},{"id":"/Is-Your-Latest-Data-Really-the-Latest-Check-the-Data-Update-Mechanism-of-Your-Database","metadata":{"permalink":"/blog/Is-Your-Latest-Data-Really-the-Latest-Check-the-Data-Update-Mechanism-of-Your-Database","source":"@site/blog/Is-Your-Latest-Data-Really-the-Latest-Check-the-Data-Update-Mechanism-of-Your-Database.md","title":"Is your latest data really the latest? check the data update mechanism of your database","description":"This is about how to support both row update and partial column update in a database in a way that is simple in execution and efficient in data quality guarantee.","date":"2023-07-24T00:00:00.000Z","tags":[{"inline":true,"label":"Tech Sharing","permalink":"/blog/tags/tech-sharing"}],"hasTruncateMarker":false,"authors":[{"name":"Apache Doris","key":null,"page":null}],"frontMatter":{"title":"Is your latest data really the latest? check the data update mechanism of your database","description":"This is about how to support both row update and partial column update in a database in a way that is simple in execution and efficient in data quality guarantee.","date":"2023-07-24","author":"Apache Doris","tags":["Tech Sharing"],"image":"/images/check-the-data-update-mechanism-of-your-database.jpg"},"unlisted":false,"prevItem":{"title":"For entry-level data engineers: how to build a simple but solid data architecture","permalink":"/blog/For-Entry-Level-Data-Engineers-How-to-Build-a-Simple-but-Solid-Data-Architecture"},"nextItem":{"title":"Apache Doris announced the official release of version 1.2.6","permalink":"/blog/release-note-1.2.6"}},"content":"\x3c!-- \\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\nIn databases, data update is to add, delete, or modify data. Timely data update is an important part of high quality data services.\\n\\nTechnically speaking, there are two types of data updates: you either update a whole row (**Row Update**) or just update part of the columns (**Partial Column Update**). Many databases supports both of them, but in different ways. This post is about one of them, which is simple in execution and efficient in data quality guarantee. \\n\\nAs an open source analytic database, Apache Doris supports both Row Update and Partial Column Update with one data model: the [**Unique Key Model**](https://doris.apache.org/docs/dev/data-table/data-model#unique-model). It is where you put data that doesn\'t need to be aggregated. In the Unique Key Model, you can specify one column or the combination of several columns as the Unique Key (a.k.a. Primary Key). For one Unique Key, there will always be one row of data: the newly ingested data record replaces the old. That\'s how data updates work.\\n\\nThe idea is straightforward, but in real-life implementation, it happens that the latest data does not arrive the last or doesn\'t even get written at all, so I\'m going to show you how Apache Doris implements data update and avoids messups with its Unique Key Model. \\n\\n![data-update](/images/Dataupdate_1.png)\\n\\n## Row Update\\n\\nFor data writing to the Unique Key Model, Apache Doris adopts the **Upsert** semantics, which means **Update or Insert**. If the new data record includes a Unique Key that already exists in the table, the new record will replace the old record; if it includes a brand new Unique Key, the new record will be inserted into the table as a whole. The Upsert operation can provide high throughput and guarantee data reliability.\\n\\n**Example**:\\n\\nIn the following table, the Unique Key is the combination of three columns: `user_id, date, group_id`.\\n\\n```SQL\\nmysql> desc test_table;\\n+-------------+--------------+------+-------+---------+-------+\\n| Field       | Type         | Null | Key   | Default | Extra |\\n+-------------+--------------+------+-------+---------+-------+\\n| user_id     | BIGINT       | Yes  | true  | NULL    |       |\\n| date        | DATE         | Yes  | true  | NULL    |       |\\n| group_id    | BIGINT       | Yes  | true  | NULL    |       |\\n| modify_date | DATE         | Yes  | false | NULL    | NONE  |\\n| keyword     | VARCHAR(128) | Yes  | false | NULL    | NONE  |\\n+-------------+--------------+------+-------+---------+-------+\\n```\\n\\nExecute `insert into` to write in a data record. Since the table was empty, by the Upsert semantics, it means to add a new row to the table.\\n\\n```SQL\\nmysql> insert into test_table values (1, \\"2023-04-28\\", 2, \\"2023-04-28\\", \\"foo\\");\\nQuery OK, 1 row affected (0.05 sec)\\n{\'label\':\'insert_2fb45d1833db4348_b612b8791c97b467\', \'status\':\'VISIBLE\', \'txnId\':\'343\'}\\n\\nmysql> select * from test_table;\\n+---------+------------+----------+-------------+---------+\\n| user_id | date       | group_id | modify_date | keyword |\\n+---------+------------+----------+-------------+---------+\\n|       1 | 2023-04-28 |        2 | 2023-04-28  | foo     |\\n+---------+------------+----------+-------------+---------+\\n```\\n\\nThen insert two more data records, one of which has the same Unique Key with the previously inserted row. Now, by the Upsert semantics, it means to replace the old row with the new one of the same Unique Key, and insert the record of the new Unique Key.\\n\\n```SQL\\nmysql> insert into test_table values (1, \\"2023-04-28\\", 2, \\"2023-04-29\\", \\"foo\\"), (2, \\"2023-04-29\\", 2, \\"2023-04-29\\", \\"bar\\");\\nQuery OK, 2 rows affected (0.04 sec)\\n{\'label\':\'insert_7dd3954468aa4ac1_a63a3852e3573b4c\', \'status\':\'VISIBLE\', \'txnId\':\'344\'}\\n\\nmysql> select * from test_table;\\n+---------+------------+----------+-------------+---------+\\n| user_id | date       | group_id | modify_date | keyword |\\n+---------+------------+----------+-------------+---------+\\n|       2 | 2023-04-29 |        2 | 2023-04-29  | bar     |\\n|       1 | 2023-04-28 |        2 | 2023-04-29  | foo     |\\n+---------+------------+----------+-------------+---------+\\n```\\n\\n## Partial Column Update\\n\\nBesides row update, under many circumstances, data analysts require the convenience of partial column update. For example, in user portraits, they would like to update certain dimensions of their users in real time. Or, if they need to maintain a flat table that is made of data from various source tables, they will prefer partial column update than complicated join operations as a way of data update. \\n\\nApache Doris supports partial column update with the UPDATE statement. It filters the rows that need to be modified, read them, changes a few values, and write the rows back to the table. \\n\\n**Example**:\\n\\nSuppose that there is an order table, in which the Order ID is the Unique Key.\\n\\n```SQL\\n+----------+--------------+-----------------+\\n| order_id | order_amount | order_status    |\\n+----------+--------------+-----------------+\\n| 1        |          100 | Payment Pending |\\n+----------+--------------+-----------------+\\n1 row in set (0.01 sec)\\n```\\n\\nWhen the buyer completes the payment, Apache Doris should change the order status of Order ID 1 from \\"Payment Pending\\" to \\"Delivery Pending\\". This is when the Update command comes into play.\\n\\n```SQL\\nmysql> UPDATE test_order SET order_status = \'Delivery Pending\' WHERE order_id = 1;\\nQuery OK, 1 row affected (0.11 sec)\\n{\'label\':\'update_20ae22daf0354fe0-b5aceeaaddc666c5\', \'status\':\'VISIBLE\', \'txnId\':\'33\', \'queryId\':\'20ae22daf0354fe0-b5aceeaaddc666c5\'}\\n```\\n\\nThis is the table after updating.\\n\\n```SQL\\n+----------+--------------+------------------+\\n| order_id | order_amount | order_status     |\\n+----------+--------------+------------------+\\n| 1        |          100 | Delivery Pending |\\n+----------+--------------+------------------+\\n1 row in set (0.01 sec)\\n```\\n\\nThe execution of the Update command consists of three steps in the system:\\n\\n- Step One: Read the row where Order ID = 1 (1, 100, \'Payment Pending\')\\n- Step Two: Modify the order status from \\"Payment Pending\\" to \\"Delivery Pending\\" (1, 100, \'Delivery Pending\')\\n- Step Three: Insert the new row into the table\\n\\n![partial-column-update-1](/images/Dataupdate_2.png)\\n\\nThe table is in the Unique Key Model, which means for rows of the same Unique Key, only the last inserted one will be reserved, so this is what the table will finally look like:\\n\\n![partial-column-update-2](/images/Dataupdate_3.png)\\n\\n## Order of Data Updates\\n\\nSo far this sounds simple, but in the actual world, data update might fail due to reasons such as data format errors, and thus mess up the data writing order. The order of data update matters more than you imagine. For example, in financial transactions, messed-up data writing order might lead to transaction data losses, errors, or duplication, which further leads to bigger problems.\\n\\nApache Doris provides two options for users to guarantee that their data is updated in the correct order:\\n\\n**1. Update by the order of transaction commit** \\n\\nIn Apache Doris, each data ingestion task is a transaction. Each successfully ingested task will be given a data version and the number of data versions is strictly increasing. If the ingestion fails, the transaction will be rolled back, and no new data version will be generated.\\n\\n By default, the Upsert semantics follows the order of the transaction commits. If there are two data ingestion tasks involving the same Unique Key, the first task generating data version 2 and the second, data version 3, then according to transaction commit order, data version 3 will replace data version 2.\\n\\n**2. Update by the user-defined order**\\n\\nIn real-time data analytics, data updates often happen in high concurrency. It is possible that there are multiple data ingestion tasks updating the same row, but these tasks are committed in unknown order, so the last saved update remains unknown, too.\\n\\nFor example, these are two data updates, with \\"2023-04-30\\" and \\"2023-05-01\\" as the `modify_data`, respectively. If they are written into the system concurrently, but the \\"2023-05-01\\" one is successfully committed first and the other later, then the \\"2023-04-30\\" record will be saved due to its higher data version number, but we know it is not the latest one.\\n\\n```Plain\\nmysql> insert into test_table values (2, \\"2023-04-29\\", 2, \\"2023-05-01\\", \\"bbb\\");\\nQuery OK, 1 row affected (0.04 sec)\\n{\'label\':\'insert_e2daf8cea5524ee1_94e5c87e7bb74d67\', \'status\':\'VISIBLE\', \'txnId\':\'345\'}\\n\\nmysql> insert into test_table values (2, \\"2023-04-29\\", 2, \\"2023-04-30\\", \\"aaa\\");\\nQuery OK, 1 row affected (0.03 sec)\\n{\'label\':\'insert_ef906f685a7049d0_b135b6cfee49fb98\', \'status\':\'VISIBLE\', \'txnId\':\'346\'}\\n\\nmysql> select * from test_table;\\n+---------+------------+----------+-------------+---------+\\n| user_id | date       | group_id | modify_date | keyword |\\n+---------+------------+----------+-------------+---------+\\n|       2 | 2023-04-29 |        2 | 2023-04-30 | aaa     |\\n|       1 | 2023-04-28 |        2 | 2023-04-29  | foo     |\\n+---------+------------+----------+-------------+---------+\\n```\\n\\nThat\'s why in high-concurrency scenarios, Apache Doris allows data update in user-defined order. Users can designate a column to the Sequence Column. In this way, the system will identity save the latest data version based on value in the Sequence Column.\\n\\n**Example:**\\n\\nYou can designate a Sequence Column by specifying the `function_column.sequence_col` property upon table creation.\\n\\n```SQL\\nCREATE TABLE test.test_table\\n(\\n    user_id bigint,\\n    date date,\\n    group_id bigint,\\n    modify_date date,\\n    keyword VARCHAR(128)\\n)\\nUNIQUE KEY(user_id, date, group_id)\\nDISTRIBUTED BY HASH (user_id) BUCKETS 32\\nPROPERTIES(\\n    \\"function_column.sequence_col\\" = \'modify_date\',\\n    \\"replication_num\\" = \\"1\\",\\n    \\"in_memory\\" = \\"false\\"\\n);\\n```\\n\\nThen check and see, the data record with the highest value in the Sequence Column will be saved:\\n\\n```SQL\\nmysql> insert into test_table values (2, \\"2023-04-29\\", 2, \\"2023-05-01\\", \\"bbb\\");\\nQuery OK, 1 row affected (0.03 sec)\\n{\'label\':\'insert_3aac37ae95bc4b5d_b3839b49a4d1ad6f\', \'status\':\'VISIBLE\', \'txnId\':\'349\'}\\n\\nmysql> insert into test_table values (2, \\"2023-04-29\\", 2, \\"2023-04-30\\", \\"aaa\\");\\nQuery OK, 1 row affected (0.03 sec)\\n{\'label\':\'insert_419d4008768d45f3_a6912e584cf1b500\', \'status\':\'VISIBLE\', \'txnId\':\'350\'}\\n\\nmysql> select * from test_table;\\n+---------+------------+----------+-------------+---------+\\n| user_id | date       | group_id | modify_date | keyword |\\n+---------+------------+----------+-------------+---------+\\n|       2 | 2023-04-29 |        2 | 2023-05-01  | bbb     |\\n|       1 | 2023-04-28 |        2 | 2023-04-29  | foo     |\\n+---------+------------+----------+-------------+---------+\\n```\\n\\n## Conclusion\\n\\nCongratulations. Now you\'ve gained an overview of how data updates are implemented in Apache Doris. With this knowledge, you can basically guarantee efficiency and accuracy of data updating. But wait, there is so much more about that. As Apache Doris 2.0 is going to provide more powerful Partial Column Update capabilities, with improved execution of the Update statement and the support for more complicated multi-table Join queries, I will show you how to take advantage of them in details in my follow-up writings. [We](https://join.slack.com/t/apachedoriscommunity/shared_invite/zt-2unfw3a3q-MtjGX4pAd8bCGC1UV0sKcw) are constantly updating our data updates!"},{"id":"/release-note-1.2.6","metadata":{"permalink":"/blog/release-note-1.2.6","source":"@site/blog/release-note-1.2.6.md","title":"Apache Doris announced the official release of version 1.2.6","description":"Dear community, Apache Doris 1.2.6 is now available, with several enhancements and bug fixes based on 1.2.0\uFF0Cenabling smoother user experience.","date":"2023-07-17T00:00:00.000Z","tags":[{"inline":true,"label":"Release Notes","permalink":"/blog/tags/release-notes"}],"hasTruncateMarker":false,"authors":[{"name":"Apache Doris","key":null,"page":null}],"frontMatter":{"title":"Apache Doris announced the official release of version 1.2.6","description":"Dear community, Apache Doris 1.2.6 is now available, with several enhancements and bug fixes based on 1.2.0\uFF0Cenabling smoother user experience.","date":"2023-07-17","author":"Apache Doris","tags":["Release Notes"],"image":"/images/1.2.6-release.png"},"unlisted":false,"prevItem":{"title":"Is your latest data really the latest? check the data update mechanism of your database","permalink":"/blog/Is-Your-Latest-Data-Really-the-Latest-Check-the-Data-Update-Mechanism-of-Your-Database"},"nextItem":{"title":"Database dissection: how fast data queries are implemented","permalink":"/blog/Database-Dissection-How-Fast-Data-Queries-Are-Implemented"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n## Behavior Change\\n\\n- Add a BE configuration item `allow_invalid_decimalv2_literal` to control whether can import data that exceeding the decimal\'s precision, for compatibility with previous logic.\\n\\n## Query\\n\\n- Fix several query planning issues.\\n- Support `sql_select_limit` session variable.\\n- Optimize query cold run performance.\\n- Fix expr context memory leak.\\n- Fix the issue that the `explode_split` function was executed incorrectly in some cases.\\n\\n### Multi Catalog\\n\\n- Fix the issue that synchronizing hive metadata caused FE replay edit log to fail.\\n- Fix `refresh catalog` operation causing FE OOM.\\n- Fix the issue that jdbc catalog cannot handle `0000-00-00` correctly.\\n- Fixed the issue that the kerberos ticket cannot be refreshed automatically.\\n- Optimize the partition pruning performance of hive.\\n- Fix the inconsistent behavior of trino and presto in jdbc catalog.\\n- Fix the issue that hdfs short-circuit read could not be used to improve query efficiency in some environments.\\n- Fix the issue that the iceberg table on CHDFS could not be read.\\n\\n## Storage\\n\\n- Fix the wrong calculation of delete bitmap in MOW table.\\n- Fix several BE memory issues.\\n- Fix snappy compression issue.\\n- Fix the issue that jemalloc may cause BE to crash in some cases.\\n\\n## Others\\n\\n- Fix several java udf related issues.\\n- Fix the issue that the `recover table` operation incorrectly triggered the creation of dynamic partitions.\\n- Fix timezone when importing orc files via broker load.\\n- Fix the issue that the newly added `PERCENT` keyword caused the replay metadata of the routine load job to fail.\\n- Fix the issue that the `truncate` operation failed to acts on a non-partitioned table.\\n- Fix the issue that the mysql connection was lost due to the `show snapshot` operation.\\n- Optimize the lock logic to reduce the probability of lock timeout errors when creating tables.\\n- Add session variable `have_query_cache` to be compatible with some old mysql clients.\\n- Optimize the error message when encountering an error of loading.\\n\\n## Big Thanks\\n\\nThanks all who contribute to this release:\\n\\n@amorynan\\n\\n@BiteTheDDDDt\\n\\n@caoliang-web\\n\\n@dataroaring\\n\\n@Doris-Extras\\n\\n@dutyu\\n\\n@Gabriel39\\n\\n@HHoflittlefish777\\n\\n@htyoung\\n\\n@jacktengg\\n\\n@jeffreys-cat\\n\\n@kaijchen\\n\\n@kaka11chen\\n\\n@Kikyou1997\\n\\n@KnightLiJunLong\\n\\n@liaoxin01\\n\\n@LiBinfeng-01\\n\\n@morningman\\n\\n@mrhhsg\\n\\n@sohardforaname\\n\\n@starocean999\\n\\n@vinlee19\\n\\n@wangbo\\n\\n@wsjz\\n\\n@xiaokang\\n\\n@xinyiZzz\\n\\n@yiguolei\\n\\n@yujun777\\n\\n@Yulei-Yang\\n\\n@zhangstar333\\n\\n@zy-kkk"},{"id":"/Database-Dissection-How-Fast-Data-Queries-Are-Implemented","metadata":{"permalink":"/blog/Database-Dissection-How-Fast-Data-Queries-Are-Implemented","source":"@site/blog/Database-Dissection-How-Fast-Data-Queries-Are-Implemented.md","title":"Database dissection: how fast data queries are implemented","description":"What\'s more important than quick performance itself is the architectural design and mechanism that enable it.","date":"2023-07-16T00:00:00.000Z","tags":[{"inline":true,"label":"Best Practice","permalink":"/blog/tags/best-practice"}],"hasTruncateMarker":false,"authors":[{"name":"Rong Hou","key":null,"page":null}],"frontMatter":{"title":"Database dissection: how fast data queries are implemented","description":"What\'s more important than quick performance itself is the architectural design and mechanism that enable it.","date":"2023-07-16","author":"Rong Hou","tags":["Best Practice"],"image":"/images/how-fast-data-queries-are-implemented.png"},"unlisted":false,"prevItem":{"title":"Apache Doris announced the official release of version 1.2.6","permalink":"/blog/release-note-1.2.6"},"nextItem":{"title":"Listen to that poor BI engineer: we need fast joins","permalink":"/blog/Listen-to-That-Poor-BI-Engineer-We-Need-Fast-Joins"}},"content":"\x3c!-- \\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n\\n\\nIn data analytics, fast query performance is more of a result than a guarantee. What\'s more important than the result itself is the architectural design and mechanism that enables quick performance. This is exactly what this post is about. I will put you into context with a typical use case of Apache Doris, an open-source MPP-based analytic database.\\n\\nThe user in this case is an all-category Q&A website. As a billion-dollar listed company, they have their own data management platform. What Doris does is to support the data filtering, packaging, analyzing, and monitoring workloads of that platform. Based on their huge data size, the user demands quick data loading and quick response to queries. \\n\\n## How to Enable Quick Queries on Huge Dataset\\n\\n- **Scenario**: user segmentation for the website\\n- **Data size**: 100 billion data objects, 2.4 million tags\\n- **Requirements**: query response time < 1 second; result packaging < 10 seconds\\n\\nFor these goals, the engineers have made three critical changes in their data processing pipeline.\\n\\n### 1.Distribute the data\\n\\nUser segmentation is when analysts pick out a group of website users that share certain characteristics (tags). In the database system, this process is implemented by a bunch of set operations (union, intersection, and difference). \\n\\n**Narration from the engineers:**\\n\\nWe realize that instead of executing set operations on one big dataset, we can divide our dataset into smaller ones, execute set operations on each of them, and then merge all the results. In this way, each small dataset is computed by one thread/queue. Then we have a queue to do the final merging. It\'s simple distributed computing thinking.\\n\\n![distributed-computing-in-database](/images/Zhihu_1.png)\\n\\nExample:\\n\\n1. Every 1 million users are put into one group with a `group_id`.\\n2. All user tags in that same group will relate to the corresponding `group_id`.\\n3. Calculate the union/intersection/difference within each group. (Enable multi-thread mode to increase computation efficiency.)\\n4. Merge the results from the groups.\\n\\nThe problem here is, since user tags are randomly distributed across various machines, the computation entails multi-time shuffling, which brings huge network overhead. That leads to the second change.\\n\\n### 2.Pre-bind a data group to a machine\\n\\nThis is enabled by the Colocate mechanism of Apache Doris. The idea of Colocate is to place data chunks that are often accessed together onto the same node, so as to reduce cross-node data transfer and thus, get lower latency.\\n\\n![colocate-mechanism](/images/Zhihu_2.png)\\n\\nThe implementation is simple: Bind one group key to one machine. Then naturally, data corresponding to that group key will be pre-bound to that machine. \\n\\nThe following is the query plan before we adopted Collocate: It is complicated, with a lot of data shuffling.\\n\\n![complicated-data-shuffling](/images/Zhihu_3.png)\\n\\nThis is the query plan after. It is much simpler, which is why queries are much faster and less costly.\\n\\n![simpler-query-plan-after-colocation-join](/images/Zhihu_4.png)\\n\\n### 3.Merge the operators\\n\\nIn data queries, the engineers realized that they often use a couple of functions in combination, so they decided to develop compound functions to further improve execution efficiency. They came to the Doris [community](https://t.co/XD4uUSROft) and talked about their thoughts. The Doris developers provided support for them and soon the compound functions are ready for use on Doris. These are a few examples:\\n\\n```\\nbitmap_and_count == bitmap_count(bitmap_and(bitmap1, bitmap2))\\nbitmap_and_not_count == bitmap_count(bitmap_not(bitmap1, bitmap_and(bitmap1, bitmap2))\\northogonal_bitmap_union_count==bitmap_and(bitmap1,bitmap_and(bitmap2,bitmap3)\\n```\\n\\nQuery execution with one compound function is much faster than that with a chain of simple functions, as you can tell from the lengths of the flow charts:\\n\\n![operator-merging](/images/Zhihu_5.png)\\n\\n- **Multiple Simple functions**: This involves three function executions and two intermediate storage. It\'s a long and slow process.\\n- **One compound function**: Simple in and out.\\n\\n## How to Quickly Ingest Large Amounts of Data\\n\\nThis is about putting the right workload on the right component. Apache Doris supports a variety of data loading methods. After trials and errors, the user settled on Spark Load and thus decreased their data loading time by 90%.  \\n\\n**Narration from the engineers:**\\n\\nIn offline data ingestion, we used to perform most computation in Apache Hive, write the data files to HDFS, and pull data regularly from HDFS to Apache Doris. However, after Doris obtains parquet files from HDFS, it performs a series of operations on them before it can turn them into segment files: decompressing, bucketing, sorting, aggregating, and compressing. These workloads will be borne by Doris backends, which have to undertake a few bitmap operations at the same time. So there is a huge pressure on the CPU. \\n\\n![Broker-Load](/images/Zhihu_6.png)\\n\\nSo we decided on the Spark Load method. It allows us to split the ingestion process into two parts: computation and storage, so we can move all the bucketing, sorting, aggregating, and compressing to Spark clusters. Then Spark writes the output to HDFS, from which Doris pulls data and flushes it to the local disks.\\n\\n![Spark-Load](/images/Zhihu_7.png)\\n\\nWhen ingesting 1.2 TB data (that\'s 110 billion rows), the Spark Load method only took 55 minutes. \\n\\n## A Vectorized Execution Engine\\n\\nIn addition to the above changes, a large part of the performance of a database relies on its execution engine. In the case of Apache Doris, it has fully vectorized its storage and computation layers since version 1.1. The longtime user also witnessed this revolution, so we invited them to test how the vectorized engine worked.\\n\\nThey compared query response time before and after the vectorization in seven of its frequent scenarios:\\n\\n- Scenario 1: Simple user segmentation (hundreds of filtering conditions), data packaging of a multi-million user group.\\n- Scenario 2: Complicated user segmentation (thousands of filtering conditions), data packaging of a tens-of-million user group.\\n- Scenario 3: Multi-dimensional filtering (6 dimensions), single-table query, **single-date flat table**, data aggregation, 180 million rows per day.\\n- Scenario 4: Multi-dimensional filtering (6 dimensions), single-table query, **multi-date flat table**, data aggregation, 180 million rows per day.\\n- Scenario 5: **Single-table query**, COUNT, 180 million rows per day.\\n- Scenario 6: **Multi-table query**, (Table A: 180 million rows, SUM, COUNT; Table B: 1.5 million rows, bitmap aggregation), aggregate Table A and Table B, join them with Table C, and then join the sub-tables, six joins in total.\\n- Scenario 7: Single-table query, 500 million rows of itemized data\\n\\nThe results are as below:\\n\\n![performance-after-vectorization](/images/Zhihu_8.png)\\n\\n## Conclusion\\n\\nIn short, what contributed to the fast data loading and data queries in this case?\\n\\n- The Colocate mechanism that\'s designed for distributed computing\\n- Collaboration between database users and [developers](https://join.slack.com/t/apachedoriscommunity/shared_invite/zt-2unfw3a3q-MtjGX4pAd8bCGC1UV0sKcw) that enables the operator merging\\n- Support for a wide range of data loading methods to choose from\\n- A vectorized engine that brings overall performance increase\\n\\nIt takes efforts from both the database developers and users to make fast performance possible. The user\'s experience and knowledge of their own status quo will allow them to figure out the quickest path, while a good database design will help pave the way and make users\' life easier."},{"id":"/Listen-to-That-Poor-BI-Engineer-We-Need-Fast-Joins","metadata":{"permalink":"/blog/Listen-to-That-Poor-BI-Engineer-We-Need-Fast-Joins","source":"@site/blog/Listen-to-That-Poor-BI-Engineer-We-Need-Fast-Joins.md","title":"Listen to that poor BI engineer: we need fast joins","description":"JOIN queries are always a hassle, but yes, you can expect fast joins from a relational database. Read this and learn how.","date":"2023-07-10T00:00:00.000Z","tags":[{"inline":true,"label":"Best Practice","permalink":"/blog/tags/best-practice"}],"hasTruncateMarker":false,"authors":[{"name":"Baoming Zhang","key":null,"page":null}],"frontMatter":{"title":"Listen to that poor BI engineer: we need fast joins","description":"JOIN queries are always a hassle, but yes, you can expect fast joins from a relational database. Read this and learn how.","date":"2023-07-10","author":"Baoming Zhang","tags":["Best Practice"],"image":"/images/listen-to-that-poor-bi-engineer.png"},"unlisted":false,"prevItem":{"title":"Database dissection: how fast data queries are implemented","permalink":"/blog/Database-Dissection-How-Fast-Data-Queries-Are-Implemented"},"nextItem":{"title":"Replacing Apache Hive, Elasticsearch and PostgreSQL with Apache Doris","permalink":"/blog/Replacing-Apache-Hive-Elasticsearch-and-PostgreSQL-with-Apache-Doris"}},"content":"\x3c!-- \\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\nBusiness intelligence (BI) tool is often the last stop of a data processing pipeline. It is where data is visualized for analysts who then extract insights from it. From the standpoint of a SaaS BI provider, what are we looking for in a database? In my job, we are in urgent need of support for fast join queries.\\n\\n## Why JOIN Query Matters\\n\\nI work as an engineer that supports a human resource management system. One prominent selling point of our services is **self-service** **BI**. That means we allow users to customize their own dashboards: they can choose the fields they need and relate them to form the dataset as they want. \\n\\n![self-service-BI](/images/Moka_1.png)\\n\\nJoin query is a more efficient way to realize self-service BI. It allows people to break down their data assets into many smaller tables instead of putting it all in a flat table. This would make data updates much faster and more cost-effective, because updating the whole flat table is not always the optimal choice when you have plenty of new data flowing in and old data being updated or deleted frequently, as is the case for most data input.\\n\\nIn order to maximize the time value of data, we need data updates to be executed really quickly. For this purpose, we looked into three OLAP databases on the market. They are all fast in some way but there are some differences.\\n\\n![Apache-Doris-VS-ClickHouse-VS-Greenplum](/images/Moka_2.png)\\n\\nGreenplum is really quick in data loading and batch DML processing, but it is not good at handling high concurrency. There is a steep decline in performance as query concurrency rises. This can be risky for a BI platform that tries to ensure stable user experience. ClickHouse is mind-blowing in single-table queries, but it only allows batch update and batch delete, so that\'s less timely.\\n\\n## Welcome to JOIN Hell\\n\\nJOIN, my old friend JOIN, is always a hassle. Join queries are demanding for both engineers and the database system. Firstly, engineers must have a thorough grasp of the schema of all tables. Secondly, these queries are resource-intensive, especially when they involve large tables. Some of the reports on our platform entail join queries across up to 20 tables. Just imagine the mess.\\n\\nWe tested our candidate OLAP engines with our common join queries and our most notorious slow queries. \\n\\n![Apache-Doris-VS-ClickHouse](/images/Moka_3.png)\\n\\nAs the number of tables joined grows, we witness a widening performance gap between Apache Doris and ClickHouse. In most join queries, Apache Doris was about 5 times faster than ClickHouse. In terms of slow queries, Apache Doris responded to most of them within less than 1 second, while the performance of ClickHouse fluctuated within a relatively large range. \\n\\nAnd just like that, we decided to upgrade our data architecture with Apache Doris. \\n\\n## Architecture that Supports Our BI Services\\n\\n**Data Input:** \\n\\nOur business data flows into DBLE, a distributed middleware based on MySQL. Then the DBLE binlogs are written into Flink, getting deduplicated, merged, and then put into Kafka. Finally, Apache Doris reads data from Kafka via its Routine Load approach. We apply the \\"delete\\" configuration in Routine Load to enable real-time deletion. The combination of Apache Flink and the idempotent write mechanism of Apache Doris is how we get exactly-once guarantee. We have a data size of billions of rows per table, and this architecture is able to finish data updates in one minute. \\n\\nIn addition, taking advantage of Apache Kafka and the Routine Load method, we are able to shave the traffic peaks and maintain cluster stability. Kafka also allows us to have multiple consumers of data and recompute intermediate data by resetting the offsets.\\n\\n**Data Output**: \\n\\nAs a self-service BI platform, we allow users to customize their own reports by configuring the rows, columns, and filters as they need. This is supported by Apache Doris with its capabilities in join queries. \\n\\nIn total, we have 400 data tables, of which 50 has over 100 million rows. That adds up to a data size measured in TB. We put all our data into two Doris clusters on 40 servers.\\n\\n## No Longer Stalled by Privileged Access Queries\\n\\nOn our BI platform, privileged queries are often much slower than non-privileged queries. Timeout is often the case and even more so for queries on large datasets.\\n\\nHuman resource data is subject to very strict and fine-grained access control policies. The role and position of users and the confidentiality level of data determine who has access to what (the data granularity here is up to fields in a table). Occasionally, we need to separately grant a certain privilege to a particular person. On top of that, we need to ensure data isolation between the multiple tenants on our platform.\\n\\nHow does all this add to complexity in engineering? Any user who inputs a query on our BI platform must go through multi-factor authentication, and the authenticated information will all be inserted into the SQL via `in` and then passed on to the OLAP engine. Therefore, the more fine-grained the privilege controls are, the longer the SQL will be, and the more time the OLAP system will spend on ID filtering. That\'s why our users are often tortured by high latency.\\n\\n![privileged-access-queries](/images/Moka_4.png)\\n\\nSo how did we fix that? We use the [Bloom Filter index](https://doris.apache.org/docs/dev/data-table/index/bloomfilter/) in Apache Doris. \\n\\n![BloomFilter-index](/images/Moka_5.png)\\n\\nBy adding Bloom Filter indexes to the relevant ID fields, we improve the speed of privileged queries by 30% and basically eliminate timeout errors.\\n\\n![faster-privileged-access-queries](/images/Moka_6.png)\\n\\nTips on when you should use the Bloom Filter index:\\n\\n- For non-prefix filtering\\n- For `in` and `=` filters on a particular column\\n- For filtering on high-cardinality columns, such as UserID. In essence, the Bloom Filter index is used to check if a certain value exists in a dataset. There is no point in using the Bloom Filter index for a low-cardinality column, like \\"gender\\", for example, because almost every data block contains all the gender values.\\n\\n## To All BI Engineers\\n\\nWe believe self-service BI is the future in the BI landscape, just like AGI is the future for artificial intelligence. Fast join queries is the way towards it, and the foregoing architectural upgrade is part of our ongoing effort to empower that. May there be less painful JOINs in the BI world. Cheers.\\n\\n\\n\\nFind the Apache Doris developers on [Slack](https://join.slack.com/t/apachedoriscommunity/shared_invite/zt-2unfw3a3q-MtjGX4pAd8bCGC1UV0sKcw)"},{"id":"/Replacing-Apache-Hive-Elasticsearch-and-PostgreSQL-with-Apache-Doris","metadata":{"permalink":"/blog/Replacing-Apache-Hive-Elasticsearch-and-PostgreSQL-with-Apache-Doris","source":"@site/blog/Replacing-Apache-Hive-Elasticsearch-and-PostgreSQL-with-Apache-Doris.md","title":"Replacing Apache Hive, Elasticsearch and PostgreSQL with Apache Doris","description":"How does a data service company build its data warehouse? Simplicity is the best policy. See how a due diligence platform increased data writing efficiency by 75%.","date":"2023-07-01T00:00:00.000Z","tags":[{"inline":true,"label":"Best Practice","permalink":"/blog/tags/best-practice"}],"hasTruncateMarker":false,"authors":[{"name":"velodb.io \xb7 Tao Wang","key":null,"page":null}],"frontMatter":{"title":"Replacing Apache Hive, Elasticsearch and PostgreSQL with Apache Doris","description":"How does a data service company build its data warehouse? Simplicity is the best policy. See how a due diligence platform increased data writing efficiency by 75%.","date":"2023-07-01","author":"velodb.io \xb7 Tao Wang","externalLink":"https://www.velodb.io/blog/1372","tags":["Best Practice"],"image":"/images/replacing-apache-hive-es-and-postgresql-with-apache-doris.png"},"unlisted":false,"prevItem":{"title":"Listen to that poor BI engineer: we need fast joins","permalink":"/blog/Listen-to-That-Poor-BI-Engineer-We-Need-Fast-Joins"},"nextItem":{"title":"Tiered storage for hot and cold data: what, why, and how?","permalink":"/blog/Tiered-Storage-for-Hot-and-Cold-Data-What-Why-and-How"}},"content":"\x3c!-- \\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\nHow does a data service company build its data warehouse? I worked as a real-time computing engineer for a due diligence platform, which is designed to allow users to search for a company\'s business data, financial, and legal details. It has collected information of over 300 million entities in more than 300 dimensions. The duty of my colleagues and I is to ensure real-time updates of such data so we can provide up-to-date information for our registered users. That\'s the customer-facing function of our data warehouse. Other than that, it needs to support our internal marketing and operation team in ad-hoc queries and user segmentation, which is a new demand that emerged with our growing business. \\n\\nOur old data warehouse consisted of the most popular components of the time, including **Apache** **Hive**, **MySQL**, **Elasticsearch**, and **PostgreSQL**. They support the data computing and data storage layers of our data warehouse: \\n\\n- **Data Computing**: Apache Hive serves as the computation engine.\\n- **Data Storage**: **MySQL** provides data for DataBank, Tableau, and our customer-facing applications. **Elasticsearch** and **PostgreSQL** serve for our DMP user segmentation system: the former stores user profiling data, and the latter stores user group data packets. \\n\\nAs you can imagine, a long and complicated data pipeline is high-maintenance and detrimental to development efficiency. Moreover, they are not capable of ad-hoc queries. So as an upgrade to our data warehouse, we replaced most of these components with [Apache Doris](https://github.com/apache/doris), a unified analytic database.\\n\\n![replace-MySQL-Elasticsearch-PostgreSQL-with-Apache-Doris-before](/images/Tianyancha_1.png)\\n\\n![replace-MySQL-Elasticsearch-PostgreSQL-with-Apache-Doris-after](/images/Tianyancha_2.png)\\n\\n## Data Flow\\n\\nThis is a lateral view of our data warehouse, from which you can see how the data flows.\\n\\n![data-flow](/images/Tianyancha_3.png)\\n\\nFor starters, binlogs from MySQL will be ingested into Kafka via Canal, while user activity logs will be transferred to Kafka via Apache Flume. In Kafka, data will be cleaned and organized into flat tables, which will be later turned into aggregated tables. Then, data will be passed from Kafka to Apache Doris, which serves as the storage and computing engine. \\n\\nWe adopt different data models in Apache Doris for different scenarios: data from MySQL will be arranged in the [Unique model](https://doris.apache.org/docs/dev/data-table/data-model/#unique-model), log data will be put in the [Duplicate model](https://doris.apache.org/docs/dev/data-table/data-model/#duplicate-model), while data in the DWS layer will be merged in the [Aggregate model](https://doris.apache.org/docs/dev/data-table/data-model/#aggregate-model).\\n\\nThis is how Apache Doris replaces the roles of Hive, Elasticsearch, and PostgreSQL in our datawarehouse. Such transformation has saved us lots of efforts in development and maintenance. It also made ad-hoc queries possible and our user segmentation more efficient. \\n\\n## Ad-Hoc Queries\\n\\n**Before**: Every time a new request was raised, we developed and tested the data model in Hive, and wrote the scheduling task in MySQL so that our customer-facing application platforms could read results from MySQL. It was a complicated process that took a lot of time and development work. \\n\\n**After**: Since Apache Doris has all the itemized data, whenever it is faced with a new request, it can simply pull the metadata and configure the query conditions. Then it is ready for ad-hoc queries. In short, it only requires low-code configuration to respond to new requests. \\n\\n![ad-hoc-queries](/images/Tianyancha_4.png)\\n\\n## User Segmentation\\n\\n**Before**: After a user segmentation task was created based on metadata, the relevant user IDs would be written into the PostgreSQL profile list and the MySQL task list. Meanwhile, Elasticsearch would execute the query according to the task conditions; after the results are produced, it would update status in the task list and write the user group bitmap package into PostgreSQL. (The PostgreSQL plug-in is capable of computing the intersection, union, and difference set of bitmap.) Then PostgreSQL would provide user group packets for downstream operation platforms.\\n\\nTables in Elasticsearch and PostgreSQL were unreusable, making this architecture cost-ineffective. Plus, we had to pre-define the user tags before we could execute a new type of query. That slowed things down.  \\n\\n**After**: The user IDs will only be written into the MySQL task list. For first-time segmentation, Apache Doris will execute the **ad-hoc query** based on the task conditions. In subsequent segmentation tasks, Apache Doris will perform **micro-batch rolling** and compute the difference set compared with the previously produced user group packet, and notify downstream platforms of any updates. (This is realized by the [bitmap functions](https://doris.apache.org/docs/dev/sql-manual/sql-functions/bitmap-functions/bitmap_union) in Apache Doris.) \\n\\nIn this Doris-centered user segmentation process, we don\'t have to pre-define new tags. Instead, tags can be auto-generated based on the task conditions. The processing pipeline has the flexibility that can make our user-group-based A/B testing easier. Also, as both the itemized data and user group packets are in Apache Doris, we don\'t have to attend to the read and write complexity between multiple components.\\n\\n![user-segmentation-pipeline](/images/Tianyancha_5.png)\\n\\n## Trick to Speed up User Segmentation by 70%\\n\\nDue to risk aversion reasons, random generation of `user_id` is the choice for many companies, but that produces sparse and non-consecutive user IDs in user group packets. Using these IDs in user segmentation, we had to endure a long waiting time for bitmaps to be generated. \\n\\nTo solve that, we created consecutive and dense mappings for these user IDs. **In this way, we decreased our user segmentation latency by 70%.**\\n\\n![user-segmentation-latency-1](/images/Tianyancha_6.png)\\n\\n![user-segmentation-latency-2](/images/Tianyancha_7.png)\\n\\n### Example\\n\\n**Step 1: Create a user ID mapping table:**\\n\\nWe adopt the Unique model for user ID mapping tables, where the user ID is the unique key. The mapped consecutive IDs usually start from 1 and are strictly increasing. \\n\\n![create-user-ID-mapping-table](/images/Tianyancha_8.png)\\n\\n**Step 2: Create a user group table:**\\n\\nWe adopt the Aggregate model for user group tables, where user tags serve as the aggregation keys. \\n\\n![create-user-group-table](/images/Tianyancha_9.png)\\n\\nSupposing that we need to pick out the users whose IDs are between 0 and 2000000. \\n\\nThe following snippets use non-consecutive (`tyc_user_id`) and consecutive (`tyc_user_id_continuous`) user IDs for user segmentation, respectively. There is a big gap between their **response time:**\\n\\n- Non-Consecutive User IDs: **1843ms**\\n- Consecutive User IDs: **543ms** \\n\\n![response-time-of-consecutive-and-non-consecutive-user-IDs](/images/Tianyancha_10.png)\\n\\n## Conclusion\\n\\nWe have 2 clusters in Apache Doris accommodating tens of TBs of data, with almost a billion new rows flowing in every day. We used to witness a steep decline in data ingestion speed as data volume expanded. But after upgrading our data warehouse with Apache Doris, we increased our data writing efficiency by 75%. Also, in user segmentation with a result set of less than 5 million, it is able to respond within milliseconds. Most importantly, our data warehouse has been simpler and friendlier to developers and maintainers. \\n\\n![user-segmentation-latency-3](/images/Tianyancha_11.png)\\n\\nLastly, I would like to share with you something that interested us most when we first talked to the [Apache Doris community](https://join.slack.com/t/apachedoriscommunity/shared_invite/zt-2unfw3a3q-MtjGX4pAd8bCGC1UV0sKcw):\\n\\n- Apache Doris supports data ingestion transactions so it can ensure data is written **exactly once**.\\n- It is well-integrated with the data ecosystem and can smoothly interface with most data sources and data formats.\\n- It allows us to implement elastic scaling of clusters using the command line interface.\\n- It outperforms ClickHouse in **join queries**.\\n\\nFind Apache Doris developers on [Slack](https://join.slack.com/t/apachedoriscommunity/shared_invite/zt-1t3wfymur-0soNPATWQ~gbU8xutFOLog)"},{"id":"/Tiered-Storage-for-Hot-and-Cold-Data-What-Why-and-How","metadata":{"permalink":"/blog/Tiered-Storage-for-Hot-and-Cold-Data-What-Why-and-How","source":"@site/blog/Tiered-Storage-for-Hot-and-Cold-Data-What-Why-and-How.md","title":"Tiered storage for hot and cold data: what, why, and how?","description":"Hot data is the frequently accessed data, while cold data is the one you seldom visit but still need. Separating them is for higher efficiency in computation and storage.","date":"2023-06-23T00:00:00.000Z","tags":[{"inline":true,"label":"Tech Sharing","permalink":"/blog/tags/tech-sharing"}],"hasTruncateMarker":false,"authors":[{"name":"Apache Doris","key":null,"page":null}],"frontMatter":{"title":"Tiered storage for hot and cold data: what, why, and how?","description":"Hot data is the frequently accessed data, while cold data is the one you seldom visit but still need. Separating them is for higher efficiency in computation and storage.","date":"2023-06-23","author":"Apache Doris","tags":["Tech Sharing"],"image":"/images/tiered-storage-for-hot-and-cold-cata.jpg"},"unlisted":false,"prevItem":{"title":"Replacing Apache Hive, Elasticsearch and PostgreSQL with Apache Doris","permalink":"/blog/Replacing-Apache-Hive-Elasticsearch-and-PostgreSQL-with-Apache-Doris"},"nextItem":{"title":"Apache Doris announced the official release of version 1.2.5","permalink":"/blog/release-note-1.2.5"}},"content":"\x3c!-- \\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\nApparently tiered storage is hot now. But first of all:\\n\\n## What is Hot/Cold Data?\\n\\nIn simple terms, hot data is the frequently accessed data, while cold data is the one you seldom visit but still need. Normally in data analytics, data is \\"hot\\" when it is new, and gets \\"colder\\" and \\"colder\\" as time goes by. \\n\\nFor example, orders of the past six months are \\"hot\\" and logs from years ago are \\"cold\\". But no matter how cold the logs are, you still need them to be somewhere you can find.  \\n\\n## Why Separate Hot and Cold Data? \\n\\nTiered storage is an idea often seen in real life: You put your favorite book on your bedside table, your Christmas ornament in the attic, and your childhood art project in the garage or a cheap self-storage space on the other side of town. The purpose is a tidy and efficient life.\\n\\nSimilarly, companies separate hot and cold data for more efficient computation and more cost-effective storage, because storage that allows quick read/write is always expensive, like SSD. On the other hand, HDD is cheaper but slower. So it is more sensible to put hot data on SSD and cold data on HDD. If you are looking for an even lower-cost option, you can go for object storage.\\n\\nIn data analytics, tiered storage is implemented by a tiered storage mechanism in the database. For example, Apache Doris supports three-tiered storage: SSD, HDD, and object storage. For newly ingested data, after a specified cooldown period, it will turn from hot data into cold data and be moved to object storage. In addition, object storage only preserves one copy of data, which further cuts down storage costs and the relevant computation/network overheads.\\n\\n![tiered-storage](/images/HCDS_1.png)\\n\\nHow much can you save by tiered storage? Here is some math.\\n\\nIn public cloud services, cloud disks generally cost 5~10 times as much as object storage. If 80% of your data asset is cold data and you put it in object storage instead of cloud disks, you can expect a cost reduction of around 70%.\\n\\nLet the percentage of cold data be \\"rate\\", the price of object storage be \\"OS\\", and the price of cloud disk be \\"CloudDisk\\", this is how much you can save by tiered storage instead of putting all your data on cloud disks: \\n\\n![cost-calculation-of-tiered-storage](/images/HCDS_2.png)\\n\\nNow let\'s put real-world numbers in this formula: \\n\\nAWS pricing, US East (Ohio):\\n\\n- **S3 Standard Storage**: 23 USD per TB per month\\n- **Throughput Optimized HDD (st 1)**: 102 USD per TB per month\\n- **General Purpose SSD (gp2)**: 158 USD per TB per month\\n\\n![cost-reduction-by-tiered-storage](/images/HCDS_3.png)\\n\\n## How Is Tiered Storage Implemented?\\n\\nTill now, hot-cold separation sounds nice, but the biggest concern is: how can we implement it without compromising query performance? This can be broken down to three questions:\\n\\n- How to enable quick reading of cold data?\\n- How to ensure high availability of data?\\n- How to reduce I/O and CPU overheads?\\n\\nIn what follows, I will show you how Apache Doris addresses them one by one.\\n\\n### Quick Reading of Cold Data\\n\\nAccessing cold data from object storage will indeed be slow. One solution is to cache cold data in local disks for use in queries. In Apache Doris 2.0, when a query requests cold data, only the first-time access will entail a full network I/O operation from object storage. Subsequent queries will be able to read data directly from local cache.\\n\\nThe granularity of caching matters, too. A coarse granularity might lead to a waste of cache space, but a fine granularity could be the reason for low I/O efficiency. Apache Doris bases its caching on data blocks. It downloads cold data blocks from object storage onto local Block Cache. This is the \\"pre-heating\\" process. With cold data fully pre-heated, queries on tables with tiered storage will be basically as fast as those on tablets without. We drew this conclusion from test results on Apache Doris:\\n\\n![query-performance-with-tiered-storage](/images/HCDS_4.png)\\n\\n- ***Test Data****: SSB SF100 dataset*\\n- ***Configuration****: 3 \xd7 16C 64G, a cluster of 1 frontend and 3 backends* \\n\\nP.S. Block Cache adopts the LRU algorithm, so the more frequently accessed data will stay in Block Cache for longer.\\n\\n### High Availability of Data\\n\\nIn object storage, only one copy of cold data is preserved. Within Apache Doris, hot data and metadata are put in the backend nodes, and there are multiple replicas of them across different backend nodes in order to ensure high data availability. These replicas are called \\"local replicas\\". The metadata of cold data is synchronized to all local replicas, so that Doris can ensure high availability of cold data without using too much storage space.\\n\\nImplementation-wise, the Doris frontend picks a local replica as the Leader. Updates to the Leader will be synchronized to all other local replicas via a regular report mechanism. Also, as the Leader uploads data to object storage, the relevant metadata will be updated to other local replicas, too.\\n\\n![data-availability-with-tiered-storage](/images/HCDS_5.png)\\n\\n### Reduced I/O and CPU Overhead\\n\\nThis is realized by cold data [compaction](https://medium.com/gitconnected/understanding-data-compaction-in-3-minutes-d2b5a1f7446f). Some scenarios require large-scale update of historical data. In this case, part of the cold data in object storage should be deleted. Apache Doris 2.0 supports cold data compaction, which ensures that the updated cold data will be reorganized and compacted, so that it will take up storage space.\\n\\nA thread in Doris backend will regularly pick N tablets from the cold data and start compaction. Every tablet has a CooldownReplica and only the CooldownReplica will execute cold data compaction for the tablet. Every time 5MB of data is compacted, it will be uploaded to object storage to clear up space locally. Once the compaction is done, the CooldownReplica will update the new metadata to object storage. Other replicas only need to synchronize the metadata from object storage. This is how I/O and CPU overheads are reduced.\\n\\n## Tutorial\\n\\nSeparating tiered storage in storage is a huge cost saver and there have been ways to ensure the same fast query performance. Executing hot-cold data separation is a simple 6-step process, so you can find out how it works yourself:\\n\\n\\n\\nTo begin with, you need **an object storage bucket** and the relevant **Access Key (AK)** and **Secret Access Key (SK)**.\\n\\nThen you can start cold/hot data separation by following these six steps.\\n\\n### 1. Create Resource\\n\\nYou can create a resource using the object storage bucket with the AK and  SK. Apache Doris supports object storage on various cloud service  providers including AWS, Azure, and Alibaba Cloud.\\n\\n```\\nCREATE RESOURCE IF NOT EXISTS \\"${resource_name}\\"\\n        PROPERTIES(\\n            \\"type\\"=\\"s3\\",\\n            \\"s3.endpoint\\" = \\"${S3Endpoint}\\",\\n            \\"s3.region\\" = \\"${S3Region}\\",\\n            \\"s3.root.path\\" = \\"path/to/root\\",\\n            \\"s3.access_key\\" = \\"${S3AK}\\",\\n            \\"s3.secret_key\\" = \\"${S3SK}\\",\\n            \\"s3.connection.maximum\\" = \\"50\\",\\n            \\"s3.connection.request.timeout\\" = \\"3000\\",\\n            \\"s3.connection.timeout\\" = \\"1000\\",\\n            \\"s3.bucket\\" = \\"${S3BucketName}\\"\\n        );\\n```\\n\\n###  2. Create Storage Policy\\n\\nWith the Storage Policy, you can specify the cooling-down period of data  (including absolute cooling-down period and relative cooling-down  period).\\n\\n```\\nCREATE STORAGE POLICY testPolicy\\nPROPERTIES(\\n  \\"storage_resource\\" = \\"remote_s3\\",\\n  \\"cooldown_ttl\\" = \\"1d\\"\\n);\\n```\\n\\nIn the above snippet, the Storage Policy is named `testPolicy`, and data will start to cool down one day after it is ingested. The cold data will be moved under the `root path` of the object storage `remote_s3`. Apart from setting the TTL, you can also specify the timepoint when the cooling down starts.\\n\\n```\\nCREATE STORAGE POLICY testPolicyForTTlDatatime\\nPROPERTIES(\\n  \\"storage_resource\\" = \\"remote_s3\\",\\n  \\"cooldown_datetime\\" = \\"2023-06-07 21:00:00\\"\\n);\\n```\\n\\n### 3. Specify Storage Policy for a Table/Partition\\n\\nWith an established Resource and a Storage Policy, you can set a Storage Policy for a data table or a specific data partition.\\n\\nThe following snippet uses the lineitem table in the TPC-H dataset as an  example. To set a Storage Policy for the whole table, specify the  PROPERTIES as follows:\\n\\n```\\nCREATE TABLE IF NOT EXISTS lineitem1 (\\n            L_ORDERKEY    INTEGER NOT NULL,\\n            L_PARTKEY     INTEGER NOT NULL,\\n            L_SUPPKEY     INTEGER NOT NULL,\\n            L_LINENUMBER  INTEGER NOT NULL,\\n            L_QUANTITY    DECIMAL(15,2) NOT NULL,\\n            L_EXTENDEDPRICE  DECIMAL(15,2) NOT NULL,\\n            L_DISCOUNT    DECIMAL(15,2) NOT NULL,\\n            L_TAX         DECIMAL(15,2) NOT NULL,\\n            L_RETURNFLAG  CHAR(1) NOT NULL,\\n            L_LINESTATUS  CHAR(1) NOT NULL,\\n            L_SHIPDATE    DATEV2 NOT NULL,\\n            L_COMMITDATE  DATEV2 NOT NULL,\\n            L_RECEIPTDATE DATEV2 NOT NULL,\\n            L_SHIPINSTRUCT CHAR(25) NOT NULL,\\n            L_SHIPMODE     CHAR(10) NOT NULL,\\n            L_COMMENT      VARCHAR(44) NOT NULL\\n            )\\n            DUPLICATE KEY(L_ORDERKEY, L_PARTKEY, L_SUPPKEY, L_LINENUMBER)\\n            PARTITION BY RANGE(`L_SHIPDATE`)\\n            (\\n                PARTITION `p202301` VALUES LESS THAN (\\"2017-02-01\\"),\\n                PARTITION `p202302` VALUES LESS THAN (\\"2017-03-01\\")\\n            )\\n            DISTRIBUTED BY HASH(L_ORDERKEY) BUCKETS 3\\n            PROPERTIES (\\n            \\"replication_num\\" = \\"3\\",\\n            \\"storage_policy\\" = \\"${policy_name}\\"\\n            )\\n```\\n\\nYou can check the Storage Policy of a tablet via the `show tablets` command. If the `CooldownReplicaId` is anything rather than `-1` and the `CooldownMetaId` is not null, that means the current tablet has been specified with a Storage Policy.\\n\\n```\\n               TabletId: 3674797\\n              ReplicaId: 3674799\\n              BackendId: 10162\\n             SchemaHash: 513232100\\n                Version: 1\\n      LstSuccessVersion: 1\\n       LstFailedVersion: -1\\n          LstFailedTime: NULL\\n          LocalDataSize: 0\\n         RemoteDataSize: 0\\n               RowCount: 0\\n                  State: NORMAL\\nLstConsistencyCheckTime: NULL\\n           CheckVersion: -1\\n           VersionCount: 1\\n              QueryHits: 0\\n               PathHash: 8030511811695924097\\n                MetaUrl: http://172.16.0.16:6781/api/meta/header/3674797\\n       CompactionStatus: http://172.16.0.16:6781/api/compaction/show?tablet_id=3674797\\n      CooldownReplicaId: 3674799\\n         CooldownMetaId: TUniqueId(hi:-8987737979209762207, lo:-2847426088899160152)\\n```\\n\\nTo set a Storage Policy for a specific partition, add the policy name to the partition PROPERTIES as follows:\\n\\n```\\nCREATE TABLE IF NOT EXISTS lineitem1 (\\n            L_ORDERKEY    INTEGER NOT NULL,\\n            L_PARTKEY     INTEGER NOT NULL,\\n            L_SUPPKEY     INTEGER NOT NULL,\\n            L_LINENUMBER  INTEGER NOT NULL,\\n            L_QUANTITY    DECIMAL(15,2) NOT NULL,\\n            L_EXTENDEDPRICE  DECIMAL(15,2) NOT NULL,\\n            L_DISCOUNT    DECIMAL(15,2) NOT NULL,\\n            L_TAX         DECIMAL(15,2) NOT NULL,\\n            L_RETURNFLAG  CHAR(1) NOT NULL,\\n            L_LINESTATUS  CHAR(1) NOT NULL,\\n            L_SHIPDATE    DATEV2 NOT NULL,\\n            L_COMMITDATE  DATEV2 NOT NULL,\\n            L_RECEIPTDATE DATEV2 NOT NULL,\\n            L_SHIPINSTRUCT CHAR(25) NOT NULL,\\n            L_SHIPMODE     CHAR(10) NOT NULL,\\n            L_COMMENT      VARCHAR(44) NOT NULL\\n            )\\n            DUPLICATE KEY(L_ORDERKEY, L_PARTKEY, L_SUPPKEY, L_LINENUMBER)\\n            PARTITION BY RANGE(`L_SHIPDATE`)\\n            (\\n                PARTITION `p202301` VALUES LESS THAN (\\"2017-02-01\\") (\\"storage_policy\\" = \\"${policy_name}\\"),\\n                PARTITION `p202302` VALUES LESS THAN (\\"2017-03-01\\")\\n            )\\n            DISTRIBUTED BY HASH(L_ORDERKEY) BUCKETS 3\\n            PROPERTIES (\\n            \\"replication_num\\" = \\"3\\"\\n            )\\n```\\n\\n**This is how you can confirm that only the target partition is set with a Storage Policy:**\\n\\nIn the above example, Table Lineitem1 has 2 partitions, each partition has 3 buckets, and `replication_num` is set to \\"3\\". That means there are 2*3 = 6 tablets and 6*3 = 18 replicas in total.\\n\\nNow, if you check the replica information of all tablets via the `show tablets` command, you will see that only the replicas of tablets of the target  partion have a CooldownReplicaId and a CooldownMetaId. (For a clear  comparison, you can check replica information of a specific partition  via the `ADMIN SHOW REPLICA STATUS FROM TABLE PARTITION(PARTITION)` command.)\\n\\nFor instance, Tablet 3691990 belongs to Partition p202301, which is the  target partition, so the 3 replicas of this tablet have a  CooldownReplicaId and a CooldownMetaId:\\n\\n```\\n*****************************************************************\\n               TabletId: 3691990\\n              ReplicaId: 3691991\\n      CooldownReplicaId: 3691993\\n         CooldownMetaId: TUniqueId(hi:-7401335798601697108, lo:3253711199097733258)\\n*****************************************************************\\n               TabletId: 3691990\\n              ReplicaId: 3691992\\n      CooldownReplicaId: 3691993\\n         CooldownMetaId: TUniqueId(hi:-7401335798601697108, lo:3253711199097733258)\\n*****************************************************************\\n               TabletId: 3691990\\n              ReplicaId: 3691993\\n      CooldownReplicaId: 3691993\\n         CooldownMetaId: TUniqueId(hi:-7401335798601697108, lo:3253711199097733258)\\n```\\n\\nAlso, the above snippet means that all these 3 replicas have been specified  with the same CooldownReplica: 3691993, so only the data in Replica  3691993 will be stored in the Resource.\\n\\n### 4. View Tablet Details\\n\\nYou can view the detailed information of Table Lineitem1 via a `show tablets from lineitem1` command. Among all the properties, `LocalDataSize` represents the size of locally stored data and `RemoteDataSize` represents the size of cold data in object storage.\\n\\nFor example, when the data is newly ingested into the Doris backends, you can see that all data is stored locally.\\n\\n```\\n*************************** 1. row ***************************\\n               TabletId: 2749703\\n              ReplicaId: 2749704\\n              BackendId: 10090\\n             SchemaHash: 1159194262\\n                Version: 3\\n      LstSuccessVersion: 3\\n       LstFailedVersion: -1\\n          LstFailedTime: NULL\\n          LocalDataSize: 73001235\\n         RemoteDataSize: 0\\n               RowCount: 1996567\\n                  State: NORMAL\\nLstConsistencyCheckTime: NULL\\n           CheckVersion: -1\\n           VersionCount: 3\\n              QueryHits: 0\\n               PathHash: -8567514893400420464\\n                MetaUrl: http://172.16.0.8:6781/api/meta/header/2749703\\n       CompactionStatus: http://172.16.0.8:6781/api/compaction/show?tablet_id=2749703\\n      CooldownReplicaId: 2749704\\n         CooldownMetaId:\\n```\\n\\nWhen the data has cooled down, you will see that the data has been moved to remote object storage.\\n\\n```\\n*************************** 1. row ***************************\\n               TabletId: 2749703\\n              ReplicaId: 2749704\\n              BackendId: 10090\\n             SchemaHash: 1159194262\\n                Version: 3\\n      LstSuccessVersion: 3\\n       LstFailedVersion: -1\\n          LstFailedTime: NULL\\n          LocalDataSize: 0\\n         RemoteDataSize: 73001235\\n               RowCount: 1996567\\n                  State: NORMAL\\nLstConsistencyCheckTime: NULL\\n           CheckVersion: -1\\n           VersionCount: 3\\n              QueryHits: 0\\n               PathHash: -8567514893400420464\\n                MetaUrl: http://172.16.0.8:6781/api/meta/header/2749703\\n       CompactionStatus: http://172.16.0.8:6781/api/compaction/show?tablet_id=2749703\\n      CooldownReplicaId: 2749704\\n         CooldownMetaId: TUniqueId(hi:-8697097432131255833, lo:9213158865768502666)\\n```\\n\\nYou can also check your cold data from the object storage side by finding  the data files under the path specified in the Storage Policy.\\n\\nData in object storage only has a single copy.\\n\\n![img](https://miro.medium.com/v2/resize:fit:1400/1*jao2TrbhDI2h6S04W0x95Q.png)\\n\\n### 5. Execute Queries\\n\\nWhen all data in Table Lineitem1 has been moved to object storage and a  query requests data from Table Lineitem1, Apache Doris will follow the  root path specified in the Storage Policy of the relevant data  partition, and download the requested data for local computation.\\n\\nApache Doris 2.0 has been optimized for cold data queries. Only the first-time access to the cold data will entail a full network I/O operation from  object storage. After that, the downloaded data will be put in cache to  be available for subsequent queries, so as to improve query speed.\\n\\n### 6. Update Cold Data\\n\\nIn Apache Doris, each data ingestion leads to the generation of a new  Rowset, so the update of historical data will be put in a Rowset that is separated from those of newly loaded data. That\u2019s how it makes sure the updating of cold data does not interfere with the ingestion of hot  data. Once the rowsets cool down, they will be moved to S3 and deleted  locally, and the updated historical data will go to the partition where  it belongs.\\n\\nIf you any questions, come find Apache Doris developers on [Slack](https://join.slack.com/t/apachedoriscommunity/shared_invite/zt-2unfw3a3q-MtjGX4pAd8bCGC1UV0sKcw). We will be happy to provide targeted support."},{"id":"/release-note-1.2.5","metadata":{"permalink":"/blog/release-note-1.2.5","source":"@site/blog/release-note-1.2.5.md","title":"Apache Doris announced the official release of version 1.2.5","description":"Dear community, Apache Doris team has fixed nearly 210 issues or performance improvements in version 1.2.5 compared to the previous verison","date":"2023-06-18T00:00:00.000Z","tags":[{"inline":true,"label":"Release Notes","permalink":"/blog/tags/release-notes"}],"hasTruncateMarker":false,"authors":[{"name":"Apache Doris","key":null,"page":null}],"frontMatter":{"title":"Apache Doris announced the official release of version 1.2.5","description":"Dear community, Apache Doris team has fixed nearly 210 issues or performance improvements in version 1.2.5 compared to the previous verison","date":"2023-06-18","author":"Apache Doris","tags":["Release Notes"],"image":"/images/1.2.5-release.png"},"unlisted":false,"prevItem":{"title":"Tiered storage for hot and cold data: what, why, and how?","permalink":"/blog/Tiered-Storage-for-Hot-and-Cold-Data-What-Why-and-How"},"nextItem":{"title":"Say goodbye to OOM crashes","permalink":"/blog/Say-Goodbye-to-OOM-Crashes"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n\\nIn version 1.2.5, the Doris team has fixed nearly 210 issues or performance improvements since the release of version 1.2.4. At the same time, version 1.2.5 is also an iterative version of version 1.2.4, which has higher stability. It is recommended that all users upgrade to this version.\\n\\n## Behavior Changed\\n\\n- The `start_be.sh` script will check that the maximum number of file handles in the system must be greater than or equal to 65536, otherwise the startup will fail.\\n\\n- The BE configuration item `enable_quick_compaction` is set to true by default. The Quick Compaction is enabled by default. This feature is used to optimize the problem of small files in the case of large batch import.\\n\\n- After modifying the dynamic partition attribute of the table, it will no longer take effect immediately, but wait for the next task scheduling of the dynamic partition table to avoid some deadlock problems.\\n\\n## Improvements\\n\\n- Optimize the use of bthread and pthread to reduce the RPC blocking problem during the query process.\\n\\n- A button to download Profile is added to the Profile page of the FE web UI.\\n\\n- Added FE configuration `recover_with_skip_missing_version`, which is used to query to skip the problematic replica under certain failure conditions.\\n\\n- The row-level permission function supports external Catalog.\\n\\n- Hive Catalog supports automatic refreshing of kerberos tickets on the BE side without manual refreshing.\\n\\n- JDBC Catalog supports tables under the MySQL/ClickHouse system database (`information_schema`).\\n\\n## Bug Fixes\\n\\n- Fixed the problem of incorrect query results caused by low-cardinality column optimization\\n\\n- Fixed several authentication and compatibility issues accessing HDFS.\\n\\n- Fixed several issues with float/double and decimal types.\\n\\n- Fixed several issues with date/datetimev2 types.\\n\\n- Fixed several query execution and planning issues.\\n\\n- Fixed several issues with JDBC Catalog.\\n\\n- Fixed several query-related issues with Hive Catalog, and Hive Metastore metadata synchronization issues.\\n\\n- Fix the problem that the result of `SHOW LOAD PROFILE` statement is incorrect.\\n\\n- Fixed several memory related issues.\\n\\n- Fixed several issues with `CREATE TABLE AS SELECT` functionality.\\n\\n- Fix the problem that the jsonb type causes BE to crash on CPU that do not support avx2.\\n\\n- Fixed several issues with dynamic partitions.\\n\\n- Fixed several issues with TOPN query optimization.\\n\\n- Fixed several issues with the Unique Key Merge-on-Write table model.\\n\\n## Big Thanks\\n\\n58 contributors participated in the improvement and release of 1.2.5, and thank them for their hard work and dedication:\\n\\n@adonis0147\\n\\n@airborne12\\n\\n@AshinGau\\n\\n@BePPPower\\n\\n@BiteTheDDDDt\\n\\n@caiconghui\\n\\n@CalvinKirs\\n\\n@cambyzju\\n\\n@caoliang-web\\n\\n@dataroaring\\n\\n@Doris-Extras\\n\\n@dujl\\n\\n@dutyu\\n\\n@fsilent\\n\\n@Gabriel39\\n\\n@gitccl\\n\\n@gnehil\\n\\n@GoGoWen\\n\\n@gongzexin\\n\\n@HappenLee\\n\\n@herry2038\\n\\n@jacktengg\\n\\n@Jibing-Li\\n\\n@kaka11chen\\n\\n@Kikyou1997\\n\\n@LemonLiTree\\n\\n@liaoxin01\\n\\n@LiBinfeng-01\\n\\n@luwei16\\n\\n@Moonm3n\\n\\n@morningman\\n\\n@mrhhsg\\n\\n@Mryange\\n\\n@nextdreamblue\\n\\n@nsnhuang\\n\\n@qidaye\\n\\n@Shoothzj\\n\\n@sohardforaname\\n\\n@stalary\\n\\n@starocean999\\n\\n@SWJTU-ZhangLei\\n\\n@wsjz\\n\\n@xiaokang\\n\\n@xinyiZzz\\n\\n@yangzhg\\n\\n@yiguolei\\n\\n@yixiutt\\n\\n@yujun777\\n\\n@Yulei-Yang\\n\\n@yuxuan-luo\\n\\n@zclllyybb\\n\\n@zddr\\n\\n@zenoyang\\n\\n@zhangstar333\\n\\n@zhannngchen\\n\\n@zxealous\\n\\n@zy-kkk\\n\\n@zzzzzzzs"},{"id":"/Say-Goodbye-to-OOM-Crashes","metadata":{"permalink":"/blog/Say-Goodbye-to-OOM-Crashes","source":"@site/blog/Say-Goodbye-to-OOM-Crashes.md","title":"Say goodbye to OOM crashes","description":"A more robust and flexible memory management solution with optimizations in memory allocation, memory tracking, and memory limit.","date":"2023-06-16T00:00:00.000Z","tags":[{"inline":true,"label":"Tech Sharing","permalink":"/blog/tags/tech-sharing"}],"hasTruncateMarker":false,"authors":[{"name":"Apache Doris","key":null,"page":null}],"frontMatter":{"title":"Say goodbye to OOM crashes","description":"A more robust and flexible memory management solution with optimizations in memory allocation, memory tracking, and memory limit.","date":"2023-06-16","author":"Apache Doris","tags":["Tech Sharing"],"image":"/images/say-goodbye-to-oom-crashes.jpg"},"unlisted":false,"prevItem":{"title":"Apache Doris announced the official release of version 1.2.5","permalink":"/blog/release-note-1.2.5"},"nextItem":{"title":"Understanding data compaction in 3 minutes","permalink":"/blog/Understanding-Data-Compaction-in-3-Minutes"}},"content":"\x3c!-- \\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\nWhat guarantees system stability in large data query tasks? It is an effective memory allocation and monitoring mechanism. It is how you speed up computation, avoid memory hotspots, promptly respond to insufficient memory, and minimize OOM errors. \\n\\n![memory-allocator](/images/OOM_1.png)\\n\\nFrom a database user\'s perspective, how do they suffer from bad memory management? This is a list of things that used to bother our users:\\n\\n- OOM errors cause backend processes to crash. To quote one of our community members: Hi, Apache Doris, it\'s okay to slow things down or fail a few tasks when you are short of memory, but throwing a downtime is just not cool.\\n- Backend processes consume too much memory space, but there is no way to find the exact task to blame or limit the memory usage for a single query.\\n- It is hard to set a proper memory size for each query, so chances are that a query gets canceled even when there is plenty of memory space.\\n- High-concurrency queries are disproportionately slow, and memory hotspots are hard to locate.\\n- Intermediate data during HashTable creation cannot be flushed to disks, so join queries between two large tables often fail due to OOM. \\n\\nLuckily, those dark days are behind us, because we have improved our memory management mechanism from the bottom up. Now get ready, things are going to be intensive.\\n\\n## Memory Allocation\\n\\nIn Apache Doris, we have a one-and-only interface for memory allocation: **Allocator**. It will make adjustments as it sees appropriate to keep memory usage efficient and under control. Also, MemTrackers are in place to track the allocated or released memory size, and three different data structures are responsible for large memory allocation in operator execution (we will get to them immediately).   \\n\\n![memory-tracker](/images/OOM_2.png)\\n\\n### Data Structures in Memory\\n\\nAs different queries have different memory hotspot patterns in execution, Apache Doris provides three different in-memory data structures: **Arena**, **HashTable**, and **PODArray**. They are all under the reign of the Allocator.\\n\\n![data-structures](/images/OOM_3.png)\\n\\n**1. Arena**\\n\\nThe Arena is a memory pool that maintains a list of chunks, which are to be allocated upon request from the Allocator. The chunks support memory alignment. They exist throughout the lifespan of the Arena, and will be freed up upon destruction (usually when the query is completed). Chunks are mainly used to store the serialized or deserialized data during Shuffle, or the serialized Keys in HashTables.\\n\\nThe initial size of a chunk is 4096 bytes. If the current chunk is smaller than the requested memory, a new chunk will be added to the list. If the current chunk is smaller than 128M, the new chunk will double its size; if it is larger than 128M, the new chunk will, at most, be 128M larger than what is required. The old small chunk will not be allocated for new requests. There is a cursor to mark the dividing line of chunks allocated and those unallocated.\\n\\n**2. HashTable**\\n\\nHashTables are applicable for Hash Joins, aggregations, set operations, and window functions. The PartitionedHashTable structure supports no more than 16 sub-HashTables. It also supports the parallel merging of HashTables and each sub-Hash Join can be scaled independently. These can reduce overall memory usage and the latency caused by scaling.\\n\\nIf the current HashTable is smaller than 8M, it will be scaled by a factor of 4; \\n\\nIf it is larger than 8M, it will be scaled by a factor of 2; \\n\\nIf it is smaller than 2G, it will be scaled when it is 50% full;\\n\\nAnd if it is larger than 2G, it will be scaled when it is 75% full. \\n\\nThe newly created HashTables will be pre-scaled based on how much data it is going to have. We also provide different types of HashTables for different scenarios. For example, for aggregations, you can apply PHmap.\\n\\n**3. PODArray**\\n\\nPODArray, as the name suggests, is a dynamic array of POD. The difference between it and `std::vector` is that PODArray does not initialize elements. It supports memory alignment and some interfaces of `std::vector`. It is scaled by a factor of 2. In destruction, instead of calling the destructor function for each element, it releases memory of the whole PODArray. PODArray is mainly used to save strings in columns and is applicable in many function computation and expression filtering.\\n\\n### Memory Interface\\n\\nAs the only interface that coordinates Arena, PODArray, and HashTable, the Allocator executes memory mapping (MMAP) allocation for requests larger than 64M. Those smaller than 4K will be directly allocated from the system via malloc/free; and those in between will be accelerated by a general-purpose caching ChunkAllocator, which brings a 10% performance increase according to our benchmarking results. The ChunkAllocator will try and retrieve a chunk of the specified size from the FreeList of the current core in a lock-free manner; if such a chunk doesn\'t exist, it will try from other cores in a lock-based manner; if that still fails, it will request the specified memory size from the system and encapsulate it into a chunk.\\n\\nWe chose Jemalloc over TCMalloc after experience with both of them. We tried TCMalloc in our high-concurrency tests and noticed that Spin Lock in CentralFreeList took up 40% of the total query time. Disabling \\"aggressive memory decommit\\" made things better, but that brought much more memory usage, so we had to use an individual thread to regularly recycle cache. Jemalloc, on the other hand, was more performant and stable in high-concurrency queries. After fine-tuning for other scenarios, it delivered the same performance as TCMalloc but consumed less memory.\\n\\n### Memory Reuse\\n\\nMemory reuse is widely executed on the execution layer of Apache Doris. For example, data blocks will be reused throughout the execution of a query. During Shuffle, there will be two blocks at the Sender end and they work alternately, one receiving data and the other in RPC transport. When reading a tablet, Doris will reuse the predicate column, implement cyclic reading, filter, copy filtered data to the upper block, and then clear. When ingesting data into an Aggregate Key table, once the MemTable that caches data reaches a certain size, it will be pre-aggregated and then more data will be written in. \\n\\nMemory reuse is executed in data scanning, too. Before the scanning starts, a number of free blocks (depending on the number of scanners and threads) will be allocated to the scanning task. During each scanner scheduling, one of the free blocks will be passed to the storage layer for data reading. After data reading, the block will be put into the producer queue for consumption of the upper operators in subsequent computation. Once an upper operator has copied the computation data from the block, the block will go back in the free blocks for next scanner scheduling. The thread the preallocates the free blocks will also be responsible for freeing them up after data scanning, so there won\'t be extra overheads. The number of free blocks somehow determines the concurrency of data scanning.\\n\\n## Memory Tracking\\n\\n Apache Doris uses MemTrackers to follow up on the allocation and releasing of memory while analyzing memory hotspots. The MemTrackers keep records of each data query, data ingestion, data compaction task, and the memory size of each global object, such as Cache and TabletMeta. It supports both manual counting and MemHook auto-tracking. Users can view the real-time memory usage in Doris backend on a Web page. \\n\\n### Structure of MemTrackers\\n\\nThe MemTracker system before Apache Doris 1.2.0 was in a hierarchical tree structure, consisting of process_mem_tracker, query_pool_mem_tracker, query_mem_tracker, instance_mem_tracker, ExecNode_mem_tracker and so on. MemTrackers of two neighbouring layers are of parent-child relationship. Hence, any calculation mistakes in a child MemTracker will be accumulated all the way up and result in a larger scale of incredibility. \\n\\n![MemTrackers](/images/OOM_4.png)\\n\\nIn Apache Doris 1.2.0 and newer, we made the structure of MemTrackers much simpler. MemTrackers are only divided into two types based on their roles: **MemTracker Limiter** and the others. MemTracker Limiter, monitoring memory usage, is unique in every query/ingestion/compaction task and global object; while the other MemTrackers traces the memory hotspots in query execution, such as HashTables in Join/Aggregation/Sort/Window functions and intermediate data in serialization, to give a picture of how memory is used in different operators or provide reference for memory control in data flushing.\\n\\nThe parent-child relationship between MemTracker Limiter and other MemTrackers is only manifested in snapshot printing. You can think of such a relationship as a symbolic link. They are not consumed at the same time, and the lifecycle of one does not affect that of the other. This makes it much easier for developers to understand and use them. \\n\\nMemTrackers (including MemTracker Limiter and the others) are put into a group of Maps. They allow users to print overall MemTracker type snapshot, Query/Load/Compaction task snapshot, and find out the Query/Load with the most memory usage or the most memory overusage. \\n\\n![Structure-of-MemTrackers](/images/OOM_5.png)\\n\\n### How MemTracker Works\\n\\nTo calculate memory usage of a certain execution, a MemTracker is added to a stack in Thread Local of the current thread. By reloading the malloc/free/realloc in Jemalloc or TCMalloc, MemHook obtains the actual size of the memory allocated or released, and records it in Thread Local of the current thread. When an execution is done, the relevant MemTracker will be removed from the stack. At the bottom of the stack is the MemTracker that records memory usage during the whole query/load execution process.\\n\\nNow let me explain with a simplified query execution process.\\n\\n- After a Doris backend node starts, the memory usage of all threads will be recorded in the Process MemTracker.\\n- When a query is submitted, a **Query MemTracker** will be added to the Thread Local Storage(TLS) Stack in the fragment execution thread.\\n- Once a ScanNode is scheduled, a **ScanNode MemTracker** will be added to Thread Local Storage(TLS) Stack in the fragment execution thread. Then, any memory allocated or released in this thread will be recorded into both the Query MemTracker and the ScanNode MemTracker.\\n- After a Scanner is scheduled, a Query MemTracker and a **Scanner MemTracker** will be added to the TLS Stack of the Scanner thread.\\n- When the scanning is done, all MemTrackers in the Scanner Thread TLS Stack will be removed. When the ScanNode scheduling is done, the ScanNode MemTracker will be removed from the fragment execution thread. Then, similarly, when an aggregation node is scheduled, an **AggregationNode MemTracker** will be added to the fragment execution thread TLS Stack, and get removed after the scheduling is done.\\n- If the query is completed, the Query MemTracker will be removed from the fragment execution thread TLS Stack. At this point, this stack should be empty. Then, from the QueryProfile, you can view the peak memory usage during the whole query execution as well as each phase (scanning, aggregation, etc.).\\n\\n![How-MemTrackers-Works](/images/OOM_6.png)\\n\\n### How to Use MemTracker\\n\\nThe Doris backend Web page demonstrates real-time memory usage, which is divided into types: Query/Load/Compaction/Global. Current memory consumption and peak consumption are shown. \\n\\n![How-to-use-MemTrackers](/images/OOM_7.png)\\n\\nThe Global types include MemTrackers of Cache and TabletMeta.\\n\\n![memory-usage-by-subsystem-1](/images/OOM_8.png)\\n\\nFrom the Query types, you can see the current memory consumption and peak consumption of the current query and the operators it involves (you can tell how they are related from the labels). For memory statistics of historical queries, you can check the Doris FE audit logs or BE INFO logs.\\n\\n![memory-usage-by-subsystem-2](/images/OOM_9.png)\\n\\n## Memory Limit\\n\\nWith widely implemented memory tracking in Doris backends, we are one step closer to eliminating OOM, the cause of backend downtime and large-scale query failures. The next step is to optimize the memory limit on queries and processes to keep memory usage under control.\\n\\n### Memory Limit on Query\\n\\nUsers can put a memory limit on every query. If that limit is exceeded during execution, the query will be canceled. But since version 1.2, we have allowed Memory Overcommit, which is a more flexible memory limit control. If there are sufficient memory resources, a query can consume more memory than the limit without being canceled, so users don\'t have to pay extra attention to memory usage; if there are not, the query will wait until new memory space is allocated; only when the newly freed up memory is not enough for the query will the query be canceled.\\n\\nWhile in Apache Doris 2.0, we have realized exception safety for queries. That means any insufficient memory allocation will immediately cause the query to be canceled, which saves the trouble of checking \\"Cancel\\" status in subsequent steps.\\n\\n### Memory Limit on Process\\n\\nOn a regular basis, Doris backend retrieves the physical memory of processes and the currently available memory size from the system. Meanwhile, it collects MemTracker snapshots of all Query/Load/Compaction tasks. If a backend process exceeds its memory limit or there is insufficient memory, Doris will free up some memory space by clearing Cache and cancelling a number of queries or data ingestion tasks. These will be executed by an individual GC thread regularly.\\n\\n![memory-limit-on-process](/images/OOM_10.png)\\n\\nIf the process memory consumed is over the SoftMemLimit (81% of total system memory, by default), or the available system memory drops below the Warning Water Mark (less than 3.2GB), **Minor GC** will be triggered. At this moment, query execution will be paused at the memory allocation step, the cached data in data ingestion tasks will be force flushed, and part of the Data Page Cache and the outdated Segment Cache will be released. If the newly released memory does not cover 10% of the process memory, with Memory Overcommit enabled, Doris will start cancelling the queries which are the biggest \\"overcommitters\\" until the 10% target is met or all queries are canceled. Then, Doris will shorten the system memory checking interval and the GC interval. The queries will be continued after more memory is available.\\n\\nIf the process memory consumed is beyond the MemLimit (90% of total system memory, by default), or the available system memory drops below the Low Water Mark (less than 1.6GB), **Full GC** will be triggered. At this time, data ingestion tasks will be stopped, and all Data Page Cache and most other Cache will be released. If, after all these steps, the newly released memory does not cover 20% of the process memory, Doris will look into all MemTrackers and find the most memory-consuming queries and ingestion tasks, and cancel them one by one. Only after the 20% target is met will the system memory checking interval and the GC interval be extended, and the queries and ingestion tasks be continued. (One garbage collection operation usually takes hundreds of \u03BCs to dozens of ms.)\\n\\n## Influences and Outcomes\\n\\nAfter optimizations in memory allocation, memory tracking, and memory limit, we have substantially increased the stability and high-concurrency performance of Apache Doris as a real-time analytic data warehouse platform. OOM crash in the backend is a rare scene now. Even if there is an OOM, users can locate the problem root based on the logs and then fix it. In addition, with more flexible memory limits on queries and data ingestion, users don\'t have to spend extra effort taking care of memory when memory space is adequate. \\n\\nIn the next phase, we plan to ensure completion of queries in memory overcommitment, which means less queries will have to be canceled due to memory shortage. We have broken this objective into specific directions of work: exception safety, memory isolation between resource groups, and the flushing mechanism of intermediate data. If you want to meet our developers, [this is where you find us](https://join.slack.com/t/apachedoriscommunity/shared_invite/zt-2unfw3a3q-MtjGX4pAd8bCGC1UV0sKcw)."},{"id":"/Understanding-Data-Compaction-in-3-Minutes","metadata":{"permalink":"/blog/Understanding-Data-Compaction-in-3-Minutes","source":"@site/blog/Understanding-Data-Compaction-in-3-Minutes.md","title":"Understanding data compaction in 3 minutes","description":"Think of your disks as a warehouse: The compaction mechanism is like a team of storekeepers who help put away the incoming data.","date":"2023-06-09T00:00:00.000Z","tags":[{"inline":true,"label":"Tech Sharing","permalink":"/blog/tags/tech-sharing"}],"hasTruncateMarker":false,"authors":[{"name":"Apache Doris","key":null,"page":null}],"frontMatter":{"title":"Understanding data compaction in 3 minutes","description":"Think of your disks as a warehouse: The compaction mechanism is like a team of storekeepers who help put away the incoming data.","date":"2023-06-09","author":"Apache Doris","tags":["Tech Sharing"],"image":"/images/understanding-data-compaction-in-3-minutes.jpg"},"unlisted":false,"prevItem":{"title":"Say goodbye to OOM crashes","permalink":"/blog/Say-Goodbye-to-OOM-Crashes"},"nextItem":{"title":"Apache Doris announced the official release of version 1.2.4","permalink":"/blog/release-note-1.2.4"}},"content":"\x3c!-- \\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\nWhat is compaction in database? Think of your disks as a warehouse: The compaction mechanism is like a team of storekeepers (with genius organizing skills like Marie Kondo) who help put away the incoming data. \\n\\nIn particular, the data (which is the inflowing cargo in this metaphor) comes in on a \\"conveyor belt\\", which does not allow cutting in line. This is how the **LSM-Tree** (Log Structured-Merge Tree) works: In data storage, data is written into **MemTables** in an append-only manner, and then the MemTables are flushed to disks to form files. (These files go by different names in different databases. In my community, we call them **Rowsets**). Just like putting small boxes of cargo into a large container, compaction means merging multiple small rowset files into a big one, but it does much more than that. Like I said, the compaction mechanism is an organizing magician: \\n\\n- Although the items (data) in each box (rowset) are orderly arranged, the boxes themselves are not. Hence, one thing that the \\"storekeepers\\" do is to sort the boxes (rowsets) in a certain order so they can be quickly found once needed (quickening data reading).\\n- If an item needs to be discarded or replaced, since no line-jump is allowed on the conveyor belt (append-only), you can only put a \\"note\\" (together with the substitution item) at the end of the queue on the belt to remind the \\"storekeepers\\", who will later perform replacing or discarding for you.\\n- If needed, the \\"storekeepers\\" are even kind enough to pre-process the cargo for you (pre-aggregating data to reduce computation burden during data reading). \\n\\n![MemTable-rowset](/images/Compaction_1.png)\\n\\nAs helpful as the \\"storekeepers\\" are, they can be troublemakers at times \u2014 that\'s why \\"team management\\" matters. For the compaction mechanism to work efficiently, you need wise planning and scheduling, or else you might need to deal with high memory and CPU usage, if not OOM in the backend or write error.\\n\\nSpecifically, efficient compaction is added up by quick triggering of compaction tasks, controllable memory and CPU overheads, and easy parameter adjustment from the engineer\'s side. That begs the question: **How**? In this post, I will show you our way, including how we trigger, execute, and fine-tune compaction for faster and less resource-hungry execution.\\n\\n## Trigger Strategies\\n\\nThe overall objective here is to trigger compaction tasks timely with the least resource consumption possible.\\n\\n### Active Trigger\\n\\nThe most intuitive way to ensure timely compaction is to scan for potential compaction tasks upon data ingestion. Every time a new data tablet version is generated, a compaction task is triggered immediately, so you will never have to worry about version buildup. But this only works for newly ingested data. This is called **Cumulative Compaction**, as opposed to **Base Compaction**, which is the compaction of existing data.\\n\\n### Passive Scan\\n\\nBase compaction is triggered by passive scan. Passive scan is a much heavier job than active trigger, because it scans all metadata in all data tablets in the node. After identifying all potential compaction tasks, the system starts compaction for the most urgent data tablet.\\n\\n### Tablet Dormancy\\n\\nFrequent metadata scanning is a waste of CPU resources, so it is better to introduce domancy: For tablets that have been producing no compaction tasks for long, the system just stops looking at them for a while. If there is a sudden data-write on a dormant tablet, that will trigger cumulative compaction as mentioned above, so no worries, you won\'t miss anything.\\n\\nThe combination of these three strategies is an example of cost-effective planning.\\n\\n## Execution\\n\\n### Vertical Compaction for Columnar Storage\\n\\nAs columnar storage is the future for analytic databases, the execution of compaction should adapt to that. We call it vertical compaction. I illustrate this mechanism with the figure below:\\n\\n![vertical-compaction](/images/Compaction_2.png)\\n\\nHope all these tiny blocks and numbers don\'t make you dizzy. Actually, vertical compaction can be broken down into four simple steps:\\n\\n1. **Separate key columns and value columns**. Split out all key columns from the input rowsets and put them into one group, and all value columns into N groups.\\n2. **Merge the key columns**. Heap sort is used in this step. The product here is a merged and ordered key column as well as a global sequence marker (**RowSources**).\\n3. **Merge the value columns**. The value columns are merged and organized based on the sequence in **RowSources**. \\n4. **Write the data**. All columns are assembled together and form one big rowset.\\n\\nAs a supporting technique for columnar storage, vertical compaction avoids the need to load all columns in every merging operation. That means it can vastly reduce memory usage compared to traditional row-oriented compaction.\\n\\n### Segment Compaction to Avoid \\"Jams\\"\\n\\nAs described in the beginning, in data ingestion, data will first be piled in the memory until it reaches a certain size, and then flushed to disks and stored in the form of files. Therefore, if you have ingested one huge batch of data at a time, you will have a large number of newly generated files on the disks. That adds to the scanning burden during data reading, and thus slows down data queries. (Imagine that suddenly you have to look into 50 boxes instead of 5, to find the item you need. That\'s overwhelming.) In some databases, such explosion of files could even trigger a protection mechanism that suspends data ingestion.\\n\\nSegment compaction is the way to avoid that. It allows you to compact data at the same time you ingest it, so that the system can ingest a larger data size quickly without generating too many files. \\n\\nThis is a flow chart that explains how segment compaction works:\\n\\n![segment-compaction](/images/Compaction_3.png)\\n\\nSegment compaction will be triggered once the number of newly generated files exceeds a certain limit (let\'s say, 10). It is executed asynchronously by a specialized merging thread. Every 10 files will be merged into one, and the original 10 files will be deleted. Segment compaction does not prolong the data ingestion process by much, but it can largely accelerate data queries.\\n\\n### Ordered Data Compaction\\n\\nTime series data analysis is an increasingly common analytic scenario. \\n\\nTime series data is \\"born orderly\\". It is already arranged chronologically, it is written at a regular pace, and every batch of it is of similar size. It is like the least-worried-about child in the family. Correspondingly, we have a tailored compaction method for it: ordered data compaction.\\n\\n![ordered-data-compaction](/images/Compaction_4.png)\\n\\nOrdered data compaction is even simpler:\\n\\n1. **Upload**: Jot down the Min/Max Keys of the input rowset files.\\n2. **Check**: Check if the rowset files are organized correctly based on the Min/Max Keys and the file size.\\n3. **Merge**: Hard link the input rowsets to the new rowset, and create metadata for the new rowset (including number of rows, file size, Min/Max Key, etc.)\\n\\nSee? It is a super neat and lightweight workload, involving only file linking and metadata creation. Statistically, **it just takes milliseconds to compact huge amounts of time series data but consumes nearly zero memory**.\\n\\nSo far, these are strategic and algorithmic optimizations for compaction, implemented by [Apache Doris 2.0.0](https://github.com/apache/doris/issues/19231), a unified analytic database. Apart from these, we, as developers for the open source project, have fine-tuned it from an engineering perspective.\\n\\n## Engineering Optimizations\\n\\n### Zero-Copy\\n\\nIn the backend nodes of Apache Doris, data goes through a few layers: Tablet -> Rowset -> Segment -> Column -> Page. The compaction process involves data transferring that consumes a lot of CPU resources. So we designed zero-copy compaction logic, which is realized by a data structure named BlockView. This brings another 5% increase in compaction efficiency.\\n\\n### Load-on-Demand\\n\\nIn most cases, the rowsets are not 100% orderless, so we can take advantage of such partial orderliness. For a group of ordered rowsets, Apache Doris only loads the first one and then starts merging. As the merging goes on, it gradually loads the rowset files it needs. This is how it decreases memory usage. \\n\\n### **Idle Schedule**\\n\\nAccording to our experience, base compaction tasks are often resource-intensive and time-consuming, so they can easily stand in the way of data queries. Doris 2.0.0 enables Idle Schedule, deprioritizing those base compaction tasks with huge data, long execution, and low compaction rate. \\n\\n## Parameter Optimizations\\n\\nEvery data engineer has somehow been harassed by complicated parameters and configurations. To protect our users from this nightmare, we have provided a streamlined set of parameters with the best-performing default configurations in the general environment.\\n\\n## Conclusion\\n\\nThis is how we keep our \\"storekeepers\\" working efficiently and cost-effectively. If you wonder how these strategies and optimization work in real practice, we tested Apache Doris with ClickBench. It reaches a **compaction speed of 300,000 row/s**; in high-concurrency scenarios, it maintains **a stable compaction score of around 50**. Also, we are planning to implement auto-tuning and increase observability for the compaction mechanism. If you are interested in the [Apache Doris](https://github.com/apache/doris) project and what we do, this is a group of visionary and passionate [developers](https://join.slack.com/t/apachedoriscommunity/shared_invite/zt-2unfw3a3q-MtjGX4pAd8bCGC1UV0sKcw) that you can talk to."},{"id":"/release-note-1.2.4","metadata":{"permalink":"/blog/release-note-1.2.4","source":"@site/blog/release-note-1.2.4.md","title":"Apache Doris announced the official release of version 1.2.4","description":"Dear community, Apache Doris 1.2.4 is now available, with several enhancements and bug fixes based on 1.2.0\uFF0Cenabling smoother user experience.","date":"2023-06-05T00:00:00.000Z","tags":[{"inline":true,"label":"Release Notes","permalink":"/blog/tags/release-notes"}],"hasTruncateMarker":false,"authors":[{"name":"Apache Doris","key":null,"page":null}],"frontMatter":{"title":"Apache Doris announced the official release of version 1.2.4","description":"Dear community, Apache Doris 1.2.4 is now available, with several enhancements and bug fixes based on 1.2.0\uFF0Cenabling smoother user experience.","date":"2023-06-05","author":"Apache Doris","tags":["Release Notes"],"image":"/images/1.2.4-release.png"},"unlisted":false,"prevItem":{"title":"Understanding data compaction in 3 minutes","permalink":"/blog/Understanding-Data-Compaction-in-3-Minutes"},"nextItem":{"title":"A/B Testing was a handful, until we found the replacement for Druid","permalink":"/blog/AB-Testing-was-a-Handful-Until-we-Found-the-Replacement-for-Druid"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n\\n\\n## Behavior Changed\\n\\n- For `DateV2`/`DatetimeV2` and `DecimalV3` type, in the results of `DESCRIBLE` and `SHOW CREATE TABLE` statements, they will no longer be displayed as `DateV2`/`DatetimeV2` or `DecimalV3`, but directly displayed as `Date`/`Datetime` or `Decimal`.\\n\\n\\t- This change is for compatibility with some BI tools. If you want to see the actual type of the column, you can check it with the `DESCRIBE ALL` statement.\\n\\n- When querying tables in the `information_schema` database, the meta information(database, table, column, etc.) in the external catalog is no longer returned by default.\\n\\n\\t- This change avoids the problem that the `information_schema` database cannot be queried due to the connection problem of some external catalog, so as to solve the problem of using some BI tools with Doris. It can be controlled by the FE configuration  `infodb_support_ext_catalog`, and the default value is `false`, that is, the meta information of external catalog will not be returned.\\n\\n## Improvements\\n\\n### JDBC Catalog\\n\\n- Supports connecting to Trino/Presto via JDBC Catalog\\n\\n\u200B        Refer to: [https://doris.apache.org/docs/dev/lakehouse/multi-catalog/jdbc#trino](https://doris.apache.org/docs/dev/lakehouse/multi-catalog/jdbc#trino)\\n\\n- JDBC Catalog connects to Clickhouse data source and supports Array type mapping\\n\\n\u200B        Refer to: [https://doris.apache.org/docs/dev/lakehouse/multi-catalog/jdbc#clickhouse](https://doris.apache.org/docs/dev/lakehouse/multi-catalog/jdbc#clickhouse)\\n\\n### Spark Load \\n\\n- Spark Load supports Resource Manager HA related configuration\\n\\n\u200B        Refer to: https://github.com/apache/doris/pull/15000\\n\\n## Bug Fixes\\n\\n- Fixed several connectivity issues with Hive Catalog.\\n\\n- Fixed ClassNotFound issues with Hudi Catalog.\\n\\n- Optimize the connection pool of JDBC Catalog to avoid too many connections.\\n\\n- Fix the problem that OOM will occur when importing data from another Doris cluster through JDBC Catalog.\\n\\n- Fixed serveral queries and imports planning issues.\\n\\n- Fixed several issues with Unique Key Merge-On-Write data model.\\n\\n- Fix several BDBJE issues and solve the problem of abnormal FE metadata in some cases.\\n\\n- Fix the problem that the `CREATE VIEW` statement does not support Table Valued Function.\\n\\n- Fixed several memory statistics issues.\\n\\n- Fixed several issues reading Parquet/ORC format.\\n\\n- Fixed several issues with DecimalV3.\\n\\n- Fixed several issues with SHOW QUERY/LOAD PROFILE."},{"id":"/AB-Testing-was-a-Handful-Until-we-Found-the-Replacement-for-Druid","metadata":{"permalink":"/blog/AB-Testing-was-a-Handful-Until-we-Found-the-Replacement-for-Druid","source":"@site/blog/AB-Testing-was-a-Handful-Until-we-Found-the-Replacement-for-Druid.md","title":"A/B Testing was a handful, until we found the replacement for Druid","description":"The recipe for successful A/B testing is quick computation, no duplication, and no data loss. For that, we used Apache Flink and Apache Doris to build our data platform.","date":"2023-06-01T00:00:00.000Z","tags":[{"inline":true,"label":"Best Practice","permalink":"/blog/tags/best-practice"}],"hasTruncateMarker":false,"authors":[{"name":"Heyu Dou, Xinxin Wang","key":null,"page":null}],"frontMatter":{"title":"A/B Testing was a handful, until we found the replacement for Druid","description":"The recipe for successful A/B testing is quick computation, no duplication, and no data loss. For that, we used Apache Flink and Apache Doris to build our data platform.","date":"2023-06-01","author":"Heyu Dou, Xinxin Wang","tags":["Best Practice"],"image":"/images/ab-testing-was-a-handful-until-we-found-the-replacement-for-druid.png"},"unlisted":false,"prevItem":{"title":"Apache Doris announced the official release of version 1.2.4","permalink":"/blog/release-note-1.2.4"},"nextItem":{"title":"Building a log analytics solution 10 times more cost-effective than Elasticsearch","permalink":"/blog/Building-A-Log-Analytics-Solution-10-Times-More-Cost-Effective-Than-Elasticsearch"}},"content":"\x3c!-- \\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\nUnlike normal reporting, A/B testing collects data of a different combination of dimensions every time. It is also a complicated kind of analysis of immense data. In our case, we have a real-time data volume of millions of OPS (Operations Per Second), with each operation involving around 20 data tags and over a dozen dimensions.\\n\\nFor effective A/B testing, as data engineers, we must ensure quick computation as well as high data integrity (which means no duplication and no data loss). I\'m sure I\'m not the only one to say this: it is hard!\\n\\nLet me show you our long-term struggle with our previous Druid-based data platform.\\n\\n## Platform Architecture 1.0\\n\\n**Components**: Apache Storm + Apache Druid + MySQL\\n\\nThis was our real-time datawarehouse, where Apache Storm was the real-time data processing engine and Apache Druid pre-aggregated the data. However, Druid did not support certain paging and join queries, so we wrote data from Druid to MySQL regularly, making MySQL the \\"materialized view\\" of Druid. But that was only a duct tape solution as it couldn\'t support our ever enlarging real-time data size. So data timeliness was unattainable.\\n\\n![Apache-Storm-Apache-Druid-MySQL](/images/360_1.png)\\n\\n## Platform Architecture 2.0\\n\\n**Components**: Apache Flink + Apache Druid + TiDB\\n\\nThis time, we replaced Storm with Flink, and MySQL with TiDB. Flink was more powerful in terms of semantics and features, while TiDB, with its distributed capability, was more maintainable than MySQL. But architecture 2.0 was nowhere near our goal of end-to-end data consistency, either, because when processing huge data, enabling TiDB transactions largely slowed down data writing. Plus, Druid itself did not support standard SQL, so there were some learning costs and frictions in usage.\\n\\n![Apache-Flink-Apache-Druid-TiDB](/images/360_2.png)\\n\\n## Platform Architecture 3.0\\n\\n**Components**: Apache Flink + [Apache Doris](https://github.com/apache/doris)\\n\\nWe replaced Apache Druid with Apache Doris as the OLAP engine, which could also serve as a unified data serving gateway. So in Architecture 3.0, we only need to maintain one set of query logic. And we layered our real-time datawarehouse to increase reusability of real-time data.\\n\\n![Apache-Flink-Apache-Doris](/images/360_3.png)\\n\\nTurns out the combination of Flink and Doris was the answer. We can exploit their features to realize quick computation and data consistency. Keep reading and see how we make it happen.\\n\\n## Quick Computation\\n\\nAs one piece of operation data can be attached to 20 tags, in A/B testing, we compare two groups of data centering only one tag each time. At first, we thought about splitting one piece of operation data (with 20 tags) into 20 pieces of data of only one tag upon data ingestion, and then importing them into Doris for analysis, but that could cause a data explosion and thus huge pressure on our clusters. \\n\\nThen we tried moving part of such workload to the computation engine. So we tried and \\"exploded\\" the data in Flink, but soon regretted it, because when we aggregated the data using the global hash windows in Flink jobs, the network and CPU usage also \\"exploded\\".\\n\\nOur third shot was to aggregate data locally in Flink right after we split it. As is shown below, we create a window in the memory of one operator for local aggregation; then we further aggregate it using the global hash windows. Since two operators chained together are in one thread, transferring data between operators consumes much less network resources. **The two-step aggregation method, combined with the** **[Aggregate model](https://doris.apache.org/docs/dev/data-table/data-model)** **of Apache Doris, can keep data explosion in a manageable range.**\\n\\n![Apache-Flink-Apache-Doris-2](/images/360_4.png)\\n\\nFor convenience in A/B testing, we make the test tag ID the first sorted field in Apache Doris, so we can quickly locate the target data using sorted indexes. To further minimize data processing in queries, we create materialized views with the frequently used dimensions. With constant modification and updates, the materialized views are applicable in 80% of our queries.\\n\\nTo sum up, with the application of sorted index and materialized views, we reduce our query response time to merely seconds in A/B testing.\\n\\n## Data Integrity Guarantee\\n\\nImagine that your algorithm designers worked sweat and tears trying to improve the business, only to find their solution unable to be validated by A/B testing due to data loss. This is an unbearable situation, and we make every effort to avoid it.\\n\\n### Develop a Sink-to-Doris Component\\n\\nTo ensure end-to-end data integrity, we developed a Sink-to-Doris component. It is built on our own Flink Stream API scaffolding and realized by the idempotent writing of Apache Doris and the two-stage commit mechanism of Apache Flink. On top of it, we have a data protection mechanism against anomalies. \\n\\nIt is the result of our long-term evolution. We used to ensure data consistency by implementing \\"one writing for one tag ID\\". Then we realized we could make good use of the transactions in Apache Doris and the two-stage commit of Apache Flink. \\n\\n![idempotent-writing-two-stage-commit](/images/360_5.png)\\n\\nAs is shown above, this is how two-stage commit works to guarantee data consistency:\\n\\n1. Write data into local files;\\n2. Stage One: pre-commit data to Apache Doris. Save the Doris transaction ID into status;\\n3. If checkpoint fails, manually abandon the transaction; if checkpoint succeeds, commit the transaction in Stage Two;\\n4. If the commit fails after multiple retries, the transaction ID and the relevant data will be saved in HDFS, and we can restore the data via Broker Load.\\n\\nWe make it possible to split a single checkpoint into multiple transactions, so that we can prevent one Stream Load from taking more time than a Flink checkpoint in the event of large data volumes.\\n\\n### Application Display\\n\\nThis is how we implement Sink-to-Doris. The component has blocked API calls and topology assembly. With simple configuration, we can write data into Apache Doris via Stream Load. \\n\\n![Sink-to-Doris](/images/360_6.png)\\n\\n### Cluster Monitoring\\n\\nFor cluster and host monitoring, we adopted the metrics templates provided by the Apache Doris community. For data monitoring, in addition to the template metrics, we added Stream Load request numbers and loading rates.\\n\\n![stream-load-cluster-monitoring](/images/360_7.png)\\n\\nOther metrics of our concerns include data writing speed and task processing time. In the case of anomalies, we will receive notifications in the form of phone calls, messages, and emails.\\n\\n![cluster-monitoring](/images/360_8.png)\\n\\n## Key Takeaways\\n\\nThe recipe for successful A/B testing is quick computation and high data integrity. For this purpose, we implement a two-step aggregation method in Apache Flink, utilize the Aggregate model, materialized view, and short indexes of Apache Doris. Then we develop a Sink-to-Doris component, which is realized by the idempotent writing of Apache Doris and the two-stage commit mechanism of Apache Flink."},{"id":"/Building-A-Log-Analytics-Solution-10-Times-More-Cost-Effective-Than-Elasticsearch","metadata":{"permalink":"/blog/Building-A-Log-Analytics-Solution-10-Times-More-Cost-Effective-Than-Elasticsearch","source":"@site/blog/Building-A-Log-Analytics-Solution-10-Times-More-Cost-Effective-Than-Elasticsearch.md","title":"Building a log analytics solution 10 times more cost-effective than Elasticsearch","description":"Apache Doris has introduced inverted indexes in version 2.0.0 and further optimized it to realize two times faster log query performance than Elasticsearch with 1/5 of the storage space it uses.","date":"2023-05-26T00:00:00.000Z","tags":[{"inline":true,"label":"Tech Sharing","permalink":"/blog/tags/tech-sharing"}],"hasTruncateMarker":false,"authors":[{"name":"Apache Doris","key":null,"page":null}],"frontMatter":{"title":"Building a log analytics solution 10 times more cost-effective than Elasticsearch","description":"Apache Doris has introduced inverted indexes in version 2.0.0 and further optimized it to realize two times faster log query performance than Elasticsearch with 1/5 of the storage space it uses.","date":"2023-05-26","author":"Apache Doris","tags":["Tech Sharing"],"image":"/images/building-a-log-analytics-solution-10-times-more-cost-effective-than-es.jpg"},"unlisted":false,"prevItem":{"title":"A/B Testing was a handful, until we found the replacement for Druid","permalink":"/blog/AB-Testing-was-a-Handful-Until-we-Found-the-Replacement-for-Druid"},"nextItem":{"title":"Building a data warehouse for traditional industry","permalink":"/blog/Building-a-Data-Warehouse-for-Traditional-Industry"}},"content":"\x3c!-- \\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n\\n\\nLogs often take up the majority of a company\'s data assets. Examples of logs include business logs (such as user activity logs), and Operation & Maintenance logs of servers, databases, and network or IoT devices.\\n\\nLogs are the guardian angel of business. On the one hand, they provide system risk alerts and help engineers in troubleshooting. On the other hand, if you zoom them out by time range, you might identify some helpful trends and patterns, not to mention that business logs are the cornerstone of user insights.\\n\\nHowever, logs can be a handful, because:\\n\\n- **They flow in like crazy.** Every system event or click from user generates a log. A company often produces tens of billions of new logs per day.\\n- **They are bulky.** Logs are supposed to stay. They might not be useful until they are. So a company can accumulate up to PBs of log data, many of which are seldom visited but take up huge storage space. \\n- **They must be quick to load and find.** Locating the target log for troubleshooting is literally like looking for a needle in a haystack. People long for real-time log writing and real-time responses to log queries. \\n\\nNow you can see a clear picture of what an ideal log processing system is like. It should support:\\n\\n- **High-throughput real-time data ingestion:** It should be able to write blogs in bulk, and make them visible immediately.\\n- **Low-cost storage:** It should be able to store substantial amounts of logs without costing too many resources.\\n- **Real-time text search:** It should be capable of quick text search.\\n\\n## Common Solutions: Elasticsearch & Grafana Loki\\n\\nThere exist two common log processing solutions within the industry, exemplified by Elasticsearch and Grafana Loki, respectively. \\n\\n- **Inverted index (Elasticsearch)**: It is well-embraced due to its support for full-text search and high performance. The downside is the low throughput in real-time writing and the huge resource consumption in index creation.\\n- **Lightweight index / no index (Grafana Loki)**: It is the opposite of inverted index because it boasts high real-time write throughput and low storage cost but delivers slow queries.\\n\\n![Elasticsearch-and-Grafana-Loki](/images/Inverted_1.png)\\n\\n## Introduction to Inverted Index\\n\\nA prominent strength of Elasticsearch in log processing is quick keyword search among a sea of logs. This is enabled by inverted indexes.\\n\\nInverted indexing was originally used to retrieve words or phrases in texts. The figure below illustrates how it works: \\n\\nUpon data writing, the system tokenizes texts into **terms**, and stores these terms in a **posting list** which maps terms to the ID of the row where they exist. In text queries, the database finds the corresponding **row ID** of the keyword (term) in the posting list, and fetches the target row based on the row ID. By doing so, the system won\'t have to traverse the whole dataset and thus improves query speeds by orders of magnitudes. \\n\\n![inverted-index](/images/Inverted_2.png)\\n\\nIn inverted indexing of Elasticsearch, quick retrieval comes at the cost of writing speed, writing throughput, and storage space. Why? Firstly, tokenization, dictionary sorting, and inverted index creation are all CPU- and memory-intensive operations. Secondly, Elasticssearch has to store the original data, the inverted index, and an extra copy of data stored in columns for query acceleration. That\'s triple redundancy. \\n\\nBut without inverted index, Grafana Loki, for example, is hurting user experience with its slow queries, which is the biggest pain point for engineers in log analysis.\\n\\nSimply put, Elasticsearch and Grafana Loki represent different tradeoffs between high writing throughput, low storage cost, and fast query performance. What if I tell you there is a way to have them all? We have introduced inverted indexes in [Apache Doris 2.0.0](https://github.com/apache/doris/issues/19231) and further optimized it to realize **two times faster log query performance than Elasticsearch with 1/5 of the storage space it uses. Both factors combined, it is a 10 times better solution.** \\n\\n## Inverted Index in Apache Doris\\n\\nGenerally, there are two ways to implement indexes: **external indexing system** or **built-in indexes**.\\n\\n**External indexing system:** You connect an external indexing system to your database. In data ingestion, data is imported to both systems. After the indexing system creates indexes, it deletes the original data within itself. When data users input a query, the indexing system provides the IDs of the relevant data, and then the database looks up the target data based on the IDs. \\n\\nBuilding an external indexing system is easier and less intrusive to the database, but it comes with some annoying flaws:\\n\\n- The need to write data into two systems can result in data inconsistency and storage redundancy.\\n- Interaction between the database and the indexing system brings overheads, so when the target data is huge, the query across the two systems can be slow.\\n- It is exhausting to maintain two systems.\\n\\nIn [Apache Doris](https://github.com/apache/doris), we opt for the other way. Built-in inverted indexes are more difficult to make, but once it is done, it is faster, more user-friendly, and trouble-free to maintain.\\n\\nIn Apache Doris, data is arranged in the following format. Indexes are stored in the Index Region:\\n\\n![index-region-in-Apache-Doris](/images/Inverted_3.png)\\n\\nWe implement inverted indexes in a non-intrusive manner:\\n\\n1. **Data ingestion & compaction**: As a segment file is written into Doris, an inverted index file will be written, too. The index file path is determined by the segment ID and the index ID. Rows in segments correspond to the docs in indexes, so are the RowID and the DocID.\\n2. **Query**: If the `where` clause includes a column with inverted index, the system will look up in the index file, return a DocID list, and convert the DocID list into a RowID Bitmap. Under the RowID filtering mechanism of Apache Doris, only the target rows will be read. This is how queries are accelerated.\\n\\n![non-intrusive-inverted-index](/images/Inverted_4.png)\\n\\nSuch non-intrusive method separates the index file from the data files, so you can make any changes to the inverted indexes without worrying about affecting the data files themselves or other indexes. \\n\\n## Optimizations for Inverted Index\\n\\n### General Optimizations\\n\\n**C++ Implementation and Vectorization**\\n\\nDifferent from Elasticsearch, which uses Java, Apache Doris implements C++ in its storage modules, query execution engine, and inverted indexes. Compared to Java, C++ provides better performance, allows easier vectorization, and produces no JVM GC overheads. We have vectorized every step of inverted indexing in Apache Doris, such as tokenization, index creation, and queries. To provide you with a perspective, **in inverted indexing, Apache Doris writes data at a speed of 20MB/s per core, which is four times that of Elasticsearch (5MB/s).**\\n\\n**Columnar Storage & Compression**\\n\\nApache Lucene lays the foundation for inverted indexes in Elasticsearch. As Lucene itself is built to support file storage, it stores data in a row-oriented format. \\n\\nIn Apache Doris, inverted indexes for different columns are isolated from each other, and the inverted index files adopt columnar storage to facilitate vectorization and data compression.\\n\\nBy utilizing Zstandard compression, Apache Doris realizes a compression ratio ranging from **5:1** to **10:1**, faster compression speeds, and 50% less space usage than GZIP compression.\\n\\n**BKD Trees for Numeric / Datetime Columns**\\n\\nApache Doris implements BKD trees for numeric and datetime columns. This not only increases performance of range queries, but is a more space-saving method than converting those columns to fixed-length strings. Other benefits of it include:\\n\\n1. **Efficient range queries**: It is able to quickly locate the target data range in numeric and datetime columns.\\n2. **Less storage space**: It aggregates and compresses adjacent data blocks to reduce storage costs.\\n3. **Support for multi-dimensional data**: BKD trees are scalable and adaptive to multi-dimensional data types, such as GEO points and ranges.\\n\\nIn addition to BKD trees, we have further optimized the queries on numeric and datetime columns.\\n\\n1. **Optimization for low-cardinality scenarios**: We have fine-tuned the compression algorithm for low-cardinality scenarios, so decompressing and de-serializing large amounts of inverted lists will consume less CPU resources.\\n2. **Pre-fetching**: For high-hit-rate scenarios, we adopt pre-fetching. If the hit rate exceeds a certain threshold, Doris will skip the indexing process and start data filtering.\\n\\n### Tailored Optimizations to OLAP\\n\\nLog analysis is a simple kind of query with no need for advanced features (e.g. relevance scoring in Apache Lucene). The bread and butter capability of a log processing tool is quick queries and low storage cost. Therefore, in Apache Doris, we have streamlined the inverted index structure to meet the needs of an OLAP database.\\n\\n- In data ingestion, we prevent multiple threads from writing data into the same index, and thus avoid overheads brought by lock contention.\\n- We discard forward index files and Norm files to clear storage space and reduce I/O overheads.\\n- We simplify the computation logic of relevance scoring and ranking to further reduce overheads and increase performance.\\n\\nIn light of the fact that logs are partitioned by time range and historical logs are visited less frequently. We plan to provide more granular and flexible index management in future versions of Apache Doris:\\n\\n- **Create inverted index for a specified data partition**: create index for logs of the past seven days, etc.\\n- **Delete** **inverted index for a specified data partition**: delete index for logs from over one month ago, etc. (so as to clear out index space).\\n\\n## Benchmarking\\n\\nWe tested Apache Doris on publicly available datasets against Elasticsearch and ClickHouse.\\n\\nFor a fair comparison, we ensure uniformity of testing conditions, including benchmarking tool, dataset, and hardware.\\n\\n### Apache Doris VS Elasticsearch\\n\\n**Benchmarking tool**: ES Rally, the official testing tool for Elasticsearch\\n\\n**Dataset**: 1998 World Cup HTTP Server Logs (self-contained dataset in ES Rally)\\n\\n**Data Size (Before Compression)**: 32G, 247 million rows, 134 bytes per row (on average)\\n\\n**Query**:  11 queries including keyword search, range query, aggregation, and ranking; Each query is serially executed 100 times.\\n\\n**Environment**: 3 \xd7 16C 64G cloud virtual machines\\n\\n- **Results of Apache Doris**:\\n\\n  - Writing Speed: 550 MB/s, **4.2 times that of Elasticsearch**\\n  - Compression Ratio: 10:1\\n  - Storage Usage: **20% that of Elasticsearch**\\n  - Response Time: **43% that of Elasticsearch**\\n\\n![Apache-Doris-VS-Elasticsearch](/images/Inverted_5.png)\\n\\n### Apache Doris VS ClickHouse\\n\\nAs ClickHouse launched inverted index as an experimental feature in v23.1, we tested Apache Doris with the same dataset and SQL as described in the ClickHouse [blog](https://clickhouse.com/blog/clickhouse-search-with-inverted-indices), and compared performance of the two under the same testing resource, case, and tool.\\n\\n**Data**: 6.7G, 28.73 million rows, the Hacker News dataset, Parquet format\\n\\n**Query**:  3 keyword searches, counting the number of occurrence of the keywords \\"ClickHouse\\", \\"OLAP\\" OR \\"OLTP\\", and \\"avx\\" AND \\"sve\\".\\n\\n**Environment**: 1 \xd7 16C 64G cloud virtual machine\\n\\n**Result**: Apache Doris was **4.7 times, 12 times, 18.5 times** faster than ClickHouse in the three queries, respectively.\\n\\n![Apache-Doris-VS-ClickHouse](/images/Inverted_6.png)\\n\\n## Usage & Example\\n\\n**Dataset**: one million comment records from Hacker News\\n\\n- **Step 1**: Specify inverted index to the data table upon table creation.\\n\\n- **Parameters**:\\n\\n- - INDEX idx_comment (`comment`): create an index named \\"idx_comment\\" comment for the \\"comment\\" column\\n  - USING INVERTED: specify inverted index for the table\\n  - PROPERTIES(\\"parser\\" = \\"english\\"): specify the tokenization language to English\\n\\n```SQL\\nCREATE TABLE hackernews_1m\\n(\\n    `id` BIGINT,\\n    `deleted` TINYINT,\\n    `type` String,\\n    `author` String,\\n    `timestamp` DateTimeV2,\\n    `comment` String,\\n    `dead` TINYINT,\\n    `parent` BIGINT,\\n    `poll` BIGINT,\\n    `children` Array<BIGINT>,\\n    `url` String,\\n    `score` INT,\\n    `title` String,\\n    `parts` Array<INT>,\\n    `descendants` INT,\\n    INDEX idx_comment (`comment`) USING INVERTED PROPERTIES(\\"parser\\" = \\"english\\") COMMENT \'inverted index for comment\'\\n)\\nDUPLICATE KEY(`id`)\\nDISTRIBUTED BY HASH(`id`) BUCKETS 10\\nPROPERTIES (\\"replication_num\\" = \\"1\\");\\n```\\n\\nNote: You can add index to an existing table via `ADD INDEX idx_comment ON hackernews_1m(`comment`) USING INVERTED PROPERTIES(\\"parser\\" = \\"english\\") `. Different from that of smart index and secondary index, the creation of inverted index only involves the reading of the comment column, so it can be much faster.\\n\\n**Step 2**: Retrieve the words\\"OLAP\\" and \\"OLTP\\" in the comment column with `MATCH_ALL`. The response time here was 1/10 of that in hard matching with `like`. (The performance gap widens as data volume increases.)\\n\\n```SQL\\nmysql> SELECT count() FROM hackernews_1m WHERE comment LIKE \'%OLAP%\' AND comment LIKE \'%OLTP%\';\\n+---------+\\n| count() |\\n+---------+\\n|      15 |\\n+---------+\\n1 row in set (0.13 sec)\\n\\nmysql> SELECT count() FROM hackernews_1m WHERE comment MATCH_ALL \'OLAP OLTP\';\\n+---------+\\n| count() |\\n+---------+\\n|      15 |\\n+---------+\\n1 row in set (0.01 sec)\\n```\\n\\nFor more feature introduction and usage guide, see documentation: [Inverted Index](https://doris.apache.org/docs/dev/data-table/index/inverted-index/)\\n\\n## Wrap-up\\n\\nIn a word, what contributes to Apache Doris\' 10-time higher cost-effectiveness than Elasticsearch is its OLAP-tailored optimizations for inverted indexing, supported by the columnar storage engine, massively parallel processing framework, vectorized query engine, and cost-based optimizer of Apache Doris. \\n\\nAs proud as we are about our own inverted indexing solution, we understand that self-published benchmarks can be controversial, so we are open to [feedback](https://join.slack.com/t/apachedoriscommunity/shared_invite/zt-2unfw3a3q-MtjGX4pAd8bCGC1UV0sKcw) from any third-party users and see how [Apache Doris](https://github.com/apache/doris) works in real-world cases."},{"id":"/Building-a-Data-Warehouse-for-Traditional-Industry","metadata":{"permalink":"/blog/Building-a-Data-Warehouse-for-Traditional-Industry","source":"@site/blog/Building-a-Data-Warehouse-for-Traditional-Industry.md","title":"Building a data warehouse for traditional industry","description":"The best component for you is the one that suits you most. In Midland Realty, we don\'t have too much data to process but want a data platform easy to use and maintain.","date":"2023-05-12T00:00:00.000Z","tags":[{"inline":true,"label":"Best Practice","permalink":"/blog/tags/best-practice"}],"hasTruncateMarker":false,"authors":[{"name":"Herman Seah","key":null,"page":null}],"frontMatter":{"title":"Building a data warehouse for traditional industry","description":"The best component for you is the one that suits you most. In Midland Realty, we don\'t have too much data to process but want a data platform easy to use and maintain.","date":"2023-05-12","author":"Herman Seah","tags":["Best Practice"],"image":"/images/building-a-data-warehouse-for-traditional-industry.png"},"unlisted":false,"prevItem":{"title":"Building a log analytics solution 10 times more cost-effective than Elasticsearch","permalink":"/blog/Building-A-Log-Analytics-Solution-10-Times-More-Cost-Effective-Than-Elasticsearch"},"nextItem":{"title":"Zipping up the lambda architecture for 40% faster performance","permalink":"/blog/Zipping-up-the-Lambda-Architecture-for-40-Percent-Faster-Performance"}},"content":"\x3c!-- \\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\nBy Herman Seah, Data Warehouse Planner & Data Analyst at Midland Realty\\n\\nThis is a part of the digital transformation of a real estate giant. For the sake of confidentiality, I\'m not going to reveal any business data, but you\'ll get a detailed view of our data warehouse and our optimization strategies.\\n\\nNow let\'s get started.\\n\\n## Architecture\\n\\nLogically, our data architecture can be divided into four parts.\\n\\n![data-processing-architecture](/images/Midland_1.png)\\n\\n- **Data integration**: This is supported by Flink CDC, DataX, and the Multi-Catalog feature of Apache Doris.\\n- **Data management**: We use Apache Dolphinscheduler for script lifecycle management, privileges in multi-tenancy management, and data quality monitoring.\\n- **Alerting**: We use Grafana, Prometheus, and Loki to monitor component resources and logs.\\n- **Data services**: This is where BI tools step in for user interaction, such as data queries and analysis.\\n\\n### 1. **Tables**\\n\\nWe create our dimension tables and fact tables centering each operating entity in business, including customers, houses, etc. If there are a series of activities involving the same operating entity, they should be recorded by one field. (This is a lesson learned from our previous chaotic data management system.)\\n\\n### 2. **Layers**\\n\\nOur data warehouse is divided into five conceptual layers. We use Apache Doris and Apache DolphinScheduler to schedule the DAG scripts between these layers.\\n\\n![ODS-DWD-DWS-ADS-DIM](/images/Midland_2.png)\\n\\nEvery day, the layers go through an overall update besides incremental updates in case of changes in historical status fields or incomplete data synchronization of ODS tables.\\n\\n### 3. **Incremental Update Strategies**\\n\\n(1) Set `where >= \\"activity time -1 day or -1 hour\\"`  instead of `where >= \\"activity time`\\n\\nThe reason for doing so is to prevent data drift caused by the time gap of scheduling scripts. Let\'s say, with the execution interval set to 10 min, suppose that the script is executed at 23:58:00 and a new piece of data arrives at 23:59:00, if we set `where >= \\"activity time`, that piece of data of the day will be missed.\\n\\n(2) Fetch the ID of the largest primary key of the table before every script execution, store the ID in the auxiliary table, and set `where >= \\"ID in auxiliary table\\"`\\n\\nThis is to avoid data duplication. Data duplication might happen if you use the Unique Key model of Apache Doris and designate a set of primary keys, because if there are any changes in the primary keys in the source table, the changes will be recorded and the relevant data will be loaded. This method can fix that, but it is only applicable when the source tables have auto-increment primary keys.\\n\\n(3) Partition the tables\\n\\nAs for time-based auto-increment data such as log tables, there might be less changes in historical data and status, but the data volume is large, so there could be huge computing pressure on overall updates and snapshot creation. Hence, it is better to partition such tables, so for each incremental update, we only need to replace one partition. (You might need to watch out for data drift, too.)\\n\\n### 4. **Overall Update Strategies**\\n\\n(1) Truncate Table\\n\\nClear out the table and then ingest all data from the source table into it. This is applicable for small tables and scenarios with no user activity in wee hours.\\n\\n(2) `ALTER TABLE tbl1 REPLACE WITH TABLE tbl2 `\\n\\nThis is an atomic operation and it is advisable for large tables. Every time before executing a script, we create a temporary table with the same schema, load all data into it, and replace the original table with it.\\n\\n## Application\\n\\n- **ETL job**: every minute\\n- **Configuration for first-time deployment**: 8 nodes, 2 frontends, 8 backends, hybrid deployment\\n- **Node configuration**: 32C * 60GB * 2TB SSD\\n\\nThis is our configuration for TBs of legacy data and GBs of incremental data. You can use it as a reference and scale your cluster on this basis. Deployment of Apache Doris is simple. You don\'t need other components.\\n\\n1. To integrate offline data and log data, we use DataX, which supports CSV format and readers of many relational databases, and Apache Doris provides a DataX-Doris-Writer.\\n\\n![DataX-Doris-Writer](/images/Midland_3.png)\\n\\n2. We use Flink CDC to synchronize data from source tables. Then we aggregate the real-time metrics utilizing the Materialized View or the Aggregate Model of Apache Doris. Since we only have to process part of the metrics in a real-time manner and we don\'t want to generate too many database connections, we use one Flink job to maintain multiple CDC source tables. This is realized by the multi-source merging and full database sync features of Dinky, or you can implement a Flink DataStream multi-source merging task yourself. It is noteworthy that Flink CDC and Apache Doris support Schema Change.\\n\\n```SQL\\nEXECUTE CDCSOURCE demo_doris WITH (\\n  \'connector\' = \'mysql-cdc\',\\n  \'hostname\' = \'127.0.0.1\',\\n  \'port\' = \'3306\',\\n  \'username\' = \'root\',\\n  \'password\' = \'123456\',\\n  \'checkpoint\' = \'10000\',\\n  \'scan.startup.mode\' = \'initial\',\\n  \'parallelism\' = \'1\',\\n  \'table-name\' = \'ods.ods_*,ods.ods_*\',\\n  \'sink.connector\' = \'doris\',\\n  \'sink.fenodes\' = \'127.0.0.1:8030\',\\n  \'sink.username\' = \'root\',\\n  \'sink.password\' = \'123456\',\\n  \'sink.doris.batch.size\' = \'1000\',\\n  \'sink.sink.max-retries\' = \'1\',\\n  \'sink.sink.batch.interval\' = \'60000\',\\n  \'sink.sink.db\' = \'test\',\\n  \'sink.sink.properties.format\' =\'json\',\\n  \'sink.sink.properties.read_json_by_line\' =\'true\',\\n  \'sink.table.identifier\' = \'${schemaName}.${tableName}\',\\n  \'sink.sink.label-prefix\' = \'${schemaName}_${tableName}_1\'\\n);\\n```\\n\\n3. We use SQL scripts or \\"Shell + SQL\\" scripts, and we perform script lifecycle management. At the ODS layer, we write a general DataX job file and pass parameters for each source table ingestion, instead of writing a DataX job for each source table. In this way, we make things much easier to maintain. We manage the ETL scripts of Apache Doris on DolphinScheduler, where we also conduct version control. In case of any errors in the production environment, we can always rollback.\\n\\n![SQL-script](/images/Midland_4.png)\\n\\n4. After ingesting data with ETL scripts, we create a page in our reporting tool. We assign different privileges to different accounts using SQL, including the privilege of modifying rows, fields, and global dictionary. Apache Doris supports privilege control over accounts, which works the same as that in MySQL. \\n\\n![privilege-control-over-accounts](/images/Midland_5.png)\\n\\nWe also use Apache Doris data backup for disaster recovery, Apache Doris audit logs to monitor SQL execution efficiency, Grafana+Loki for cluster metric alerts, and Supervisor to monitor the daemon processes of node components.\\n\\n## Optimization\\n\\n### 1. Data Ingestion\\n\\nWe use DataX to Stream Load offline data. It allows us to adjust the size of each batch. The Stream Load method returns results synchronously, which meets the needs of our architecture. If we execute asynchronous data import using DolphinScheduler, the system might assume that the script has been executed, and that can cause a messup. If you use a different method, we recommend that you execute `show load` in the shell script, and check the regex filtering status to see if the ingestion succeeds.\\n\\n### 2. Data Model\\n\\nWe adopt the Unique Key model of Apache Doris for most of our tables. The Unique Key model ensures idempotence of data scripts and effectively avoids upstream data duplication. \\n\\n### 3. Reading External Data\\n\\nWe use the Multi-Catalog feature of Apache Doris to connect to external data sources. It allows us to create mappings of external data at the Catalog level.\\n\\n### 4. Query Optimization\\n\\nWe suggest that you put the most frequently used fields of non-character types (such as int and where clauses) in the first 36 bytes, so you can filter these fields within milliseconds in point queries.\\n\\n### 5. Data Dictionary\\n\\nFor us, it is important to create a data dictionary because it largely reduces personnel communication costs, which can be a headache when you have a big team. We use the `information_schema` in Apache Doris to generate a data dictionary. With it, we can quickly grasp the whole picture of the tables and fields and thus increase development efficiency.\\n\\n## Performance\\n\\n**Offline data ingestion time**: Within minutes\\n\\n**Query latency**: For tables containing over 100 million rows, Apache Doris responds to ad-hoc queries within one second, and complicated queries in five seconds.\\n\\n**Resource consumption**: It only takes up a small number of servers to build this data warehouse. The 70% compression ratio of Apache Doris saves us lots of storage resources.\\n\\n## **Experience and Conclusion**\\n\\nActually, before we evolved into our current data architecture, we tried Hive, Spark and Hadoop to build an offline data warehouse. It turned out that Hadoop was overkill for a traditional company like us since we didn\'t have too much data to process. It is important to find the component that suits you most.\\n\\n![old-offline-data warehouse](/images/Midland_6.png)\\n\\n(Our old off-line data warehouse)\\n\\nOn the other hand, to smoothen our  big data transition, we need to make our data platform as simple as possible in terms of usage and maintenance. That\'s why we landed on Apache Doris. It is compatible with MySQL protocol and provides a rich collection of functions so we don\'t have to develop our own UDFs. Also, it is composed of only two types of processes: frontends and backends, so it is easy to scale and track.\\n\\nFind Apache Doris developers on [Slack](https://join.slack.com/t/apachedoriscommunity/shared_invite/zt-2unfw3a3q-MtjGX4pAd8bCGC1UV0sKcw)."},{"id":"/Zipping-up-the-Lambda-Architecture-for-40-Percent-Faster-Performance","metadata":{"permalink":"/blog/Zipping-up-the-Lambda-Architecture-for-40-Percent-Faster-Performance","source":"@site/blog/Zipping-up-the-Lambda-Architecture-for-40-Percent-Faster-Performance.md","title":"Zipping up the lambda architecture for 40% faster performance","description":"Instead of pooling real-time and offline data after they are fully ready for queries, Douyu engineers use Apache Doris to share part of the pre-query computation burden.","date":"2023-05-05T00:00:00.000Z","tags":[{"inline":true,"label":"Best Practice","permalink":"/blog/tags/best-practice"}],"hasTruncateMarker":false,"authors":[{"name":"Tongyang Han","key":null,"page":null}],"frontMatter":{"title":"Zipping up the lambda architecture for 40% faster performance","description":"Instead of pooling real-time and offline data after they are fully ready for queries, Douyu engineers use Apache Doris to share part of the pre-query computation burden.","date":"2023-05-05","author":"Tongyang Han","tags":["Best Practice"],"image":"/images/zipping-up-the-lambda-architecture-for-40-percent-faster-performance.png"},"unlisted":false,"prevItem":{"title":"Building a data warehouse for traditional industry","permalink":"/blog/Building-a-Data-Warehouse-for-Traditional-Industry"},"nextItem":{"title":"Step-by-step guide to building a high-performing risk data mart","permalink":"/blog/Step-by-step-Guide-to-Building-a-High-Performing-Risk-Data-Mart"}},"content":"\x3c!-- \\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\nAuthor: Tongyang Han, Senior Data Engineer at Douyu\\n\\nThe Lambda architecture has been common practice in big data processing. The concept is to separate stream (real time data) and batch (offline data) processing, and that\'s exactly what we did. These two types of data of ours were processed in two isolated tubes before they were pooled together and ready for searches and queries.\\n\\n![Lambda-architecture](/images/Douyu_1.png)\\n\\nThen we run into a few problems:\\n\\n1. **Isolation of real-time and offline data warehouses**\\n   1.  I know this is kind of the essence of Lambda architecture, but that means we could not reuse real-time data since it was not layered as offline data, so further customized development was required.\\n2. **Complex Pipeline from Data Sources to Data Application**\\n   1.  Data had to go through multi-step processing before it reached our data users. As our architecture involved too many components, navigating and maintaining these tech stacks was a lot of work.\\n3. **Lack of management of real-time data sources**\\n   1.  In extreme cases, this worked like a data silo and we had no way to find out whether the ingested data was duplicated or reusable.\\n\\nSo we decided to \\"zip up\\" the Lambda architecture a little bit. By \\"zipping up\\", I mean to introduce an OLAP engine that is capable of processing, storing, and analyzing data, so real-time data and offline data converge a little earlier than they used to. It is not a revolution of Lambda, but a minor change in the choice of components, which made our real-time data processing 40% faster.\\n\\n## **Zipping up Lambda Architecture**\\n\\nI am going to elaborate on how this is done using our data tagging process as an example.\\n\\nPreviously, our offline tags were produced by the data warehouse, put into a flat table, and then written in **HBase**, while real-time tags were produced by **Flink**, and put into **HBase** directly. Then **Spark** would work as the computing engine.\\n\\n![HBase-Redis-Spark](/images/Douyu_2.png)\\n\\nThe problem with this stemmed from the low computation efficiency of **Flink** and **Spark**. \\n\\n- **Real-time tag production**: When computing real-time tags that involve data within a long time range, Flink did not deliver stable performance and consumed more resources than expected. And when a task failed, it would take a really long time for checkpoint recovery.\\n- **Tag query**: As a tag query engine, Spark could be slow.\\n\\nAs a solution, we replaced **HBase** and **Spark** with **Apache Doris**, a real-time analytic database, and moved part of the computational logic of the foregoing wide-time-range real-time tags from **Flink** to **Apache Doris**.\\n\\n![Apache-Doris-Redis](/images/Douyu_3.png)\\n\\nInstead of putting our flat tables in HBase, we place them in Apache Doris. These tables are divided into partitions based on time sensitivity. Offline tags will be updated daily while real-time tags will be updated in real time. We organize these tables in the Aggregate Model of Apache Doris, which allows partial update of data.\\n\\nInstead of using Spark for queries, we parse the query rules into SQL for execution in Apache Doris. For pattern matching, we use Redis to cache the hot data from Apache Doris, so the system can respond to such queries much faster.\\n\\n![Real-time-and-offline-data-processing-in-Apache-Doris](/images/Douyu_4.png)\\n\\n## **Computational Pipeline of Wide-Time-Range Real-Time Tags**\\n\\nIn some cases, the computation of wide-time-range real-time tags entails the aggregation of historical (offline) data with real-time data. The following figure shows our old computational pipeline for these tags. \\n\\n![offline-data-processing-link](/images/Douyu_5.png)\\n\\nAs you can see, it required multiple tasks to finish computing one real-time tag. Also, in complicated aggregations that involve a collection of aggregation operations, any improper resource allocation could lead to back pressure or waste of resources. This adds to the difficulty of task scheduling. The maintenance and stability guarantee of such a long pipeline could be an issue, too.\\n\\nTo improve on that, we decided to move such aggregation workload to Apache Doris.\\n\\n![real-time-data-processing-link](/images/Douyu_6.png)\\n\\nWe have around 400 million customer tags in our system, and each customer is attached with over 300 tags. We divide customers into more than 10,000 groups, and we have to update 5000 of them on a daily basis. The above improvement has sped up the computation of our wide-time-range real-time queries by **40%**.\\n\\n## Overwrite\\n\\nTo atomically replace data tables and partitions in Apache Doris, we customized the [Doris-Spark-Connector](https://github.com/apache/doris-spark-connector), and added an \\"Overwrite\\" mode to the Connector.\\n\\nWhen a Spark job is submitted, Apache Doris will call an interface to fetch information of the data tables and partitions.\\n\\n- If it is a non-partitioned table, we create a temporary table for the target table, ingest data into it, and then perform atomic replacement. If the data ingestion fails, we clear the temporary table;\\n- If it is a dynamic partitioned table, we create a temporary partition for the target partition, ingest data into it, and then perform atomic replacement. If the data ingestion fails, we clear the temporary partition;\\n- If it is a non-dynamic partitioned table, we need to extend the Doris-Spark-Connector parameter configuration first. Then we create a temporary partition and take steps as above.\\n\\n##  Conclusion\\n\\nOne prominent advantage of Lambda architecture is the stability it provides. However, in our practice, the processing of real-time data and offline data sometimes intertwines. For example, the computation of certain real-time tags requires historical (offline) data. Such interaction becomes a root cause of instability. Thus, instead of pooling real-time and offline data after they are fully ready for queries, we use an OLAP engine to share part of the pre-query computation burden and make things faster, simpler, and more cost-effective."},{"id":"/Step-by-step-Guide-to-Building-a-High-Performing-Risk-Data-Mart","metadata":{"permalink":"/blog/Step-by-step-Guide-to-Building-a-High-Performing-Risk-Data-Mart","source":"@site/blog/Step-by-step-Guide-to-Building-a-High-Performing-Risk-Data-Mart.md","title":"Step-by-step guide to building a high-performing risk data mart","description":"The key step is to leverage the Multi Catalog feature of Apache Doris to unify the heterogenous data sources. This removed a lot of our performance bottlenecks.","date":"2023-04-20T00:00:00.000Z","tags":[{"inline":true,"label":"Best Practice","permalink":"/blog/tags/best-practice"}],"hasTruncateMarker":false,"authors":[{"name":"Jacob Chow","key":null,"page":null}],"frontMatter":{"title":"Step-by-step guide to building a high-performing risk data mart","description":"The key step is to leverage the Multi Catalog feature of Apache Doris to unify the heterogenous data sources. This removed a lot of our performance bottlenecks.","date":"2023-04-20","author":"Jacob Chow","tags":["Best Practice"],"image":"/images/step-by-step-guide-to-building-a-high-performing-risk-data-mart.png"},"unlisted":false,"prevItem":{"title":"Zipping up the lambda architecture for 40% faster performance","permalink":"/blog/Zipping-up-the-Lambda-Architecture-for-40-Percent-Faster-Performance"},"nextItem":{"title":"How We increased database query concurrency by 20 times","permalink":"/blog/How-We-Increased-Database-Query-Concurrency-by-20-Times"}},"content":"\x3c!-- \\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\nPursuing data-driven management at a consumer financing company, we aim to serve four needs in our data platform development: monitoring and alerting, query and analysis, dashboarding, and data modeling. For these purposes, we built our data processing architecture based on Greenplum and CDH. The most essential part of it is the risk data mart. \\n\\n## Risk Data Mart:  Apache Hive\\n\\nI will walk you through how the risk data mart works following the data flow: \\n\\n1. Our **business data** is imported into **Greenplum** for real-time analysis to generate BI reports. Part of this data also goes into Apache Hive for queries and modeling analysis. \\n2. Our **risk control variables** are updated into **Elasticsearch** in real time via message queues, while Elasticsearch ingests data into Hive for analysis, too.\\n3. The **risk management decision data** is passed from **MongoDB** to Hive for risk control analysis and modeling.\\n\\nSo these are the three data sources of our risk data mart.\\n\\n![risk-data-mart](/images/RDM_1.png)\\n\\nThis whole architecture is built with CDH 6.0. The workflows in it can be divided into real-time data streaming and offline risk analysis.\\n\\n- **Real-time data streaming**: Real-time data from Apache Kafka will be cleaned by Apache Flink, and then written into Elasticsearch. Elasticsearch will aggregate part of the data it receives and send it for reference in risk management. \\n- **Offline risk analysis**: Based on the CDH solution and utilizing Sqoop, we ingest data from Greenplum in an offline manner. Then we put this data together with the third-party data from MongoDB. Then, after data cleaning, we pour all this data into Hive for daily batch processing and data queries.\\n\\nTo give a brief overview, these are the components that support the four features of our data processing platform:\\n\\n![features-of-a-data-processing-platform](/images/RDM_2.png)\\n\\nAs you see, Apache Hive is central to this architecture. But in practice, it takes minutes for Apache Hive to execute analysis, so our next step is to increase query speed.\\n\\n### What are Slowing Down Our Queries?\\n\\n1. **Huge data volume in external tables**\\n\\nOur Hive-based data mart is now carrying more than 300 terabytes of data. That\'s about 20,000 tables and 5 million fields. To put them all in external tables is maintenance-intensive. Plus, data ingestion can be a big headache.\\n\\n1. **Big flat tables**\\n\\nDue to the complexity of the rule engine in risk management, our company invests a lot in the derivation of variables. In some dimensions, we have thousands of variables or even more. As a result, a few of the frequently used flat tables in Hive have over 3000 fields. So you can imagine how time consuming these queries can be.\\n\\n1. **Unstable interface**\\n\\nResults produced by daily offline batch processing will be regularly sent to our Elasticsearch clusters. (The data volume in these updates is huge, and the call of interface can get expired.) This process might cause high I/O and introduce garbage collection jitter, and further leads to unstable interface services. \\n\\nIn addition, since our risk control analysts and modeling engineers are using Hive with Spark, the expanding data architecture is also dragging down query performance.\\n\\n## A Unified Query Gateway\\n\\nWe wanted a unified gateway to manage our heterogenous data sources. That\'s why we introduced Apache Doris.\\n\\n![unified-query-gateway](/images/RDM_3.png)\\n\\nBut doesn\'t that make things even more complicated? Actually, no.\\n\\nWe can connect various data sources to Apache Doris and simply conduct queries on it. This is made possible by the **Multi-Catalog** feature of Apache Doris: It can interface with various data sources, including datalakes like Apache Hive, Apache Iceberg, and Apache Hudi, and databases like MySQL, Elasticsearch, and Greenplum. That happens to cover our toolkit. \\n\\nWe create Elasticsearch Catalog and Hive Catalog in Apache Doris. These catalogs map to the external data in Elasticsearch and Hive, so we can conduct federated queries across these data sources using Apache Doris as a unified gateway. Also, we use the [Spark-Doris-Connector](https://github.com/apache/doris-spark-connector) to allow data communication between Spark and Doris. So basically, we replace Apache Hive with Apache Doris as the central hub of our data architecture. \\n\\n![Apache-Doris-as-center-of-data-architecture](/images/RDM_4.png)\\n\\nHow does that affect our data processing efficiency?\\n\\n- **Monitoring & Alerting**: This is about real-time data querying. We access our real-time data in Elasticsearch clusters using Elasticsearch Catalog in Apache Doris. Then we perform queries directly in Apache Doris. It is able to return results within seconds, as opposed to the minute-level response time when we used Hive.\\n- **Query & Analysis**: As I said, we have 20,000 tables in Hive so it wouldn\'t make sense to map all of them to external tables in Hive. That would mean a hell of maintenance. Instead, we utilize the Multi Catalog feature of Apache Doris 1.2. It enables data mapping at the catalog level, so we can simply create one Hive Catalog in Doris before we can conduct queries. This separates query operations from the daily batching processing workload in Hive, so there will be less resource conflict.\\n- **Dashboarding**: We use Tableau and Doris to provide dashboard services. This reduces the query response time to seconds and milliseconds, compared with the several minutes back in the \\"Tableau + Hive\\" days.\\n- **Modeling**: We use Spark and Doris for aggregation modeling. The Spark-Doris-Connector allows mutual synchronization of data, so data from Doris can also be used in modeling for more accurate analysis.\\n\\n### **Cluster Monitoring in Production Environment**\\n\\nWe tested this new architecture in our production environment. We built two clusters.\\n\\n**Configuration**:\\n\\nProduction cluster: 4 frontends + 8 backends, m5d.16xlarge\\n\\nBackup cluster: 4 frontends + 4 backends, m5d.16xlarge\\n\\nThis is the monitoring board: \\n\\n![cluster-monitoring-board](/images/RDM_5.png)\\n\\nAs is shown, the queries are fast. We expected that it would take at least 10 nodes but in real cases, we mainly conduct queries via Catalogs, so we can handle this with a relatively small cluster size. The compatibility is good, too. It doesn\'t rock the rest of our existing system.\\n\\n## Guide to Faster Data Integration\\n\\nTo accelerate the regular data ingestion from Hive to Apache Doris 1.2.2, we have a solution that goes as follows:\\n\\n![faster-data-integration](/images/RDM_6.png)\\n\\n**Main components:**\\n\\n- DolphinScheduler 3.1.4\\n- SeaTunnel 2.1.3\\n\\nWith our current hardware configuration, we use the Shell script mode of DolphinScheduler and call the SeaTunnel script on a regular basis. This is the configuration file of the data synchronization tasks:\\n\\n```undefined\\n  env{\\n  spark.app.name = \\"hive2doris-template\\"\\n  spark.executor.instances = 10\\n  spark.executor.cores = 5\\n  spark.executor.memory = \\"20g\\"\\n}\\nspark {\\n  spark.sql.catalogImplementation = \\"hive\\"\\n}\\nsource {\\n  hive {\\n    pre_sql = \\"select * from ods.demo_tbl where dt=\'2023-03-09\'\\"\\n    result_table_name = \\"ods_demo_tbl\\"\\n  }\\n}\\n \\ntransform {\\n}\\n \\nsink {\\n  doris {\\n      fenodes = \\"192.168.0.10:8030,192.168.0.11:8030,192.168.0.12:8030,192.168.0.13:8030\\"\\n      user = root\\n      password = \\"XXX\\"\\n      database = ods\\n      table = ods_demo_tbl\\n      batch_size = 500000\\n      max_retries = 1\\n      interval = 10000\\n      doris.column_separator = \\"\\\\t\\"\\n    }\\n}\\n```\\n\\nThis solution consumes less resources and memory but brings higher performance in queries and data ingestion.\\n\\n1. **Less storage costs**\\n\\n**Before**: The original table in Hive had 500 fields. It was divided into partitions by day, with 150 million pieces of data per partition. It takes **810G** to store in HDFS.\\n\\n**After**: For data synchronization, we call Spark on YARN using SeaTunnel. It can be finished within 40 minutes, and the ingested data only takes up **270G** of storage space.\\n\\n1. **Less memory usage & higher performance in queries**\\n\\n**Before**: For a GROUP BY query on the foregoing table in Hive, it occupied 720 Cores and 1.44T in YARN, and took a response time of **162 seconds**. \\n\\n**After**: We perform an aggregate query using Hive Catalog in Doris, `set exec_mem_limit=16G`, and receive the result after **58.531 seconds**. We also try and put the table in Doris and conduct the same query in Doris itself, that only takes **0.828 seconds**.\\n\\nThe corresponding statements are as follows:\\n\\n- Query in Hive, response time: 162 seconds\\n\\n```SQL\\nselect count(*),product_no   FROM ods.demo_tbl where dt=\'2023-03-09\'\\ngroup by product_no;\\n```\\n\\n- Query in Doris using Hive Catalog, response time: 58.531 seconds\\n\\n```SQL\\nset exec_mem_limit=16G\uFF1B\\nselect count(*),product_no   FROM hive.ods.demo_tbl where dt=\'2023-03-09\'\\ngroup by product_no;\\n```\\n\\n- Query in Doris directly, response time: 0.828 seconds\\n\\n```SQL\\nselect count(*),product_no   FROM ods.demo_tbl where dt=\'2023-03-09\'\\ngroup by product_no;\\n```\\n\\n1. **Faster data ingestion**\\n\\n**Before**: The original table in Hive had 40 fields. It was divided into partitions by day, with 1.1 billion pieces of data per partition. It takes **806G** to store in HDFS.\\n\\n**After**: For data synchronization, we call Spark on YARN using SeaTunnel. It can be finished within 11 minutes (100 million pieces per minute ), and the ingested data only takes up **378G** of storage space.\\n\\n![faster-data-ingestion](/images/RDM_7.png)\\n\\n## Summary\\n\\nThe key step to building a high-performing risk data mart is to leverage the Multi Catalog feature of Apache Doris to unify the heterogenous data sources. This not only increases our query speed but also solves a lot of the problems coming with our previous data architecture.\\n\\n1. Deploying Apache Doris allows us to decouple daily batch processing workloads with ad-hoc queries, so they don\'t have to compete for resources. This reduces the query response time from minutes to seconds.\\n2. We used to build our data ingestion interface based on Elasticsearch clusters, which could lead to garbage collection jitter when transferring large batches of offline data. When we stored the interface service dataset on Doris, no jitter was found during data writing and we were able to transfer 10 million rows within 10 minutes.\\n3. Apache Doris has been optimizing itself in many scenarios including flat tables. As far as we know, compared with ClickHouse, Apache Doris 1.2 is twice as fast in SSB-Flat-table benchmark and dozens of times faster in TPC-H benchmark.\\n4. In terms of cluster scaling and updating, we used to suffer from a big window of restoration time after configuration revision. But Doris supports hot swap and easy scaling out, so we can reboot nodes within a few seconds and minimize interruption to users caused by cluster scaling.\\n\\n(One last piece of advice for you: If you encounter any problems with deploying Apache Doris, don\'t hesitate to contact the Doris community for help, they and a bunch of SelectDB engineers will be more than happy to make your adaption journey quick and easy.)"},{"id":"/How-We-Increased-Database-Query-Concurrency-by-20-Times","metadata":{"permalink":"/blog/How-We-Increased-Database-Query-Concurrency-by-20-Times","source":"@site/blog/How-We-Increased-Database-Query-Concurrency-by-20-Times.md","title":"How We increased database query concurrency by 20 times","description":"In the upcoming Apache Doris 2.0, we have optimized it for high-concurrency point queries. Long story short, it can achieve over 30,000 QPS for a single node.","date":"2023-04-14T00:00:00.000Z","tags":[{"inline":true,"label":"Tech Sharing","permalink":"/blog/tags/tech-sharing"}],"hasTruncateMarker":false,"authors":[{"name":"Apache Doris","key":null,"page":null}],"frontMatter":{"title":"How We increased database query concurrency by 20 times","description":"In the upcoming Apache Doris 2.0, we have optimized it for high-concurrency point queries. Long story short, it can achieve over 30,000 QPS for a single node.","date":"2023-04-14","author":"Apache Doris","tags":["Tech Sharing"],"image":"/images/how-we-increased-database-query-concurrency-by-20-times.jpg"},"unlisted":false,"prevItem":{"title":"Step-by-step guide to building a high-performing risk data mart","permalink":"/blog/Step-by-step-Guide-to-Building-a-High-Performing-Risk-Data-Mart"},"nextItem":{"title":"Apache Doris announced the official release of version 1.2.3","permalink":"/blog/release-note-1.2.3"}},"content":"\x3c!-- \\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\nA unified analytic database is the holy grail for data engineers, but what does it look like specifically? It should evolve with the needs of data users.\\n\\nVertically, companies now have an ever enlarging pool of data and expect a higher level of concurrency in data processing. Horizontally, they require a wider range of data analytics services. Besides traditional OLAP scenarios such as statistical reporting and ad-hoc queries, they are also leveraging data analysis in recommender systems, risk control, customer tagging and profiling, and IoT.\\n\\nAmong all these data services, point queries are the most frequent operations conducted by data users. Point query means to retrieve one or several rows from the database based on the Key. A point query only returns a small piece of data, such as the details of a shopping order, a transaction, a consumer profile, a product description, logistics status, and so on. Sounds easy, right? But the tricky part is, **a database often needs to handle tens of thousands of point queries at a time and respond to all of them in milliseconds**.\\n\\nMost current OLAP databases are built with a columnar storage engine to process huge data volumes. They take pride in their high throughput, but often underperform in high-concurrency scenarios. As a complement, many data engineers invite Key-Value stores like Apache HBase for point queries, and Redis as a cache layer to ease the burden. The downside is redundant storage and high maintenance costs.\\n\\nSince Apache Doris was born, we have been striving to make it a unified database for data queries of all sizes, including ad-hoc queries and point queries. Till now, we have already taken down the monster of high-throughput OLAP scenarios. In the upcoming Apache Doris 2.0, we have optimized it for high-concurrency point queries. Long story short, it can achieve over 30,000 QPS for a single node. \\n\\n## Five Ways to Accelerate High-Concurrency Queries\\n\\nHigh-concurrency queries are thorny because you need to handle high loads with limited system resources. That means you have to reduce the CPU, memory and I/O overheads of a single SQL as much as possible. The key is to minimize the scanning of underlying data and follow-up computing. \\n\\nApache Doris uses five methods to achieve higher QPS.\\n\\n### Partioning and Bucketing\\n\\nApache Doris shards data into a two-tiered structure: Partition and Bucket. You can use time information as the Partition Key. As for bucketing, you distribute the data into various nodes after data hashing. A wise bucketing plan can largely increase concurrency and throughput in data reading. \\n\\nThis is an example:\\n\\n```SQL\\nselect * from user_table where id = 5122 and create_date = \'2022-01-01\'\\n```\\n\\nIn this case, the user has set 10 buckets. `create_date` is the Partition Key and `id` is the Bucket Key. After dividing the data into partitions and buckets, the system only needs to scan one bucket in one partition before it can locate the needed data. This is a huge time saver.\\n\\n### Index\\n\\nApache Doris uses various data indexes to speed up data reading and filtering, including smart indexes and secondary indexes. Smart indexes are auto-generated by Doris upon data ingestion, which requires no action from the user\'s side. \\n\\nThere are two types of smart indexes:\\n\\n- **Sorted Index**: Apache Doris stores data in an orderly way. It creates a sorted index for every 1024 rows of data. The Key in the index is the value of the sorted column in the first row of the current 1024 rows. If the query involves the sorted column, the system will locate the first row of the relevant 1024 row group and start scanning there.\\n- **ZoneMap Index**: These are indexes on the Segment and Page level. The maximum and minimum values of each column within a Page will be recorded, so are those within a Segment. Hence, in equivalence queries and range queries, the system can narrow down the filter range with the help of the MinMax indexes.\\n\\nSecondary indexes are created by users. These include Bloom Filter indexes, Bitmap indexes, [Inverted indexes](https://doris.apache.org/docs/dev/data-table/index/inverted-index/), and [NGram Bloom Filter indexes](https://doris.apache.org/docs/dev/data-table/index/ngram-bloomfilter-index/). (If you are interested, I will go into details about them in future articles.)\\n\\nExample:\\n\\n```SQL\\nselect * from user_table where id > 10 and id < 1024\\n```\\n\\nSuppose that the user has designated `id` as the Key during table creation, the data will be sorted by `id` on Memtable and the disks. So any queries involving `id` as a filter condition will be executed much faster with the aid of sorted indexes. Specifically, the data in storage will be put into multiple ranges based on `id`, and the system will implement binary search to locate the exact range according to the sorted indexes. But that could still be a large range since the sorted indexes are sparse. You can further narrow it down based on ZoneMap indexes, Bloom Filter indexes, and Bitmap indexes. \\n\\nThis is another way to reduce data scanning and improve overall concurrency of the system.\\n\\n### Materialized View\\n\\nThe idea of materialized view is to trade space for time: You execute pre-computation with pre-defined SQL statements, and perpetuate the results in a table that is visible to users but occupies some storage space. In this way, Apache Doris can respond much faster to queries for aggregated data and breakdown data and those involve the matching of sorted indexes once it hits a materialized view. This is a good way to lessen computation, improve query performance, and reduce resource consumption.\\n\\n```SQL\\n// For an aggregation query, the system reads the pre-aggregated columns in the materialized view.\\n\\ncreate materialized view store_amt as select store_id, sum(sale_amt) from sales_records group by store_id;\\nSELECT store_id, sum(sale_amt) FROM sales_records GROUP BY store_id;\\n\\n// For a query where k3 matches the sorted column in the materialized view, the system directly performs the query on the materialized view. \\n\\nCREATE MATERIALIZED VIEW mv_1 as SELECT k3, k2, k1 FROM tableA ORDER BY k3;\\nselect k1, k2, k3 from table A where k3=3;\\n```\\n\\n### Runtime Filter\\n\\nApart from filtering data by indexes, Apache Doris has a dynamic filtering mechanism: Runtime Filter. \\n\\nIn multi-table Join queries, the left table is usually called ProbeTable while the right one is called BuildTable, with the former much bigger than the latter. In query execution, firstly, the system reads the right table and creates a HashTable (Build) in the memory. Then, it starts reading the left table row by row, during which it also compares data between the left table and the HashTable and returns the matched data (Probe). \\n\\nSo what\'s new about that in Apache Doris? During the creation of HashTable, Apache Doris generates a filter for the columns. It can be a Min/Max filter or an IN filter. Then it pushes down the filter to the left table, which can use the filter to screen out data and thus reduces the amount of data that the Probe node has to transfer and compare. \\n\\nThis is how the Runtime Filter works. In most Join queries, the Runtime Filter can be automatically pushed down to the most underlying scan nodes or to the distributed Shuffle Join. In other words, Runtime Filter is able to reduce data reading and shorten response time for most Join queries.\\n\\n### TOP-N Optimization\\n\\nTOP-N query is a frequent scenario in data analysis. For example, users want to fetch the most recent 100 orders, or the 5 highest/lowest priced products. The performance of such queries determines the quality of real-time analysis. For them, Apache Doris implements TOP-N optimization. Here is how it goes:\\n\\n1. Apache Doris reads the sorted fields and query fields from the Scanner layer, reserves only the TOP-N pieces of data by means of Heapsort, updates the real-time TOP-N results as it continues reading, and dynamically pushes them down to the Scanner. \\n2. Combing the received TOP-N range and the indexes, the Scanner can skip a large proportion of irrelevant files and data chunks and only read a small number of rows.\\n3. Queries on flat tables usually mean the need to scan massive data, but TOP-N queries only retrieve a small amount of data. The strategy here is to divide the data reading process into two stages. In stage one, the system sorts the data based on a few columns (sorted column, or condition column) and locates the TOP-N rows. In stage two, it fetches the TOP-N rows of data after data sorting, and then it retrieves the target data according to the row numbers. \\n\\nTo sum up, Apache Doris prunes the data that needs to be read and sorted, and thus substantially reduces consumption of I/O, CPU, and memory resources.\\n\\nIn addition to the foregoing five methods, Apache Doris also improves concurrency by SQL Cache, Partition Cache, and a variety of Join optimization techniques.\\n\\n## How We Bring Concurrency to the Next Level\\n\\nBy adopting the above methods, Apache Doris was able to achieve thousands of QPS per node. However, in scenarios requiring tens of thousands of QPS, it was still bottlenecked by several issues:\\n\\n- With Doris\' columnar storage engine, it was inconvenient to read rows. In flat table models, columnar storage could result in much larger I/O usage.\\n- The execution engine and query optimizer of OLAP databases were sometimes too complicated for simple queries (point queries, etc.). Such queries needed to be processed with a shorter pipeline, which should be considered in query planning.\\n- FE modules of Doris, implementing Java, were responsible for interfacing with SQL requests and parsing query plans. These processes could produce high CPU overheads in high-concurrency scenarios.\\n\\nWe optimized Apache Doris to solve these problems. ([Pull Request on Github](https://github.com/apache/doris/pull/15491))\\n\\n \\n\\n### Row Storage Format\\n\\nAs we know, row storage is much more efficient when the user only queries for a single row of data. So we introduced row storage format in Apache Doris 2.0. Users can enable row storage by specifying the following property in the table creation statement.\\n\\n```SQL\\n\\"store_row_column\\" = \\"true\\"\\n```\\n\\nWe chose JSONB as the encoding format for row storage for three reasons:\\n\\n- **Flexible schema change**: If a user has added or deleted a field, or modified the type of a field, these changes must be updated in row storage in real time. So we choose to adopt the JSONB format and encode columns into JSONB fields. This makes changes in fields very easy.\\n- **High performance**:  Accessing rows in row-oriented storage is much faster than doing that in columnar storage, and it requires much less disk access in high-concurrency scenarios. Also, in some cases, you can map the column ID to the corresponding JSONB value so you can quickly access a certain column.\\n- **Less storage space**: JSONB is a compacted binary format. It consumes less space on the disk and is more cost-effective.\\n\\nIn the storage engine, row storage will be stored as a hidden column (DORIS_ROW_STORE_COL). During Memtable Flush, the columns will be encoded into JSONB and cached into this hidden column. In data reading, the system uses the Column ID to locate the column, finds the target row based on the row number, and then deserializes the relevant columns.\\n\\n### Short-Circuit\\n\\nNormally, an SQL statement is executed in three steps:\\n\\n1.  SQL Parser parses the statement to generate an abstract syntax tree (AST).\\n2.  The Query Optimizer produces an executable plan.\\n3.  Execute the plan and return the results.\\n\\nFor complex queries on massive data, it is better to follow the plan created by the Query Optimizer. However, for high-concurrency point queries requiring low latency, that plan is not only unnecessary but also brings extra overheads. That\'s why we implement a short-circuit plan for point queries. \\n\\n![short-circuit-plan](/images/high-concurrency_1.png)\\n\\nOnce the FE receives a point query request, a short-circuit plan will be produced. It is a lightweight plan that involves no equivalent transformation, logic optimization or physical optimization. Instead, it conducts some basic analysis on the AST, creates a fixed plan accordingly, and finds ways to reduce overhead of the optimizer.\\n\\nFor a simple point query involving primary keys, such as `select * from tbl where pk1 = 123 and pk2 = 456`, since it only involves one single Tablet, it is better to use a lightweight RPC interface for interaction with the Storage Engine. This avoids the creation of a complicated Fragment Plan and eliminates the performance overhead brought by the scheduling under the MPP query framework.\\n\\nDetails of the RPC interface are as follows:\\n\\n```Java\\nmessage PTabletKeyLookupRequest {\\n    required int64 tablet_id = 1;\\n    repeated KeyTuple key_tuples = 2;\\n    optional Descriptor desc_tbl = 4;\\n    optional ExprList  output_expr = 5;\\n}\\n\\nmessage PTabletKeyLookupResponse {\\n    required PStatus status = 1;\\n    optional bytes row_batch = 5;\\n    optional bool empty_batch = 6;\\n}\\nrpc tablet_fetch_data(PTabletKeyLookupRequest) returns (PTabletKeyLookupResponse);\\n```\\n\\n`tablet_id` is calculated based on the primary key column, while `key_tuples` is the string format of the primary key. In this example, the `key_tuples` is similar to [\'123\', \'456\']. As BE receives the request, `key_tuples` will be encoded into primary key storage format. Then, it will locate the corresponding row number of the Key in the Segment File with the help of the primary key index, and check if that row exists in `delete bitmap`. If it does, the row number will be returned; if not, the system returns NotFound. The returned row number will be used for point query on `__DORIS_ROW_STORE_COL__`. That means we only need to locate one row in that column, fetch the original value of the JSONB format, and deserialize it.\\n\\n### Prepared Statement\\n\\nIn high-concurrency queries, part of the CPU overhead comes from SQL analysis and parsing in FE. To reduce such overhead, in FE, we provide prepared statements that are fully compatible with MySQL protocol. With prepared statements, we can achieve a four-time performance increase for primary key point queries.\\n\\n![prepared-statement-map](/images/high-concurrency_2.png)\\n\\nThe idea of prepared statements is to cache precomputed SQL and expressions in HashMap in memory, so they can be directly used in queries when applicable.\\n\\nPrepared statements adopt MySQL binary protocol for transmission. The protocol is implemented in the mysql_row_buffer.[h|cpp] file, and uses MySQL binary encoding. Under this protocol, the client (for example, JDBC Client) sends a pre-compiled statement to FE via `PREPARE` MySQL Command. Next, FE will parse and analyze the statement and cache it in the HashMap as shown in the figure above. Next, the client, using `EXECUTE` MySQL Command, will replace the placeholder, encode it into binary format, and send it to FE. Then, FE will perform deserialization to obtain the value of the placeholder, and generate query conditions.\\n\\n![prepared-statement-execution](/images/high-concurrency_3.png)\\n\\nApart from caching prepared statements in FE, we also cache reusable structures in BE. These structures include pre-allocated computation blocks, query descriptors, and output expressions. Serializing and deserializing these structures often cause a CPU hotspot, so it makes more sense to cache them. The prepared statement for each query comes with a UUID named CacheID. So when BE executes the point query, it will find the corresponding class based on the CacheID, and then reuse the structure in computation.\\n\\n \\n\\nThe following example demonstrates how to use a prepared statement in JDBC:\\n\\n1. Set a JDBC URL and enable prepared statement at the server end.\\n\\n```shell\\nurl = jdbc:mysql://127.0.0.1:9030/ycsb?useServerPrepStmts=true\\n```\\n\\n1. Use a prepared statement.\\n\\n```Java\\n// Use `?` as placeholder, reuse readStatement.\\nPreparedStatement readStatement = conn.prepareStatement(\\"select * from tbl_point_query where key = ?\\");\\n...\\nreadStatement.setInt(1234);\\nResultSet resultSet = readStatement.executeQuery();\\n...\\nreadStatement.setInt(1235);\\nresultSet = readStatement.executeQuery();\\n...\\n```\\n\\n### Row Storage Cache\\n\\nApache Doris has a Page Cache feature, where each page caches the data of one column. \\n\\n![page-cache](/images/high-concurrency_4.png)\\n\\nAs mentioned above, we have introduced row storage in Doris. The problem with this is, one row of data consists of multiple columns, so in the case of big queries, the cached data might be erased. Thus, we also introduced row cache to increase row cache hit rate.\\n\\nRow cache reuses the LRU Cache mechanism in Apache Doris. When the caching starts, the system will initialize a threshold value. If that threshold is hit, the old cached rows will be phased out. For a primary key query statement, the performance gap between cache hit and cache miss can be huge (we are talking about dozens of times less disk I/O and memory access here). So the introduction of row cache can remarkably enhance point query performance.\\n\\n![row-cache](/images/high-concurrency_5.png)\\n\\nTo enable row cache, you can specify the following configuration in BE:\\n\\n```JSON\\ndisable_storage_row_cache=false // This specifies whether to enable row cache; it is set to false by default.\\nrow_cache_mem_limit=20% // This specifies the percentage of row cache in the memory; it is set to 20% by default.\\n```\\n\\n## Benchmark Performance\\n\\nWe tested Apache Doris with YCSB (Yahoo! Cloud Serving Benchmark) to see how all these optimizations work.\\n\\n**Configurations and data size:**\\n\\n- Machines: a single 16 Core 64G cloud server with 4\xd71T hard drives\\n- Cluster size: 1 Frontend + 2 Backends\\n- Data volume: 100 million rows of data, with each row taking 1KB to store; preheated\\n- Table schema and query statement:\\n\\n```JavaScript\\n// Table creation statement:\\n\\nCREATE TABLE `usertable` (\\n  `YCSB_KEY` varchar(255) NULL,\\n  `FIELD0` text NULL,\\n  `FIELD1` text NULL,\\n  `FIELD2` text NULL,\\n  `FIELD3` text NULL,\\n  `FIELD4` text NULL,\\n  `FIELD5` text NULL,\\n  `FIELD6` text NULL,\\n  `FIELD7` text NULL,\\n  `FIELD8` text NULL,\\n  `FIELD9` text NULL\\n) ENGINE=OLAP\\nUNIQUE KEY(`YCSB_KEY`)\\nCOMMENT \'OLAP\'\\nDISTRIBUTED BY HASH(`YCSB_KEY`) BUCKETS 16\\nPROPERTIES (\\n\\"replication_allocation\\" = \\"tag.location.default: 1\\",\\n\\"in_memory\\" = \\"false\\",\\n\\"persistent\\" = \\"false\\",\\n\\"storage_format\\" = \\"V2\\",\\n\\"enable_unique_key_merge_on_write\\" = \\"true\\",\\n\\"light_schema_change\\" = \\"true\\",\\n\\"store_row_column\\" = \\"true\\",\\n\\"disable_auto_compaction\\" = \\"false\\"\\n);\\n\\n// Query statement:\\n\\nSELECT * from usertable WHERE YCSB_KEY = ?\\n```\\n\\nWe run the test with the optimizations (row storage, short-circuit, and prepared statement) enabled, and then did it again with all of them disabled. Here are the results:\\n\\n![performance-before-and-after-concurrency-optimization](/images/high-concurrency_6.png)\\n\\nWith optimizations enabled, **the average query latency decreased by a whopping 96%, the 99th percentile latency was only 1/28 of that without optimizations, and it has achieved a query concurrency of over 30,000 QPS.** This is a huge leap in performance and an over 20-time increase in concurrency.\\n\\n## Best Practice\\n\\nIt should be noted that these optimizations for point queries are implemented in the Unique Key model of Apache Doris, and you should enable Merge-on-Write and Light Schema Change for this model.\\n\\nThis is a table creation statement example for point queries:\\n\\n```undefined\\nCREATE TABLE `usertable` (\\n  `USER_KEY` BIGINT NULL,\\n  `FIELD0` text NULL,\\n  `FIELD1` text NULL,\\n  `FIELD2` text NULL,\\n  `FIELD3` text NULL\\n) ENGINE=OLAP\\nUNIQUE KEY(`USER_KEY`)\\nCOMMENT \'OLAP\'\\nDISTRIBUTED BY HASH(`USER_KEY`) BUCKETS 16\\nPROPERTIES (\\n\\"enable_unique_key_merge_on_write\\" = \\"true\\",\\n\\"light_schema_change\\" = \\"true\\",\\n\\"store_row_column\\" = \\"true\\",\\n); \\n```\\n\\n**Note:**\\n\\n- Enable `light_schema_change` to support JSONB row storage for encoding ColumnID\\n- Enable `store_row_column` to store row storage format\\n\\nFor a primary key-based point query like the one below, after table creation, you can use row storage and short-circuit execution to improve performance to a great extent.\\n\\n```SQL\\nselect * from usertable where USER_KEY = xxx;\\n```\\n\\nTo further unleash performance, you can apply prepared statement. If you have enough memory space, you can also enable row cache in the BE configuration.\\n\\n## Conclusion\\n\\nIn high-concurrency scenarios, Apache Doris realizes over 30,000 QPS per node after optimizations including row storage, short-circuit, prepared statement, and row cache. Also, Apache Doris is easily scaled out since it is built on MPP architecture, on top of which you can scale it up by upgrading the hardware and machine configuration. This is how Apache Doris manages to achieve both high throughput and high concurrency. It allows you to deal with various data analytic workloads on one single platform and experience quick data analytics for various scenarios. Thanks to the great efforts of the Apache Doris community and a group of excellent SelectDB engineers, Apache Doris 2.0 is about to be released soon."},{"id":"/release-note-1.2.3","metadata":{"permalink":"/blog/release-note-1.2.3","source":"@site/blog/release-note-1.2.3.md","title":"Apache Doris announced the official release of version 1.2.3","description":"Dear community, Apache Doris 1.2.3 is now available, with several enhancements and bug fixes based on 1.2.0\uFF0Cenabling smoother user experience.","date":"2023-03-19T00:00:00.000Z","tags":[{"inline":true,"label":"Release Notes","permalink":"/blog/tags/release-notes"}],"hasTruncateMarker":false,"authors":[{"name":"Apache Doris","key":null,"page":null}],"frontMatter":{"title":"Apache Doris announced the official release of version 1.2.3","description":"Dear community, Apache Doris 1.2.3 is now available, with several enhancements and bug fixes based on 1.2.0\uFF0Cenabling smoother user experience.","date":"2023-03-19","author":"Apache Doris","tags":["Release Notes"],"image":"/images/1.2.3-release.png"},"unlisted":false,"prevItem":{"title":"How We increased database query concurrency by 20 times","permalink":"/blog/How-We-Increased-Database-Query-Concurrency-by-20-Times"},"nextItem":{"title":"Building the next-generation data lakehouse: 10X performance","permalink":"/blog/Building-the-Next-Generation-Data-Lakehouse-10X-Performance"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n\\n## Improvements\\n\\n### JDBC Catalog \\n\\n- Support connecting to Doris clusters through JDBC Catalog.\\n\\nCurrently, Jdbc Catalog only support to use 5.x version of JDBC jar package to connect another Doris database. If you use 8.x version of JDBC jar package, the data type of column may not be matched.\\n\\nReference: [https://doris.apache.org/docs/dev/lakehouse/multi-catalog/jdbc/#doris](https://doris.apache.org/docs/dev/lakehouse/multi-catalog/jdbc/#doris)\\n\\n- Support to synchronize only the specified database through the `only_specified_database` attribute.\\n\\n- Support synchronizing table names in the form of lowercase through `lower_case_table_names` to solve the problem of case sensitivity of table names.\\n\\nReference: [https://doris.apache.org/docs/dev/lakehouse/multi-catalog/jdbc](https://doris.apache.org/docs/dev/lakehouse/multi-catalog/jdbc)\\n\\n- Optimize the read performance of JDBC Catalog.\\n\\n### Elasticsearch Catalog\\n\\n- Support Array type mapping.\\n\\n- Support whether to push down the like expression through the `like_push_down` attribute to control the CPU overhead of the ES cluster.\\n\\nReference: [https://doris.apache.org/docs/dev/lakehouse/multi-catalog/es](https://doris.apache.org/docs/dev/lakehouse/multi-catalog/es)\\n\\n### Hive Catalog\\n\\n- Support Hive table default partition `_HIVE_DEFAULT_PARTITION_`.\\n\\n- Hive Metastore metadata automatic synchronization supports notification event in compressed format.\\n\\n### Dynamic Partition Improvement\\n\\n- Dynamic partition supports specifying the `storage_medium` parameter to control the storage medium of the newly added partition.\\n\\nReference: [https://doris.apache.org/docs/dev/advanced/partition/dynamic-partition](https://doris.apache.org/docs/dev/advanced/partition/dynamic-partition)\\n\\n\\n### Optimize BE\'s Threading Model\\n\\n- Optimize BE\'s threading model to avoid stability problems caused by frequent thread creation and destroy.\\n\\n## Bug Fixes\\n\\n- Fixed issues with Merge-On-Write Unique Key tables.\\n\\n- Fixed compaction related issues.\\n\\n- Fixed some delete statement issues causing data errors.\\n\\n- Fixed several query execution errors.\\n\\n- Fixed the problem of using JDBC catalog to cause BE crash on some operating system.\\n\\n- Fixed Multi-Catalog issues.\\n\\n- Fixed memory statistics and optimization issues.\\n\\n- Fixed decimalV3 and date/datetimev2 related issues.\\n\\n- Fixed load transaction stability issues.\\n\\n- Fixed light-weight schema change issues.\\n\\n- Fixed the issue of using `datetime` type for batch partition creation.\\n\\n- Fixed the problem that a large number of failed broker loads would cause the FE memory usage to be too high.\\n\\n- Fixed the problem that stream load cannot be canceled after dropping the table.\\n\\n- Fixed querying `information_schema` timeout in some cases.\\n\\n- Fixed the problem of BE crash caused by concurrent data export using `select outfile`.\\n\\n- Fixed transactional insert operation memory leak.\\n\\n- Fixed several query/load profile issues, and supports direct download of profiles through FE web ui.\\n\\n- Fixed the problem that the BE tablet GC thread caused the IO util to be too high.\\n\\n- Fixed the problem that the commit offset is inaccurate in Kafka routine load."},{"id":"/Building-the-Next-Generation-Data-Lakehouse-10X-Performance","metadata":{"permalink":"/blog/Building-the-Next-Generation-Data-Lakehouse-10X-Performance","source":"@site/blog/Building-the-Next-Generation-Data-Lakehouse-10X-Performance.md","title":"Building the next-generation data lakehouse: 10X performance","description":"This article explains how to connect various data sources quickly and ensure high query performance.","date":"2023-03-14T00:00:00.000Z","tags":[{"inline":true,"label":"Tech Sharing","permalink":"/blog/tags/tech-sharing"}],"hasTruncateMarker":false,"authors":[{"name":"Apache Doris","key":null,"page":null}],"frontMatter":{"title":"Building the next-generation data lakehouse: 10X performance","description":"This article explains how to connect various data sources quickly and ensure high query performance.","date":"2023-03-14","author":"Apache Doris","tags":["Tech Sharing"],"image":"/images/building-the-next-generation-data-lakehouse.jpg"},"unlisted":false,"prevItem":{"title":"Apache Doris announced the official release of version 1.2.3","permalink":"/blog/release-note-1.2.3"},"nextItem":{"title":"Tencent data engineer: why we went from ClickHouse to Apache Doris?","permalink":"/blog/Tencent-Data-Engineers-Why-We-Went-from-ClickHouse-to-Apache-Doris"}},"content":"\x3c!-- \\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\nA data warehouse was defined by Bill Inmon as \\"a subject-oriented, integrated, nonvolatile, and time-variant collection of data in support of management\'s decisions\\" over 30 years ago. However, the initial data warehouses unable to store massive heterogeneous data, hence the creation of data lakes. In modern times, data lakehouse emerges as a new paradigm. It is an open data management architecture featured by strong data analytics and governance capabilities, high flexibility, and open storage.\\n\\nIf I could only use one word to describe the next-gen data lakehouse, it would be **unification:**\\n\\n- **Unified data storage** to avoid the trouble and risks brought by redundant storage and cross-system ETL.\\n- **Unified governance** of both data and metadata with support for ACID, Schema Evolution, and Snapshot.\\n- **Unified data application** that supports data access via a single interface for multiple engines and workloads.\\n\\nLet\'s look into the architecture of a data lakehouse. We will find that it is not only supported by table formats such as Apache Iceberg, Apache Hudi, and Delta Lake, but more importantly, it is powered by a high-performance query engine to extract value from data.\\n\\nUsers are looking for a query engine that allows quick and smooth access to the most popular data sources. What they don\'t want is for their data to be locked in a certain database and rendered unavailable for other engines or to spend extra time and computing costs on data transfer and format conversion.\\n\\nTo turn these visions into reality, a data query engine needs to figure out the following questions:\\n\\n- How to access more data sources and acquire metadata more easily?\\n- How to improve query performance on data coming from various sources?\\n- How to enable more flexible resource scheduling and workload management?\\n\\n[Apache Doris](https://github.com/apache/doris) provides a possible answer to these questions. It is a real-time OLAP database that aspires to build itself into a unified data analysis gateway. This means it needs to be easily connected to various RDBMS, data warehouses, and data lake engines (such as Hive, Iceberg, Hudi, Delta Lake, and Flink Table Store) and allow for quick data writing from and queries on these heterogeneous data sources. The rest of this article is an in-depth explanation of Apache Doris\' techniques in the above three aspects: metadata acquisition, query performance optimization, and resource scheduling.\\n\\n## Metadata Acquisition and Data Access\\n\\nApache Doris 1.2.2 supports a wide variety of data lake formats and data access from various external data sources. Besides, via the Table Value Function, users can analyze files in object storage or HDFS directly.\\n\\n![data-sources-supported-in-data-lakehouse](/images/Lakehouse/Lakehouse_1.png)\\n\\n\\n\\nTo support multiple data sources, Apache Doris puts efforts into metadata acquisition and data access.\\n\\n### Metadata Acquisition\\n\\nMetadata consists of information about the databases, tables, partitions, indexes, and files from the data source. Thus, metadata of various data sources come in different formats and patterns, adding to the difficulty of metadata connection. An ideal metadata acquisition service should include the following:\\n\\n1. A **metadata structure** that can accommodate heterogeneous metadata.\\n2. An **extensible metadata connection framework** that enables quick and low-cost data connection.\\n3. Reliable and **efficient** **metadata access** that supports real-time metadata capture.\\n4. **Custom authentication** services to interface with external privilege management systems and thus reduce migration costs. \\n\\n#### Metadata Structure\\n\\nOlder versions of Doris support a two-tiered metadata structure: database and table. As a result, users need to create mappings for external databases and tables one by one, which is heavy work. Thus, Apache Doris 1.2.0 introduced the Multi-Catalog functionality. With this, you can map to external data at the catalog level, which means:\\n\\n1. You can map to the whole external data source and ingest all metadata from it.\\n2. You can manage the properties of the specified data source at the catalog level, such as connection, privileges, and data ingestion details, and easily handle multiple data sources.\\n\\n![metadata-structure](/images/Lakehouse/Lakehouse_2.png)\\n\\n\\n\\nData in Doris falls into two types of catalogs:\\n\\n1. Internal Catalog: Existing Doris databases and tables all belong to the Internal Catalog.\\n2. External Catalog: This is used to interface with external data sources. For example, HMS External Catalog can be connected to a cluster managed by Hive Metastore, and Iceberg External Catalog can be connected to an Iceberg cluster.\\n\\nYou can use the `SWITCH` statement to switch catalogs. You can also conduct federated queries using fully qualified names. For example:\\n\\n```\\nSELECT * FROM hive.db1.tbl1 a JOIN iceberg.db2.tbl2 b\\nON a.k1 = b.k1;\\n```\\n\\nSee more details [here](https://doris.apache.org/docs/dev/lakehouse/multi-catalog/).\\n\\n#### **Extensible Metadata Connection Framework**\\n\\nThe introduction of the catalog level also enables users to add new data sources simply by using the `CREATE CATALOG` statement:\\n\\n```\\nCREATE CATALOG hive PROPERTIES (\\n    \'type\'=\'hms\',\\n    \'hive.metastore.uris\' = \'thrift://172.21.0.1:7004\',\\n);\\n```\\n\\nIn data lake scenarios, Apache Doris currently supports the following metadata services:\\n\\n- Hive Metastore-compatible metadata services\\n- Alibaba Cloud Data Lake Formation\\n- AWS Glue\\n\\nThis also paves the way for developers who want to connect to more data sources via External Catalog. All they need is to implement the access interface.\\n\\n#### **Efficient Metadata Access**\\n\\nAccess to external data sources is often hindered by network conditions and data resources. This requires extra efforts of a data query engine to guarantee reliability, stability, and real-timeliness in metadata access.\\n\\n![metadata-access-Hive-MetaStore](/images/Lakehouse/Lakehouse_3.png)\\n\\nDoris enables high efficiency in metadata access by **Meta Cache**, which includes Schema Cache, Partition Cache, and File Cache. This means that Doris can respond to metadata queries on thousands of tables in milliseconds. In addition, Doris supports manual refresh of metadata at the Catalog/Database/Table level. Meanwhile, it enables auto synchronization of metadata in Hive Metastore by monitoring Hive Metastore Event, so any changes can be updated within seconds.\\n\\n#### **Custom Authorization**\\n\\nExternal data sources usually come with their own privilege management services. Many companies use one single tool (such as Apache Ranger) to provide authorization for their multiple data systems. Doris supports a custom authorization plugin, which can be connected to the user\'s own privilege management system via the Doris Access Controller interface. As a user, you only need to specify the authorization plugin for a newly created catalog, and then you can readily perform authorization, audit, and data encryption on external data in Doris.\\n\\n![custom-authorization](/images/Lakehouse/Lakehouse_4.png)\\n\\n### Data Access\\n\\nDoris supports data access to external storage systems, including HDFS and S3-compatible object storage:\\n\\n![access-to-external-storage-systems](/images/Lakehouse/Lakehouse_5.png)\\n\\n\\n\\n## Query Performance Optimization\\n\\nAfter clearing the way for external data access, the next step for a query engine would be to accelerate data queries. In the case of Apache Doris, efforts are made in data reading, execution engine, and optimizer.\\n\\n### **Data Reading**\\n\\nReading data on remote storage systems is often bottlenecked by access latency, concurrency, and I/O bandwidth, so reducing reading frequency will be a better choice.\\n\\n#### **Native File Format Reader**\\n\\nImproving data reading efficiency entails optimizing the reading of Parquet files and ORC files, which are the most commonly seen data files. Doris has refactored its File Reader, which is fine-tuned for each data format. Take the Native Parquet Reader as an example:\\n\\n- **Reduce format conversion**: It can directly convert files to the Doris storage format or to a format of higher performance using dictionary encoding. \\n- **Smart indexing of finer granularity**: It supports Page Index for Parquet files, so it can utilize Page-level smart indexing to filter Pages. \\n- **Predicate pushdown and late materialization**: It first reads columns with filters first and then reads the other columns of the filtered rows. This remarkably reduces file read volume since it avoids reading irrelevant data.\\n- **Lower read frequency**: Building on the high throughput and low concurrency of remote storage, it combines multiple data reads into one in order to improve overall data reading efficiency.\\n\\n#### File Cache\\n\\nDoris caches files from remote storage in local high-performance disks as a way to reduce overhead and increase performance in data reading. In addition, it has developed two new features that make queries on remote files as quick as those on local files:\\n\\n1. **Block cache**: Doris supports the block cache of remote files and can automatically adjust the block size from 4KB to 4MB based on the read request. The block cache method reduces read/write amplification and read latency in cold caches.\\n2. **Consistent hashing for caching**: Doris applies consistent hashing to manage cache locations and schedule data scanning. By doing so, it prevents cache failures brought about by the online and offlining of nodes. It can also increase cache hit rate and query service stability.\\n\\n![file-cache](/images/Lakehouse/Lakehouse_6.png)\\n\\n#### Execution Engine\\n\\nDevelopers surely don\'t want to rebuild all the general features for every new data source. Instead, they hope to reuse the vectorized execution engine and all operators in Doris in the data lakehouse scenario. Thus, Doris has refactored the scan nodes:\\n\\n- **Layer the logic**: All data queries in Doris, including those on internal tables, use the same operators, such as Join, Sort, and Agg. The only difference between queries on internal and external data lies in data access. In Doris, anything above the scan nodes follows the same query logic, while below the scan nodes, the implementation classes will take care of access to different data sources.\\n- **Use a general framework for scan operators**: Even for the scan nodes, different data sources have a lot in common, such as task splitting logic, scheduling of sub-tasks and I/O, predicate pushdown, and Runtime Filter. Therefore, Doris uses interfaces to handle them. Then, it implements a unified scheduling logic for all sub-tasks. The scheduler is in charge of all scanning tasks in the node. With global information of the node in hand, the schedular is able to do fine-grained management. Such a general framework makes it easy to connect a new data source to Doris, which will only take a week of work for one developer.\\n\\n![execution-engine](/images/Lakehouse/Lakehouse_7.png)\\n\\n#### Query Optimizer\\n\\nDoris supports a range of statistical information from various data sources, including Hive Metastore, Iceberg Metafile, and Hudi MetaTable. It has also refined its cost model inference based on the characteristics of different data sources to enhance its query planning capability. \\n\\n#### Performance\\n\\nWe tested Doris and Presto/Trino on HDFS in flat table scenarios (ClickBench) and multi-table scenarios (TPC-H). Here are the results:\\n\\n![Apache-Doris-VS-Trino-Presto-ClickBench](/images/Lakehouse/Lakehouse_8.png)\\n\\n![Apache-Doris-VS-Trino-Presto-TPCH](/images/Lakehouse/Lakehouse_9.png)\\n\\n\\n\\nAs is shown, with the same computing resources and on the same dataset, Apache Doris takes much less time to respond to SQL queries in both scenarios, delivering a 3~10 times higher performance than Presto/Trino.\\n\\n## Workload Management and Elastic Computing\\n\\nQuerying external data sources requires no internal storage of Doris. This makes elastic stateless computing nodes possible. Apache Doris 2.0 is going to implement Elastic Compute Node, which is dedicated to supporting query workloads of external data sources.\\n\\n![stateless-compute-nodes](/images/Lakehouse/Lakehouse_10.png)\\n\\nStateless computing nodes are open for quick scaling so users can easily cope with query workloads during peaks and valleys and strike a balance between performance and cost. In addition, Doris has optimized itself for Kubernetes cluster management and node scheduling. Now Master nodes can automatically manage the onlining and offlining of Elastic Compute Nodes, so users can govern their cluster workloads in cloud-native and hybrid cloud scenarios without difficulty.\\n\\n## Use Case\\n\\nApache Doris has been adopted by a financial institution for risk management. The user\'s high demands for data timeliness makes their data mart built on Greenplum and CDH, which could only process data from one day ago, no longer a great fit. In 2022, they incorporated Apache Doris in their data production and application pipeline, which allowed them to perform federated queries across Elasticsearch, Greenplum, and Hive. A few highlights from the user\'s feedback include:\\n\\n- Doris allows them to create one Hive Catalog that maps to tens of thousands of external Hive tables and conducts fast queries on them.\\n- Doris makes it possible to perform real-time federated queries using Elasticsearch Catalog and achieve a response time of mere milliseconds.\\n- Doris enables the decoupling of daily batch processing and statistical analysis, bringing less resource consumption and higher system stability.\\n\\n![use-case-of-data-lakehouse](/images/Lakehouse/Lakehouse_11.png)\\n\\n\\n\\n# Future Plans\\n\\nApache Doris is going to support a wider range of data sources, improve its data reading and write-back functionality, and optimizes its resource isolation and scheduling.\\n\\n## More Data Sources\\n\\nWe are working closely with various open source communities to expand and improve Doris\' features in data lake analytics. We plan to provide:\\n\\n- Support for Incremental Query of Hudi Merge-on-Read tables;\\n- Lower query latency utilizing the indexing of Iceberg/Hudi in combination with the query optimizer;\\n- Support for more data lake formats such as Delta Lake and Flink Table Store.\\n\\n## Data Integration\\n\\n **Data reading:**\\n\\nApache Doris is going to:\\n\\n- Support CDC and Incremental Materialized Views for data lakes in order to provide users with near real-time data views;\\n- Support a Git-Like data access mode and enable easier and safer data management via the multi-version and Branch mechanisms. \\n\\n**Data Write-Back:**\\n\\nWe are going to enhance Apache Doris\' data analysis gateway. In the future, users will be able to use Doris as a unified data management portal that is in charge of the write-back of processed data, export of data, and the generation of a unified data view.\\n\\n## Resource Isolation & Scheduling\\n\\nApache Doris is undertaking a wider variety of workloads as it is interfacing with more and more data sources. For example, it needs to provide low-latency online services while batch processing T-1 data in Hive. To make it work, resource isolation within the same cluster is critical, which is where efforts will be made.\\n\\nMeanwhile, we will continue optimizing the scheduling logic of elastic computing nodes in various scenarios and develop intra-node resource isolation at a finer granularity, such as CPU, I/O, and memory. \\n\\n## Join us\\n\\nContact dev@apache.doris.org to join the Lakehouse SIG(Special Interest Group) in the Apache Doris community and talk to developers from all walks of life.\\n\\n**# Links:**\\n\\n**Apache Doris:**\\n\\nhttp://doris.apache.org\\n\\n**Apache Doris Github:**\\n\\nhttps://github.com/apache/doris\\n\\nFind Apache Doris developers on [Slack](https://join.slack.com/t/apachedoriscommunity/shared_invite/zt-2unfw3a3q-MtjGX4pAd8bCGC1UV0sKcw)."},{"id":"/Tencent-Data-Engineers-Why-We-Went-from-ClickHouse-to-Apache-Doris","metadata":{"permalink":"/blog/Tencent-Data-Engineers-Why-We-Went-from-ClickHouse-to-Apache-Doris","source":"@site/blog/Tencent-Data-Engineers-Why-We-Went-from-ClickHouse-to-Apache-Doris.md","title":"Tencent data engineer: why we went from ClickHouse to Apache Doris?","description":"Evolution of the data processing architecture of Tencent Music Entertainment towards better performance and simpler maintenance.","date":"2023-03-07T00:00:00.000Z","tags":[{"inline":true,"label":"Best Practice","permalink":"/blog/tags/best-practice"}],"hasTruncateMarker":false,"authors":[{"name":"Jun Zhang & Kai Dai","key":null,"page":null}],"frontMatter":{"title":"Tencent data engineer: why we went from ClickHouse to Apache Doris?","description":"Evolution of the data processing architecture of Tencent Music Entertainment towards better performance and simpler maintenance.","date":"2023-03-07","author":"Jun Zhang & Kai Dai","tags":["Best Practice"],"image":"/images/tencent-data-engineer.png"},"unlisted":false,"prevItem":{"title":"Building the next-generation data lakehouse: 10X performance","permalink":"/blog/Building-the-Next-Generation-Data-Lakehouse-10X-Performance"},"nextItem":{"title":"Best practice in Duyansoft, improving query speed to make the most out of your data","permalink":"/blog/Improving-Query-Speed-to-Make-the-Most-out-of-Your-Data"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n![Tencent-use-case-of-Apache-Doris](/images/TME/TME.png)\\n\\nThis article is co-written by me and my colleague Kai Dai. We are both data platform engineers at [Tencent Music](https://www.tencentmusic.com/en-us/) (NYSE: TME), a music streaming service provider with a whopping 800 million monthly active users. To drop the number here is not to brag but to give a hint of the sea of data that my poor coworkers and I have to deal with everyday.\\n\\n# What We Use ClickHouse For?\\n\\nThe music library of Tencent Music contains data of all forms and types: recorded music, live music, audios, videos, etc. As data platform engineers, our job is to distill information from the data, based on which our teammates can make better decisions to support our users and musical partners.\\n\\nSpecifically, we do all-round analysis of the songs, lyrics, melodies, albums, and artists, turn all this information into data assets, and pass them to our internal data users for inventory counting, user profiling, metrics analysis, and group targeting.\\n\\n![data-pipeline](/images/TME/TME_1.png)\\n\\nWe stored and processed most of our data in Tencent Data Warehouse (TDW), an offline data platform where we put the data into various tag and metric systems and then created flat tables centering each object (songs, artists, etc.).\\n\\nThen we imported the flat tables into ClickHouse for analysis and Elasticsearch for data searching and group targeting.\\n\\nAfter that, our data analysts used the data under the tags and metrics they needed to form datasets for different usage scenarios, during which they could create their own tags and metrics.\\n\\nThe data processing pipeline looked like this:\\n\\n![data-warehouse-architecture-1.0](/images/TME/TME_2.png)\\n\\n# The Problems with ClickHouse\\n\\nWhen working with the above pipeline, we encountered a few difficulties:\\n\\n1. **Partial Update**: Partial update of columns was not supported. Therefore, any latency from any one of the data sources could delay the creation of flat tables, and thus undermine data timeliness.\\n2. **High storage cost**: Data under different tags and metrics was updated at different frequencies. As much as ClickHouse excelled in dealing with flat tables, it was a huge waste of storage resources to just pour all data into a flat table and partition it by day, not to mention the maintenance cost coming with it.\\n3. **High maintenance cost**: Architecturally speaking, ClickHouse was characterized by the strong coupling of storage nodes and compute nodes. Its components were heavily interdependent, adding to the risks of cluster instability. Plus, for federated queries across ClickHouse and Elasticsearch, we had to take care of a huge amount of connection issues. That was just tedious.\\n\\n# Transition to Apache Doris\\n\\n[Apache Doris](https://github.com/apache/doris), a real-time analytical database, boasts a few features that are exactly what we needed in solving our problems:\\n\\n1. **Partial update**: Doris supports a wide variety of data models, among which the Aggregate Model supports real-time partial update of columns. Building on this, we can directly ingest raw data into Doris and create flat tables there. The ingestion goes like this: Firstly, we use Spark to load data into Kafka; then, any incremental data will be updated to Doris and Elasticsearch via Flink. Meanwhile, Flink will pre-aggregate the data so as to release burden on Doris and Elasticsearch.\\n2. **Storage cost**: Doris supports multi-table join queries and federated queries across Hive, Iceberg, Hudi, MySQL, and Elasticsearch. This allows us to split the large flat tables into smaller ones and partition them by update frequency. The benefits of doing so include a relief of storage burden and an increase of query throughput.\\n3. **Maintenance cost**: Doris is of simple architecture and is compatible with MySQL protocol. Deploying Doris only involves two processes (FE and BE) with no dependency on other systems, making it easy to operate and maintain. Also, Doris supports querying external ES data tables. It can easily interface with the metadata in ES and automatically map the table schema from ES so we can conduct queries on Elasticsearch data via Doris without grappling with complex connections.\\n\\nWhat\u2019s more, Doris supports multiple data ingestion methods, including batch import from remote storage such as HDFS and S3, data reads from MySQL binlog and Kafka, and real-time data synchronization or batch import from MySQL, Oracle, and PostgreSQL. It ensures service availability and data reliability through a consistency protocol and is capable of auto debugging. This is great news for our operators and maintainers.\\n\\nStatistically speaking, these features have cut our storage cost by 42% and development cost by 40%.\\n\\nDuring our usage of Doris, we have received lots of support from the open source Apache Doris community and timely help from the SelectDB team, which is now running a commercial product based on Apache Doris.\\n\\n![data-warehouse-architecture-2.0](/images/TME/TME_3.png)\\n\\n# Further Improvement to Serve Our Needs\\n\\n## Introduce a Semantic Layer\\n\\nSpeaking of the datasets, on the bright side, our data analysts are given the liberty of redefining and combining the tags and metrics at their convenience. But on the dark side, high heterogeneity of the tag and metric systems leads to more difficulty in their usage and management.\\n\\nOur solution is to introduce a semantic layer in our data processing pipeline. The semantic layer is where all the technical terms are translated into more comprehensible concepts for our internal data users. In other words, we are turning the tags and metrics into first-class citizens for data definement and management.\\n\\n![data-warehouse-architecture-3.0](/images/TME/TME_4.png)\\n\\n**Why would this help?**\\n\\nFor data analysts, all tags and metrics will be created and shared at the semantic layer so there will be less confusion and higher efficiency.\\n\\nFor data users, they no longer need to create their own datasets or figure out which one is applicable for each scenario but can simply conduct queries on their specified tagset and metricset.\\n\\n## Upgrade the Semantic Layer\\n\\nExplicitly defining the tags and metrics at the semantic layer was not enough. In order to build a standardized data processing system, our next goal was to ensure consistent definition of tags and metrics throughout the whole data processing pipeline.\\n\\nFor this sake, we made the semantic layer the heart of our data management system:\\n\\n![data-warehouse-architecture-4.0](/images/TME/TME_5.png)\\n\\n**How does it work?**\\n\\nAll computing logics in TDW will be defined at the semantic layer in the form of a single tag or metric.\\n\\nThe semantic layer receives logic queries from the application side, selects an engine accordingly, and generates SQL. Then it sends the SQL command to TDW for execution. Meanwhile, it might also send configuration and data ingestion tasks to Doris and decide which metrics and tags should be accelerated.\\n\\nIn this way, we have made the tags and metrics more manageable. A fly in the ointment is that since each tag and metric is individually defined, we are struggling with automating the generation of a valid SQL statement for the queries. If you have any idea about this, you are more than welcome to talk to us.\\n\\n# Give Full Play to Apache Doris\\n\\nAs you can see, Apache Doris has played a pivotal role in our solution. Optimizing the usage of Doris can largely improve our overall data processing efficiency. So in this part, we are going to share with you what we do with Doris to accelerate data ingestion and queries and reduce costs.\\n\\n## What We Want?\\n\\n![goals-of-a-data-analytic-solution](/images/TME/TME_6.png)\\n\\nCurrently, we have 800+ tags and 1300+ metrics derived from the 80+ source tables in TDW.\\n\\nWhen importing data from TDW to Doris, we hope to achieve:\\n\\n- **Real-time availability:** In addition to the traditional T+1 offline data ingestion, we require real-time tagging.\\n- **Partial update**: Each source table generates data through its own ETL task at various paces and involves only part of the tags and metrics, so we require the support for partial update of columns.\\n- **High performance**: We need a response time of only a few seconds in group targeting, analysis and reporting scenarios.\\n- **Low costs**: We hope to reduce costs as much as possible.\\n\\n## What We Do?\\n\\n1. **Generate Flat Tables in Flink Instead of TDW**\\n\\n![generate-flat-tables-in-Flink](/images/TME/TME_7.png)\\n\\nGenerating flat tables in TDW has a few downsides:\\n\\n- **High storage cost**: TDW has to maintain an extra flat table apart from the discrete 80+ source tables. That\u2019s huge redundancy.\\n- **Low real-timeliness**: Any delay in the source tables will be augmented and retard the whole data link.\\n- **High development cost**: To achieve real-timeliness would require extra development efforts and resources.\\n\\nOn the contrary, generating flat tables in Doris is much easier and less expensive. The process is as follows:\\n\\n- Use Spark to import new data into Kafka in an offline manner.\\n- Use Flink to consume Kafka data.\\n- Create a flat table via the primary key ID.\\n- Import the flat table into Doris.\\n\\nAs is shown below, Flink has aggregated the five lines of data, of which \u201CID\u201D=1, into one line in Doris, reducing the data writing pressure on Doris.\\n\\n![flat-tables-in-Flink-2](/images/TME/TME_8.png)\\n\\nThis can largely reduce storage costs since TDW no long has to maintain two copies of data and KafKa only needs to store the new data pending for ingestion. What\u2019s more, we can add whatever ETL logic we want into Flink and reuse lots of development logic for offline and real-time data ingestion.\\n\\n**2. Name the Columns Smartly**\\n\\nAs we mentioned, the Aggregate Model of Doris allows partial update of columns. Here we provide a simple introduction to other data models in Doris for your reference:\\n\\n**Unique Model**: This is applicable for scenarios requiring primary key uniqueness. It only keeps the latest data of the same primary key ID. (As far as we know, the Apache Doris community is planning to include partial update of columns in the Unique Model, too.)\\n\\n**Duplicate Model**: This model stores all original data exactly as it is without any pre-aggregation or deduplication.\\n\\nAfter determining the data model, we had to think about how to name the columns. Using the tags or metrics as column names was not a choice because:\\n\\nI. Our internal data users might need to rename the metrics or tags, but Doris 1.1.3 does not support modification of column names.\\n\\nII. Tags might be taken online and offline frequently. If that involves the adding and dropping of columns, it will be not only time-consuming but also detrimental to query performance.\\n\\nInstead, we do the following:\\n\\n- **For flexible renaming of tags and metrics**, we use MySQL tables to store the metadata (name, globally unique ID, status, etc.). Any change to the names will only happen in the metadata but will not affect the table schema in Doris. For example, if a `song_name` is given an ID of 4, it will be stored with the column name of a4 in Doris. Then if the `song_name`is involved in a query, it will be converted to a4 in SQL.\\n- **For the onlining and offlining of tags**, we sort out the tags based on how frequently they are being used. The least used ones will be given an offline mark in their metadata. No new data will be put under the offline tags but the existing data under those tags will still be available.\\n- **For real-time availability of newly added tags and metrics**, we prebuild a few ID columns in Doris tables based on the mapping of name IDs. These reserved ID columns will be allocated to the newly added tags and metrics. Thus, we can avoid table schema change and the consequent overheads. Our experience shows that only 10 minutes after the tags and metrics are added, the data under them can be available.\\n\\nNoteworthily, the recently released Doris 1.2.0 supports Light Schema Change, which means that to add or remove columns, you only need to modify the metadata in FE. Also, you can rename the columns in data tables as long as you have enabled Light Schema Change for the tables. This is a big trouble saver for us.\\n\\n**3. Optimize Date Writing**\\n\\nHere are a few practices that have reduced our daily offline data ingestion time by 75% and our CUMU compaction score from 600+ to 100.\\n\\n- Flink pre-aggregation: as is mentioned above.\\n- Auto-sizing of writing batch: To reduce Flink resource usage, we enable the data in one Kafka Topic to be written into various Doris tables and realize the automatic alteration of batch size based on the data amount.\\n- Optimization of Doris data writing: fine-tune the the sizes of tablets and buckets as well as the compaction parameters for each scenario:\\n\\n```\\nmax_XXXX_compaction_thread\\nmax_cumulative_compaction_num_singleton_deltas\\n```\\n\\n- Optimization of the BE commit logic: conduct regular caching of BE lists, commit them to the BE nodes batch by batch, and use finer load balancing granularity.\\n\\n![stable-compaction-score](/images/TME/TME_9.png)\\n\\n**4. Use Dori-on-ES in Queries**\\n\\nAbout 60% of our data queries involve group targeting. Group targeting is to find our target data by using a set of tags as filters. It poses a few requirements for our data processing architecture:\\n\\n- Group targeting related to APP users can involve very complicated logic. That means the system must support hundreds of tags as filters simultaneously.\\n- Most group targeting scenarios only require the latest tag data. However, metric queries need to support historical data.\\n- Data users might need to perform further aggregated analysis of metric data after group targeting.\\n- Data users might also need to perform detailed queries on tags and metrics after group targeting.\\n\\nAfter consideration, we decided to adopt Doris-on-ES. Doris is where we store the metric data for each scenario as a partition table, while Elasticsearch stores all tag data. The Doris-on-ES solution combines the distributed query planning capability of Doris and the full-text search capability of Elasticsearch. The query pattern is as follows:\\n\\n```\\nSELECT tag, agg(metric) \\n   FROM Doris \\n   WHERE id in (select id from Es where tagFilter)\\n   GROUP BY tag\\n```\\n\\nAs is shown, the ID data located in Elasticsearch will be used in the sub-query in Doris for metric analysis.\\n\\nIn practice, we find that the query response time is related to the size of the target group. If the target group contains over one million objects, the query will take up to 60 seconds. If it is even larger, a timeout error might occur.\\n\\nAfter investigation, we identified our two biggest time wasters:\\n\\nI. When Doris BE pulls data from Elasticsearch (1024 lines at a time by default), for a target group of over one million objects, the network I/O overhead can be huge.\\n\\nII. After the data pulling, Doris BE needs to conduct Join operations with local metric tables via SHUFFLE/BROADCAST, which can cost a lot.\\n\\n![Doris-on-Elasticsearch](/images/TME/TME_10.png)\\n\\nThus, we make the following optimizations:\\n\\n- Add a query session variable `es_optimize` that specifies whether to enable optimization.\\n- In data writing into ES, add a BK column to store the bucket number after the primary key ID is hashed. The algorithm is the same as the bucketing algorithm in Doris (CRC32).\\n- Use Doris BE to generate a Bucket Join execution plan, dispatch the bucket number to BE ScanNode and push it down to ES.\\n- Use ES to compress the queried data; turn multiple data fetch into one and reduce network I/O overhead.\\n- Make sure that Doris BE only pulls the data of buckets related to the local metric tables and conducts local Join operations directly to avoid data shuffling between Doris BEs.\\n\\n![Doris-on-Elasticsearch-2](/images/TME/TME_11.png)\\n\\nAs a result, we reduce the query response time for large group targeting from 60 seconds to a surprising 3.7 seconds.\\n\\nCommunity information shows that Doris is going to support inverted indexing since version 2.0.0, which is soon to be released. With this new version, we will be able to conduct full-text search on text types, equivalence or range filtering of texts, numbers, and datetime, and conveniently combine AND, OR, NOT logic in filtering since the inverted indexing supports array types. This new feature of Doris is expected to deliver 3~5 times better performance than Elasticsearch on the same task.\\n\\n**5. Refine the Management of Data**\\n\\nDoris\u2019 capability of cold and hot data separation provides the foundation of our cost reduction strategies in data processing.\\n\\n- Based on the TTL mechanism of Doris, we only store data of the current year in Doris and put the historical data before that in TDW for lower storage cost.\\n- We vary the numbers of copies for different data partitions. For example, we set three copies for data of the recent three months, which is used frequently, one copy for data older than six months, and two copies for data in between.\\n- Doris supports turning hot data into cold data so we only store data of the past seven days in SSD and transfer data older than that to HDD for less expensive storage.\\n\\n# Conclusion\\n\\nThank you for scrolling all the way down here and finishing this long read. We\u2019ve shared our cheers and tears, lessons learned, and a few practices that might be of some value to you during our transition from ClickHouse to Doris. We really appreciate the help from the Apache Doris community, but we might still be chasing them around for a while since we attempt to realize auto-identification of cold and hot data, pre-computation of frequently used tags/metrics, simplification of code logic using Materialized Views, and so on and so forth.\\n\\n\\n\\n**# Links**\\n\\n**Apache Doris**:\\n\\nhttp://doris.apache.org\\n\\n**Apache Doris Github**:\\n\\nhttps://github.com/apache/doris\\n\\nFind Apache Doris developers on [Slack](https://join.slack.com/t/apachedoriscommunity/shared_invite/zt-2unfw3a3q-MtjGX4pAd8bCGC1UV0sKcw)"},{"id":"/Improving-Query-Speed-to-Make-the-Most-out-of-Your-Data","metadata":{"permalink":"/blog/Improving-Query-Speed-to-Make-the-Most-out-of-Your-Data","source":"@site/blog/Improving-Query-Speed-to-Make-the-Most-out-of-Your-Data.md","title":"Best practice in Duyansoft, improving query speed to make the most out of your data","description":"This is about how Duyansoft improved its overall data processing efficiency by optimizing the choice and usage of data warehouses.","date":"2023-02-27T00:00:00.000Z","tags":[{"inline":true,"label":"Best Practice","permalink":"/blog/tags/best-practice"}],"hasTruncateMarker":false,"authors":[{"name":"Junfei Liu","key":null,"page":null}],"frontMatter":{"title":"Best practice in Duyansoft, improving query speed to make the most out of your data","description":"This is about how Duyansoft improved its overall data processing efficiency by optimizing the choice and usage of data warehouses.","date":"2023-02-27","author":"Junfei Liu","tags":["Best Practice"],"image":"/images/best-practice-in-duyansoft.png"},"unlisted":false,"prevItem":{"title":"Tencent data engineer: why we went from ClickHouse to Apache Doris?","permalink":"/blog/Tencent-Data-Engineers-Why-We-Went-from-ClickHouse-to-Apache-Doris"},"nextItem":{"title":"Apache Doris announced the official release of version 1.2.2","permalink":"/blog/release-note-1.2.2"}},"content":"\x3c!-- \\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n> Author: Junfei Liu, Senior Architect of Duyansoft\\n\\n![Duyansoft-use-case-of-Apache-Doris](/images/Duyansoft/Duyansoft.png)\\n\\nThe world is getting more and more value out of data, as exemplified by the currently much-talked-about ChatGPT, which I believe is a robotic data analyst. However, in today\u2019s era, what\u2019s more important than the data itself is the ability to locate your wanted information among all the overflowing data quickly. So in this article, I will talk about how I improved overall data processing efficiency by optimizing the choice and usage of data warehouses.\\n\\n# Too Much Data on My Plate\\n\\nThe choice of data warehouses was never high on my worry list until 2021. I have been working as a data engineer for a Fintech SaaS provider since its incorporation in 2014. In the company\u2019s infancy, we didn\u2019t have too much data to juggle. We only needed a simple tool for OLTP and business reporting, and the traditional databases would cut the mustard.\\n\\n![data-processing-pipeline-Duyansoft](/images/Duyansoft/Duyan_1.png)\\n\\nBut as the company grew, the data we received became overwhelmingly large in volume and increasingly diversified in sources. Every day, we had tons of user accounts logging in and sending myriads of requests. It was like collecting water from a thousand taps to put out a million scattered pieces of fire in a building, except that you must bring the exact amount of water needed for each fire spot. Also, we got more and more emails from our colleagues asking if we could make data analysis easier for them. That\u2019s when the company assembled a big data team to tackle the beast.\\n\\nThe first thing we did was to revolutionize our data processing architecture. We used DataHub to collect all our transactional or log data and ingest it into an offline data warehouse for data processing (analyzing, computing. etc.). Then the results would be exported to MySQL and then forwarded to QuickBI to display the reports visually. We also replaced MongoDB with a real-time data warehouse for business queries.\\n\\n![Data-ingestion-ETL-ELT-application](/images/Duyansoft/Duyan_2.png)\\n\\nThis new architecture worked, but there remained a few pebbles in our shoes:\\n\\n- **We wanted faster responses.** MySQL could be slow in aggregating large tables, but our product guys requested a query response time of fewer than five seconds. So first, we tried to optimize MySQL. Then we also tried to skip MySQL and directly connect the offline data warehouse with QuickBI, hoping that the combination of query acceleration capability of the former and caching of the latter would do the magic. Still, that five-second goal seemed to be unreachable. There was a time when I believed the only perfect solution was for the product team to hire people with more patience.\\n- **We wanted less pain in maintaining dimension tables.** The offline data warehouse conducted data synchronization every five minutes, making it not applicable for frequent data updates or deletions scenarios. If we needed to maintain dimension tables in it, we would have to filter and deduplicate the data regularly to ensure data consistency. Out of our trouble-averse instinct, we chose not to do so.\\n- **We wanted support for point queries of high concurrency.** The real-time database that we previously used required up to 500ms to respond to highly concurrent point queries in both columnar storage and row storage, even after optimization. That was not good enough.\\n\\n# Hit It Where It Hurts Most\\n\\nIn March, 2022, we started our hunt for a better data warehouse. To our disappointment, there was no one-size-fits-all solution. Most of the tools we looked into were only good at one or a few of the tasks, but if we gathered the best performer for each usage scenario, that would add up to a heavy and messy toolkit, which was against instinct.\\n\\nSo we decided to solve our biggest headache first: slow response, as it was hurting both the experience of our users and our internal work efficiency.\\n\\nTo begin with, we tried to move the largest tables from MySQL to [Apache Doris](https://github.com/apache/doris), a real-time analytical database that supports MySQL protocol. That reduced the query execution time by a factor of eight. Then we tried and used Doris to accommodate more data.\\n\\nAs for now, we are using two Doris clusters: one to handle point queries (high QPS) from our users and the other for internal ad-hoc queries and reporting. As a result, users have reported smoother experience and we can provide more features that are used to be bottlenecked by slow query execution. Moving our dimension tables to Doris also brought less data errors and higher development efficiency.\\n\\n![Data-ingestion-ETL-ELT-Doris-application](/images/Duyansoft/Duyan_3.png)\\n\\nBoth the FE and BE processes of Doris can be scaled out, so tens of PBs of data stored in hundreds of devices can be put into one single cluster. In addition, the two types of processes implement a consistency protocol to ensure service availability and data reliability. This removes dependency on Hadoop and thus saves us the cost of deploying Hadoop clusters.\\n\\n# Tips\\n\\nHere are a few of our practices to share with you:\\n\\n## **Data Model:**\\n\\nOut of the three Doris data models, we find the Unique Model and the Aggregate Model suit our needs most. For example, we use the Unique Model to ensure data consistency while ingesting dimension tables and original tables and the Aggregate Model to import report data.\\n\\n## **Data Ingestion:**\\n\\nFor real-time data ingestion, we use the Flink-Doris-Connector: After our business data, the MySQL-based binlogs, is written into Kafka, it will be parsed by Flink and then loaded into Doris in a real-time manner.\\n\\nFor offline data ingestion, we use DataX: This mainly involves the computed report data in our offline data warehouse.\\n\\n## **Data Management:**\\n\\nWe back up our cluster data in a remote storage system via Broker. Then, it can restore the data from the backups to any Doris cluster if needed via the restore command.\\n\\n## **Monitoring and Alerting:**\\n\\nIn addition to the various monitoring metrics of Doris, we deployed an audit log plugin to keep a closer eye on certain slow SQL of certain users for optimization.\\n\\nSlow SQL queries:\\n\\n![slow-SQL-queries-monitoring](/images/Duyansoft/Duyan_4.png)\\n\\nSome of our often-used monitoring metrics:\\n\\n![monitoring-metrics](/images/Duyansoft/Duyan_5.png)\\n\\n**Tradeoff Between Resource Usage and Real-Time Availability:**\\n\\nIt turned out that using Flink-Doris-Connector for data ingestion can result in high cluster resource usage, so we increased the interval between each data writing from 3s to 10 or 20s, compromising a little bit on the real-time availability of data in exchange for much less resource usage.\\n\\n# Communication with Developers\\n\\nWe have been in close contact with the open source Doris community all the way from our investigation to our adoption of the data warehouse, and we\u2019ve provided a few suggestions to the developers:\\n\\n- Enable Flink-Doris-Connector to support simultaneous writing of multiple tables in a single sink.\\n- Enable Materialized Views to support Join of multiple tables.\\n- Optimize the underlying compaction of data and reduce resource usage as much as possible.\\n- Provide optimization suggestions for slow SQL and warnings for abnormal table creation behaviors.\\n\\nIf the perfect data warehouse is not there to be found, I think providing feedback for the second best is a way to help make one. We are also looking into a commercialized products based on Apach Doris called SelectDB to see if more custom-tailored advanced features can grease the wheels.\\n\\n# Conclusion\\n\\nAs we set out to find a single data warehouse that could serve all our needs, we ended up finding something less than perfect but good enough to improve our query speed by a wide margin and discovered some surprising features of it along the way. So if you wiggle between different choices, you may bet on the one with the thing you want most badly, and taking care of the rest wouldn\u2019t be so hard.\\n\\n**Try** [**Apache Doris**](https://github.com/apache/doris) **out!**\\n\\nIt is an open source real-time analytical database based on MPP architecture. It supports both high-concurrency point queries and high-throughput complex analysis."},{"id":"/release-note-1.2.2","metadata":{"permalink":"/blog/release-note-1.2.2","source":"@site/blog/release-note-1.2.2.md","title":"Apache Doris announced the official release of version 1.2.2","description":"Dear community, Apache Doris 1.2.2 is now available, with several enhancements and bug fixes based on 1.2.0\uFF0Cenabling smoother user experience.","date":"2023-02-15T00:00:00.000Z","tags":[{"inline":true,"label":"Release Notes","permalink":"/blog/tags/release-notes"}],"hasTruncateMarker":false,"authors":[{"name":"Apache Doris","key":null,"page":null}],"frontMatter":{"title":"Apache Doris announced the official release of version 1.2.2","description":"Dear community, Apache Doris 1.2.2 is now available, with several enhancements and bug fixes based on 1.2.0\uFF0Cenabling smoother user experience.","date":"2023-02-15","author":"Apache Doris","tags":["Release Notes"],"image":"/images/1.2.2-release.png"},"unlisted":false,"prevItem":{"title":"Best practice in Duyansoft, improving query speed to make the most out of your data","permalink":"/blog/Improving-Query-Speed-to-Make-the-Most-out-of-Your-Data"},"nextItem":{"title":"ClickHouse & Kudu to Doris: 10X concurrency increased, 70% latency down","permalink":"/blog/linkedcare"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n\\n## New Features\\n\\n### Lakehouse \\n\\n- Support automatic synchronization of Hive metastore.\\n\\n- Support reading the Iceberg Snapshot, and viewing the Snapshot history.\\n\\n- JDBC Catalog supports PostgreSQL, Clickhouse, Oracle, SQLServer\\n\\n- JDBC Catalog supports Insert operation\\n\\nReference: [https://doris.apache.org/docs/dev/lakehouse/multi-catalog/](https://doris.apache.org/docs/dev/lakehouse/multi-catalog/)\\n\\n### Auto Bucket\\n\\n Set and scale the number of buckets for different partitions to keep the number of tablet in a relatively appropriate range.\\n\\n### New Functions\\n\\nAdd the new function `width_bucket`. \\n\\nReference: [https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-functions/width-bucket/#description](https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-functions/width-bucket/#description)\\n\\n## Behavior Changes\\n\\n- Disable BE\'s page cache by default: `disable_storage_page_cache=true`\\n\\nTurn off this configuration to optimize memory usage and reduce the risk of memory OOM.\\nBut it will reduce the query latency of some small queries.\\nIf you are sensitive to query latency, or have high concurrency and small query scenarios, you can configure *disable_storage_page_cache=false* to enable page cache again.\\n\\n- Add new session variable `group_by_and_having_use_alias_first`, used to control whether the group and having clauses use alias.\\n\\nReference: [https://doris.apache.org/docs/dev/advanced/variables](https://doris.apache.org/docs/dev/advanced/variables)\\n\\n## Improvements\\n\\n### Compaction\\n\\n- Support `Vertical Compaction`. To optimize the compaction overhead and efficiency of wide tables.\\n\\n- Support `Segment ompaction`. Fix -238 and -235 issues with high frequency imports.\\n\\n### Lakehouse\\n\\n- Hive Catalog can be compatible with Hive version 1/2/3\\n\\n- Hive Catalog can access JuiceFS based HDFS with Broker.\\n\\n- Iceberg Catalog Support Hive Metastore and Rest Catalog type.\\n\\n- ES Catalog support _id column mapping\u3002\\n\\n- Optimize Iceberg V2 read performance with large number of delete rows.\\n\\n- Support for reading Iceberg tables after Schema Evolution\\n\\n- Parquet Reader handles column name case correctly.\\n\\n### Other\\n\\n- Support for accessing Hadoop KMS-encrypted HDFS.\\n\\n- Support to cancel the Export export task in progress.\\n\\n- Optimize the performance of `explode_split` with 1x.\\n\\n- Optimize the read performance of nullable columns with 3x.\\n\\n- Optimize some problems of Memtracker, improve memory management accuracy, and optimize memory application.\\n\\n\\n\\n## Bug Fixes\\n \\n- Fixed memory leak when loading data with Doris Flink Connector.\\n\\n- Fixed the possible thread scheduling problem of BE and reduce the `Fragment sent timeout` error caused by BE thread exhaustion.\\n\\n- Fixed various correctness and precision issues of column type datetimev2/decimalv3.\\n\\n- Fixed the problem data correctness issue with Unique Key Merge-on-Read table.\\n\\n- Fixed various known issues with the Light Schema Change feature.\\n\\n- Fixed various data correctness issues of bitmap type Runtime Filter.\\n\\n- Fixed the problem of poor reading performance of csv reader introduced in version 1.2.1.\\n\\n- Fixed the problem of BE OOM caused by Spark Load data download phase. \\n\\n- Fixed possible metadata compatibility issues when upgrading from version 1.1 to version 1.2. \\n\\n- Fixed the metadata problem when creating JDBC Catalog with Resource.\\n\\n- Fixed the problem of high CPU usage caused by load operation.\\n\\n- Fixed the problem of FE OOM caused by a large number of failed Broker Load jobs.\\n\\n- Fixed the problem of precision loss when loading floating-point types.\\n\\n- Fixed the problem of memory leak when useing 2PC stream load\\n\\n## Other\\n\\nAdd metrics to view the total rowset and segment numbers on BE\\n\\n- doris_be_all_rowsets_num and doris_be_all_segments_num\\n\\n\\n## Big Thanks\\n\\nThanks to ALL who contributed to this release!\\n\\n\\n@adonis0147\\n\\n@AshinGau\\n\\n@BePPPower\\n\\n@BiteTheDDDDt\\n\\n@ByteYue\\n\\n@caiconghui\\n\\n@cambyzju\\n\\n@chenlinzhong\\n\\n@DarvenDuan\\n\\n@dataroaring\\n\\n@Doris-Extras\\n\\n@dutyu\\n\\n@englefly\\n\\n@freemandealer\\n\\n@Gabriel39\\n\\n@HappenLee\\n\\n@Henry2SS\\n\\n@htyoung\\n\\n@isHuangXin\\n\\n@JackDrogon\\n\\n@jacktengg\\n\\n@Jibing-Li\\n\\n@kaka11chen\\n\\n@Kikyou1997\\n\\n@Lchangliang\\n\\n@LemonLiTree\\n\\n@liaoxin01\\n\\n@liqing-coder\\n\\n@luozenglin\\n\\n@morningman\\n\\n@morrySnow\\n\\n@mrhhsg\\n\\n@nextdreamblue\\n\\n@qidaye\\n\\n@qzsee\\n\\n@spaces-X\\n\\n@stalary\\n\\n\\n@starocean999\\n\\n@weizuo93\\n\\n@wsjz\\n\\n@xiaokang\\n\\n@xinyiZzz\\n\\n@xy720\\n\\n@yangzhg\\n\\n@yiguolei\\n\\n@yixiutt\\n\\n@Yukang-Lian\\n\\n@Yulei-Yang\\n\\n@zclllyybb\\n\\n@zddr\\n\\n@zhangstar333\\n\\n@zhannngchen\\n\\n@zy-kkk"},{"id":"/linkedcare","metadata":{"permalink":"/blog/linkedcare","source":"@site/blog/linkedcare.md","title":"ClickHouse & Kudu to Doris: 10X concurrency increased, 70% latency down","description":"The value-added report provided by Linkedcare to customers was initially provided by ClickHouse, which was later replaced by Apache Doris","date":"2023-01-28T00:00:00.000Z","tags":[{"inline":true,"label":"Best Practice","permalink":"/blog/tags/best-practice"}],"hasTruncateMarker":false,"authors":[{"name":"velodb.io \xb7 Yi Yang","key":null,"page":null}],"frontMatter":{"title":"ClickHouse & Kudu to Doris: 10X concurrency increased, 70% latency down","language":"en","description":"The value-added report provided by Linkedcare to customers was initially provided by ClickHouse, which was later replaced by Apache Doris","date":"2023-01-28","author":"velodb.io \xb7 Yi Yang","externalLink":"https://www.velodb.io/blog/1373","tags":["Best Practice"],"image":"/images/clickhouse-kudu-to-doris.png"},"unlisted":false,"prevItem":{"title":"Apache Doris announced the official release of version 1.2.2","permalink":"/blog/release-note-1.2.2"},"nextItem":{"title":"A glimpse of the next-generation analytical database","permalink":"/blog/summit"}},"content":"\x3c!-- \\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n![kv](/images/linkedcare/kv.png)\\n\\n## Author: \\nYiYang, Senior Big Data Developer, Linkedcare\\n\\n# About Linkedcare\\nLinkedcare is a leading SaaS software company in the health technology industry, focusing on the medical dental and cosmetic plastic surgery. In 2021, it was selected as one of the top 150 digital healthcare companies in the world by CB Insights. Linkedcare has served thousands of plastic surgery institutions in Los Angeles, Taiwan, and Hong Kong. Linkedcare also provides integrated management system services for dental clinics, covering electronic medical records, customer relationship management, intelligent marketing, B2B trading platform, insurance payment, BI tools, etc.\\n\\n# Doris\' Evolution in Linkedcare\\nLet me briefly introduce Doris\'s development in Linkedcare first. In general, the application of Doris in Linkedcare can be divided into two stages:\\n1. The value-added report provided by Linkedcare to customers was initially provided by ClickHouse, which was later replaced by Apache Doris;\\n2. Due to the continuous improvement of real-time data analysis requirements, T+1\'s data reporting gradually cannot meet business needs. Linkedcare needs a data warehouse that can handle real-time processing, and Doris has been introduced into the company\'s data warehouse since then. With the support of the Apache Doris community and the SelectDB professional technical team, our business data has been gradually migrated from Kudu to Doris.\\n\\n![1](/images/linkedcare/1.png)\\n\\n# Data Service Architecture: From ClickHouse to Doris\\n## Data Service Architecture Requirements\\n- Support complex queries: When customers do self-service on the dashboard, a complex SQL query statement will be generated to directly query the database, and the complexity of the statement is unknown, which adds a lot of pressure on the database and affects query performance.\\n- High concurrency and low latency: At least 100 concurrent queries can be supported, and query results can be return within 1 second;\\n- Real-time data update: The report data comes from the SaaS system. When the customer modifies the historical data in the system, the report data must be changed accordingly to ensure consistentency, which requires real-time processing.\\n- Low cost and easy deployment: There are a lot of private cloud customers in our SaaS business. In order to reduce labor costs, the business requires that the architecture deployment and operation and maintenance be simple enough.\\n\\n## Early Problems Found: ClickHouse Shuts Down When High-concurrency Occurs\\nThe previous project chose ClickHouse to provide data query services, but serious concurrency problems occurred during use:\\n10 concurrent queries will cause ClickHouse to shut down, resulting in the inability to provide services to customers normally, which is the direct reason for us to replace ClickHouse.\\n\\nIn addition, there are several severe problems:\\n1. The cost of ClickHouse services on the cloud is very high, and the dependency on ClickHouse components is relatively high. The frequent interaction between ClickHouse and Zookeeper during data ingestion will put greater pressure on stability.\\n2. How to seamlessly migrate data without affecting the normal use of customers is another problem.\\n\\n## Selection between Doris, Clickhouse and Kudu\\nTo deal with the existing problems and meet the business requirements, we decided to conduct research on Doris (0.14), Clickhouse, and Kudu respectively.\\n\\n![2](/images/linkedcare/2.png)\\n\\nAs shown in the table above, we made a deep comparison of these 3 databases. And we can see that Doris has excellent performance in many aspects:\\n- High concurrency: Doris can handle high-concurrency of 1,000 and more. So it will easily solve the problem of 10 concurrent queries which led ClickHouse to shut down.\\n- Query performance: Doris can achieve millisecond-level query response. In single-table query, although Doris and ClickHouse are almost equivalent in query performance, in multi-table query, Doris is far better than ClickHouse. Doris can make sure that the QPS won\'t drop when high-concurrency happens.\\n- Data update: Doris\' data model can meet our needs for data update to ensure the consistency of system data and business data, which will be described in detail below.\\n- Ease of use: Doris has a flat architecture, simple and fast deployment, fully-completed data ingest functions, and good at scaling out; At the same time, Doris can automatically perform replica balancing internally, and the operation and maintenance cost is extremely low. However, ClickHouse and Kudu rely heavily on components and require a lot of preparatory work for use. This requires a professional team to handle a large number of daily operation and maintenance tasks.\\n- Standard SQL: Doris is compatible with the MySQL protocol and uses standard SQL. It is easy for developers to get started and does not require additional learning costs.\\n- Distributed JOINs: Doris supports distributed JOINs, but ClickHouse has limitations in JOIN queries and functions as well as poor maintainability.\\n- Active community: The Apache Doris open source community is active with passion. At the same time, SelectDB provides a professional and full-time team for technical support for the Doris community. If you encounter problems, you can directly contact the community and find out a solution in time.\\n\\nFrom the above research, we can find that Doris has excellent capabilities in all aspects and is very in line with our needs. Therefore, we adopt Doris instead of ClickHouse, which solves the problems of poor concurrency and the shutdown of ClickHouse.\\n\\n# Data Warehouse Architecture: From Kudu+Impala to Doris\\nIn the process of using data reports, we have gradually discovered many advantages of Doris, so we decided to introduce Doris to the company\'s data warehouse.\\n\\n## Data Warehouse Architecture Requirements\\n- When the customer modifies the historical data in the system, the report data should also be changed accordingly. At the same time, there should be a feature that can help customers to change the value of a single column;\\n- When Flink extracts the full amount of data from the business database and writes it into the data warehouse frequently, the version compaction must keep up with the speed of new version generation, and will not cause version accumulation;\\n- Through resource isolation and other functions, Doris reduces the possibility of resource preemption, improves resource utilization, and makes full use of resources on the core computing nodes;\\n- Due to the limited memory resources in the company, overloaded tasks must be completed without increasing the number of clusters.\\n\\n## Early Problems Found: Kudu+Impala Underperforms\\nThe early company data warehouse architecture used Kudu and Impala for computing and storage. But we found the following problems during use:\\n1. When the number of concurrent queries (QPS) is large, the simple query response time of Kudu+Impala is always more than a few seconds, which cannot reach the millisecond-level required by the business. The long waiting time has brought bad user experience to customers. \\n2. The Kudu+Impala engine cannot perform incremental aggregation of factual data, and can barely support real-time data analysis.\\n3. Kudu relies on a large number of primary key lookups when ingesting data. The batch processing efficiency is low and Kudu consumes a lot of CPU, which is not friendly to resource utilization.\\n\\n## New Data Warehouse Architecture Design Based on Doris\\n\\n![3](/images/linkedcare/3.png)\\n\\nAs shown in the figure above, Apache Doris is used in the new architecture and is responsible for data warehouse storage and computing; Data ingestion of real-time data and ODS data through Kafka has been replaced with Flink; We use Duckula as our stream computing platform; While we introduce DolphinSchedular for our task scheduling.\\n\\n# Benefits of the new architecture based on Apache Doris:\\n- The new data warehouse architecture based on Doris no longer depends on Hadoop related components, and the operation and maintenance cost is low.\\n- Higher performance. Doris uses less server resources but provides stronger data processing capabilities;\\n- Doris supports high concurrency and can directly support WebApp query services;\\n- Doris supports the access to external tables, which enable easy data publishing and data ingestion;\\n- Doris supports dynamic scaling out and automatic data balance;\\n- Doris supports multiple federated queries, including Hive, ES, MySQL, etc.;\\n- Doris\' Aggregate Model supports users updating a single column;\\n- By adjusting BE parameters and cluster size, the problem of version accumulation can be effectively solved;\\n- Through the Resource Tag and Query Block function, cluster resource isolation can be realized, resource usage rate can be reduced, and query performance can be improved.\\n\\nThanks to the excellent capabilities of the new architecture, the cluster we use has been reduced from 18 pieces of 16Cores 128G to 12 pieces of 16Cores 128G, saving up to 33% of resources compared to before; Further, the computing performance has been greatly improved. Doris can complete an ETL task that was completed in 3 hours on Kudu in only 1 hour. In addition, in frequent updates, Kudu\'s internal data fragmentation files cannot be automatically merged so that the performance will become worse and worse, requiring regular rebuilding; While the compaction function of Doris can effectively solve this problem.\\n\\n# Highly Recommended\\nThe cost of using Doris is very low. Only 3 low-end servers or even desktops can be used to deploy easily a data warehouse based on Apache Doris; For enterprises with limited investment and do not want to be left behind by the market, it is highly recommended to try Apache Doris.\\n\\nDoris is also a mature analytical database with MPP architecture. At the same time, its community is very active and easy to communicate with. SelectDB, a commercial company with products based on Apache Doris, has set up a full-time technical team for the community. Any questions can be answered within 1 hour. In the last year, the community has been continuously promoted by SelectDB and introduced a series of industry-leading new features. In addition, the community will seriously consider the user habits when iterating, which will bring a lot of convenience.\\n\\nI really appreciate the full support from the Doris community and the SelectDB team. And I sincerely recommend developers and enterprises to start with Apache Doris today.\\n\\n# Apache Doris \\nApache Doris is a real-time analytical database based on MPP architecture, known for its high performance and ease of use. It supports both high-concurrency point queries and high-throughput complex analysis. (https://github.com/apache/doris)\\n\\n# Links\\n## GitHub:\\nhttps://github.com/apache/doris\\n\\n## Apache Doris Website:\\nhttps://doris.apache.org"},{"id":"/summit","metadata":{"permalink":"/blog/summit","source":"@site/blog/summit.md","title":"A glimpse of the next-generation analytical database","description":"My name is Mingyu Chen and I am the PMC Chair of the Apache Doris.In this lecture, you will go through the development of Doris in 2022 and look into the new trends that Doris is exploring in 2023.","date":"2023-01-19T00:00:00.000Z","tags":[{"inline":true,"label":"Top News","permalink":"/blog/tags/top-news"}],"hasTruncateMarker":false,"authors":[{"name":"Mingyu Chen","key":null,"page":null}],"frontMatter":{"title":"A glimpse of the next-generation analytical database","language":"en","description":"My name is Mingyu Chen and I am the PMC Chair of the Apache Doris.In this lecture, you will go through the development of Doris in 2022 and look into the new trends that Doris is exploring in 2023.","date":"2023-01-19","author":"Mingyu Chen","tags":["Top News"],"image":"/images/a-glimpse-of-the-next-generation-analytical-database.png"},"unlisted":false,"prevItem":{"title":"ClickHouse & Kudu to Doris: 10X concurrency increased, 70% latency down","permalink":"/blog/linkedcare"},"nextItem":{"title":"Apache Doris announced the official release of version 1.2.1","permalink":"/blog/release-note-1.2.1"}},"content":"\x3c!-- \\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n# Self-Intro\\n\\nHello everyone, welcome to the Doris Summit 2022, the first summit of Apache Doris since it was open-sourced. In this lecture, you will go through the development of Doris in 2022 and look into the new trends that Doris is exploring in 2023. My name is Mingyu Chen and I am the PMC Chair of the Apache Doris. I have been developing for Doris since 2014, and witnessed its whole process from open-source to graduation from Apache. My sharing will cover the following aspects. Let\'s get started.\\n\\nAs the beginning, I will briefly introduce what Doris is and why we should choose Doris in case you are new to Apache Doris. In 2022, Doris has became one of the most active open-sourced big data analysis engine projects in the world while the Doris community became one of the most active open-source communities in China, which you may get interested in. Moreover, the cutting-edge features, such as vectorized execution engine, cloud-native and efficient semi-structured data analysis, real-time processing and Lakehouse will be the focus of my lecture today. Also, it is important to prioritize tasks at the beginning of the year, so I will go through our job list with you shortly.\\n\\n# About Doris\\n\\nBriefly speaking, Apache Doris is an easy-to-use, high-performance and unified analytical database. As shown in this enterprise data flow chart, you may have a clear vision of where Apache Doris stands. Data from various upstream data sources, such as transactional databases, log systems, event tracking, etc., as well as data from ETL components, such as Flink, Spark and Hive is ingested into Doris through data processing and integration tools. \\n\\n![flow](/images/summit/en/flow.png)\\n\\nAs a fully-complete database system, Doris can provide various direct query functions including report analysis, multi-dimensional analysis, log analysis, user portrait and lakehouse, etc. Thanks to Doris\' MPP SQL distributed query engine. Doris can also be used to query external data sources from Hive, Iceberg, Hudi, Elasticsearch and various transactional database systems connected through JDBC, without data import and maintaining the schema of other data sources. There are several core features that can help users solve practical problems, which are as follows:\\n\\n- NO.1 is the ease of use. It supports ANSI SQL syntax, including single table aggregation, sorting, filtering and multi table join, sub query, etc. It also supports complex SQL syntax such as window function and grouping sets. At the same time, users can expand system functions through UDF, UDAF. In addition, Apache Doris is also compatible with MySQL protocol, which allows users access Doris through various BI tools. \\n- NO.2 is high performance. Doris is equipped with an efficient column storage engine, which not only reduces the amount of data scanning, but also implements an ultra-high data compression ratio. At the same time, Doris also uses various index technology to speed up data reading and filtering. Using the partition and bucket pruning function, Doris can support ultra-high concurrency of online service business, and a single node can support up to thousands of QPS. Further, Apache Doris combines the vectorized execution engine to give full play to the modern CPU parallel computing power. Doris supports materialized view. In terms of the optimizer, Doris uses a combination of CBO and RBO, with RBO supporting constant folding, subquery rewriting, predicate pushdown, etc.\\n- NO.3 is unified data warehouse. Thanks to the well-designed architecture, Doris can easily handle both low-latency, high-concurrency scenarios and high-throughput scenarios . \\n- NO.4 is the federated query analysis. With the help of Doris\'s complete distributed query engine, Doris can access data lake such as Hive, Iceberg and Hudi, as well as high-speed queries to external data sources such as Elasticsearch and MySQL. \\n- NO.5 is ecological enrichment. Doris provides rich data ingest methods, supports fast loading of data from localhost, Hadoop, Flink, Spark, Kafka, SeaTunnel and other systems, and can also directly access data in MySQL, PostgreSQL, Oracle, S3, Hive, Iceberg, Elasticsearch and other systems without data replication. At the same time, the data stored in Doris can also be read by Spark and Flink, and can be output to the upstream data application for display and analysis.\\n\\nNext, we will review what remarkable achievements the Doris community has achieved in 2022.\\n\\n# How should we look back on 2022?\\n\\nIn 2022, the world has witnessed  unprecedented changes, and countless magical moments are happening in reality. Thankfully, the power of technology and open source has navigated us to the right path. And 2022 is absolutely a fruitful year for Apache Doris. Let\'s review the development of Apache Doris in the past year from several angles:\\n\\n## Important Indicators of the Community\\n![community](/images/summit/en/community.png)\\n\\nIn the past year:\\n- The number of cumulative community contributors has increased from 200 to nearly 420, a year-on-year increase of more than 100%, which is still rising.\\n- The number of monthly active contributors has doubled from 50 to 100.\\n- The number of GitHub Stars has increased from 3.6k to 6.8k, and has been on the daily/weekly/monthly  GitHub Trending list many times.\\n- The number of all Commits increased from 3.7k to 7.6k. The amount of newly submitted code in the past year exceeded the total of previous years.\\n\\n![community 2](/images/summit/en/community 2.png)\\n\\nFrom these data, we can see that in 2022, there was an explosive growth in Apache Doris. The data indicators of all dimensions are grown by nearly 100%. The great effort has also made Apache Doris one of the most active open-source communities in the big data and database world. As is the growth shown in the trending of GitHub Contribution above, users and developers have made tremendous contribution to the community .\\nIt is memorable that in June 2022, Apache Doris graduated from the Apache incubator and became a Top-Level Project, which is the biggest milestone since open-souced.\\n\\n![top level](/images/summit/en/top level.png)\\n\\n## Open Source User Scale\\n\\nThanks to the voluntary technical support from the developers of SelectDB, a commercial company with products based on Apache Doris. In 2022 Doris became smoother in user connection and communication, and we were able to interact with users more directly and listen to their real voices.\\nLast year, Apache Doris was applied in dozens of industries, such as the Internet, fintech, telecommunications, education, automobiles, manufacturing, logistics, energy, and government affairs, and especially in the Internet industry, which is known for massive data. 80% of the TOP 50 Chinese Internet companies have been using Apache Doris for a long time to solve data analysis problems in their own business, including Baidu, Meituan, Xiaomi, Tencent, JD.com, ByteDance , NetEase, Sina, 360 Total Security, MiHoYo, ZHIHU.COM, etc.\\n\\n![logo wall](/images/summit/en/logo wall.png)\\n\\nGlobally, Apache Doris has served thousands of enterprise users, and this number is still growing rapidly. Most enterprise users are glad to contact the community and participate in community building through various means. Moreover, many of the enterprise users participated in Doris Summit, giving a lecture of their own practical experience based on real business.\\n\\n## Releases\\n\\nIn the early versions, ease of use has been frequently emphasized. The versions released in 2022  mainly focus on performance, stability, ease of use, which is a comprehensive evolution.\\n- In April, the community released Apache Doris V1.0.0, whose major version first changed from 0 to 1(V0.1.5 to V1.0.0) since open-sourced. In version 1.0, the extraordinary vectorized execution engine was first published, marking the beginning of Apache Doris to the era of ultra-high speed data analysis.\\n- In version 1.1 released in June, we further improved and optimized the vectorized engine, and set it as default. Simultaneously, the community has also prepared LTS(Long-Term-Support) versions released to quickly fix bugs and optimize functions for version 1.1 on a monthly basis, aiming to ensure higher stability required by the growing community users.\\n- Launched in early December, Version 1.2 not only introduces many important functions, such as Merge-on-Write for Unique Key model, Multi-Catalog, Java UDF, Array type, JSONB type, etc., but also improves the query performance by nearly ten times. These features allow Apache Doris to be more adaptable and possible for more data analysis.\\n- In version 1.2, stability and quality assurance were strongly stressed. On the one hand, using automated testing tools such as SQL Smith and test cases from various well-known open source projects, we have built millions of test case sets; On the other hand, the community access pipeline and perfect regression testing framework ensure the quality of code-merge. \\n\\n## Evolution of Core Features\\n\\nIn 2022, the community\'s research and development was mainly focused on four aspects, high performance, real-time processing, semi-structured data support and Lakehouse.\\n\\n![2022](/images/summit/en/2022.png)\\n\\n- Query performance improvement. From the released version 1.0 to 1.2, Apache Doris has made remarkable achievement in performance. In the single-table test, Apache Doris won 3rd place in Clickbench database performance list launched by Clickhouse. In the multi-table association, thanks to the vectorized execution engine and various query optimization, compared to the released version 0.15 at the end of 2021, Apache Doris was 10 times faster in standard test data sets under SSB and TPC-H, which marks Apache Doris one of the best databases in the world!\\n\\n![performance](/images/summit/en/performance.png)\\n\\n- Real-time processing optimization. In version 1.2, we have implemented the Merge-On-Write data update method on the original Unique Key, with a query performance improved by 5-10 times during high-frequency updates and low latency on updateable data in real-time analytics. In addition, the lightweight Schema Change enables easier column adding and substraction of data, which is unecessary for users to convert historical data any more. Tools such as Flink CDC can be used instantly to synchronize DML or DDL operations in transaction databases, making data synchronization smoother and unified.\\n\\n![realtime](/images/summit/en/realtime.png)\\n\\n- Semi-structured data analysis. At present, Apache Doris supports Array and JSONB types. The Array type can not only store complex data structures, but also support user behavior analysis through Array functions. JSONB is a binary JSON storage type, which not only has 4 times faster access performance than Text JSON, but has lower memory consumption as well. Various log data structures in JSON format can be easily ingested through JSONB efficiently. \\n\\n![semi](/images/summit/en/semi.png)\\n\\n- Lakehouse. In version 1.2.0, through multiple performance optimizations for external data sources such as Native Format Reader, late materialization, asynchronous IO, data prefetching, high-performance execution engine and query optimizer, Apache Doris can easily access external data sources, for instance, Hive, Iceberg and Hudi. And the speed of access is 3-5 times faster than  Trino/Presto and 10-100 times faster than Hive.\\n\\n![lakehouse](/images/summit/en/lakehouse.png)\\n\\n# 2023 RoadMap\\n\\nIn 2023, the Apache Doris community will deep dive into new features development, as you can refer to the 2023 RoadMap and the specific plan for next year below:\\n\\n![roadmap](/images/summit/en/roadmap.png)\\n\\nIn 2023, we will start the iteration of Apache Doris 2.x version on a quarterly basis . At the same time, for each 2-bit version, bug fixes and upgrades will be done on a monthly basis.\\nFrom a functional point of view, the follow-up research and development will focus on the following main directions:\\n\\n## High Performance\\n\\nHigh performance is the goal that Apache Doris is constantly pursuing. Doris\' excellent performance on public test datasets such as Clickbench and TPC-H has proved that it has become industry-leading. In the future, we will further enhance performance, including:\\n- More complex SQL: The new query optimizer will be available in the first quarter of 2023. The new query optimizer supports the strategy of combining RBO and CBO, and it can support complex queries more efficiently and fully execute all 99 SQLs of TPC-DS. \\n- Higher concurrency point query: High concurrency is always what Apache Doris is good at. And in 2023 we will further strengthen this capability through a series of features such as Short-Circuit Plan, Prepare Statement, Query Cache, etc., to support ultra-high concurrency of 10,000 QPS with single node and has higher-concurrency scaling out.\\n- More flexible multi-table materialized views: In previous versions, Apache Doris accelerated the analysis efficiency of fixed-dimensional data through strengthen single-table materialized views. The new multi-table materialized view will decouple the lifecycle of Base table and the MV table. In this way, Doris can easily deal with the multi-table JOINs and the pre-calculation acceleration of more complex SQL queries. And Doris is capable of asynchronous refresh and flexible incremental calculation methods. This feature will be available in the first quarter of 2023.\\n\\n## Cost-effective\\n\\nCost efficiency is the key to winning market competition for enterprises, which is true for databases as well. In the past, Apache Doris helped users greatly save the cost in computing and storage resources with many designs of ease of use. In the future, we will introduce a series of cloud-native capabilities to further reduce costs without affecting business efficiency, including:\\n- Lower storage costs: We will explore the combination of object storage systems and file systems on the cloud to help users further reduce storage costs, including better separation of hot and cold data, and migrate cold data to cheaper object storage or file system. Combining technologies such as a single remote replica, cold data cache, and hot & cold data conversion, we can ensure that query efficiency is not affected while saving up storage costs. This feature will be released in the first quarter of 2023.\\n- More elastic computing resources: We plan to separate storage and computing state and adopt Elastic Compute Node for computing. Since no data is stored, Elastic Computing Nodes have faster elastic scaling capabilities, which is convenient for users to quickly scale out during peak business periods, and further improve the analysis efficiency in massive data computing, such as lakehouse analysis. This function will be released shortly. \\n\\n## Hybrid Workload\\n\\nLots of users nowadays are building a unified analysis platform within the enterprise based on Apache Doris. On the one hand, Apache Doris is required to execute larger-scale data processing and analysis. On the other hand, Apache Doris is also required to deal with more analytical load challenges, such as real-time reports and Ad-hoc to ELT/ETL, log retrieval and more unified analysis. In order to better adapt to these cases, new features are about to be released in 2023, which include:\\n- Pipeline execution engine: Compared with the traditional volcano model, the Pipeline model does not need to set the concurrency manually, but instead, it can do parallel computing between different pipelines, making full usage of CPUs and is more flexible in execution scheduling, which improves the overall performance under mixed load cases.\\n- Workload Manager: It is also urgent to improve resource isolation and division capabilities. Based on the Pipeline execution engine, we will launch features such as flexible load management, resource queues, and isolation in shared services to balance query performance and stability in various mixed load cases.\\n- Lightweight fault tolerance: It can not only take advantage of the high efficiency of MPP structure but also tolerate errors to better adapt to the challenges of users in ETL/ELT.\\n- Function compatibility and UDF in multiple languages: At the same time, we will be more compatible with Hive/Trino/Spark function and support multiple UDF in the future to help users process data more flexibly. And data migration to Apache Doris will be easier than before.\\n\\n## Multi-model Data Analysis\\n\\nIn the past, Apache Doris was quite good at structured data analysis. As the demand for semi-structured and unstructured data analysis increased, we added Array and JSONB types from version 1.2 to support these data types naturally. In the future release, we will continue providing more cost-effective and better-performance solutions for log analysis cases, including:\\n- Richer complex data types: In addition to Array/JSONB types, we will increase support for Map/Struct types in the first quarter of 2023, including efficient writing, storage, analysis functions to better perform multi-model data analysis. In the future, more data types will be supported, such as IP and GEO geographic information, and more time series data.\\n- More efficient text analysis algorithms: For text data, we will introduce text analysis algorithms, including adaptive Like, high-performance substring matching, high-performance regular matching, predicate pushdown of Like statements, Ngram Bloomfilter, etc. The full-text search is based on the inverted index and it provides higher performance and is more cost-effective in analysis compared with that of  Elasticsearch in the log analysis. These features will come out in early 2023.\\n-Dynamic Schema table: In other databases, the schema is relatively static and DDL needs to be executed manually when the schema is changed. In recent cases, the table structure changes all the time, so we plan to launch Dynamic Table, which can automatically adapt to the Schema according to data writing without DDL execution, replacing manual adjusting. This feature will be released in the first quarter of 2023.\\n\\n## Lakehouse\\n\\nWith the development of data lake technology, analysis performance has become the biggest constraint to data-mining. Building analysis services on top of data lakes based on an easy-to-use and high-performance query analysis engine has become a new trend. In the last year, through many performance optimizations on the data lake, high-performance execution engine and query optimizer, Apache Doris has become extremely fast in analysis and easy-to-use on the data lake with a performance 3-5 times higher than that of Presto/Trino. In 2023, we will continue to go deeper, including:\\n- Easier data access: In version 1.2, we released Multi-Catalog, which supports automatic metadata mapping and synchronization of multiple heterogeneous data sources and is used for accessing data lakes. Delta Lake, Iceberg and Hudi will be better supported.\\n- More complete data lake capabilities: We provide incremental update and query of data on the data lake. Analysis result will be sent to data lake and the data from external tables will be ingested into internal tables. At the same time, Doris will also support multi-version Snapshot\'s read & delete and materialized views.\\n\\n## Real-time and storage engine optimization\\n\\nThe value of data will decrease over time, so real-time performance is very important for users. The Merge-on-Write data update in version 1.2 allows Apache Doris to be fast in both real-time updating and query. In 2023, we will upgrade the storage engine with the following:\\n- More stable data writing: Through a series of compaction operations and optimization of batch processing, resource cost is able to be saved. And through a new memory management framework, stability of the writing process will be improved.\\n- More mature data-updating mechanism: In the past, column updates were implemented through Replace_if_not_null on the Agg model. In the future, we will increase support for partial column updates with the Unique Key model, and data updates such as Delete, Update, and Merge.\\n- A unified data model: Currently, the three data models of Apache Doris are widely used in various cases. In the future, we will try to unify the existing data models to provide a better user experience.\\n\\n## Ease of use and stability\\n\\nIn addition to improving functions, simplicity, ease of use and stability is also the goal that Apache Doris has been pursuing. In 2023, we will dive deeper in the following:\\n- Simplified table creation: Currently, Apache Doris already supports time functions in table partitioning. In the future, we will further simplify Bucket settings to help users build models easily.\\n- Security: At present, a permission management mechanism based on the RBAC model has been launched, which makes user permissions more secure and reliable. Functions such as ID-federation, Row&Column-level permissions, data desensitization will be further improved in the future.\\n- Observability: Profile is an important means of locating query performance problems. In the future, we will strengthen the monitoring of Profile and provide visualized Profile tools to help users locate problems faster.\\n- Better BI compatibility and data migration solution: Currently, various BI tools can be connected with Apache Doris through MySQL protocol, and we will further adapt mainstream BI software in the future to ensure a better query experience. With the rise of emerging data integration and migration tools such as DBT and Airbyte, more and more users synchronize data to Apache Doris in this way. So we should provide support for these users in the future.\\n\\n# How to join the community\\n\\nLast but not the least, we hope that more developers can participate in the community to jointly create a powerful database. There are 3 ways to participate in the community. First of all, users can subscribe to our developer mailing group through this address: dev@doris.apache.org, which is recommended by the Apache Way as well. You can send any related topics that you want to discuss with the community. Secondly, you can reach out to us virtually on developer\'s biweekly meeting. The biweekly meeting is held on Wednesdays at 8pm(UTC+8). The topic will cover new features, disclosure and development progress and more. Thirdly, the DSIP. DSIP is short for Doris Improvement Proposal. All Doris designed functions are recorded in this document. Both users and developers can follow and see detailed design and development of  important functions on this Wiki.\\n\\n## Links:\\n\\nApache Doris Repository\\n\\nhttps://github.com/apache/doris\\n\\nApache Doris Website\\n\\nhttps://doris.apache.org"},{"id":"/release-note-1.2.1","metadata":{"permalink":"/blog/release-note-1.2.1","source":"@site/blog/release-note-1.2.1.md","title":"Apache Doris announced the official release of version 1.2.1","description":"Dear community, Apache Doris 1.2.1 is now available, with several enhancements and bug fixes based on 1.2.0\uFF0Cenabling smoother user experience.","date":"2023-01-04T00:00:00.000Z","tags":[{"inline":true,"label":"Release Notes","permalink":"/blog/tags/release-notes"}],"hasTruncateMarker":false,"authors":[{"name":"Apache Doris","key":null,"page":null}],"frontMatter":{"title":"Apache Doris announced the official release of version 1.2.1","description":"Dear community, Apache Doris 1.2.1 is now available, with several enhancements and bug fixes based on 1.2.0\uFF0Cenabling smoother user experience.","date":"2023-01-04","author":"Apache Doris","tags":["Release Notes"],"image":"/images/1.2.1-release.png"},"unlisted":false,"prevItem":{"title":"A glimpse of the next-generation analytical database","permalink":"/blog/summit"},"nextItem":{"title":"The Efficiency of the data warehouse greatly improved in LY Digital\\"","permalink":"/blog/LY"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n\\n\\n## Improvements\\n\\n### Supports new type DecimalV3\\n\\nDecimalV3, which supports higher precision and better performance, has the following advantages over past versions.\\n\\n- Larger representable range, the range of values are significantly expanded, and the valid number range [1,38].\\n\\n- Higher performance, adaptive adjustment of the storage space occupied according to different precision.\\n\\n- More complete precision derivation support, for different expressions, different precision derivation rules are applied to the accuracy of the result.\\n\\n[DecimalV3](https://doris.apache.org/docs/2.0/sql-manual/sql-reference/Data-Types/DECIMAL)\\n\\n### Support Iceberg V2\\n\\nSupport Iceberg V2 (only Position Delete is supported, Equality Delete will be supported in subsequent versions).\\n\\nTables in Iceberg V2 format can be accessed through the Multi-Catalog feature.\\n\\n### Support OR condition to IN\\n\\nSupport converting  OR condition to IN condition, which can improve the execution efficiency in some scenarios.[#15437](https://github.com/apache/doris/pull/15437) [#12872](https://github.com/apache/doris/pull/12872)\\n\\n### Optimize the import and query performance of JSONB type\\n\\nOptimize the import and query performance of JSONB type. [#15219](https://github.com/apache/doris/pull/15219)  [#15219](https://github.com/apache/doris/pull/15219)\\n\\n### Stream load supports quoted csv data\\n\\nSearch trim_double_quotes in Document:[https://doris.apache.org/docs/dev/sql-manual/sql-reference/Data-Manipulation-Statements/Load/STREAM-LOAD](https://doris.apache.org/docs/dev/sql-manual/sql-reference/Data-Manipulation-Statements/Load/STREAM-LOAD)\\n\\n### Broker supports Tencent Cloud CHDFS and Baidu Cloud BOS, AFS\\n\\nData on CHDFS, BOS, and AFS can be accessed through Broker. [#15297](https://github.com/apache/doris/pull/15297) [#15448](https://github.com/apache/doris/pull/15448)\\n\\n### New function\\n\\nAdd function `substring_index`. [#15373](https://github.com/apache/doris/pull/15373)\\n\\n## Bug Fixes\\n\\n- In some cases, after upgrading from version 1.1 to version 1.2, the user permission information will be lost. [#15144](https://github.com/apache/doris/pull/15144)\\n\\n- Fix the problem that the partition value is wrong when using datev2/datetimev2 type for partitioning. [#15094](https://github.com/apache/doris/pull/15094)\\n\\n- Bug fixes for a large number of released features. For a complete list see: [PR List](https://github.com/apache/doris/pulls?q=is%3Apr+label%3Adev%2F1.2.1-merged+is%3Aclosed)\\n\\n## Upgrade Notices\\n\\n### Known Issues\\n\\n- Do not use JDK11 as the runtime JDK of BE, it will cause BE Crash.\\n- The reading performance of the csv format in this version has declined, which will affect the import and reading efficiency of the csv format. We will fix it as soon as possible in the next three-digit version\\n\\n### Behavior Changed\\n\\n- The default value of the BE configuration item `high_priority_flush_thread_num_per_store` is changed from 1 to 6, to improve the write efficiency of Routine Load. (https://github.com/apache/doris/pull/14775)\\n\\n- The default value of the FE configuration item `enable_new_load_scan_node` is changed to true. Import tasks will be performed using the new File Scan Node. No impact on users.[#14808](https://github.com/apache/doris/pull/14808)\\n\\n- Delete the FE configuration item `enable_multi_catalog`. The Multi-Catalog function is enabled by default.\\n\\n- The vectorized execution engine is forced to be enabled by default.[#15213](https://github.com/apache/doris/pull/15213)\\n\\nThe session variable enable_vectorized_engine will no longer take effect. Enabled by default.\\n\\nTo make it valid again, set the FE configuration item `disable_enable_vectorized_engine` to false, and restart FE to make `enable_vectorized_engine` valid again.\\n\\n\\n## Big Thanks\\n\\nThanks to ALL who contributed to this release!\\n\\n\\n@adonis0147\\n\\n@AshinGau\\n\\n@BePPPower\\n\\n@BiteTheDDDDt\\n\\n@ByteYue\\n\\n@caiconghui\\n\\n@cambyzju\\n\\n@chenlinzhong\\n\\n@dataroaring\\n\\n@Doris-Extras\\n\\n@dutyu\\n\\n@eldenmoon\\n\\n@englefly\\n\\n@freemandealer\\n\\n@Gabriel39\\n\\n@HappenLee\\n\\n@Henry2SS\\n\\n@hf200012\\n\\n@jacktengg\\n\\n@Jibing-Li\\n\\n@Kikyou1997\\n\\n@liaoxin01\\n\\n@luozenglin\\n\\n@morningman\\n\\n@morrySnow\\n\\n@mrhhsg\\n\\n@nextdreamblue\\n\\n@qidaye\\n\\n@spaces-X\\n\\n@starocean999\\n\\n@wangshuo128\\n\\n@weizuo93\\n\\n@wsjz\\n\\n@xiaokang\\n\\n@xinyiZzz\\n\\n@xutaoustc\\n\\n@yangzhg\\n\\n@yiguolei\\n\\n@yixiutt\\n\\n@Yulei-Yang\\n\\n@yuxuan-luo\\n\\n@zenoyang\\n\\n@zhangstar333\\n\\n@zhannngchen\\n\\n@zhengshengjun"},{"id":"/LY","metadata":{"permalink":"/blog/LY","source":"@site/blog/LY.md","title":"The Efficiency of the data warehouse greatly improved in LY Digital\\"","description":"Established in 2015, LY Digital is a financial service platform for tourism industry under LY. Com. In 2020, LY Digital introduced Apache Doris to build a data warehouse because of its rich data import methods, excellent parallel computing capabilities, and low maintenance costs. This article describes the evolution of data warehouse in LY Digital and why we switch to Apache Doris. ","date":"2022-12-19T00:00:00.000Z","tags":[{"inline":true,"label":"Best Practice","permalink":"/blog/tags/best-practice"}],"hasTruncateMarker":false,"authors":[{"name":"Xing Wang","key":null,"page":null}],"frontMatter":{"title":"The Efficiency of the data warehouse greatly improved in LY Digital\\"","language":"en","description":"Established in 2015, LY Digital is a financial service platform for tourism industry under LY. Com. In 2020, LY Digital introduced Apache Doris to build a data warehouse because of its rich data import methods, excellent parallel computing capabilities, and low maintenance costs. This article describes the evolution of data warehouse in LY Digital and why we switch to Apache Doris. ","date":"2022-12-19","author":"Xing Wang","tags":["Best Practice"],"image":"/images/best-practice.png"},"unlisted":false,"prevItem":{"title":"Apache Doris announced the official release of version 1.2.1","permalink":"/blog/release-note-1.2.1"},"nextItem":{"title":"Apache Doris announced the official release of version 1.1.5","permalink":"/blog/release-note-1.1.5"}},"content":"\x3c!-- \\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n> Guide: Established in 2015, LY Digital is a financial service platform for tourism industry under LY. Com. In 2020, LY Digital introduced Apache Doris to build a data warehouse because of its rich data import methods, excellent parallel computing capabilities, and low maintenance costs. This article describes the evolution of data warehouse in LY Digital and why we switch to Apache Doris. I hope you like it.\\n\\n> Author: XingWang, Lead Developer of LY Digital\\n\\n![kv](/images/LY/en/kv.png)\\n\\n# 1. Background\\n\\n## 1.1 About LY Digital\\n\\nLY Digital is a tourism financial service platform under LY. Com. Formally established in 2015, LY Digital takes \\"Digital technology empowers the tourism industry.\\" as its vision.\\nAt present, LY Digital\'s business covers financial services, consumer financial services, financial technology and digital technology. So far, more than 10 million users and 76 cities have enjoyed our services.\\n\\n## 1.2 Requirements for Data Warehouse\\n\\n- Dashboard: Needs dashboard for T+1 business, etc.\\n- Early Warning System: Needs risk control, anomaly capital management and traffic monitoring, etc.\\n- Business Analysis: Needs timely data query analysis and temporary data retrieval, etc.\\n- Finance: Needs liquidation and payment reconciliation.\\n\\n# 2. Previous Data Warehouse\\n\\n## 2.1 Architecture\\n\\n![page_1](/images/LY/en/page_1.png)\\n\\nOur previous data warehouse adopted the combination of SteamSets and Apache Kudu, which was very popular in the past few years. In this architecture, Binlog is ingested into Apache Kudu after passing through StreamSets in real-time, and is finally queried and used through Apache Impala and visualization tools.\\n\\n### 2.1.2 Downside\\n\\n- The previous data warehouse has a sophisticated structure that consists of many components that interact with one another, which requires huge operation and maintenance costs. \\n- The previous data warehouse has a sophisticated structure that consists of many components that interact with one another, which requires huge operation and maintenance costs.\\n- Apache Kudu\'s performance in wide tables Join is not so good.\\n- SLA is not fully guaranteed because tenant isolation is not provided.\\n- Although SteamSets are equipped with early warning capabilities, job recovery capabilities are still poor. When configuring multiple tasks, the JVM consumes a lot, resulting in slow recovery.\\n\\n# 3. New Data Warehouse\\n\\n## 3.1 Research of Popular Data Warehouses\\n\\nDue to so many shortcomings, we had to give up the previous data warehouse. In 2020, we conducted an in-depth research on the popular data warehouses in the market.\\n\\nDuring the research, we focused on comparing Clickhouse and Apache Doris. ClickHouse has a high utilization rate of CPU, so it performs well in single-table query. But it does not perform well in multitable Joins and high QPS. On the other hand, Doris can not only support thousands of QPS per node. Thanks to the function of partitioning, it can also support high-concurrency queries at the QPS level of 10,000. Moreover, the horiziontal scaling in and out of ClickHouse are complex, which cannot be done automatically at present. Doris supports online dynamic scaling, and can be expanded horizontally according to the development of the business.\\n\\nIn the research, Apache Doris stood out. Doris\'s high-concurrency query capability is very attractive. Its dynamic scaling capabilities are also suitable for our flexible advertising business. So we chose Apache Doris for sure.\\n\\n![page_2](/images/LY/en/page_2.png)\\n\\nAfter introducing Apache Doris, we upgraded the entire data warehouse:\\n- We collect MySQL Binlog through Canal and then it is ingested into Kafka. Because Apache Doris is highly capatible with Kafka, we can easily use Routine Load to load and import data.\\n- We have made minor adjustments to the batch processing. For data stored in Hive, Apache Doris can ingest data from Hive through Broker Load. In this way, the data in batch processing can be directly ingested into Doris.\\n\\n## 3.2 Why We Choose Doris\\n\\n![page_3](/images/LY/en/page_3.png)\\n\\nThe overall performance of Apache Doris is impressive:\\n- Data access: It provides rich data import methods and can support the access of many types of data sources;\\n- Data connection: Doris supports JDBC and ODBC connections. And it can easily connect with BI tools. In addition, Doris uses the MySQL protocol for communication. Users can directly access Doris through various Client tools;\\n- SQL syntax: Doris adopts MySQL protocol and it is highly compatible with MySQL syntax, supporting standard SQL, and is low in learning costs for developers;\\n- MPP parallel computing: Doris provides excellent parallel computing capabilities and has obvious advantages in complex Join and wide table Join;\\n- Fully-completed documentation: Doris official documentation is very profound, which is friendly for new users. \\n\\n\\n## 3.3  Architecture of Real-time Processing \\n\\n![page_4](/images/LY/en/page_4.png)\\n\\n- Data source: In real-time processing, data sources come from business branches such as industrial finance, consumer finance, and risk control. They are all collected through Canal and API.\\n- Data collection: After data collection through Canal-Admin, Canal sends the data to Kafka message queue. After that, the data is ingested into the Doris through Routine Load.\\n- Inside Doris: The Doris cluster constitutes a  three-level layer of the data warehouse, namely: the DWD layer with the Unique model, the DWS layer with the Aggregation model, and the ADS application layer.\\n- Data application: The data is applied in three aspects: real-time dashboard, data timeliness analysis and data service.\\n\\n## 3.4 New Features\\n\\nThe data import method is simple and adopts 3 different import methods according to different scenarios:\\n- Routine Load: When we submit the Rountine Load task, there will be a process within Doris that consumes Kafka in real time, continuously reads data from Kafka and ingestes it into Doris.\\n- Broker Load: Offline data such as dim-tables and historical data are ingested into Doris in an orderly manner.\\n- Insert Into: Used for batch processing tasks, Insert into is responsible for processing data in the DWD layer\\n\\nDoris\' data model improves our development efficiency:\\n- The Unique model is used when accessing the DWD layer, which can effectively prevent repeated consumption of data.\\n-  In Doris, aggregation supports 4 models, such as Sum, Replace, Min, and Max. In this way, it may reduce a large amount of SQL code,  and no longer allow us to manually write Sum, Min, Max and other codes.\\n\\nDoris query is efficient:\\n- It supports materialized view and Rollup materialized index. The bottom layer of the materialized view is similar to the concept of Cube and the precomputation process. As a way of exchanging space for time, special tables are generated at the bottom layer. In the query, materialized view maps to the tables and responds quickly.\\n\\n# 4. Benefits of the New Data Warehouse\\n\\n- Data access: In the previous architecture, the Kudu table needs to be created manually during the imports through SteamSets. Lack of tools, the entire process of creating tables and tasks takes 20-30 minutes. Nowadays, fast data access can be realized through the platform. The access process of each table has been shortened from the previous 20-30 minutes to the current 3-5 minutes, which is to say that the performance has been improved by 5-6 times.\\n- Data development: After using Doris, we can directly use the data models, such as Unique and Aggregation.  The Duplicate model can well support logs, greatly speeding up the development process in ETL.\\n- Query analysis: The bottom layer of Doris has functions such as materialized view and Rollup materialized index. Moreover, Doris has made many optimizations for wide table associations, such as Runtime Filter and other Joins. Compared with Doris, Apache Kudu requires more complex optimization to be better used.\\n- Data report: It took 1-2 minutes to complete the rendering when we used Kudu to query before, but Doris responded in seconds or even milliseconds.\\n- Easy maintenance: Doris is not as complex as Hadoop. In March, our IDC was relocated, and 12 Doris virtual machines were all migrated within three days. The overall operation is relatively simple. In addition to physically moving the machine, FE\'s scaling only requires simple commands such as Add and Drop, which do not take a long time to do.\\n\\n# 5. Look ahead\\n\\n- Realize data access based on Flink CDC: At present, Flink CDC is not introduced, but Kafka through Canal instead. The development efficiency can be even faster if we use Flink CDC. Flink CDC still needs us to write a certain amount of code, which is not friendly for data analysts to use directly. We hope that data analysts only need to write simple SQL or directly operate. In the future planning, we plan to introduce Flink CDC.\\n- Keep up with the latest release: Now the latest version Apache Doris V1.2.0 has made great achievements in vectorization, multi-catalog, and light schema change. We will keep up with the community to upgrade the cluster and make full use of new features.\\n- Strengthen the construction of related systems: Our current index system management, such as report metadata, business metadata, and other management levels still need to be improved. Although we have data quality monitoring functions, it still needs to be strengthened and improved in automation."},{"id":"/release-note-1.1.5","metadata":{"permalink":"/blog/release-note-1.1.5","source":"@site/blog/release-note-1.1.5.md","title":"Apache Doris announced the official release of version 1.1.5","description":"Dear community, Apache Doris team has fixed about 36 issues or performance improvements in version 1.1.5 compared to previous version.","date":"2022-12-19T00:00:00.000Z","tags":[{"inline":true,"label":"Release Notes","permalink":"/blog/tags/release-notes"}],"hasTruncateMarker":false,"authors":[{"name":"Apache Doris","key":null,"page":null}],"frontMatter":{"title":"Apache Doris announced the official release of version 1.1.5","description":"Dear community, Apache Doris team has fixed about 36 issues or performance improvements in version 1.1.5 compared to previous version.","date":"2022-12-19","author":"Apache Doris","tags":["Release Notes"],"image":"/images/release-notes.png"},"unlisted":false,"prevItem":{"title":"The Efficiency of the data warehouse greatly improved in LY Digital\\"","permalink":"/blog/LY"},"nextItem":{"title":"Best practice in Kwai: Apache Doris on Elasticsearch","permalink":"/blog/BestPractice_Kwai"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n\\n\\nIn this release, Doris Team has fixed about 36 issues or performance improvement since 1.1.4. This release is a bugfix release on 1.1 and all users are encouraged to upgrade to this release.\\n\\n# Behavior Changes\\n\\nWhen alias name is same as the original column name like \\"select year(birthday) as birthday\\" and use it in group by, order by , having clause, doris\'s behavior is different from MySQL in the past. In this release, we make it follow MySQL\'s behavior. Group by and having clause will use original column at first and order by will use alias first. It maybe a litter confuse here so there is a simple advice here, you\'d better not use an alias the same as original column name.\\n\\n# Features\\n\\nAdd support of murmur_hash3_64. [#14636](https://github.com/apache/doris/pull/14636)\\n\\n# Improvements\\n\\nAdd timezone cache for convert_tz to improve performance. [#14616](https://github.com/apache/doris/pull/14616)\\n\\nSort result by tablename when call show clause. [#14492](https://github.com/apache/doris/pull/14492)\\n\\n# Bug Fix\\n\\nFix coredump when there is a if constant expr in select clause.  [#14858](https://github.com/apache/doris/pull/14858)\\n\\nColumnVector::insert_date_column may crashed. [#14839](https://github.com/apache/doris/pull/14839)\\n\\nUpdate high_priority_flush_thread_num_per_store default value to 6 and it will improve the load performance. [#14775](https://github.com/apache/doris/pull/14775)\\n\\nFix quick compaction core.  [#14731](https://github.com/apache/doris/pull/14731)\\n\\nPartition column is not duplicate key, spark load will throw IndexOutOfBounds error. [#14661](https://github.com/apache/doris/pull/14661)\\n\\nFix a memory leak problem in VCollectorIterator. [#14549](https://github.com/apache/doris/pull/14549)\\n\\nFix create table like when having sequence column. [#14511](https://github.com/apache/doris/pull/14511)\\n\\nUsing avg rowset to calculate batch size instead of using total_bytes since it costs a lot of cpu. [#14273](https://github.com/apache/doris/pull/14273)\\n\\nFix right outer join core with conjunct. [#14821](https://github.com/apache/doris/pull/14821)\\n\\nOptimize policy of tcmalloc gc.  [#14777](https://github.com/apache/doris/pull/14777) [#14738](https://github.com/apache/doris/pull/14738) [#14374](https://github.com/apache/doris/pull/14374)"},{"id":"/BestPractice_Kwai","metadata":{"permalink":"/blog/BestPractice_Kwai","source":"@site/blog/BestPractice_Kwai.md","title":"Best practice in Kwai: Apache Doris on Elasticsearch","description":"This article mainly focuses on the practice of Apache Doris on Elasticsearch (DOE) in Kwai\'s business.Kwai\u2019s commercial report engine provides advertisers with real-time query service for multi-dimensional analysis reports. And it also provides query service for multi-dimensional analysis reports for internal users. The engine is committed to dealing with high-performance, high-concurrency, and high-stability query problems in multi-dimensional analysis report cases. After using Doris, query becomes simple. We only need to synchronize the fact table and dim-table on a daily basis and Join while querying. By replacing Druid and Clickhouse with Doris, Doris basically covers all scenarios when we use Druid. In this way, Kwai\'s commercial report engine greatly improves the aggregation and analysis capabilities of massive data. During the use of Apache Doris, we also found some unexpected benefits: For example, the import method of Routine Load and Broker Load is relatively simple, which improves the query speed; The data occupation is greatly reduced; Doris supports the MySQL protocol, which is much easier for data analyst to fetch data and make charts.","date":"2022-12-14T00:00:00.000Z","tags":[{"inline":true,"label":"Best Practice","permalink":"/blog/tags/best-practice"}],"hasTruncateMarker":false,"authors":[{"name":"Xiang He","key":null,"page":null}],"frontMatter":{"title":"Best practice in Kwai: Apache Doris on Elasticsearch","language":"en","description":"This article mainly focuses on the practice of Apache Doris on Elasticsearch (DOE) in Kwai\'s business.Kwai\u2019s commercial report engine provides advertisers with real-time query service for multi-dimensional analysis reports. And it also provides query service for multi-dimensional analysis reports for internal users. The engine is committed to dealing with high-performance, high-concurrency, and high-stability query problems in multi-dimensional analysis report cases. After using Doris, query becomes simple. We only need to synchronize the fact table and dim-table on a daily basis and Join while querying. By replacing Druid and Clickhouse with Doris, Doris basically covers all scenarios when we use Druid. In this way, Kwai\'s commercial report engine greatly improves the aggregation and analysis capabilities of massive data. During the use of Apache Doris, we also found some unexpected benefits: For example, the import method of Routine Load and Broker Load is relatively simple, which improves the query speed; The data occupation is greatly reduced; Doris supports the MySQL protocol, which is much easier for data analyst to fetch data and make charts.","date":"2022-12-14","author":"Xiang He","tags":["Best Practice"],"image":"/images/best-practice.png"},"unlisted":false,"prevItem":{"title":"Apache Doris announced the official release of version 1.1.5","permalink":"/blog/release-note-1.1.5"},"nextItem":{"title":"Practice and optimization of Apache Doris in Xiaomi","permalink":"/blog/xiaomi_vector"}},"content":"\x3c!-- \\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n> Author: Xiang He, Head Developer of Big Data, Commercialization Team of Kwai\\n\\n![kv](/images/Kwai/en/kv.png)\\n\\n# 1 About Kwai\\n\\n## 1.1 Kwai\\nKwai(HKG:1024) is a social network for short videos and trends. Discover funny short videos, contribute to the virtual community with recordings, videos of your life, playing daily challenges or likes the best memes and videos. Share your life with short videos and choose from dozens of magical effects and filters for them.\\n\\n## 1.2 Kwai\'s Commercial Report Engine\\nKwai\u2019s commercial report engine provides advertisers with real-time query service for multi-dimensional analysis reports. And it also provides query service for multi-dimensional analysis reports for internal users. The engine is committed to dealing with high-performance, high-concurrency, and high-stability query problems in multi-dimensional analysis report cases.\\n\\n# 2 Previous Architecture\\n\\n## 2.1 Background\\nTraditional OLAP engines deal with multi-dimensional analysis in a more pre-modeled way, by building a data cube (Cube) to perform operations such as Drill-down, Roll-up, Slice, and Dice and Pivot. Modern OLAP analysis introduces the idea of \u200B\u200Ba relational model, representing data in two-dimensional relational tables. In the modeling process, usually there are two modeling methods. One is to ingest the data of multiple tables into one wide table through Join; the other is to use the star schema, divide the data into fact table and dim-table.  And then Join them when querying. \\nBoth options have some pros and cons:\\n\\nWide table:\\n\\nTaking the idea of \u200B\u200Bexchanging space for time. The primary key of the dim-table is the unique ID to fill all dimensions, and multiple dimension data is stored in redundant storage. Its advantage is that it is convenient to query, unnecessary to associate additional dim-tables, which is way better. The disadvantage is that if there is a change in dimension data, the entire table needs to be refreshed, which is bad for high-frequency Update.\\n\\nStar Schema:\\n\\nDimension data is completely separated from fact data. Dimension data is often stored in a dedicated engine (such as MySQL, Elasticsearch, etc.). When querying, dimension data is associated with the primary key. The advantage is that changes in dimension data do not affect fact data, which can support high-frequency Update operations. The disadvantage is that the query logic is relatively more complex, and multi-table Join may lead to performance loss.\\n\\n## 2.2 Requirement for an OLAP Engine\\n\\nIn Kwai\u2019s business, the commercial reports engine supports the real-time query of the advertising effect for advertisers. When building the report engine, we expect to meet the following requirements:\\n- Immersive data: the original data of a single table increases by ten billion every day\\n- High QPS in Query: thousand-level QPS on average\\n- High stability requirements: SLA level of 99.9999 %\\n\\nMost importantly, due to frequent changes in dimension data, dim-tables need to support Update operations up to thousand-level QPS and further support requirements such as fuzzy matching and word segmentation retrieval.\\nBased on the above requirements, we chose star schema and built a report engine architecture with Apache Druid and Elasticsearch.\\n\\n## 2.3 Previous Architecture: Based on Apache Druid\\n\\nWe chose the combination of Elasticsearch and Apache Druid. In data import, we use Flink to pre-aggregate the data at minute-evel, and use Kafka to pre-aggregate the data at hour-level. In data query, the application initiates a query request through RE Front API, and Re Query initiates queries to the dim-table engine (Elasticsearch and MySQL) and the extension engine respectively.\\n\\nDruid is a timing-based query engine that supports real-time data ingestion and is used to store and query large amounts of fact data. We adopt Elasticseach based on those concerns:\\n- High update frequency, QPS is around 1000\\n- Support word segmentation and fuzzy search, which is suitable for Kwai\\n- Supports high-level dim-table data, which can be directly qualified without adopting sub-database and sub-table just like MySQL database\\n- Supports data synchronization monitoring, and has check and recovery services as well\\n\\n## 2.4 Engine of the Reports\\n\\nThe report engine can be divided into two layers: REFront and REQuery. REMeta is an independent metadata management module. The report engine implements MEMJoin inside REQuery. It supports associative query between fact data in Druid and dimension data in Elasticsearch. And it also provides virtual cube query for upper-layer business, avoiding the explosion of complex cross-engine management and query logic.\\n\\n![page_1](/images/Kwai/en/page_1.png)\\n\\n# 3 New Architecture Based on Apache Doris\\n\\n## 3.1 Problems Remained \\nFirst, we came across a problem when we build the report engine. Mem Join is single-machine with serial execution. When the amount of data pulled from Elasticsearch exceeds 100,000 at a single time, the response time is close to 10s, and the user experience is poor. Moreover, using a single node to execute large-scale data Join will consume a lot of memory, causing Full GC.\\n\\nSecond, Druid\'s Lookup Join function is not so perfect, which is a big problem, and it cannot fully meet our business needs.\\n\\n## 3.2 Database Research\\n\\nSo we conducted a survey on popular OLAP databases in the industry, the most representative of which are Apache Doris and Clickhouse. We found out that Apache Doris is more capable of Join between large and wide tables. ClickHouse can support Broadcast memory-based Join, but the performance  is not good for the Join between large and wide tables with a large data volume. Both Doris and Clickhouse support detailed data storage, but the capability for concurrency of Clickhouse is low. On the contrary, Doris supports high-concurrency and low-latency query services, and a single machine supports up to thousands of QPS. When the concurrency increases, horizontal expansion of FE and BE can be supported. However, Clickhouse\'s data import is not able to support Transaction SQL, which cannot realize Exactly-once semantics and has limited ablility for standard SQL. In contrast, Doris provides Transaction SQL and atomicity for data import. Doris itself can ensure that messages in Kafka are not lost or re-subscribed, which is to say, Exactly-Once semantics is supported. ClickHouse has high learning cost, high operation and maintenance costs, and weak in distribution. The fact that it requires more customization and deeper technical strength is another problem. Doris is different. There are only two core components, FE and BE, and there are fewer external dependencies. We also found that because Doris is closer to the MySQL protocol, it is more convenient than Clickhouse and the cost of migration is not so large. In terms of horizontal expansion, Doris\' expansion and contraction can also achieve self-balancing, which is much better than that of Clickhouse.\\n\\nFrom this point of view, Doris can better improve the performance of Join and is much better in other aspects such as migration cost, horizontal expansion, and concurrency. However, Elasticsearch has inherent advantages in high-frequency Update.\\n\\nIt would be an ideal solution to deal with high-frequency Update and Join performance at the same time by building engines through Doris on Elasticsearch.\\n\\n## 3.3 Good Choice: Doris on Elasticsearch\\n\\nWhat is the query performance of Doris on Elasticsearch?\\n\\nFirst of all, Apache Doris is a real-time analytical database based on MPP architecture, with strong performance and strong horizontal expansion capability. Doris on Elasticsearch takes advantage on this capability and does a lot of query optimization. Secondly, after integrating Elasticsearch, we have also made a lot of optimizations to the query:\\n- Shard-level concurrency\\n- Automatic adaptation of row and column scanning, priority to column scanning\\n- Sequential read, terminated early\\n- Two-phase query becomes one-phase query\\n- Broadcast Join is especially friendly for small batch data\\n\\n![page_2](/images/Kwai/en/page_2.png)\\n\\n## 3.4 Doris on Elasticsearch\\n\\n### 3.4.1 Data Link Upgrade\\n\\nThe upgrade of the data link is relatively simple. In the first step, in Doris we build a new Olap table and configure the materialized view. Second, the routine load is initiated based on the Kafka topic of the previous fact data, and then real-time data is ingested. The third step is to ingest offline data from Hive\'s broker load. The last step is to create an Elasticsearch external table through Doris.\\n\\n![page_3](/images/Kwai/en/page_3.png)\\n\\n### 3.4.2 Upgrades of the Report Engine\\n\\n![page_4](/images/Kwai/en/page_4.png)\\n\\nNote: The MySQL dim-table associated above is based on future planning. Currently, Elasticsearch is mainly used as the dim-table engine\\n\\nReport Engine Adaptation\\n- Generate virtual cube table based on Doris\'s star schema\\n- Adapt to cube table query analysis, intelligent Push-down\\n- Gray Release\\n\\n# 4  Online Performance\\n\\n## 4.1 Fact Table Query Performance Comparison\\n\\nDruid\\n\\n![page_5](/images/Kwai/en/page_5.png)\\n\\nDoris\\n\\n![page_6](/images/Kwai/en/page_6.png)\\n\\n99th percentile of response time: \\nDruid: 270ms, Doris: 150ms and which is reduced by 45%\\n\\n## 4.2 Comparison of Cube Table Query Performance in Join\\n\\nDruid\\n\\n![page_7](/images/Kwai/en/page_7.png)\\n\\nDoris\\n\\n![page_8](/images/Kwai/en/page_8.png)\\n\\n99th percentile of response time: \\nDruid: 660ms, Doris: 440ms and which is reduced by 33%\\n\\n## 4.3 Benefits\\n\\n- The overall time consumption of 99 percentile is reduced by about 35%\\n- Resource saving about 50%\\n- Remove the complex logic of MemJoin from the report engine; Realize through DO(in the case of large query: dim-table results exceed 100,000, performance improvement exceeds 10 times, 10s to 1s)\\n- Richer query semantics (Mem Join is relatively simple and does not support complex queries)\\n\\n# 5  Summary and Plans\\n\\nIn Kwai\'s commercial business, Join queries between dimension data and fact data is very common. After using Doris, query becomes simple. We only need to synchronize the fact table and dim-table on a daily basis and Join while querying. By replacing Druid and Clickhouse with Doris, Doris basically covers all scenarios when we use Druid. In this way, Kwai\'s commercial report engine greatly improves the aggregation and analysis capabilities of massive data. During the use of Apache Doris, we also found some unexpected benefits: For example, the import method of Routine Load and Broker Load is relatively simple, which improves the query speed; The data occupation is greatly reduced; Doris supports the MySQL protocol, which is much easier for data analyst to fetch data and make charts.\\n\\nAlthough the Doris on Elasticsearch has fully meet our requirement, Elasticsearch external table still requires manual creation. However, Apache Doris recently released the latest version V1.2.0. The new version has added Multi-Catlog, which provides the ability to seamlessly access external table sources such as Hive, Elasticsearch, Hudi, and Iceberg. Users can connect to external tables through the CREATE CATALOG command, and Doris will automatically map the library and table information of the external dable. In this way, we don\'t need to manually create the Elasticsearch external tables to complete the mapping in the future, which greatly saves us time and cost of development and improves the efficiency of research and development. The power of other new functions such as Vectorization and Ligt Schema Change also gives us new expectations for Apache Doris. Bless Apache Doris!\\n\\n\\n# Contact Us\\n\\nApache Doris Website\uFF1Ahttp://doris.apache.org\\n\\nGithub\uFF1Ahttps://github.com/apache/doris\\n\\nDev Email\uFF1Adev@doris.apache.org"},{"id":"/xiaomi_vector","metadata":{"permalink":"/blog/xiaomi_vector","source":"@site/blog/xiaomi_vector.md","title":"Practice and optimization of Apache Doris in Xiaomi","description":"Xiaomi Group introduced Apache Doris in 2019. At present, Apache Doris has been widely used in dozens of business departments within Xiaomi. A set of data ecology with Apache Doris has been formed. This article is transcribed from an online meetup speech of the Doris community, aiming to share the practice of Apache Doris in Xiaomi.","date":"2022-12-08T00:00:00.000Z","tags":[{"inline":true,"label":"Best Practice","permalink":"/blog/tags/best-practice"}],"hasTruncateMarker":false,"authors":[{"name":"ZuoWei","key":null,"page":null}],"frontMatter":{"title":"Practice and optimization of Apache Doris in Xiaomi","description":"Xiaomi Group introduced Apache Doris in 2019. At present, Apache Doris has been widely used in dozens of business departments within Xiaomi. A set of data ecology with Apache Doris has been formed. This article is transcribed from an online meetup speech of the Doris community, aiming to share the practice of Apache Doris in Xiaomi.","date":"2022-12-08","author":"ZuoWei","tags":["Best Practice"],"image":"/images/best-practice.png"},"unlisted":false,"prevItem":{"title":"Best practice in Kwai: Apache Doris on Elasticsearch","permalink":"/blog/BestPractice_Kwai"},"nextItem":{"title":"Apache Doris announced the official release of version 1.2.0","permalink":"/blog/release-note-1.2.0"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n> Guide: Xiaomi Group introduced Apache Doris in 2019. At present, Apache Doris has been widely used in dozens of business departments within Xiaomi. A set of data ecology with Apache Doris has been formed. This article is transcribed from an online meetup speech of the Doris community, aiming to share the practice of Apache Doris in Xiaomi.\\n\\n> Author: ZuoWei, OLAP Engineer, Xiaomi\\n\\n![kv](/images/xiaomi/en/kv.png)\\n\\n# About Xiaomi\\n[Xiaomi Corporation](https://www.mi.com/global) (\u201CXiaomi\u201D or the \u201CGroup\u201D; HKG:1810), a consumer electronics and smart manufacturing company with smartphones and smart hardware connected by an Internet of Things (IoT) platform.  In 2021, Xiaomi\'s total revenue amounted to RMB328.3 billion(USD472,231,316,200), an increase of 33.5% year-over-year; Adjusted net profit was RMB22.0 billion(USD3,164,510,800), an increase of 69.5% year-over-year.\\n\\nDue to the growing need of data analysis, Xiaomi Group introduced Apache Doris in 2019. As one of the earliest users of Apache Doris, Xiaomi Group has been deeply involved in the open-source community. After three years of development, Apache Doris has been widely used in dozens of business departments within Xiaomi, such as Advertising, New Retail, Growth Analysis, Dashboards, UserPortraits, [AISTAR](https://airstar.com/home), [Xiaomi Youpin](https://www.xiaomiyoupin.com). Within Xiaomi, a data ecosystem has been built around Apache Doris. \\n\\n![page_1](/images/xiaomi/en/page_1.jpg)\\n\\nAt present, Apache Doris already has dozens of clusters in Xiaomi, with an overall scale of hundreds of virtual machines . Among them, the largest single cluster reaches nearly 100 nodes, with dozens of real-time data synchronization tasks. And the largest daily increment of a single table rocket to 12 billion, supporting PB-level storage. And a single cluster can support more than 20,000 multi-dimensional analysis queries per day.\\n\\n# Architecture Evolution\\nThe original intention of Xiaomi to introduce Apache Doris is to solve the problems encountered in user behavior analysis. With the development of Xiaomi\'s Internet business, the demand for growth analysis using user behavior data is becoming stronger and stronger. If each business branch builds its own growth analysis system, it will not only be costly, but also inefficient. Therefore, if there is a product that can help them stop worrying about underlying complex technical details, it would be great to have relevant business personnel focus on their own technical work. In this way, it can greatly improve work efficiency. Therefore, Xiaomi Big Data and the cloud platform jointly developed the growth analysis system called Growing Analytics (referred to as GA), which aims to provide a flexible multi-dimensional real-time query and analysis platform, which can manage data access and query solutions in a unified way, and help business branches to refine operation.\\n\\n## Previous Architecture\\nThe growth analysis platform project was established in mid-2018. At that time, based on the consideration of development time and cost, Xiaomi reused various existing big data basic components (HDFS, Kudu, SparkSQL, etc.) to build a growth analysis query system based on Lambda architecture. The architecture of the first version of the GA system is shown in the figure below, including the following aspects:\\n\\n- Data Source: The data source is the front-end embedded data and user behavior data.\\n- Data Access: The event tracking data is uniformly cleaned and ingested into Xiaomi\'s internal self-developed message queue, and the data is imported into Kudu through Spark Streaming.\\n- Storage: Separate hot and cold data in the storage layer. Hot data is stored in Kudu, and cold data is stored in HDFS. At the same time, partitioning is carried out in the storage layer. When the partition unit is day, part of the data will be cooled and stored on HDFS every night.\\n- Compute and Query: In the query layer, use SparkSQL to perform federated queries on the data on Kudu and HDFS, and finally display the query results on the front-end page.\\n\\n![page_2](/images/xiaomi/en/page_2.jpg)\\n\\nAt that time, the first version of the growth analysis platform helped us solve a series of problems in the user operation process, but there were also two problems:\\n\\n### Problem No.1: Scattered components\\nSince the historical architecture is based on the combination of SparkSQL + Kudu + HDFS, too many dependent components lead to high operation and maintenance costs. The original design is that each component uses the resources of the public cluster, but in practice, it is found that during the execution of the query job, the query performance is easily affected by other jobs in the public cluster, and query jitter is prone to occur, especially when reading data from the HDFS public cluster , sometimes slower.\\n\\n### Problem No.2: High resource consumption\\nWhen querying through SparkSQL, the latency is relatively high. SparkSQL is a query engine designed based on a batch processing system. In the process of exchanging data shuffle between each stage, it still needs to be placed on the disk, and the delay in completing the SQL query is relatively high. In order to ensure that SQL queries are not affected by resources, we ensure query performance by adding machines. However, in practice, we find that there is limited room for performance improvement. This solution cannot make full use of machine resources to achieve efficient queries. A certain waste of resources.\\n\\nIn response to the above two problems, our goal is to seek an MPP database that integrates computing and storage to replace our current storage and computing layer components. After technical selection, we finally decided to use Apache Doris to replace the older generation of historical architecture.\\n\\n## New Choice\\nPopular MPP-based query engines such as Impala and Presto, can efficiently support SQL queries, but they still need to rely on Kudu, HDFS, Hive Metastore and other storage system, which increase the operation and maintenance costs. At the same time, due to the separation of storage and compute, the query engine cannot easily find the data changes in the storage layer, resulting in bad performance in detailed query optimization. If you want to cache at the SQL layer, you cannot guarantee that the query results are up-to-date.\\n\\nApache Doris is a top-level project of the Apache Foundation. It is mainly positioned as a high-performance, real-time analytical database, and is mainly used to solve reports and multi-dimensional analysis. It integrates Google Mesa and Cloudera Impala technologies. We conducted an in-depth performance tests on Doris and communicated with the community many times. And finally, we determined to replace the previous computing and storage components with Doris. \\n\\n## New Architecture Based on Apache Doris\\nThe new architecture obtains event tracking data from the data source. Then data is ingested  into Apache Doris. Query results can be directly displayed in the applications. In this way, Doris has truly realized the unification of computing, storage, and resource management tools.\\n\\n![page_3](/images/xiaomi/en/page_3.jpg)\\n\\nWe chose Doris because:\\n- Doris has excellent query performance and can meet our business needs.\\n- Doris supports standard SQL, and the learning cost is low.\\n- Doris does not depend on other external components and is easy to operate and maintain.\\n- The Apache Doris community is very active and friendly, crowded with contributors. It is easier for further versions upgrades and convenient for maintenance.\\n\\n## Query Performance Comparision between Apache Doris & Spark SQL\\nNote: The comparison is based on Apache Doris V0.13\\n\\n![page_4](/images/xiaomi/en/page_4.jpg)\\n\\nWe selected a business model with an average daily data volume of about 1 billion, and conducted performance tests on Doris in different scenarios, including 6 event analysis scenarios, 3 retention analysis scenarios, and 3 funnel analysis scenarios. After comparing it with the previous architecture(SparkSQL+Kudu+HDFS), we found out:\\n- In the event analysis scenario, the average query time was reduced by 85%.\\n- In the scenarios of retention analysis and funnel analysis, the average query time was reduced by 50%.\\n\\n# Real Practice\\nBelow we will introduce our experience of data import, data query, A/B test in the business application of Apache Doris.\\n\\n## Data Import\\n\\n![page_5](/images/xiaomi/en/page_5.jpg)\\n\\nXiaomi writes data into Doris mainly through Stream Load, Broker Load and a small amount of data by Insert. Usually data is generally ingested into the message queue first, which is divided into real-time and offline data.\\n\\n### How to write real-time data into Apache Doris: \\nAfter part of real-time data processed by Flink, they will be ingested into Doris through  Flink-Doris-Connector provided by Apache Doris. The rest of the data is ingested through Spark Streaming. The bottom layer of these two writing approaches both rely on the Stream Load provided by Apache Doris.\\n\\n### How to write offline data into Apache Doris: \\nAfter offline data is partially ingested into Hive, they will be ingested into Doris through Xiaomi\'s data import tool. Users can directly submit Broker Load tasks to the Xiaomi\'s data import tool and import data directly into Doris, or import data through Spark SQL, which relies on the Spark-Doris-Connector provided by Apache Doris. Spark Doris Connector is actually the encapsulation of Stream Load.\\n\\n## Data Qurey\\n\\n![page_6](/images/xiaomi/en/page_6.jpg)\\n\\nUsers can query after data import is done. Inside Xiaomi, we query through our data platform. Users can perform visual queries on Doris through Xiaomi\'s data platform, and conduct user behavior analysis and user portrait analysis. In order to help our teams conduct event analysis, retention analysis, funnel analysis, path analysis and other behavioral analysis, we have added corresponding UDF (User Defined Function) and UDAF (User Defined Aggregate Function) to Doris.\\n\\nIn the upcoming version 1.2, Apache Doris adds the function of synchronizing metadata through external table, such as Hive/Hudi/Iceberg and Multi Catalog tool. External table query improves performance, and the ability to access external tables greatly increases ease of use. In the future, we will consider querying Hive and Iceberg data directly through Doris, which builds an architecture of datalake.\\n\\n## A/B Test\\nIn real business, the A/B test is a method of comparing two versions of strategies against each other to determine which one performs better. A/B test is essentially an experiment where two or more variants of a page are shown to users at random, and statistical analysis. It is popular approach used to determine which variation performs better for a given conversion goal. Xiaomi\'s A/B test platform is an operation tool product that conducts the A/B test with experimental grouping, traffic splitting, and scientific evaluation to assist in decision making. Xiaomi\'s A/B test platform has several query applications: user deduplication, indicator summation, covariance calculation, etc. The query types will involve Count (distinct), Bitmap, Like, etc.\\n\\nApache Doris also provides services to Xiaomi\'s A/B test platform. Everyday, Xiaomi\'s A/B test platform needs to process a temendous amount of data with billions of queries. That\'s why Xiaomi\'s A/B test platform is eager to improve the query performance. \\n\\nApache Doris V1.1 released just in time and has fully supported vectorization in the processing and storage. Compared with the non-vectorized version, the query performance has been significantly improved. It is time to update Xiaomi\'s Doris cluster to the latest version. That\'s why we first launched the latest vectorized version of Doris on Xiaomi\'s A/B test platform.\\n\\n## Test before Launch\\nNote: The following tests are based on Apache Doris V1.1.2\\n\\nWe built a test cluster for Apache Doris V1.1.2, which is as big as that of the Xiaomi online Apache Doris V0.13 version, to test before the vectorization version goes online. The test is divided into two aspects: single SQL parrellel query test and batch SQL concurrent query test.\\n\\nThe configurations of the two clusters are exactly the same, and the specific configuration information is as follows:\\n- Scale: 3 FEs + 89 virtual machines\\n- CPU: Intel(R) Xeon(R) Silver 4216 CPU @ 2.10GHz 16 cores 32 threads \xd7 2\\n- Memory: 256GB\\n- Disk: 7.3TB \xd7 12 HDD\\n\\n### Single SQL Parrellel Query Test\\nWe choose 7 classic queries in the Xiaomi A/B test. For each query, we limited the time range to 1 day, 7 days, and 20 days for testing, where the daily partition data size is about 3.1 billion (the data volume is about 2 TB). The test results are shown in the figures:\\n\\n![page_7](/images/xiaomi/en/page_7.jpg)\\n\\n![page_8](/images/xiaomi/en/page_8.jpg)\\n\\n![page_9](/images/xiaomi/en/page_9.jpg)\\n\\nThe Apache Doris V1.1.2 has at least 3~5 times performance improvement compared to the Xiaomi online Doris V0.13, which is remarkable.\\n\\n## Optimization\\nNote: The following tests are based on Apache Doris V1.1.2\\n\\nBased on Xiaomi\'s A/B test business data, we tuned Apache Doris V1.1.2 and conducted concurrent query tests on the tuned Doris V1.1.2 and Xiaomi\'s online Doris V0.13. The test results are as follows.\\n\\n### Optimization in Test 1\\nWe choose user deduplication, index summation, and covariance calculation query(the total number of SQL is 3245) in the A/B test to conduct concurrent query tests on the two versions. The single-day partition data of the table is about 3.1 billion (the amount of data is about 2 TB) and the query will be based on the latest week\'s data. The test results are shown in the figures:\\n\\n![page_10](/images/xiaomi/en/page_10.jpg)\\n\\nCompared with Apache Doris V0.13, the overall average latency of Doris V1.1.2 is reduced by about 48%, and the P95 latency is reduced by about 49%. In this test, the query performance of Doris V1.1.2 was nearly doubled compared to Doris V0.13.\\n\\n### Optimization in Test 2\\nWe choose 7 A/B test reports to test the two versions. Each A/B test report is corresponded to two modules in Xiaomi A/B test platform and each module represents thousands of SQL query. Each report submits query tasks to the cluster where the two versions reside at the same concurrency. The test results are shown in the figure:\\n\\n![page_11](/images/xiaomi/en/page_11.jpg)\\n\\nCompared with Doris V0.13, Doris V1.1.2 reduces the overall average latency by around 52%. In the test, the query performance of Doris V1.1.2 version was more than 1 time higher than that of Doris V0.13. \\n\\n### Optimization in Test 3\\nTo verify the performance of the tuned Apache Doris V1.1.2 in other cases, we choose the Xiaomi user behavior analysis to conduct concurrent query performance tests of Doris V1.1.2 and Doris V0.13. We choose behavior analysis query for 4 days on October 24, 25, 26 and 27, 2022. The test results are shown in the figures:\\n\\n![page_12](/images/xiaomi/en/page_12.jpg)\\n\\nCompared with Doris V0.13, the overall average latency of Doris V1.1.2 has been reduced by about 77%, and the P95 latency has been reduced by about 83%. In this test, the query performance of Doris V1.1.2 version is 4~6 times higher than that of Doris V0.13.\\n\\n# Conclusion\\nSince we adopted Apache Doris in 2019, Apache Doris has currently served dozens of businesses and sub-brands within Xiaomi, with dozens of clusters and hundreds of nodes. It completes more than 10,000 user online analysis queries every day and is responsible for most of the online analysis in Xiaomi.\\n\\nAfter performance test and tuning, Apache Doris V1.1.2 has met the launch requirements of the Xiaomi A/B test platform and does well in query performance and stability. In some cases, it even exceeds our expectations, such as the overall average latency being reduced by about 77% in our tuned version.\\n\\nMeanwhile, some functions have in the above been released in Apache Doris V1.0 or V1.1,  some PRs have been merged into the community Master Fork and should be released soon. Recently the activity of the community has been greatly enhanceed. We are glad to see that Apache Doris has become more and more mature, and stepped forward to an integrated datalake. We truly believe that in the future, more data analysis will be explored and realized within Apache Doris.\\n\\n\\n# Contact Us\\nApache Doris Website\uFF1Ahttp://doris.apache.org\\n\\nGithub Homepage\uFF1Ahttps://github.com/apache/doris\\n\\nEmail to DEV\uFF1Adev@doris.apache.org"},{"id":"/release-note-1.2.0","metadata":{"permalink":"/blog/release-note-1.2.0","source":"@site/blog/release-note-1.2.0.md","title":"Apache Doris announced the official release of version 1.2.0","description":"Dear community, we are pleased to announce that we have officially released Apache Doris 1.2.0 on December 7, 2022","date":"2022-12-07T00:00:00.000Z","tags":[{"inline":true,"label":"Release Notes","permalink":"/blog/tags/release-notes"}],"hasTruncateMarker":false,"authors":[{"name":"Apache Doris","key":null,"page":null}],"frontMatter":{"title":"Apache Doris announced the official release of version 1.2.0","description":"Dear community, we are pleased to announce that we have officially released Apache Doris 1.2.0 on December 7, 2022","date":"2022-12-7","author":"Apache Doris","tags":["Release Notes"],"image":"/images/release-notes.png"},"unlisted":false,"prevItem":{"title":"Practice and optimization of Apache Doris in Xiaomi","permalink":"/blog/xiaomi_vector"},"nextItem":{"title":"JD.com\'s exploration and practice with Apache Doris in real time OLAP","permalink":"/blog/JD_OLAP"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n\\n\\nDear Community, after months of polishing, we are pleased to announce the release of Apache Doris 1.2.0 on December 07, 2022! \\n\\n\\n## Highlights\\n\\n1. Full Vectorizied-Engine support, greatly improved performance\\n\\n\\tIn the standard ssb-100-flat benchmark, the performance of 1.2 is 2 times faster than that of 1.1; in complex TPCH 100 benchmark, the performance of 1.2 is 3 times faster than that of 1.1.\\n\\n2. Merge-on-Write Unique Key\\n\\n\\tSupport Merge-On-Write on Unique Key Model. This mode marks the data that needs to be deleted or updated when the data is written, thereby avoiding the overhead of Merge-On-Read when querying, and greatly improving the reading efficiency on the updateable data model.\\n\\n3. Multi Catalog\\n\\n\\tThe multi-catalog feature provides Doris with the ability to quickly access external data sources for access. Users can connect to external data sources through the `CREATE CATALOG` command. Doris will automatically map the library and table information of external data sources. After that, users can access the data in these external data sources just like accessing ordinary tables. It avoids the complicated operation that the user needs to manually establish external mapping for each table.\\n   \\n    Currently this feature supports the following data sources:\\n   \\n    1. Hive Metastore: You can access data tables including Hive, Iceberg, and Hudi. It can also be connected to data sources compatible with Hive Metastore, such as Alibaba Cloud\'s DataLake Formation. Supports data access on both HDFS and object storage.\\n    2. Elasticsearch: Access ES data sources.\\n    3. JDBC: Access MySQL through the JDBC protocol.\\n   \\n    Documentation: https://doris.apache.org/zh-CN/docs/dev/ecosystem/external-table/multi-catalog)\\n\\n    > Note: The corresponding permission level will also be changed automatically, see the \\"Upgrade Notes\\" section for details.\\n   \\n4. Light table structure changes\\n\\nIn the new version, it is no longer necessary to change the data file synchronously for the operation of adding and subtracting columns to the data table, and only need to update the metadata in FE, thus realizing the millisecond-level Schema Change operation. Through this function, the DDL synchronization capability of upstream CDC data can be realized. For example, users can use Flink CDC to realize DML and DDL synchronization from upstream database to Doris.\\n\\nDocumentation: https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Definition-Statements/Create/CREATE-TABLE\\n\\nWhen creating a table, set `\\"light_schema_change\\"=\\"true\\"` in properties.\\n\\n5. JDBC facade\\n\\n\\tUsers can connect to external data sources through JDBC. Currently supported:\\n\\n\\t  - MySQL\\n\\t  - PostgreSQL\\n\\t  - Oracle\\n\\t  - SQL Server\\n\\t  - Clickhouse\\n\\n\\tDocumentation: https://doris.apache.org/zh-CN/docs/dev/ecosystem/external-table/jdbc-of-doris/\\n\\n\\t> Note: The ODBC feature will be removed in a later version, please try to switch to the JDBC.\\n\\n6. JAVA UDF\\n\\n\\tSupports writing UDF/UDAF in Java, which is convenient for users to use custom functions in the Java ecosystem. At the same time, through technologies such as off-heap memory and Zero Copy, the efficiency of cross-language data access has been greatly improved.\\n\\n\\tDocument: https://doris.apache.org/zh-CN/docs/dev/ecosystem/udf/java-user-defined-function\\n\\n\\tExample: https://github.com/apache/doris/tree/master/samples/doris-demo\\n\\t\\n7. Remote UDF\\n\\n\\tSupports accessing remote user-defined function services through RPC, thus completely eliminating language restrictions for users to write UDFs. Users can use any programming language to implement custom functions to complete complex data analysis work.\\n\\n\\tDocumentation: https://doris.apache.org/zh-CN/docs/ecosystem/udf/remote-user-defined-function\\n\\n\\tExample: https://github.com/apache/doris/tree/master/samples/doris-demo\\n   \\n8. More data types support\\n\\n\\t- Array type\\n\\n\\t\\tArray types are supported. It also supports nested array types. In some scenarios such as user portraits and tags, the Array type can be used to better adapt to business scenarios. At the same time, in the new version, we have also implemented a large number of data-related functions to better support the application of data types in actual scenarios.\\n\\n\\tDocumentation: https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Types/ARRAY\\n\\n\\tRelated functions: https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-functions/array-functions/array_max\\n        \\n\\t- Jsonb type\\n\\n\\t\\tSupport binary Json data type: Jsonb. This type provides a more compact json encoding format, and at the same time provides data access in the encoding format. Compared with json data stored in strings, it is several times newer and can be improved.\\n\\n\\tDocumentation: https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Types/JSONB\\n\\n\\tRelated functions: https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-functions/json-functions/jsonb_parse\\n          \\n\\t- Date V2\\n\\t\\n\\t\\tSphere of influence:\\n\\n\\t\\t1. The user needs to specify datev2 and datetimev2 when creating the table, and the date and datetime of the original table will not be affected.\\n\\t\\t2. When datev2 and datetimev2 are calculated with the original date and datetime (for example, equivalent connection), the original type will be cast into a new type for calculation\\n\\t\\t3. The example is in the documentation\\n\\n\\t\\tDocumentation: https://doris.apache.org/docs/dev/sql-manual/sql-reference/Data-Types/DATEV2\\n\\t\\n\\t \\n## More\\n\\n1. A new memory management framework\\n\\n\\tDocumentation: https://doris.apache.org/zh-CN/docs/dev/admin-manual/maint-monitor/memory-management/memory-tracker\\n\\n2. Table Valued Function\\n\\n\\tDoris implements a set of Table Valued Function (TVF). TVF can be regarded as an ordinary table, which can appear in all places where \\"table\\" can appear in SQL.\\n\\n\\tFor example, we can use S3 TVF to implement data import on object storage:\\n\\n\\t```\\n\\tinsert into tbl select * from s3(\\"s3://bucket/file.*\\", \\"ak\\" = \\"xx\\", \\"sk\\" = \\"xxx\\") where c1 > 2;\\n\\t```\\n\\n\\tOr directly query data files on HDFS:\\n\\t\\n\\t```\\n\\tinsert into tbl select * from hdfs(\\"hdfs://bucket/file.*\\") where c1 > 2;\\n\\t```\\n\\t\\n\\tTVF can help users make full use of the rich expressiveness of SQL and flexibly process various data.\\n\\n    Documentation:\\n   \\n    https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-functions/table-functions/s3\\n   \\n    https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-functions/table-functions/hdfs\\n   \\n3. A more convenient way to create partitions\\n\\n\\tSupport for creating multiple partitions within a time range via the `FROM TO` command.\\n\\n4. Column renaming\\n\\n\\tFor tables with Light Schema Change enabled, column renaming is supported.\\n\\n\\tDocumentation: https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Definition-Statements/Alter/ALTER-TABLE-RENAME\\n\\t\\n5. Richer permission management\\n\\n\\t- Support row-level permissions\\n\\t\\n\\t\\tRow-level permissions can be created with the `CREATE ROW POLICY` command.\\n\\t\\n\\t\\tDocumentation: https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Definition-Statements/Create/CREATE-POLICY\\n\\t\\n\\t- Support specifying password strength, expiration time, etc.\\n\\t\\n\\t- Support for locking accounts after multiple failed logins.\\n\\t\\n\\t\\tDocumentation: https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Account-Management-Statements/ALTER-USER\\n\\n6. Import\\n\\n\\t- CSV import supports csv files with header.\\n\\t\\n\\t\\tSearch for `csv_with_names` in the documentation: https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Manipulation-Statements/Load/STREAM-LOAD/\\n\\t\\n\\t- Stream Load adds `hidden_columns`, which can explicitly specify the delete flag column and sequence column.\\n\\t\\n\\t\\tSearch for `hidden_columns` in the documentation: https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Manipulation-Statements/Load/STREAM-LOAD\\n\\t\\n\\t- Spark Load supports Parquet and ORC file import.\\n\\t\\n\\t- Support for cleaning completed imported Labels\\n\\t  \\n\\t  Documentation: https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Manipulation-Statements/Load/CLEAN-LABEL\\n\\t\\n\\t- Support batch cancellation of import jobs by status\\n\\t\\n\\t\\tDocumentation: https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Manipulation-Statements/Load/CANCEL-LOAD\\n\\t\\n\\t- Added support for Alibaba Cloud oss, Tencent Cloud cos/chdfs and Huawei Cloud obs in broker load.\\n\\t\\t\\n\\t\\tDocumentation: https://doris.apache.org/zh-CN/docs/dev/advanced/broker\\n\\t\\n\\t- Support access to hdfs through hive-site.xml file configuration.\\n\\t\\n\\t\\tDocumentation: https://doris.apache.org/zh-CN/docs/dev/admin-manual/config/config-dir\\n\\n7. Support viewing the contents of the catalog recycle bin through `SHOW CATALOG RECYCLE BIN` function.\\n\\n\\tDocumentation: https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Show-Statements/SHOW-CATALOG-RECYCLE-BIN\\n\\n8. Support `SELECT * EXCEPT` syntax.\\n\\n\\tDocumentation: https://doris.apache.org/zh-CN/docs/dev/data-table/basic-usage\\n\\n9. OUTFILE supports ORC format export. And supports multi-byte delimiters.\\n   \\n\\tDocumentation: https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Manipulation-Statements/OUTFILE\\n\\n10. Support to modify the number of Query Profiles that can be saved through configuration.\\n\\n\\tDocument search FE configuration item: max_query_profile_num\\n\\t\\n11. The DELETE statement supports IN predicate conditions. And it supports partition pruning.\\n\\n\\tDocumentation: https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Manipulation-Statements/Manipulation/DELETE\\n\\n12. The default value of the time column supports using `CURRENT_TIMESTAMP`\\n\\n\\tSearch for \\"CURRENT_TIMESTAMP\\" in the documentation: https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Definition-Statements/Create/CREATE-TABLE\\n\\n13. Add two system tables: backends, rowsets\\n\\n\\tDocumentation:\\n\\n\\thttps://doris.apache.org/zh-CN/docs/dev/admin-manual/system-table/backends\\n\\n\\thttps://doris.apache.org/zh-CN/docs/dev/admin-manual/system-table/rowsets\\n\\n14. Backup and restore\\n\\n\\t- The Restore job supports the `reserve_replica` parameter, so that the number of replicas of the restored table is the same as that of the backup.\\n\\t\\n\\t- The Restore job supports `reserve_dynamic_partition_enable` parameter, so that the restored table keeps the dynamic partition enabled.\\n\\t\\n\\tDocumentation: https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Definition-Statements/Backup-and-Restore/RESTORE\\n\\t\\n\\t- Support backup and restore operations through the built-in libhdfs, no longer rely on broker.\\n\\t\\n\\tDocumentation: https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Definition-Statements/Backup-and-Restore/CREATE-REPOSITORY\\n\\n15. Support data balance between multiple disks on the same machine\\n\\n\\tDocumentation:\\n\\t\\n\\thttps://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Database-Administration-Statements/ADMIN-REBALANCE-DISK\\n\\t\\n\\thttps://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Database-Administration-Statements/ADMIN-CANCEL-REBALANCE-DISK\\n\\n16. Routine Load supports subscribing to Kerberos-authenticated Kafka services.\\n\\n\\tSearch for kerberos in the documentation: https://doris.apache.org/zh-CN/docs/dev/data-operate/import/import-way/routine-load-manual\\n\\n17. New built-in-function\\n\\n\\tAdded the following built-in functions:\\n\\t\\n\\t- `cbrt`\\n\\t- `sequence_match/sequence_count`\\n\\t- `mask/mask_first_n/mask_last_n`\\n\\t- `elt`\\n\\t- `any/any_value`\\n\\t- `group_bitmap_xor`\\n\\t- `ntile`\\n\\t- `nvl`\\n\\t- `uuid`\\n\\t- `initcap`\\n\\t- `regexp_replace_one/regexp_extract_all`\\n\\t- `multi_search_all_positions/multi_match_any`\\n\\t- `domain/domain_without_www/protocol`\\n\\t- `running_difference`\\n\\t- `bitmap_hash64`\\n\\t- `murmur_hash3_64`\\n\\t- `to_monday`\\n\\t- `not_null_or_empty`\\n\\t- `window_funnel`\\n\\t- `group_bit_and/group_bit_or/group_bit_xor`\\n\\t- `outer combine`\\n\\t- and all array functions\\n\\n\\n## Upgrade Notices\\n\\n### Known Issues\\n\\n- Use JDK11 will cause BE crash, please use JDK8 instead.\\n\\n### Behavior Changes\\n\\n- Permission level changes\\n\\n\\tBecause the catalog level is introduced, the corresponding user permission level will also be changed automatically. The rules are as follows:\\n\\t\\n\\t- GlobalPrivs and ResourcePrivs remain unchanged\\n\\t- Added CatalogPrivs level.\\n\\t- The original DatabasePrivs level is added with the internal prefix (indicating the db in the internal catalog)\\n\\t- Add the internal prefix to the original TablePrivs level (representing tbl in the internal catalog)\\n\\n- In GroupBy and Having clauses, match on column names in preference to aliases. (#14408)\\n\\n- Creating columns starting with `mv_` is no longer supported. `mv_` is a reserved keyword in materialized views (#14361)\\n\\n- Removed the default limit of 65535 rows added by the order by statement, and added the session variable `default_order_by_limit` to configure this limit. (#12478)\\n\\n- In the table generated by \\"Create Table As Select\\", all string columns use the string type uniformly, and no longer distinguish varchar/char/string (#14382)\\n\\n- In the audit log, remove the word `default_cluster` before the db and user names. (#13499) (#11408)\\n\\n- Add sql digest field in audit log (#8919)\\n\\n- The union clause always changes the order by logic. In the new version, the order by clause will be executed after the union is executed, unless explicitly associated by parentheses. (#9745)\\n\\n- During the decommission operation, the tablet in the recycle bin will be ignored to ensure that the decomission can be completed. (#14028)\\n\\n- The returned result of Decimal will be displayed according to the precision declared in the original column, or according to the precision specified in the cast function. (#13437)\\n\\n- Changed column name length limit from 64 to 256 (#14671)\\n\\n- Changes to FE configuration items\\n\\n  - The `enable_vectorized_load` parameter is enabled by default. (#11833)\\n\\n  - Increased `create_table_timeout` value. The default timeout for table creation operations will be increased. (#13520)\\n\\n  - Modify `stream_load_default_timeout_second` default value to 3 days.\\n\\n  - Modify the default value of `alter_table_timeout_second` to one month.\\n\\n  - Increase the parameter `max_replica_count_when_schema_change` to limit the number of replicas involved in the alter job, the default is 100000. (#12850)\\n\\n  - Add `disable_iceberg_hudi_table`. The iceberg and hudi appearances are disabled by default, and the multi catalog function is recommended. (#13932)\\n\\n- Changes to BE configuration items\\n\\n  - Removed `disable_stream_load_2pc` parameter. 2PC\'s stream load can be used directly. (#13520)\\n\\n  - Modify `tablet_rowset_stale_sweep_time_sec` from 1800 seconds to 300 seconds.\\n\\n  - Redesigned configuration item name about compaction (#13495)\\n\\n  - Revisited parameter about memory optimization (#13781)\\n\\n- Session variable changes\\n\\n   - Modify the variable `enable_insert_strict` to true by default. This will cause some insert operations that could be executed before, but inserted illegal values, to no longer be executed. (11866)\\n\\n   - Modified variable `enable_local_exchange` to default to true (#13292)\\n\\n   - Default data transmission via lz4 compression, controlled by variable `fragment_transmission_compression_codec` (#11955)\\n\\n   - Add `skip_storage_engine_merge` variable for debugging unique or agg model data (#11952)\\n   \\n     Documentation: https://doris.apache.org/zh-CN/docs/dev/advanced/variables\\n\\n- The BE startup script will check whether the value is greater than 200W through `/proc/sys/vm/max_map_count`. Otherwise, the startup fails. (#11052)\\n\\n- Removed mini load interface (#10520)\\n\\n- FE Metadata Version\\n\\n\\tFE Meta Version changed from 107 to 114, and cannot be rolled back after upgrading.\\n\\t\\n### During Upgrade\\n\\n1. Upgrade preparation\\n  \\n   - Need to replace: lib, bin directory (start/stop scripts have been modified)\\n   \\n   - BE also needs to configure JAVA_HOME, and already supports JDBC Table and Java UDF.\\n   \\n   - The default JVM Xmx parameter in fe.conf is changed to 8GB.\\n\\n2. Possible errors during the upgrade process\\n  \\n   - The repeat function cannot be used and an error is reported: `vectorized repeat function cannot be executed`, you can turn off the vectorized execution engine before upgrading. (#13868)\\n   \\n   - schema change fails with error: `desc_tbl is not set. Maybe the FE version is not equal to the BE` (#13822)\\n   \\n   - Vectorized hash join cannot be used and an error will be reported. `vectorized hash join cannot be executed`. You can turn off the vectorized execution engine before upgrading. (#13753)\\n\\n\\tThe above errors will return to normal after a full upgrade.\\n\\t\\n### Performance Impact\\n\\n- By default, JeMalloc is used as the memory allocator of the new version BE, replacing TcMalloc (#13367)\\n\\n- The batch size in the tablet sink is modified to be at least 8K. (#13912)\\n\\n- Disable chunk allocator by default (#13285)\\n\\n### API Changes\\n\\n- BE\'s http api error return information changed from `{\\"status\\": \\"Fail\\", \\"msg\\": \\"xxx\\"}` to more specific ``{\\"status\\": \\"Not found\\", \\"msg\\": \\"Tablet not found. tablet_id=1202\\"}``(#9771)\\n\\n- In `SHOW CREATE TABLE`, the content of comment is changed from double quotes to single quotes (#10327)\\n\\n- Support ordinary users to obtain query profile through http command. (#14016)\\nDocumentation: https://doris.apache.org/zh-CN/docs/dev/admin-manual/http-actions/fe/manager/query-profile-action\\n\\n- Optimized the way to specify the sequence column, you can directly specify the column name. (#13872)\\nDocumentation: https://doris.apache.org/zh-CN/docs/dev/data-operate/update-delete/sequence-column-manual\\n\\n- Increase the space usage of remote storage in the results returned by `show backends` and `show tablets` (#11450)\\n\\n- Removed Num-Based Compaction related code (#13409)\\n\\n- Refactored BE\'s error code mechanism, some returned error messages will change (#8855)\\nother\\n\\n- Support Docker official image.\\n\\n- Support compiling Doris on MacOS(x86/M1) and ubuntu-22.04\\n  Documentation: https://doris.apache.org/zh-CN/docs/dev/install/source-install/compilation-mac/\\n\\n- Support for image file verification.\\n\\n  Documentation: https://doris.apache.org/zh-CN/docs/dev/admin-manual/maint-monitor/metadata-operation/\\n\\n- script related\\n\\n  - The stop scripts of FE and BE support exiting FE and BE via the `--grace` parameter (use kill -15 signal instead of kill -9)\\n\\n  - FE start script supports checking the current FE version via --version (#11563)\\n\\n - Support to get the data and related table creation statement of a tablet through the `ADMIN COPY TABLET` command, for local problem debugging (#12176)\\n\\n\\tDocumentation: https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Database-Administration-Statements/ADMIN-COPY-TABLET\\n\\n- Support to obtain a table creation statement related to a SQL statement through the http api for local problem reproduction (#11979)\\n\\n\\tDocumentation: https://doris.apache.org/zh-CN/docs/dev/admin-manual/http-actions/fe/query-schema-action\\n\\n- Support to close the compaction function of this table when creating a table, for testing (#11743)\\n\\n\\tSearch for \\"disble_auto_compaction\\" in the documentation: https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Definition-Statements/Create/CREATE-TABLE\\n\\t\\n\\n## Big Thanks\\n\\nThanks to ALL who contributed to this release! (alphabetically)\\n```\\n@924060929\\n@a19920714liou\\n@adonis0147\\n@Aiden-Dong\\n@aiwenmo\\n@AshinGau\\n@b19mud\\n@BePPPower\\n@BiteTheDDDDt\\n@bridgeDream\\n@ByteYue\\n@caiconghui\\n@CalvinKirs\\n@cambyzju\\n@caoliang-web\\n@carlvinhust2012\\n@catpineapple\\n@ccoffline\\n@chenlinzhong\\n@chovy-3012\\n@coderjiang\\n@cxzl25\\n@dataalive\\n@dataroaring\\n@dependabot[bot]\\n@dinggege1024\\n@DongLiang-0\\n@Doris-Extras\\n@eldenmoon\\n@EmmyMiao87\\n@englefly\\n@FreeOnePlus\\n@Gabriel39\\n@gaodayue\\n@geniusjoe\\n@gj-zhang\\n@gnehil\\n@GoGoWen\\n@HappenLee\\n@hello-stephen\\n@Henry2SS\\n@hf200012\\n@huyuanfeng2018\\n@jacktengg\\n@jackwener\\n@jeffreys-cat\\n@Jibing-Li\\n@JNSimba\\n@Kikyou1997\\n@Lchangliang\\n@LemonLiTree\\n@lexoning\\n@liaoxin01\\n@lide-reed\\n@link3280\\n@liutang123\\n@liuyaolin\\n@LOVEGISER\\n@lsy3993\\n@luozenglin\\n@luzhijing\\n@madongz\\n@morningman\\n@morningman-cmy\\n@morrySnow\\n@mrhhsg\\n@Myasuka\\n@myfjdthink\\n@nextdreamblue\\n@pan3793\\n@pangzhili\\n@pengxiangyu\\n@platoneko\\n@qidaye\\n@qzsee\\n@SaintBacchus\\n@SeekingYang\\n@smallhibiscus\\n@sohardforaname\\n@song7788q\\n@spaces-X\\n@ssusieee\\n@stalary\\n@starocean999\\n@SWJTU-ZhangLei\\n@TaoZex\\n@timelxy\\n@Wahno\\n@wangbo\\n@wangshuo128\\n@wangyf0555\\n@weizhengte\\n@weizuo93\\n@wsjz\\n@wunan1210\\n@xhmz\\n@xiaokang\\n@xiaokangguo\\n@xinyiZzz\\n@xy720\\n@yangzhg\\n@Yankee24\\n@yeyudefeng\\n@yiguolei\\n@yinzhijian\\n@yixiutt\\n@yuanyuan8983\\n@zbtzbtzbt\\n@zenoyang\\n@zhangboya1\\n@zhangstar333\\n@zhannngchen\\n@ZHbamboo\\n@zhengshiJ\\n@zhenhb\\n@zhqu1148980644\\n@zuochunwei\\n@zy-kkk\\n```"},{"id":"/JD_OLAP","metadata":{"permalink":"/blog/JD_OLAP","source":"@site/blog/JD_OLAP.md","title":"JD.com\'s exploration and practice with Apache Doris in real time OLAP","description":"This article discusses the exploration and practice of the search engine team in JD.com  using Apache Flink and Apache Doris in real-time data analysis. The popularity of stream computing is increasing day by day: More papers are published on Google Dataflow; Apache Flink has become the one of the most popular engine in the world; There is wide application of real-time analytical databases more than ever before, such as Apache Doris; Stream computing engines are really flourishing. However, no engine is perfect enough to solve every problem. It is important to find a  suitable OLAP engine for the business. We hope that JD.com\'s practice in  real-time OLAP and stream computing may give you some inspiration.","date":"2022-12-02T00:00:00.000Z","tags":[{"inline":true,"label":"Best Practice","permalink":"/blog/tags/best-practice"}],"hasTruncateMarker":false,"authors":[{"name":"Li Zhe","key":null,"page":null}],"frontMatter":{"title":"JD.com\'s exploration and practice with Apache Doris in real time OLAP","description":"This article discusses the exploration and practice of the search engine team in JD.com  using Apache Flink and Apache Doris in real-time data analysis. The popularity of stream computing is increasing day by day: More papers are published on Google Dataflow; Apache Flink has become the one of the most popular engine in the world; There is wide application of real-time analytical databases more than ever before, such as Apache Doris; Stream computing engines are really flourishing. However, no engine is perfect enough to solve every problem. It is important to find a  suitable OLAP engine for the business. We hope that JD.com\'s practice in  real-time OLAP and stream computing may give you some inspiration.","date":"2022-12-02","author":"Li Zhe","tags":["Best Practice"],"image":"/images/user-jd.jpg"},"unlisted":false,"prevItem":{"title":"Apache Doris announced the official release of version 1.2.0","permalink":"/blog/release-note-1.2.0"},"nextItem":{"title":"Apache Doris helped Netease create a refined operation DMP system","permalink":"/blog/Netease"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n![kv](/images/jd/en/kv.png)\\n\\n> Guide:\\nThis article discusses the exploration and practice of the search engine team in JD.com  using Apache Flink and Apache Doris in real-time data analysis. The popularity of stream computing is increasing day by day: More papers are published on Google Dataflow; Apache Flink has become the one of the most popular engine in the world; There is wide application of real-time analytical databases more than ever before, such as Apache Doris; Stream computing engines are really flourishing. However, no engine is perfect enough to solve every problem. It is important to find a  suitable OLAP engine for the business. We hope that JD.com\'s practice in  real-time OLAP and stream computing may give you some inspiration.\\n\\n>Author: Li Zhe, data engineer of JD.com, who focused on offline data, stream computing and application development.\\n\\n## About JD.com\\nJD.com (NASDAQ: JD), a leading e-commerce company in China, had a net income of RMB 951.6 billion in 2021. JD Group owns JD Retail, JD Global, JD Technology, JD Logistics, JD Cloud, etc. Jingdong Group was officially listed on the NASDAQ Stock Exchange in May 2014.\\n\\n## JD Search Box\'s Requirement: Real-time Data Analysis\\nJD search box, as the entrance of the e-commerce platform, provides a link between merchants and users. Users can express their needs through the search box. In order to better understand user intentions and quickly improve the conversion rate, multiple A/B tests are running online at the same time, which apply to multiple products. The category, organization, and brand all need to be monitored online for better conversion. At present, JD search box demands real-time data in application mainly includes three parts:\\n\\n1. The overall data of JD search box.\\n2. Real-time monitoring of the A/B test.\\n3. Top list of hot search words to reflect changes in public opinion. Words trending can reflect what users care\\n\\nThe analysis mentioned above needs to refine the data to the SKU-level. At the same time, we also undertake the task of building a real-time data platform to show our business analysists different real-time stream computing data.\\n\\nAlthough different business analysists care about different data granularity, time frequency, and dimensions, we are hoping to establish a unified real-time OLAP data warehouse and provide a set of safe, reliable and flexible real-time data services.\\n\\nAt present, the newly generated exposure logs every day reach hundreds of millions. The logs will increase by 10 times if they are stored as SKU. And they would grow to billions of records if based on A/B test. Aggregation queries cross multi-dimension require second-level response time. \\n\\nSuch an amount of data also brings huge challenges to the team: 2 billion rows have been created daily; Up to 60 million rows need to be imported per minute; Data latency should be limited to 1 minute; MDX query needs to be executed within 3 seconds; QPS has reached above 20. Yet a new reliable OLAP database with high stability should be able to respond to priority 0 emergency.\\n\\n## The Evolution of the Real-time Architecture\\nOur previous architecture is based on Apache Storm for a point-to-point data processing. This approach can quickly meet the needs of real-time reports during the stage of rapid business growth in the early days. However, with the continuous development of business, disadvantages gradually appear. For example, poor flexibility, poor data consistency, low development efficiency and increased resource costs.\\n\\n![page_2](/images/jd/en/page_2.png)\\n\\nIn order to solve the problems of the previous architecture, we first upgraded the architecture and replaced Apache Storm with Apache Flink to achieve high throughput. At the same time, according to the characteristics of the search data, the real-time data is processed hierarchically, which means the PV data flow, the SKU data flow and the A/B test data flow are created. It is expected to build the upper real-time OLAP layer based on the real-time flow.\\n\\nWhen selecting OLAP database, the following points need to be considered:\\n\\n1. The data latency is at minute-level and the query response time is at second-level\\n2. Suppots standard SQL, which reduces the cost of use\\n3. Supports JOIN to facilitate adding dimension\\n4. Traffic data can be deduplicated approximately, but order data must be exact deduplicated \\n5. High throughput with tens of millions of records per minute and tens of billions of new records every day\\n6. Query concurrency needs to be high because Front-end may need it\\n\\nBy comparing the OLAP engines that support real-time import , we made an in-depth comparison among Apache Druid, Elasticsearch, Clickhouse and Apache Doris:\\n\\n![page_3](/images/jd/en/page_3.png)\\n\\n\\nWe found out that Doris and Clickhouse can meet our needs. But the concurrency of Clickhouse is low for us, which is a potential risk. Moreover, the data import of Clickhouse has no TRANSACTION and cannot achieve Exactly-once semantics. Clickhouse is not fully supportive of SQL.\\n\\n\\nFinally, we chose Apache Doris as our real-time OLAP database. For user behavior log data, we use Aggregation Key data table; As for E-commerce orders data, we use Unique Key data table. Moreover, we split the previous tasks and reuse the logic we tried before. Therefore, when Flink is processing, there will be new topic flow and real-time flow of different granularities generated in DWD. The new architecture is as follows:\\n\\n![page_4](/images/jd/en/page_4.png)\\n\\nIn the current technical architecture, flink task is very light. Based on the production data detail layer, we directly use Doris to act as the aggregation layer function.  And we ask Doris to complete window calculation which previously belongs to Flink. We also take advantage of the routine load to consume real-time data. Although the data is fine-grained before importing, based on the Aggregation Key, asynchronous aggregation will be automatically performed. The degree of aggregation is completely determined by the number of dimensions. By creating Rollup on the base table, double-write or multi-write and pre-aggregate operations are performed during import, which is similar to the function of materialized view, which can highly aggregate data to improve query performance.\\n\\nAnother advantage of using Kafka to directly connect to Doris at the detail layer is that it naturally supports data backtracking. Data backtracking means that when real-time data is out of order, the \\"late\\" data can be recalculated and the previous results can be updated. This is because delayed data can be written to the table whenever it arrives. The final solution is as follows:\\n\\n![page_5](/images/jd/en/page_5.png)\\n\\n## Optimization during the Promotion\\nAs mentioned above, we have established Aggregation Key of different granularities in Doris, including PV, SKU, and A/B test granularity. Here we take the exposure A/B test model with the largest amount of daily production data as an example to explain how to support the query of tens of billions of records per day during the big promotion period.\\n\\nStrategy we used:\\n- Monitoring: 10, 30, 60 minutes A/B test with indicators, such as exposure PV, UV, exposure SKU pieces, click PV, click UV and CTR.\\n- Data Modeling: Use exposed real-time data to establish Aggregation Key; And perform HyperLogLog approximate calculation with UV and PV\\n\\nClusters we had:\\n- 30+ virtual machines with storage of NVMe SSD\\n- 40+ partitions exposed by A/B test\\n- Tens of billions of new data are created every day\\n- 2 Rollups\\n\\nBenefits overall:\\n- Bucket Field can quickly locate tablet partition when querying\\n- Import 600 million records in 10 minutes\\n- 2 Rollups have relatively low IO, which meet the requirement of the query\\n\\n## Look Ahead\\nJD search box introduced Apache Doris in May 2020, with a scale of 30+ BEs, 10+ routine load tasks running online at the same time. Replacing Flink\'s window computing with Doris can not only improve development efficiency, adapt to dimension changes, but also reduce computing resources. Apache Doris provides unified interface services ensuring data consistency and security.\\nWe are also pushing the upgrade of JD search box\'s OLAP platform to the latest version. After upgrading, we plan to use the bitmap function to support accurate deduplication operations of UV and other indicators. In addition, we also plan to use the appropriate Flink window to develop the real-time stream computing of the aggregation layer to increase the richness and completeness of the data."},{"id":"/Netease","metadata":{"permalink":"/blog/Netease","source":"@site/blog/Netease.md","title":"Apache Doris helped Netease create a refined operation DMP system","description":"Better data analysis enables users to get better experience. Currently, the normal analysis method is to build a user tags system to accurately generate user portraits and improve user experience. The topic we shared today is the practice of Netease DMP tags system.","date":"2022-11-30T00:00:00.000Z","tags":[{"inline":true,"label":"Best Practice","permalink":"/blog/tags/best-practice"}],"hasTruncateMarker":false,"authors":[{"name":"Xiaodong Liu","key":null,"page":null}],"frontMatter":{"title":"Apache Doris helped Netease create a refined operation DMP system","description":"Better data analysis enables users to get better experience. Currently, the normal analysis method is to build a user tags system to accurately generate user portraits and improve user experience. The topic we shared today is the practice of Netease DMP tags system.","date":"2022-11-30","author":"Xiaodong Liu","tags":["Best Practice"],"image":"/images/best-practice.png"},"unlisted":false,"prevItem":{"title":"JD.com\'s exploration and practice with Apache Doris in real time OLAP","permalink":"/blog/JD_OLAP"},"nextItem":{"title":"The application of Apache Doris in NIO","permalink":"/blog/NIO"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n# Apache Doris Helped Netease Create a Refined Operation DMP System\\n\\n![1280X1280](/images/netease/kv.png)\\n\\n> Guide: Refined operation is a trend of the future Internet, which requires excellent data analysis. In this article, you will get knowledge of: the construction of Netease Lifease\'s DMP system and the application of Apache Doris.\\n\\n> Author | Xiaodong Liu, Lead Developer, Netease\\n\\n\\nBetter data analysis enables users to get better experience. Currently, the normal analysis method is to build a user tags system to accurately generate user portraits and improve user experience. The topic we shared today is the practice of Netease DMP tags system.\\n\\n## About Netease and Lifease\\nNetEase (NASDAQ: NTES) is a leading Internet technology company in China, providing users with free emails, gaming, search engine services, news and entertainment, sports, e-commerce and other services.\\n\\nLifease is Netease\'s self-operated home furnishing e-commerce brand. Its products cover 8 categories in total: home life, apparel, food and beverages, personal care and cleaning, baby products, outdoor sport, digital home appliances, and Lifease\'s Special. In Q1 of 2022, Lifease launches \\"Pro \\" membership and other multiple memberships for different users. The number of Pro members has increased by 65% \u200B\u200Bcompared with the previous year.\\n\\n## About the DMP System\\nDMP system plays an important role in Lifease\'s data analysis. \\nThe data sources of DMP mainly include:\\n- Business logs of APPs, H5s, PCs and other terminals\\n- Basic data constructed within NetEase Group\\n- Data from products sold by third-party such as JD.com, Alibaba, and Bytedance\\nThrough data collection and data cleaning, the above data is ingested into data assets. Based on these data, DMP has created a system of functions, such as tag creation, grouping and portrait analysis, which supports the business including: intelligent product matching, user engagement, and user insight. In general, the DMP system concentrates on building a data-centric tagging system and portrait system to assist the business.\\n\\nYou can get basic knowledge of the DMP system starting from the concepts below:\\n- Tagging: Tagging is one of the user monitoring abilities to uniquely identify individual users across different browsers, devices, and user sessions. This approach to user tagging works by capturing available data in your application\'s page source: age, address, preference and other variables. \\n- Targeting: Target audience may be dictated by age, gender, income, location, interests or a myriad of other factors.\\n- User Portrait Analysis: User portrait analysis is to develop user profiles, actions and attributes after targeting audience. For instance, check the behavior paths and consumption models of users whose portraits are \\"City: Hangzhou, Gender: Female\\" on Lifease APP.\\n\\n![1280X1280](/images/netease/1__core_capability.png)\\n\\nLlifease\'s tagging system mainly provides two core capabilities: \\n1. Tag Query: the ability to query the specified tag of a specific entity, which is often used to display basic information. \\n2. Targeting Audience: for both real-time and offline targets. Result after targeting is mainly used for:\\n- As Grouping Criteria: It can be used to tell if the user is in one or more specified groups. This occasionally occurs in scenarios such as advertising and contact marketing. \\n- Resultset Pull: Extract specified data to business system for customized development.\\n- Portrait Analysis: Analyze the behavioral and consumption models in specific groups of people for more refined operations.\\n\\nThe overall business process is as follows:\\n\\n![1280X1280](/images/netease/2__business_process.png)\\n\\n- First define the rules for tags and grouping;\\n- After defining the DSL, the task can be submitted to Spark for processing;\\n- After the processing is done, the results can be stored in Hive and Doris;\\n- Data from Hive or Doris can be queried and used according to the actual business needs.\\n\\n![1280X1280](/images/netease/3__dmp_architecture.png)\\n\\nThe DMP platform is divided into four modules: Processing&storage layer, scheduling layer, service layer, and metadata management.\\nAll tag meta-information is stored in the source data table; The scheduling layer schedules tasks for the entire business process: Data processing and aggregation are converted into basic tags, and the data in the basic tags and source tables are converted into something that can be used for data query through SQL; The scheduling layer dispatches tasks to Spark to process, and then stores results in both Hive and Doris. The service layer consists of four parts: tag service, entity grouping service, basic tag data service, and portrait analysis service.\\n\\n![1280X1280](/images/netease/4__tag_lifecycle.png)\\n\\nThe lifecycle of tag consists of 5 phases:\\n- Tag requirements: At this stage, the operation team demands and the product manager team evaluates the rationality and urgency of the requirements.\\n- Scheduling production: Developers first sort out the data from ODS to DWD, which is the entire link of DM layer. Secondly, they build a model based on data, and at the same time, monitor the production process.\\n- Targeting Audience: After the tag is produced, group the audience by those tags.\\n- Precision marketing: Carry out precision marketing strategy to people grouped by.\\n- Effect evaluation: In the end, tage usage rate and use effect need to be evaluated for future optimization.\\n\\n## Production of Tags\\n\\n![1280X1280](/images/netease/5__production_of_tags.png)\\n\\nTag data layering:\\n- The bottom layer is the ODS layer, including user login logs, event tracking records, transaction data, and Binlog data of various databases\\n- The data processed by the ODS layer, such as user login table, user activity table and order information table reaches the DWD detail layer\\n- The DWD layer data is aggregated to the DM layer and the tags are all implemented based on the DM layer data.\\nAt present, we have fully automated the data output from the original database to the ODS layer. And we also realized partial automation from the ODS layer to the DWD layer. And there are a small number of automated operations from the DWD to the DM layer, which will be our focus in the future.\\n\\n![1280X1280](/images/netease/6__type_of__tags.png)\\n\\nTags are devided based on timeliness: offline tags, quasi-real-time tags and real-time tags. According to the scale of data, it is divided into: aggregation tags and detail tags. In other cases, tags can also be divided into: account attribute tags, consumption behavior tags, active behavior tags, user preference tags, asset information tags, etc. \\n\\n![1280X1280](/images/netease/7__tags_settings.png)\\n\\nIt is inconvenient to use the data of the DM layer directly because the basic data is relatively primitive. The abstraction level is lacking and it is not easy to use. By combining basic data with AND, OR, and NOT, business tags are formed for further use, which can reduce the cost of understanding operations and make it easier to use.\\n\\n![1280X1280](/images/netease/8__target_audience.png)\\n\\nAfter the tags are merged, it is necessary to apply the tags to specific business scenarios, such as grouping. The configuration is shown on the left side of the figure above, which supports offline crowd packages and real-time behaviors (need to be configured separately). After configuration, generate the DSL rules shown on the right side of the figure above, expressed in Json format, which is more friendly to FE, and can also be converted into query statements of the datebase engine.\\n\\n![1280X1280](/images/netease/9__target_audience-mapping.png)\\n\\n![1280X1280](/images/netease/10__automation.png)\\n\\nTagging is partially automated. The degree of automation in grouping is relatively high. For example, group refresh can be done regularly every day; Advanced processing, such as intersection/merge/difference between groups; Data cleaning means timely cleaning up expired and invalid data.\\n\\n## Tags Storage\\nLifease\'s DMP labeling system needs to carry relatively large customer end traffic, and has relatively high requirements for real-time performance. Our storage requirements include:\\n- Need support high-performance query to deal with large-scale customer end traffic\\n- Need support SQL to facilitate data analysis scenarios\\n- Need support data update mechanism\\n- Can store large amount of data\\n- Need support for extension functions to handle custom data structures\\n- Closely integrated with big data ecology\\n\\nIn the field of big data, multiple engines vary in different applicable scenarios. We used the popular engines in the chart below to optimize our database architecture for 2 times.\\n\\n![1280X1280](/images/netease/11__comparision.png)\\n\\nOur architecture V1.0 is shown below:\\n\\n![1280X1280](/images/netease/12__architecture_v1_0.png)\\n\\nMost of the offline data is stored in Hive while a small part is stored in Hbase (mainly used for querying basic tags). Part of the real-time data is stored in Hbase for basic tags query and the rest is double-written into KUDU and Elasticsearch for real-time grouping and data query. The data offline is processed by Impala and cached in Redis. \\nDisadvantages :\\n- Too many database engines.\\n- Double writing has hidden problems with data quality. One side may succeed while the other side fails, resulting in data inconsistency.\\n- The project is complex and maintainability is poor.\\nIn order to reduce the usage of engine and storage, we improved and implemented version 2.0 :\\n\\n![1280X1280](/images/netease/13__architecture_v2_0.png)\\n\\nIn storage architecture V2.0, Apache Doris is adopted. Offline data is mainly stored in Hive. At the same time, basic tags are imported into Doris, and real-time data as well. The query federation of Hive and Doris is performed based on Spark, and the results are stored in Redis. After this improvement, an storage engine which can manages offline and real-time data has been created. We are currently use Apache Doris 1.0, which enables : 1. The query performance can be controlled within 20ms at 99% 2.  The query performance can be controlled within 50ms at 99.9%.  Now the architecture is simplified, which greatly reduces operation and maintenance costs.\\n\\n## Advantages of Apache Doris in Practice\\n\\n![1280X1280](/images/netease/14__advantages_in_practice.png)\\n\\nLifeuse has adopted Apache Doris to check, batch query, path analyse and grouping. The advantages are as follows:\\n- The query federation performance of  key query and a small number of tables exceeds 10,000 QPS, with RT99<50MS.\\n- The horizontal expansion capability is relatively strong and maintenance cost is relatively low.\\n- The offlin and real-time data are unified to reduce the complexity of the tags model.\\n\\nThe downside is that importing a large amount of small data takes up more resources. But this problem has been optimized in Doris 1.1. Apache Doris has greatly enhanced the data compaction capability in version 1.1, and can quickly complete aggregation of new data, avoiding the -235 error caused by too many versions of sharded data and the low query efficiency problems.\\n\\n## Future Plan\\n\\n![1280X1280](/images/netease/15__future_plan.png)\\n\\nHive and Spark are gradually turning into Apache Doris. \\nOptimize the tagging system:\\n- Establish a rich and accurate tag evaluation system\\n- Improve tag quality and output speed\\n- Improve tag coverage\\nMore precision operation:\\n- Build a rich user analysis model\\n- Improve the user insight model evaluation system based on the frequency of use and user value\\n- Establish general image analysis capabilities to assist intelligent decision-making in operations"},{"id":"/NIO","metadata":{"permalink":"/blog/NIO","source":"@site/blog/NIO.md","title":"The application of Apache Doris in NIO","description":"NIO Inc. (NYSE: NIO)is a leading company in the premium smart electric vehicle market. Founded in November 2014, NIO designs, develops, jointly manufactures and sells premium smart electric vehicles, driving innovations in autonomous driving, digital technologies, electric powertrains and batteries. Recently, NIO planned to enter the U.S. market alongside other western markets by the end of 2025. The company has already established a U.S. headquarters in San Jose, California, where they started hiring people..","date":"2022-11-28T00:00:00.000Z","tags":[{"inline":true,"label":"Best Practice","permalink":"/blog/tags/best-practice"}],"hasTruncateMarker":false,"authors":[{"name":"Huaidong Tang","key":null,"page":null}],"frontMatter":{"title":"The application of Apache Doris in NIO","description":"NIO Inc. (NYSE: NIO)is a leading company in the premium smart electric vehicle market. Founded in November 2014, NIO designs, develops, jointly manufactures and sells premium smart electric vehicles, driving innovations in autonomous driving, digital technologies, electric powertrains and batteries. Recently, NIO planned to enter the U.S. market alongside other western markets by the end of 2025. The company has already established a U.S. headquarters in San Jose, California, where they started hiring people..","date":"2022-11-28","author":"Huaidong Tang","tags":["Best Practice"],"image":"/images/nio.png"},"unlisted":false,"prevItem":{"title":"Apache Doris helped Netease create a refined operation DMP system","permalink":"/blog/Netease"},"nextItem":{"title":"How does Apache Doris help AISPEECH build a data warehouse in AI chatbots scenario","permalink":"/blog/Use-Apache-Doris-with-AI-chatbots"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n\\n# The Application of Apache Doris in NIO\\n\\n![NIO](/images/NIO_kv.png)\\n\\n>Guide: The topic of this sharing is the application of Apache Doris in NIO, which mainly includes the following topics:\\n>1. Introduction about NIO\\n>2. The Development of OLAP in NIO\\n>3. Apache Doris-the Unified OLAP Data warehouse\\n>4. Best Practice of Apache Doris on CDP Architecture\\n>5. Summery and Benefits\\n\\nAuthor\uFF1AHuaidong Tang, Data Team Leader, NIO INC\\n\\n## About NIO\\n\\nNIO Inc. (NYSE: NIO)is a leading company in the premium smart electric vehicle market. Founded in November 2014, NIO designs, develops, jointly manufactures and sells premium smart electric vehicles, driving innovations in autonomous driving, digital technologies, electric powertrains and batteries.\\n\\nRecently, NIO planned to enter the U.S. market alongside other western markets by the end of 2025. The company has already established a U.S. headquarters in San Jose, California, where they started hiring people.\\n\\n## The Architecture Evolution of OLAP in NIO\\n\\nThe architectural evolution of OLAP in NIO took several steps for years.\\n\\n### 1. Introduced Apache Druid\\n\\nAt that time, there were not so many OLAP storage and query engines to choose from. The more common ones were Apache Druid and Apache Kylin. There are 2 reasons why we didn\'t choose Kylin.\\n\\n- The most suitable and optimal storage at the bottom of Kylin is HBase and adding it would increase the cost of operation and maintenance.\\n\\n- Kylin\'s precalculation involves various dimensions and indicators. Too many dimensions and indicators would cause great pressure on storage.\\n\\nWe prefer Druid because we used to be users and are familiar with it. Apache Druid has obvious advantages. It supports real-time and offline data import, columnar storage, high concurrency, and high query efficiency. But it has downsides as well:\\n\\n- Standard protocols such as JDBC are not used\\n\\n- The capability of JOIN is weak\\n\\n- Significant performance downhill when performing dedeplication\\n\\n- High in operation and maintenance costs, different components have separate installation methods and different dependencies; Data import needs extra integration with Hadoop and the dependencies of JAR packages\\n\\n### 2. Introduced TiDB\\n\\n**TiDB is a mature datawarehouse focused on OLTP+OLAP, which also has distinctive advantages and disadvantages:**\\n\\nAdvantage:\\n\\n- OLTP database, can be updated friendly\\n\\n- Supports detailed and aggregated query, which can handle dashboard statistical reports or query of detailed data at the same time\\n\\n- Supports standard SQL, which has low cost of use\\n\\n- Low operation and maintenance cost\\n\\nDisadvantages:\\n\\n- It is not an independent OLAP. TiFlash relies on OLTP and will increase storage. Its OLAP ability is insufficient\\n\\n- The overall performance should be measured separately by each scene\\n\\n### 3. Introduced Apache Doris\\n\\nSince 2021, we have officially introduced Apache Doris. In the process of selection, we are most concerned about various factors such as product performance, SQL protocol, system compatibility, learning and operation and maintenance costs. After deep research and detailed comparison of the following systems, we came to the following conclusions:\\n\\n**Apache Doris, whose advantages fully meet our demands:**\\n\\n- Supports high concurrent query (what we concerned most)\\n\\n- Supports both real-time and offline data\\n\\n- Supports detailed and aggregated query\\n\\n- UNIQ model can be updated\\n\\n- The ability of Materialized View can greatly speed up query efficiency\\n\\n- Fully compatible with the MySQL protocol and the cost of development is relatively low\\n\\n- The performance fully meets our requirements\\n\\n- Lower operation and maintenance costs\\n\\n**Moreover, there is another competitor, Clickhouse. Its stand-alone performance is extremely strong, but its disadvantages are hard to accept:**\\n\\n- In some cases, its multi-table JOIN is weak\\n\\n- Relatively low in concurrency\\n\\n- High operation and maintenance costs\\n\\nWith multiple good performances, Apache Doris outstands Druid and TiDB. Meanwhile Clickhouse did not fit well in our business, which lead us to Apache Doris.\\n\\n## Apache Doris-the Unified OLAP Datawarehouse\\n![NIO](/images/olap.png)\\n\\nThis diagram basically describes our OLAP Architecuture, including data source, data import, data processing, data warehouse, data service and application.\\n\\n### 1. Data Source\\n\\nIn NIO, the data source not only refers to database, but also event tracking data, device data, vehicle data, etc. The data will be ingested into the big data platform. \\n### 2. Data Import\\n\\nFor business data, you can trigger CDC and convert it into a data stream, store it in Kafka, and then perform stream processing. Some data that can only be passed in batches will directly enter our distributed storage.\\n\\n### 3. Data Processing\\n\\nWe took the Lambda architecture rather than stream-batch integration.\\n\\nOur own business determines that our Lambda architecture should be divided into two paths: offline and real-time:\\n\\n- Some data is streamed.\\n\\n- Some data can be stored in the data stream, and some historical data will not be stored in Kafka.\\n\\n- Some data requires high precision in some circumstances. In order to ensure the accuracy of the data, an offline pipeline will recalculate and refresh the entire data.\\n\\n### 4. Data Warehouse\\n\\nFrom data processing to the data warehouse, we did not adopt Flink or Spark Doris Connector. We use Routine Load to connect Apache Doris and Flink, and Broker Load to connect Doris and Spark. The data generated in batches by Spark will be backed up to Hive for further use in other scenarios. In this way, each calculation is used for multiple scenarios at the same time, which greatly improves the efficiency. It also works for Flink.\\n\\n### 5. Data Service\\n\\nWhat behind Doris is One Service. By registering the data source or flexible configuration, the API with flow and authority control is automatically generated, which greatly improves flexibility. And with the k8s serverless solution, the entire service is much more flexible.\\n\\n### 6. Application\\n\\nIn the application layer, we mainly deploy some reporting applications and other services.\\n\\nWe mainly have two types of scenarios:\\n\\n- **User-oriented** , which is similar to the Internet, contains a data dashboard and data indicators.\\n\\n- **Car-oriented** , car data enters Doris in this way. After certain aggregation, the volume of Doris data is about billions. But the overall performance can still meet our requirements.\\n\\n## Best Practice of Apache Doris on CDP Architecture\\n\\n### 1. CDP Architecture\\n\\n![NIO](/images/cdp.png)\\n\\nNext, let me introduce Doris\' practice on the operating platform. This is what happens in our real business. Nowadays, Internet companies will make their own CDP, which includes several modules:\\n\\n- **Tags** , which is the most basic part.\\n\\n- **Target** , based on tags, select people according to some certain logic.\\n\\n- **Insight** , aiming at a group of people, clarify the distribution and characteristics of the group.\\n\\n- **Touch** , use methods such as text messages, phone calls, voices, APP notifications, IM, etc. to reach users, and cooperate with flow control.\\n\\n- **Effect analysis,** to improve the integrity of the operation platform, with action, effect and feedback.\\n\\nDoris plays the most important role here, including: tags storage, groups storage, and effect analysis.\\n\\nTags are divided into basic tags and basic data of user behavior. We can flexibly customize other tags based on those facts. From the perspective of time effectiveness, tags are also divided into real-time tags and offline tags.\\n\\n### 2. Considerations for CDP Storage Selection\\n\\nWe took five dimensions into account when we select CDP storage.\\n\\n**(1) Unification of Offline and Real-time**\\n\\nAs mentioned earlier, there are offline tags and real-time tags. Currently we are close to quasi-real-time. For some data, quasi-real-time is good enough to meet our needs. A large number of tags are still offline tags. The methods used are Doris\'s Routine Load and Broker Load.\\n\\n| **Scenes** | **Requirements** | **Apache Doris\'s Function** |\\n| --- | --- | --- |\\n| Real-time tags | Real-time data updates | Routine Load |\\n| Offline tags | Highly efficient batch import | Broker Load |\\n| Unification of offline and real-time | Unification of offline and real-time data storage | Routine Load and Broker Load update different columns of the same table |\\n\\nIn addition, on the same table, the update frequency of different columns is also different. For example, we need to update the user\'s identity in real time because the user\'s identity changes all the time. T+1\'s update does not meet our needs. Some tags are offline, such as the user\'s gender, age and other basic tags, T+1 update is sufficient to meet our standards. The maintenance cost caused by putting the tags of basic users on the same table is very low. When customizing tags later, the number of tables will be greatly reduced, which benefits the overall performance.\\n\\n**(2) Efficient Targets**\\n\\nWhen users tags are done, is time to target right group of people. The target is to filter out all the people who meet the conditions according to different combinations of tags. At this time, there will be queries with different combinations of tag conditions. There was an obvious improvement when Apache Doris upgraded to vectorization.\\n\\n| **Scenes** | **Requirements** | **Apache Doris\'s Function** |\\n| --- | --- | --- |\\n| Complex Condition Targets | Highly efficient combination of tags | Optimization of SIMD |\\n\\n**(3) Efficient Polymerization**\\n\\nThe user insights and effect analysis statistics mentioned above require statistical analysis of the data, which is not a simple thing of obtaining tags by user ID. The amount of data read and query efficiency have a great impact on the distribution of our tags, the distribution of groups, and the statistics of effect analysis. Apache Doris helps a lot:\\n\\n- Data Partition. We shard the data by time order and the analysis and statistics will greatly reduce the amount of data, which can greatly speed up the efficiency of query and analysis.\\n\\n- Node aggregation. Then we collect them for unified aggregation.\\n\\n- Vectorization. The vectorization execution engine has significant performance improvement.\\n\\n| **Scenes** | **Requirements** | **Apache Doris\'s Function** |\\n| --- | --- | --- |\\n| Distribution of Tags Values | The distribution values \u200B\u200Bof all tags need to be updated every day. Fast and efficient statistics are required  | Data partition lessens data transfer and calculation |\\n| Distribution of Groups | Same as Above | Unified storage and calculation, each node aggregates first |\\n| Statistics for Performance Analysis | Same as Above | Speed up SIMD\\n\\n\\n**(4) Multi-table Association**\\n\\nOur CDP might be different from common CDP scenarios in the industry, because common CDP tags in some scenarios are estimated in advance and no custom tags, which leaves the flexibility to users who use CDP to customize tags themselves. The underlying data is scattered in different database tables. If you want to create a custom tag, you must associate the tables.\\n\\nA very important reason we chose Doris is the ability to associate multiple tables. Through performance tests, Apache Doris is able to meet our requirements. And Doris provides users with powerful capabilities because tags are dynamic.\\n\\n| **Scenes** | **Requirements** | **Apache Doris\'s Function** |\\n| --- | --- | --- |\\n| Distributed Characteristics of the Population | The distribution of statistical groups under a certain characteristic | Table Association |\\n| Single Tag | Display tags |\\n\\n**(5) Query Federation**\\n\\nWhether the user is successfully reached or not will be recorded in TiDB. Notifications during operations may only affect user experience. If a transaction is involved, such as gift cards or coupons, the task execution must be done without repetition. TiDB is more suitable for this OLTP scenario.\\n\\nBut for effect analysis, it is necessary to understand the extent to which the operation plan is implemented, whether the goal is achieved and its distribution. It is necessary to combine task execution and group selection for analysis, which requires the query association between Doris and TiDB.\\n\\nThe size of the tag is probably small, so we would like to save it into Elasticsearch. However, it proves us wrong later.\\n\\n| **Scenes** | **Requirements** | **Apache Doris\'s Function** |\\n| --- | --- | --- |\\n| Effect Analysis Associated with Execution Details | Doris query associated with TiDB | Query Association with other databases |\\n| Group Tags Associated with Behavior Aggregation | Doris query associated with Elasticsearch |\\n\\n## Summery and Benefits\\n\\n1. **bitmap**. Our volume are not big enough to test its full efficiency. If the volume reaches a certain level, using bitmap might have a good performance improvement. For example, when calculating UV , bitmap aggregation can be considered if the full set of Ids is greater than 50 million.\\n\\n2. **The performance is good** when Elasticsearch single-table query is associated with Doris.\\n\\n3. **Better to update columns in batches**. In order to reduce the number of tables and improve the performance of the JOIN table, the table designed should be as streamlined as possible and aggregated as much as possible. However, fields of the same type may have different update frequencies. Some fields need to be updated at daily level, while others may need to be updated at hourly level. Updating a column alone is an important requirement. The solution from Apache Doris is to use REPLACE\\\\_IF\\\\_NOT\\\\_NULL. Note: It is impossible to replace the original non-null value with null. You can replace all nulls with meaningful default values, such as unknown.\\n\\n4. **Online Services**. Apache Doris serves online and offline scenarios at the same time, which requires high resource isolation."},{"id":"/Use-Apache-Doris-with-AI-chatbots","metadata":{"permalink":"/blog/Use-Apache-Doris-with-AI-chatbots","source":"@site/blog/Use-Apache-Doris-with-AI-chatbots.md","title":"How does Apache Doris help AISPEECH build a data warehouse in AI chatbots scenario","description":"Guide: In 2019, AISPEACH built a real-time and offline datawarehouse based on Apache Doris. Reling on its flexible query model, extremely low maintenance costs, high development efficiency, and excellent query performance, Apache Doris has been used in many business scenarios such as real-time business operations, AI chatbots analysis. It meets various data analysis needs such as device portrait/user label, real-time operation, data dashboard, self-service BI and financial reconciliation. And now I will share our experience through this article.","date":"2022-11-24T00:00:00.000Z","tags":[{"inline":true,"label":"Best Practice","permalink":"/blog/tags/best-practice"}],"hasTruncateMarker":false,"authors":[{"name":"Zhao Wei","key":null,"page":null}],"frontMatter":{"title":"How does Apache Doris help AISPEECH build a data warehouse in AI chatbots scenario","description":"Guide: In 2019, AISPEACH built a real-time and offline datawarehouse based on Apache Doris. Reling on its flexible query model, extremely low maintenance costs, high development efficiency, and excellent query performance, Apache Doris has been used in many business scenarios such as real-time business operations, AI chatbots analysis. It meets various data analysis needs such as device portrait/user label, real-time operation, data dashboard, self-service BI and financial reconciliation. And now I will share our experience through this article.","date":"2022-11-24","author":"Zhao Wei","tags":["Best Practice"],"image":"/images/best-practice.png"},"unlisted":false,"prevItem":{"title":"The application of Apache Doris in NIO","permalink":"/blog/NIO"},"nextItem":{"title":"Apache Doris 1.2 star-schema-benchmark performance test report","permalink":"/blog/ssb"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n# How Does Apache Doris Help AISPEECH Build a Data warehouse in AI Chatbots Scenario\\n\\n![kv](/images/kv.png)\\n\\n>Guide: In 2019, AISPEACH built a real-time and offline datawarehouse based on Apache Doris. Reling on its flexible query model, extremely low maintenance costs, high development efficiency, and excellent query performance, Apache Doris has been used in many business scenarios such as real-time business operations, AI chatbots analysis. It meets various data analysis needs such as device portrait/user label, real-time operation, data dashboard, self-service BI and financial reconciliation. And now I will share our experience through this article.\\n\\nAuthor\uFF5CZhao Wei, Head Developer of AISPEACH\'s Big Data Departpment\\n\\n## Backgounds\\n\\nAISPEACH is a professional conversational artificial intelligence company in China. It has full-link intelligent voice and language technology. It is committed to becoming a platform-based enterprise for full-link intelligent voice and language interaction. Recently it has developed a new generation of human-computer interaction platform DUI and artificial intelligence chip TH1520, providing natural language interaction solutions for partners in many industry scenarios such as Internet of Vehicles, IoT, government affairs and fintech.\\n\\nAspire introduced Apache Doris for the first time in 2019 and built a real-time and offline data warehouse based on Apache Doris. Compared with the previous architecture, Apache Doris has many advantages such as flexible query model, extremely low maintenance cost, high development efficiency and excellent query performance. Multiple business scenarios have been applied to meet various data analysis needs such as device portraits/user tags, real-time operation of business scenarios, data analysis dashboards, self-service BI, and financial reconciliation.\\n\\n## Architecture Evolution\\n\\nOffline data analysis in the early business was our main requirement. Recently, with the continuous development of business, the requirements for real-time data analysis in business scenarios have become higher and higher. The early datawarehouse architecture failed to meet our requirements. In order to meet the higher requirements of business scenarios for query performance, response time, and concurrency capabilities, Apache Doris was officially introduced in 2019 to build a real-time and offline integrated datawarehouse architecture.\\n\\nIn the following I will introduce the evolution of the AISPEACH Data Warehouse architecture, and share the reasons why we chose Apache Doris to build a new architecture.\\n\\n### Early Data Warehouse Architecture\\n\\nAs shown in the architecture diagram below, the offline data warehouse is based on Hive + Kylin while the real-time data warehouse is based on Spark + MySQL.\\n\\n![data_wharehouse_architecture_v1_0_git](/images/data_wharehouse_architecture_v1_0_git.png)\\n\\nThere are three main types of data sources in our business, business databases such as MySQL, application systems such as K8s container service logs, and logs of automotive T-Box. Data sources are first written to Kafka through various methods such as MQTT/HTTP protocol, business database Binlog, and Filebeat log collection. In the early time, the data will be divided into real-time and offline links after passing through Kafka. Real-time part has a shorter link. The data buffered by Kafka is processed by Spark and put into MySQL for further analysis. MySQL can basically meet the early analysis requirements. After data cleaning and processing by Spark, an offline datawarehouse is built in Hive, and Apache Kylin is used to build Cube. Before building Cube, it is necessary to design the data model in advance, including association tables, dimension tables, index fields, and aggregation functions. After construction through the scheduling system, we can finally use HBase to store the Cube.\\n\\n#### Pain Points of Early Architecture\uFF1A\\n\\n1. **There are many dependent components.** Kylin strongly relies on Hadoop and HBase in versions 2.x and 3.x. The large number of application components leads to low development efficiency, many hidden dangers of architecture stability, and high maintenance costs.\\n\\n2. **The construction process of Kylin is complicated and the construction task always fail.** When we do construction for Kylin, we always need to do the following: widen tables, de-duplicate columns, generate dictionaries, build cubes, etc. If there are 1000-2000 or more tasks per day, at least 10 or more tasks will fail to build, resulting in a lot of time to write automatic operation and maintenance scripts.\\n\\n3. **Dimension/dictionary expansion is heavy.** Dimension expansion refers to the need for multiple analysis conditions and fields in some business scenarios. If many fields are selected in the data analysis model without pruning, it will lead to severe cube dimension expansion and longer construction time. Dictionary inflation means that in some scenarios, it takes a long time to do global accurate deduplication, which will make the dictionary construction bigger and bigger, and the construction time will become longer and longer, resulting in a continuous decline in data analysis performance.\\n\\n4. **The data analysis model is fixed and low in flexibility.** In the actual application, if a calculation field or business scenario is changed, some or even all of the data needs to be backtracked.\\n\\n5. **Data detail query is not supported.** The early data warehouse architecture could not provide detailed data query. The official Kylin solution is to relate to Presto for detailed query, which introduces another architecture and increases development costs.\\n\\n### Architecture Selection\\n\\nIn order to solve the problems above, we began to explore other datawarehouse architecture solutions. And we conducted a series of research on OLAP engines such as Apache Doris and Clickhouse, which are most widely used in the market.\\n\\nAs the original creator, SelectDB provides commercial products based on Apache Doris. With the new Apache Doris, SelectDB is now providing global users with a fully-managed database option for deployment.\\n\\nComparing with ClickHouse\'s heavy maintenance, various table types, and lack of support for associated queries, Apache Doris performed better. And combined with our OLAP analysis scenario, we finally decided to introduce Apache Doris.\\n\\n### New Data Warehouse Architecture\\n\\n![data_wharehouse_architecture_v2_0_git](/images/data_wharehouse_architecture_v2_0_git.png)\\n\\nAs shown in the figure above, we built a new real-time + offline data warehouse architecture based on Apache Doris. Unlike the previous architecture, real-time and offline data are processed separately and written to Apache Doris for analysis.\\n\\nDue to some historical reasons, data migration is difficult. The offline data is basically consistent with the previous datawarehouse architecture, and it is entirely possible to directly build an offline data warehouse on Apache Doris.\\n\\nComparing with the earlier architecture, the offline data is cleaned and processed by Spark, which is possible to build data warehouse in Hive. Then the data stored in Hive can be written to Apache Doris through Broker Load. What I want to explain here is that the data import speed of Broker Load is very fast and it only takes 10-20 minutes to import 100-200G data into Apache Doris on a daily basis.\\n\\nWhen it comes to the real-time data flow, the new architecture uses Doris-Spark-Connector to consume data in Kafka and write it to Apache Doris after simple tasks. As shown in the architecture diagram, real-time and offline data are analyzed and processed in Apache Doris, which meets the business requirements of data applications for both real-time and offline.\\n\\n#### Benefits of the New Architecture:\\n\\n1. **Simplified operation, low maintenance cost, and does not depend on Hadoop ecological components.** The deployment of Apache Doris is simple. There are only two processes of FE and BE. Both FE and BE processes can be scaled out. A single cluster supports hundreds of machines and tens of PB storage capacity. These two types of processes pass the consistency agreement to ensure high availability of services and high reliability of data. This highly integrated architecture design greatly reduces the operation and maintenance cost of a distributed system. The operation and maintenance time spent in the three years of using Doris is very small. Comparing with the previous architecture based on Kylin, the new architecture spends little time on operation and maintenance.\\n\\n2. **The difficulty of developing and troubleshooting problems is greatly reduced.** The real-time and offline unified data warehouse based on Doris supports real-time data services, interactive data analysis, and offline data processing scenarios, which greatly reduces the difficulty of troubleshooting.\\n\\n3. **Apache Doris supports JOIN query in Runtime format.** Runtime is similar to MySQL\'s table association, which is friendly to the scene where the data analysis model changes frequently, and solves the problem of low flexibility in the early structured data model.\\n\\n4. **Apache Doris supports JOIN, aggregation, and detailed query at the same time.** Meanwhile, it solves the problem that data details could not be queried in the previous architecture.\\n\\n5. **Apache Doris supports multiple accelerated query methods.** And it also supports rollup index, materialized view, and implements secondary index through rollup index to speed up query, which greatly improves query response time.\\n\\n6. **Apache Doris supports multiple types of Query Federation.** And it supports Federation Query analysis on data lakes such as Hive, Iceberg, and Hudi, and also databases such as MySQL and Elasticsearch.\\n\\n## Applications\\n\\nApache Doris was first applied in real-time business and AI Chatbots analysis scenarios in AISPEACH. This chapter will introduce the requirements and applications of the two scenarios.\\n\\n### Real-time Business\\n\\n![real-time_operation_git](/images/real-time_operation_git.png)\\n\\nAs shown in the figure above, the technical architecture of the real-time operation business is basically the same as the new version of the data warehouse architecture mentioned above:\\n\\n- Data Source: The data source is consistent in the new version with the architecture diagram in the new version, including business data in MySQL, event tracking data of the application system, device and terminal logs.\\n\\n- Data Import: Broker Load is used for offline data import, and Doris-Spark-Connector is used for real-time data import.\\n\\n- Data Storage and Development: Almost all real-time data warehouses are built on Apache Doris, and some offline data is placed on Airflow to perform DAG batch tasks.\\n\\n- Data Application: The top layer is the business analysis requirements, including large-screen display, real-time dashboard for data operation, user portrait, BI tools, etc.\\n\\n**In real-time operation business, there are two main requirements for data analysis:**\\n\\n- Due to the large amount of real-time imported data, the query efficiency requirement is high.\\n\\n- In this scenario, a team of 20+ people is in charge. The data operation dashboard needs to be opened at the same time, so there will be relatively high requirements for real-time writing performance and query concurrency.\\n\\n### AI Chatbots Analysis\\n\\nIn addition, the second application of Apache Doris in AISPEACG is a AI Chatbots analysis.\\n\\n![ai_chatbots_git](/images/ai_chatbots_git.png)\\n\\nAs shown in the figure above, different from normal BI cases, our users only needs to describe the data analysis needs by typing. Based on our company\'s NLP capabilities, AI Chatbots BI will convert natural language into SQL, which similar to NL2SQL technology. It should be noted that the natural language analysis used here is customized. Comparing with open source NL2SQL, the hit rate is high and the analysis is more precise. After the natural language is converted into SQL, the SQL will give Apache Doris query to get the analysis result. As a result, users can view detailed data in any cases at any time by typing. **Compared with pre-computed OLAP engines such as Apache Kylin and Apache Druid, Apache Doris performs better for the following reasons:**\\n\\n- The query is flexible and the model is not fixed, which supports customization.\\n\\n- It needs to support table association, aggregation calculation, and detailed query.\\n\\n- Response time needs to be fast.\\n\\nTherefore, we have successfully implemented AI Chatbots analysis by using Apache Doris. At the same time, feedback on the application in our company is awesome.\\n\\n## Experience\\n\\nBased on the above two scenarios, we have accumulated some experience and insights and I will share them with you now.\\n\\n### Datawarehouse Table Design:\\n\\n1. Tables which contain about tens of millions of data(for reference, related to the size of the cluster) is better to use the Duplicate table type. The Duplicate table type supports aggregation and detailed query at the same time, without additional detailed tables required.\\n\\n2. When the amount of data is relatively large, we suggest to use the Aggregate aggregation table type, build a rollup index on the aggregation table type, use materialized views to optimize queries, and optimize aggregation fields.\\n\\n3. When the amount of data is large with many associated tables, ETL can be used to write wide tables, imports to Doris, combined with Aggregate to optimize the aggregation table type. Or we suggest you use the official Doris JOIN optimization refer to: https://doris .apache.org/en-US/docs/dev/advanced/join-optimization/doris-join-optimization\\n\\n### Storage:\\n\\nWe use SSD and HDD to separate hot and warm data storage. Data within the past year is stored in SSD, and data more than one year is stored in HDD. Apache Doris supports setting cooling time for partitions. The current solution is to set automatic synchronization to migrate historical data from SSD to HDD to ensure that the data within one year is placed in on the SSD.\\n\\n### Upgrade\\n\\nMake sure to back up the metadata before upgrading. You can also use the method of starting a new cluster to back up the data files to a remote storage system such as S3 or HDFS through Broker, and then import the previous cluster data into the new cluster through backup and recovery.\\n\\n### Performance Comparison\\n\\nAspire started using Apache Doris from version 0.12. This year we completed the upgrade from version 0.15 to the latest version 1.1, and conducted performance tests based on real business data.\\n\\n![doris_1_1_performance_test_git](/images/doris_1_1_performance_test_git.png)\\n\\nAs can be seen from the test report, among the 13 SQLs test in total, the performance difference of the first 3 SQLs after the upgrade is not obvious, because these 3 scenarios are mainly simple aggregation functions, which do not require high performance of Apache Doris. Version 0.15 can meet demand. In the scenario after Q4, SQL is more complex while Group By needs multiple fields, aggregation functions and complex functions. Therefore, the performance improvement after upgrading is obvious to see: the average query performance is 2- 3 times. We highly recommend that you upgrade to the latest version of Apache Doris.\\n\\n## Summary and Benefits\\n\\n1. Apache Doris supports the construction of offline plus real-time unified data warehouses. One ETL script can support both real-time and offline data warehouses, which greatly greatly improved efficiency, reduces storage costs, and avoids problems such as inconsistencies between offline and real-time indicators.\\n\\n2. Apache Doris 1.1.x version fully supports vectorization, which improves the query performance by 2-3 times compared with the previous version. After testing, the query performance of Apache Doris version 1.1.x in the wide table is equal to that of ClickHouse.\\n\\n3. Apache Doris is powerful and does not depend on other components. Compared with Apache Kylin, Apache Druid, ClickHouse, Apache Doris does not need a second component to fill the technical gap. Apache Doris supports aggregation, detailed queries, and associated queries. Currently, more than 90% of AISPEACH\' analysis have migrated to Apache Doris. Thanks to this advantage, developers operate and maintain fewer components, which greatly reduces the cost of operation and maintenance.\\n\\n4. It is extremely easy to use, supporting MySQL protocol and standard SQL, which greatly reduces user learning costs.\\n\\n_Special thanks to SelectDB, a company with products powered by Apache Doris, helps us work with the community and get sufficient technical support._"},{"id":"/ssb","metadata":{"permalink":"/blog/ssb","source":"@site/blog/ssb.md","title":"Apache Doris 1.2 star-schema-benchmark performance test report","description":"On the SSB flat wide table, the overall performance of Apache Doris 1.2.0-rc01 has been improved by nearly 4 times compared with Apache Doris 1.1.3, and nearly 10 times compared with Apache Doris 0.15.0 RC04. On the SQL test with standard SSB, the overall performance of Apache Doris 1.2.0-rc01 has been improved by nearly 2 times compared with Apache Doris 1.1.3, and nearly 31 times compared with Apache Doris 0.15.0 RC04.","date":"2022-11-22T00:00:00.000Z","tags":[{"inline":true,"label":"Tech Sharing","permalink":"/blog/tags/tech-sharing"}],"hasTruncateMarker":false,"authors":[{"name":"Apache Doris","key":null,"page":null}],"frontMatter":{"title":"Apache Doris 1.2 star-schema-benchmark performance test report","description":"On the SSB flat wide table, the overall performance of Apache Doris 1.2.0-rc01 has been improved by nearly 4 times compared with Apache Doris 1.1.3, and nearly 10 times compared with Apache Doris 0.15.0 RC04. On the SQL test with standard SSB, the overall performance of Apache Doris 1.2.0-rc01 has been improved by nearly 2 times compared with Apache Doris 1.1.3, and nearly 31 times compared with Apache Doris 0.15.0 RC04.","date":"2022-11-22","author":"Apache Doris","tags":["Tech Sharing"],"image":"/images/tech-sharing.png"},"unlisted":false,"prevItem":{"title":"How does Apache Doris help AISPEECH build a data warehouse in AI chatbots scenario","permalink":"/blog/Use-Apache-Doris-with-AI-chatbots"},"nextItem":{"title":"Apache Doris 1.2 TPC-H performance test report","permalink":"/blog/tpch"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n# Star Schema Benchmark\\n\\n[Star Schema Benchmark(SSB)](https://www.cs.umb.edu/~poneil/StarSchemaB.PDF) is a lightweight performance test set in the data warehouse scenario. SSB provides a simplified star schema data based on [TPC-H](http://www.tpc.org/tpch/), which is mainly used to test the performance of multi-table JOIN query under star schema.  In addition, the industry usually flattens SSB into a wide table model (Referred as: SSB flat) to test the performance of the query engine, refer to [Clickhouse](https://clickhouse.com/docs/zh/getting-started).\\n\\nThis document mainly introduces the performance of Doris on the SSB 100G test set.\\n\\n> Note 1: The standard test set including SSB usually has a large gap with the actual business scenario, and some tests will perform parameter tuning for the test set. Therefore, the test results of the standard test set can only reflect the performance of the database in a specific scenario. It is recommended that users use actual business data for further testing.\\n>\\n> Note 2: The operations involved in this document are all performed in the Ubuntu Server 20.04 environment, and CentOS 7 as well.\\n\\nWith 13 queries on the SSB standard test data set, we conducted a comparison test based on Apache Doris 1.2.0-rc01, Apache Doris 1.1.3 and Apache Doris 0.15.0 RC04 versions.\\n\\nOn the SSB flat wide table, the overall performance of Apache Doris 1.2.0-rc01 has been improved by nearly 4 times compared with Apache Doris 1.1.3, and nearly 10 times compared with Apache Doris 0.15.0 RC04.\\n\\nOn the SQL test with standard SSB, the overall performance of Apache Doris 1.2.0-rc01 has been improved by nearly 2 times compared with Apache Doris 1.1.3, and nearly 31 times compared with Apache Doris 0.15.0 RC04.\\n\\n## 1. Hardware Environment\\n\\n| Number of machines | 4 Tencent Cloud Hosts (1 FE, 3 BEs)        |\\n| ------------------ | ----------------------------------------- |\\n| CPU                | AMD EPYC\u2122 Milan (2.55GHz/3.5GHz) 16 Cores |\\n| Memory             | 64G                                       |\\n| Network Bandwidth  | 7Gbps                                     |\\n| Disk               | High-performance Cloud Disk               |\\n\\n## 2. Software Environment\\n\\n- Doris deployed 3BEs and 1FE;\\n- Kernel version: Linux version 5.4.0-96-generic (buildd@lgw01-amd64-051)\\n- OS version: Ubuntu Server 20.04 LTS 64-bit\\n- Doris software versions: Apache Doris 1.2.0-rc01, Apache Doris 1.1.3 and Apache Doris 0.15.0 RC04\\n- JDK: openjdk version \\"11.0.14\\" 2022-01-18\\n\\n## 3. Test Data Volume\\n\\n| SSB Table Name | Rows | Annotation                          |\\n| :------------- | :------------- | :------------------------------- |\\n| lineorder      | 600,037,902    | Commodity Order Details             |\\n| customer       | 3,000,000      | Customer Information        |\\n| part           | 1,400,000      | Parts Information          |\\n| supplier       | 200,000        | Supplier Information        |\\n| date           | 2,556          | Date                        |\\n| lineorder_flat | 600,037,902    | Wide Table after Data Flattening |\\n\\n## 4. Test Results\\n\\nWe use Apache Doris 1.2.0-rc01, Apache Doris 1.1.3 and Apache Doris 0.15.0 RC04 for comparative testing. The test results are as follows:\\n\\n| Query | Apache Doris 1.2.0-rc01(ms) | Apache Doris 1.1.3 (ms) |  Doris 0.15.0 RC04 (ms) |\\n| ----- | ------------- | ------------- | ----------------- |\\n| Q1.1  | 20            | 90            | 250               |\\n| Q1.2  | 10            | 10            | 30                |\\n| Q1.3  | 30            | 70            | 120               |\\n| Q2.1  | 90            | 360           | 900               |\\n| Q2.2  | 90            | 340           | 1,020              |\\n| Q2.3  | 60            | 260           | 770               |\\n| Q3.1  | 160           | 550           | 1,710              |\\n| Q3.2  | 80            | 290           | 670               |\\n| Q3.3  | 90            | 240           | 550               |\\n| Q3.4  | 20            | 20            | 30                |\\n| Q4.1  | 140           | 480           | 1,250              |\\n| Q4.2  | 50            | 240           | 400               |\\n| Q4.3  | 30            | 200           | 330               |\\n| Total  | 880           | 3,150          | 8,030              |\\n\\n![ssb_v11_v015_compare](/images/ssb_flat.png)\\n\\n**Interpretation of Results**\\n\\n- The data set corresponding to the test results is scale 100, about 600 million.\\n- The test environment is configured as the user\'s common configuration, with 4 cloud servers, 16-core 64G SSD, and 1 FE, 3 BEs deployment.\\n- We select the user\'s common configuration test to reduce the cost of user selection and evaluation, but the entire test process will not consume so many hardware resources.\\n\\n\\n## 5. Standard SSB Test Results\\n\\nHere we use Apache Doris 1.2.0-rc01, Apache Doris 1.1.3 and Apache Doris 0.15.0 RC04 for comparative testing. In the test, we use Query Time\uFF08ms\uFF09 as the main performance indicator. The test results are as follows:\\n\\n| Query | Apache Doris 1.2.0-rc01 (ms) | Apache Doris 1.1.3 (ms) | Doris 0.15.0 RC04 (ms) |\\n| ----- | ------- | ---------------------- | ------------------------------- |\\n| Q1.1  | 40      | 18                    | 350                           |\\n| Q1.2  | 30      | 100                    | 80                             |\\n| Q1.3  | 20      | 70                     | 80                            |\\n| Q2.1  | 350     | 940                  | 20,680                     |\\n| Q2.2  | 320     | 750                  | 18,250                    |\\n| Q2.3  | 300     | 720                  | 14,760                   |\\n| Q3.1  | 650     | 2,150                 | 22,190                   |\\n| Q3.2  | 260     | 510                 | 8,360                          |\\n| Q3.3  | 220     | 450                  | 6,200                        |\\n| Q3.4  | 60      | 70                   | 160                            |\\n| Q4.1  | 840     | 1,480                   | 24,320                      |\\n| Q4.2  | 460     | 560                 | 6,310                          |\\n| Q4.3  | 610     | 660                  | 10,170                    |\\n| Total  | 4,160    | 8,478                | 131,910 |\\n\\n![ssb_12_11_015](/images/ssb.png)\\n\\n**Interpretation of Results**\\n\\n- The data set corresponding to the test results is scale 100, about 600 million.\\n- The test environment is configured as the user\'s common configuration, with 4 cloud servers, 16-core 64G SSD, and 1 FE 3 BEs deployment.\\n- We select the user\'s common configuration test to reduce the cost of user selection and evaluation, but the entire test process will not consume so many hardware resources.\\n\\n## 6. Environment Preparation\\n\\nPlease first refer to the [official documentation](../install/deploy-manually/storage-compute-decoupled-deploy-manually) to install and deploy Apache Doris first to obtain a Doris cluster which is working well(including at least 1 FE 1 BE, 1 FE 3 BEs is recommended).\\n\\nThe scripts mentioned in the following documents are stored in the Apache Doris codebase: [ssb-tools](https://github.com/apache/doris/tree/master/tools/ssb-tools)\\n\\n## 7. Data Preparation\\n\\n### 7.1 Download and Install the SSB Data Generation Tool.\\n\\nExecute the following script to download and compile the [ssb-dbgen](https://github.com/electrum/ssb-dbgen.git) tool.\\n\\n```shell\\nsh build-ssb-dbgen.sh\\n```\\n\\nAfter successful installation, the `dbgen` binary will be generated under the `ssb-dbgen/` directory.\\n\\n### 7.2 Generate SSB Test Set\\n\\nExecute the following script to generate the SSB dataset:\\n\\n```shell\\nsh gen-ssb-data.sh -s 100 -c 100\\n```\\n\\n> Note 1: Check the script help via `sh gen-ssb-data.sh -h`.\\n>\\n> Note 2: The data will be generated under the `ssb-data/` directory with the suffix `.tbl`. The total file size is about 60GB and may need a few minutes to an hour to generate.\\n>\\n> Note 3: `-s 100` indicates that the test set size factor is 100, `-c 100` indicates that 100 concurrent threads generate the data of the lineorder table. The `-c` parameter also determines the number of files in the final lineorder table. The larger the parameter, the larger the number of files and the smaller each file.\\n\\nWith the `-s 100` parameter, the resulting dataset size is:\\n\\n| Table     | Rows             | Size | File Number |\\n| --------- | ---------------- | ---- | ----------- |\\n| lineorder | 600,037,902 | 60GB | 100         |\\n| customer  | 3,000,000 | 277M | 1           |\\n| part      | 1,400,000 | 116M | 1           |\\n| supplier  | 200,000   | 17M  | 1           |\\n| date      | 2,556             | 228K | 1           |\\n\\n### 7.3 Create Table\\n\\n#### 7.3.1 Prepare the `doris-cluster.conf` File.\\n\\nBefore import the script, you need to write the FE\u2019s ip port and other information in the `doris-cluster.conf` file.\\n\\nThe file location is at the same level as `load-ssb-dimension-data.sh`.\\n\\nThe content of the file includes FE\'s ip, HTTP port, user name, password and the DB name of the data to be imported:\\n\\n```shell\\nexport FE_HOST=\\"xxx\\"\\nexport FE_HTTP_PORT=\\"8030\\"\\nexport FE_QUERY_PORT=\\"9030\\"\\nexport USER=\\"root\\"\\nexport PASSWORD=\'xxx\'\\nexport DB=\\"ssb\\"\\n```\\n\\n#### 7.3.2 Execute the Following Script to Generate and Create the SSB Table:\\n\\n```shell\\nsh create-ssb-tables.sh\\n```\\n\\nOr copy the table creation statements in [create-ssb-tables.sql](https://github.com/apache/incubator-doris/tree/master/tools/ssb-tools/ddl/create-ssb-tables.sql) and [ create-ssb-flat-table.sql](https://github.com/apache/incubator-doris/tree/master/tools/ssb-tools/ddl/create-ssb-flat-table.sql) and then execute them in the MySQL client.\\n\\nThe following is the `lineorder_flat` table build statement. Create the `lineorder_flat` table in the above `create-ssb-flat-table.sh` script, and perform the default number of buckets (48 buckets). You can delete this table and adjust the number of buckets according to your cluster scale node configuration, so as to obtain a better test result.\\n\\n```sql\\nCREATE TABLE `lineorder_flat` (\\n  `LO_ORDERDATE` date NOT NULL COMMENT \\"\\",\\n  `LO_ORDERKEY` int(11) NOT NULL COMMENT \\"\\",\\n  `LO_LINENUMBER` tinyint(4) NOT NULL COMMENT \\"\\",\\n  `LO_CUSTKEY` int(11) NOT NULL COMMENT \\"\\",\\n  `LO_PARTKEY` int(11) NOT NULL COMMENT \\"\\",\\n  `LO_SUPPKEY` int(11) NOT NULL COMMENT \\"\\",\\n  `LO_ORDERPRIORITY` varchar(100) NOT NULL COMMENT \\"\\",\\n  `LO_SHIPPRIORITY` tinyint(4) NOT NULL COMMENT \\"\\",\\n  `LO_QUANTITY` tinyint(4) NOT NULL COMMENT \\"\\",\\n  `LO_EXTENDEDPRICE` int(11) NOT NULL COMMENT \\"\\",\\n  `LO_ORDTOTALPRICE` int(11) NOT NULL COMMENT \\"\\",\\n  `LO_DISCOUNT` tinyint(4) NOT NULL COMMENT \\"\\",\\n  `LO_REVENUE` int(11) NOT NULL COMMENT \\"\\",\\n  `LO_SUPPLYCOST` int(11) NOT NULL COMMENT \\"\\",\\n  `LO_TAX` tinyint(4) NOT NULL COMMENT \\"\\",\\n  `LO_COMMITDATE` date NOT NULL COMMENT \\"\\",\\n  `LO_SHIPMODE` varchar(100) NOT NULL COMMENT \\"\\",\\n  `C_NAME` varchar(100) NOT NULL COMMENT \\"\\",\\n  `C_ADDRESS` varchar(100) NOT NULL COMMENT \\"\\",\\n  `C_CITY` varchar(100) NOT NULL COMMENT \\"\\",\\n  `C_NATION` varchar(100) NOT NULL COMMENT \\"\\",\\n  `C_REGION` varchar(100) NOT NULL COMMENT \\"\\",\\n  `C_PHONE` varchar(100) NOT NULL COMMENT \\"\\",\\n  `C_MKTSEGMENT` varchar(100) NOT NULL COMMENT \\"\\",\\n  `S_NAME` varchar(100) NOT NULL COMMENT \\"\\",\\n  `S_ADDRESS` varchar(100) NOT NULL COMMENT \\"\\",\\n  `S_CITY` varchar(100) NOT NULL COMMENT \\"\\",\\n  `S_NATION` varchar(100) NOT NULL COMMENT \\"\\",\\n  `S_REGION` varchar(100) NOT NULL COMMENT \\"\\",\\n  `S_PHONE` varchar(100) NOT NULL COMMENT \\"\\",\\n  `P_NAME` varchar(100) NOT NULL COMMENT \\"\\",\\n  `P_MFGR` varchar(100) NOT NULL COMMENT \\"\\",\\n  `P_CATEGORY` varchar(100) NOT NULL COMMENT \\"\\",\\n  `P_BRAND` varchar(100) NOT NULL COMMENT \\"\\",\\n  `P_COLOR` varchar(100) NOT NULL COMMENT \\"\\",\\n  `P_TYPE` varchar(100) NOT NULL COMMENT \\"\\",\\n  `P_SIZE` tinyint(4) NOT NULL COMMENT \\"\\",\\n  `P_CONTAINER` varchar(100) NOT NULL COMMENT \\"\\"\\n) ENGINE=OLAP\\nDUPLICATE KEY(`LO_ORDERDATE`, `LO_ORDERKEY`)\\nCOMMENT \\"OLAP\\"\\nPARTITION BY RANGE(`LO_ORDERDATE`)\\n(PARTITION p1 VALUES [(\'0000-01-01\'), (\'1993-01-01\')),\\nPARTITION p2 VALUES [(\'1993-01-01\'), (\'1994-01-01\')),\\nPARTITION p3 VALUES [(\'1994-01-01\'), (\'1995-01-01\')),\\nPARTITION p4 VALUES [(\'1995-01-01\'), (\'1996-01-01\')),\\nPARTITION p5 VALUES [(\'1996-01-01\'), (\'1997-01-01\')),\\nPARTITION p6 VALUES [(\'1997-01-01\'), (\'1998-01-01\')),\\nPARTITION p7 VALUES [(\'1998-01-01\'), (\'1999-01-01\')))\\nDISTRIBUTED BY HASH(`LO_ORDERKEY`) BUCKETS 48\\nPROPERTIES (\\n\\"replication_num\\" = \\"1\\",\\n\\"colocate_with\\" = \\"groupxx1\\",\\n\\"in_memory\\" = \\"false\\",\\n\\"storage_format\\" = \\"DEFAULT\\"\\n);\\n```\\n\\n### 7.4 Import data\\n\\nWe use the following command to complete all data import of SSB test set and SSB FLAT wide table data synthesis and then import into the table.\\n\\n```shell\\n sh bin/load-ssb-data.sh -c 10\\n```\\n\\n`-c 5` means start 10 concurrent threads to import (5 by default). In the case of a single BE node, the lineorder data generated by `sh gen-ssb-data.sh -s 100 -c 100` will also generate the data of the ssb-flat table in the end. If more threads are enabled, the import speed can be accelerated. But it will cost extra memory.\\n\\n> Notes.\\n>\\n> 1. To get faster import speed, you can add `flush_thread_num_per_store=5` in be.conf and then restart BE. This configuration indicates the number of disk writing threads for each data directory, 2 by default. Larger data can improve write data throughput, but may increase IO Util. (Reference value: 1 mechanical disk, with 2 by default, the IO Util during the import process is about 12%. When it is set to 5, the IO Util is about 26%. If it is an SSD disk, it is almost 0%) .\\n>\\n> 2. The flat table data is imported by \'INSERT INTO ... SELECT ... \'.\\n\\n### 7.5 Checking Imported data\\n\\n\\n```sql\\nselect count(*) from part;\\nselect count(*) from customer;\\nselect count(*) from supplier;\\nselect count(*) from date;\\nselect count(*) from lineorder;\\nselect count(*) from lineorder_flat;\\n```\\n\\nThe amount of data should be consistent with the number of rows of generated data.\\n\\n| Table          | Rows             | Origin Size | Compacted Size(1 Replica) |\\n| -------------- | ---------------- | ----------- | ------------------------- |\\n| lineorder_flat | 600,037,902 |             | 59.709 GB                 |\\n| lineorder      | 600,037,902 | 60 GB       | 14.514 GB                 |\\n| customer       | 3,000,000 | 277 MB      | 138.247 MB                |\\n| part           | 1,400,000 | 116 MB      | 12.759 MB                 |\\n| supplier       | 200,000   | 17 MB       | 9.143 MB                  |\\n| date           | 2,556             | 228 KB      | 34.276 KB                 |\\n\\n### 7.6 Query Test\\n\\n- SSB-Flat Query Statement: [ ssb-flat-queries](https://github.com/apache/doris/tree/master/tools/ssb-tools/ssb-flat-queries)\\n- Standard SSB Queries: [ ssb-queries](https://github.com/apache/doris/tree/master/tools/ssb-tools/ssb-queries)\\n\\n#### 7.6.1 SSB FLAT Test for SQL\\n\\n```sql\\n--Q1.1\\nSELECT SUM(LO_EXTENDEDPRICE * LO_DISCOUNT) AS revenue\\nFROM lineorder_flat\\nWHERE  LO_ORDERDATE >= 19930101  AND LO_ORDERDATE <= 19931231 AND LO_DISCOUNT BETWEEN 1 AND 3  AND LO_QUANTITY < 25;\\n--Q1.2\\nSELECT SUM(LO_EXTENDEDPRICE * LO_DISCOUNT) AS revenue\\nFROM lineorder_flat\\nWHERE LO_ORDERDATE >= 19940101 AND LO_ORDERDATE <= 19940131  AND LO_DISCOUNT BETWEEN 4 AND 6 AND LO_QUANTITY BETWEEN 26 AND 35;\\n\\n--Q1.3\\nSELECT SUM(LO_EXTENDEDPRICE * LO_DISCOUNT) AS revenue\\nFROM lineorder_flat\\nWHERE  weekofyear(LO_ORDERDATE) = 6 AND LO_ORDERDATE >= 19940101  AND LO_ORDERDATE <= 19941231 AND LO_DISCOUNT BETWEEN 5 AND 7  AND LO_QUANTITY BETWEEN 26 AND 35;\\n\\n--Q2.1\\nSELECT SUM(LO_REVENUE), (LO_ORDERDATE DIV 10000) AS YEAR, P_BRAND\\nFROM lineorder_flat WHERE P_CATEGORY = \'MFGR#12\' AND S_REGION = \'AMERICA\'\\nGROUP BY YEAR, P_BRAND\\nORDER BY YEAR, P_BRAND;\\n\\n--Q2.2\\nSELECT  SUM(LO_REVENUE), (LO_ORDERDATE DIV 10000) AS YEAR, P_BRAND\\nFROM lineorder_flat\\nWHERE P_BRAND >= \'MFGR#2221\' AND P_BRAND <= \'MFGR#2228\'  AND S_REGION = \'ASIA\'\\nGROUP BY YEAR, P_BRAND\\nORDER BY YEAR, P_BRAND;\\n\\n--Q2.3\\nSELECT SUM(LO_REVENUE), (LO_ORDERDATE DIV 10000) AS YEAR, P_BRAND\\nFROM lineorder_flat\\nWHERE P_BRAND = \'MFGR#2239\' AND S_REGION = \'EUROPE\'\\nGROUP BY YEAR, P_BRAND\\nORDER BY YEAR, P_BRAND;\\n\\n--Q3.1\\nSELECT C_NATION, S_NATION, (LO_ORDERDATE DIV 10000) AS YEAR, SUM(LO_REVENUE) AS revenue\\nFROM lineorder_flat\\nWHERE C_REGION = \'ASIA\' AND S_REGION = \'ASIA\' AND LO_ORDERDATE >= 19920101  AND LO_ORDERDATE <= 19971231\\nGROUP BY C_NATION, S_NATION, YEAR\\nORDER BY YEAR ASC, revenue DESC;\\n\\n--Q3.2\\nSELECT C_CITY, S_CITY, (LO_ORDERDATE DIV 10000) AS YEAR, SUM(LO_REVENUE) AS revenue\\nFROM lineorder_flat\\nWHERE C_NATION = \'UNITED STATES\' AND S_NATION = \'UNITED STATES\' AND LO_ORDERDATE >= 19920101 AND LO_ORDERDATE <= 19971231\\nGROUP BY C_CITY, S_CITY, YEAR\\nORDER BY YEAR ASC, revenue DESC;\\n\\n--Q3.3\\nSELECT C_CITY, S_CITY, (LO_ORDERDATE DIV 10000) AS YEAR, SUM(LO_REVENUE) AS revenue\\nFROM lineorder_flat\\nWHERE C_CITY IN (\'UNITED KI1\', \'UNITED KI5\') AND S_CITY IN (\'UNITED KI1\', \'UNITED KI5\') AND LO_ORDERDATE >= 19920101 AND LO_ORDERDATE <= 19971231\\nGROUP BY C_CITY, S_CITY, YEAR\\nORDER BY YEAR ASC, revenue DESC;\\n\\n--Q3.4\\nSELECT C_CITY, S_CITY, (LO_ORDERDATE DIV 10000) AS YEAR, SUM(LO_REVENUE) AS revenue\\nFROM lineorder_flat\\nWHERE C_CITY IN (\'UNITED KI1\', \'UNITED KI5\') AND S_CITY IN (\'UNITED KI1\', \'UNITED KI5\') AND LO_ORDERDATE >= 19971201  AND LO_ORDERDATE <= 19971231\\nGROUP BY C_CITY, S_CITY, YEAR\\nORDER BY YEAR ASC, revenue DESC;\\n\\n--Q4.1\\nSELECT (LO_ORDERDATE DIV 10000) AS YEAR, C_NATION, SUM(LO_REVENUE - LO_SUPPLYCOST) AS profit\\nFROM lineorder_flat\\nWHERE C_REGION = \'AMERICA\' AND S_REGION = \'AMERICA\' AND P_MFGR IN (\'MFGR#1\', \'MFGR#2\')\\nGROUP BY YEAR, C_NATION\\nORDER BY YEAR ASC, C_NATION ASC;\\n\\n--Q4.2\\nSELECT (LO_ORDERDATE DIV 10000) AS YEAR,S_NATION, P_CATEGORY, SUM(LO_REVENUE - LO_SUPPLYCOST) AS profit\\nFROM lineorder_flat\\nWHERE C_REGION = \'AMERICA\' AND S_REGION = \'AMERICA\' AND LO_ORDERDATE >= 19970101 AND LO_ORDERDATE <= 19981231 AND P_MFGR IN (\'MFGR#1\', \'MFGR#2\')\\nGROUP BY YEAR, S_NATION, P_CATEGORY\\nORDER BY YEAR ASC, S_NATION ASC, P_CATEGORY ASC;\\n\\n--Q4.3\\nSELECT (LO_ORDERDATE DIV 10000) AS YEAR, S_CITY, P_BRAND, SUM(LO_REVENUE - LO_SUPPLYCOST) AS profit\\nFROM lineorder_flat\\nWHERE S_NATION = \'UNITED STATES\' AND LO_ORDERDATE >= 19970101 AND LO_ORDERDATE <= 19981231 AND P_CATEGORY = \'MFGR#14\'\\nGROUP BY YEAR, S_CITY, P_BRAND\\nORDER BY YEAR ASC, S_CITY ASC, P_BRAND ASC;\\n```\\n\\n#### 7.6.2 SSB Standard Test for SQL\\n\\n```SQL\\n--Q1.1\\nSELECT SUM(lo_extendedprice * lo_discount) AS REVENUE\\nFROM lineorder, dates\\nWHERE\\n    lo_orderdate = d_datekey\\n    AND d_year = 1993\\n    AND lo_discount BETWEEN 1 AND 3\\n    AND lo_quantity < 25;\\n--Q1.2\\nSELECT SUM(lo_extendedprice * lo_discount) AS REVENUE\\nFROM lineorder, dates\\nWHERE\\n    lo_orderdate = d_datekey\\n    AND d_yearmonth = \'Jan1994\'\\n    AND lo_discount BETWEEN 4 AND 6\\n    AND lo_quantity BETWEEN 26 AND 35;\\n    \\n--Q1.3\\nSELECT\\n    SUM(lo_extendedprice * lo_discount) AS REVENUE\\nFROM lineorder, dates\\nWHERE\\n    lo_orderdate = d_datekey\\n    AND d_weeknuminyear = 6\\n    AND d_year = 1994\\n    AND lo_discount BETWEEN 5 AND 7\\n    AND lo_quantity BETWEEN 26 AND 35;\\n    \\n--Q2.1\\nSELECT SUM(lo_revenue), d_year, p_brand\\nFROM lineorder, dates, part, supplier\\nWHERE\\n    lo_orderdate = d_datekey\\n    AND lo_partkey = p_partkey\\n    AND lo_suppkey = s_suppkey\\n    AND p_category = \'MFGR#12\'\\n    AND s_region = \'AMERICA\'\\nGROUP BY d_year, p_brand\\nORDER BY p_brand;\\n\\n--Q2.2\\nSELECT SUM(lo_revenue), d_year, p_brand\\nFROM lineorder, dates, part, supplier\\nWHERE\\n    lo_orderdate = d_datekey\\n    AND lo_partkey = p_partkey\\n    AND lo_suppkey = s_suppkey\\n    AND p_brand BETWEEN \'MFGR#2221\' AND \'MFGR#2228\'\\n    AND s_region = \'ASIA\'\\nGROUP BY d_year, p_brand\\nORDER BY d_year, p_brand;\\n\\n--Q2.3\\nSELECT SUM(lo_revenue), d_year, p_brand\\nFROM lineorder, dates, part, supplier\\nWHERE\\n    lo_orderdate = d_datekey\\n    AND lo_partkey = p_partkey\\n    AND lo_suppkey = s_suppkey\\n    AND p_brand = \'MFGR#2239\'\\n    AND s_region = \'EUROPE\'\\nGROUP BY d_year, p_brand\\nORDER BY d_year, p_brand;\\n\\n--Q3.1\\nSELECT\\n    c_nation,\\n    s_nation,\\n    d_year,\\n    SUM(lo_revenue) AS REVENUE\\nFROM customer, lineorder, supplier, dates\\nWHERE\\n    lo_custkey = c_custkey\\n    AND lo_suppkey = s_suppkey\\n    AND lo_orderdate = d_datekey\\n    AND c_region = \'ASIA\'\\n    AND s_region = \'ASIA\'\\n    AND d_year >= 1992\\n    AND d_year <= 1997\\nGROUP BY c_nation, s_nation, d_year\\nORDER BY d_year ASC, REVENUE DESC;\\n\\n--Q3.2\\nSELECT\\n    c_city,\\n    s_city,\\n    d_year,\\n    SUM(lo_revenue) AS REVENUE\\nFROM customer, lineorder, supplier, dates\\nWHERE\\n    lo_custkey = c_custkey\\n    AND lo_suppkey = s_suppkey\\n    AND lo_orderdate = d_datekey\\n    AND c_nation = \'UNITED STATES\'\\n    AND s_nation = \'UNITED STATES\'\\n    AND d_year >= 1992\\n    AND d_year <= 1997\\nGROUP BY c_city, s_city, d_year\\nORDER BY d_year ASC, REVENUE DESC;\\n\\n--Q3.3\\nSELECT\\n    c_city,\\n    s_city,\\n    d_year,\\n    SUM(lo_revenue) AS REVENUE\\nFROM customer, lineorder, supplier, dates\\nWHERE\\n    lo_custkey = c_custkey\\n    AND lo_suppkey = s_suppkey\\n    AND lo_orderdate = d_datekey\\n    AND (\\n        c_city = \'UNITED KI1\'\\n        OR c_city = \'UNITED KI5\'\\n    )\\n    AND (\\n        s_city = \'UNITED KI1\'\\n        OR s_city = \'UNITED KI5\'\\n    )\\n    AND d_year >= 1992\\n    AND d_year <= 1997\\nGROUP BY c_city, s_city, d_year\\nORDER BY d_year ASC, REVENUE DESC;\\n\\n--Q3.4\\nSELECT\\n    c_city,\\n    s_city,\\n    d_year,\\n    SUM(lo_revenue) AS REVENUE\\nFROM customer, lineorder, supplier, dates\\nWHERE\\n    lo_custkey = c_custkey\\n    AND lo_suppkey = s_suppkey\\n    AND lo_orderdate = d_datekey\\n    AND (\\n        c_city = \'UNITED KI1\'\\n        OR c_city = \'UNITED KI5\'\\n    )\\n    AND (\\n        s_city = \'UNITED KI1\'\\n        OR s_city = \'UNITED KI5\'\\n    )\\n    AND d_yearmonth = \'Dec1997\'\\nGROUP BY c_city, s_city, d_year\\nORDER BY d_year ASC, REVENUE DESC;\\n\\n--Q4.1\\nSELECT /*+SET_VAR(parallel_fragment_exec_instance_num=4, enable_vectorized_engine=true, batch_size=4096, enable_cost_based_join_reorder=true, enable_projection=true) */\\n    d_year,\\n    c_nation,\\n    SUM(lo_revenue - lo_supplycost) AS PROFIT\\nFROM dates, customer, supplier, part, lineorder\\nWHERE\\n    lo_custkey = c_custkey\\n    AND lo_suppkey = s_suppkey\\n    AND lo_partkey = p_partkey\\n    AND lo_orderdate = d_datekey\\n    AND c_region = \'AMERICA\'\\n    AND s_region = \'AMERICA\'\\n    AND (\\n        p_mfgr = \'MFGR#1\'\\n        OR p_mfgr = \'MFGR#2\'\\n    )\\nGROUP BY d_year, c_nation\\nORDER BY d_year, c_nation;\\n\\n--Q4.2\\nSELECT /*+SET_VAR(parallel_fragment_exec_instance_num=2, enable_vectorized_engine=true, batch_size=4096, enable_cost_based_join_reorder=true, enable_projection=true) */  \\n    d_year,\\n    s_nation,\\n    p_category,\\n    SUM(lo_revenue - lo_supplycost) AS PROFIT\\nFROM dates, customer, supplier, part, lineorder\\nWHERE\\n    lo_custkey = c_custkey\\n    AND lo_suppkey = s_suppkey\\n    AND lo_partkey = p_partkey\\n    AND lo_orderdate = d_datekey\\n    AND c_region = \'AMERICA\'\\n    AND s_region = \'AMERICA\'\\n    AND (\\n        d_year = 1997\\n        OR d_year = 1998\\n    )\\n    AND (\\n        p_mfgr = \'MFGR#1\'\\n        OR p_mfgr = \'MFGR#2\'\\n    )\\nGROUP BY d_year, s_nation, p_category\\nORDER BY d_year, s_nation, p_category;\\n\\n--Q4.3\\nSELECT /*+SET_VAR(parallel_fragment_exec_instance_num=2, enable_vectorized_engine=true, batch_size=4096, enable_cost_based_join_reorder=true, enable_projection=true) */\\n    d_year,\\n    s_city,\\n    p_brand,\\n    SUM(lo_revenue - lo_supplycost) AS PROFIT\\nFROM dates, customer, supplier, part, lineorder\\nWHERE\\n    lo_custkey = c_custkey\\n    AND lo_suppkey = s_suppkey\\n    AND lo_partkey = p_partkey\\n    AND lo_orderdate = d_datekey\\n    AND s_nation = \'UNITED STATES\'\\n    AND (\\n        d_year = 1997\\n        OR d_year = 1998\\n    )\\n    AND p_category = \'MFGR#14\'\\nGROUP BY d_year, s_city, p_brand\\nORDER BY d_year, s_city, p_brand;\\n```"},{"id":"/tpch","metadata":{"permalink":"/blog/tpch","source":"@site/blog/tpch.md","title":"Apache Doris 1.2 TPC-H performance test report","description":"On 22 queries on the TPC-H standard test data set, we conducted a comparison test based on Apache Doris 1.2.0-rc01, Apache Doris 1.1.3 and Apache Doris 0.15.0 RC04 versions. Compared with Apache Doris 1.1.3, the overall performance of Apache Doris 1.2.0-rc01 has been improved by nearly 3 times, and by nearly 11 times compared with Apache Doris 0.15.0 RC04.","date":"2022-11-22T00:00:00.000Z","tags":[{"inline":true,"label":"Tech Sharing","permalink":"/blog/tags/tech-sharing"}],"hasTruncateMarker":false,"authors":[{"name":"Apache Doris","key":null,"page":null}],"frontMatter":{"title":"Apache Doris 1.2 TPC-H performance test report","description":"On 22 queries on the TPC-H standard test data set, we conducted a comparison test based on Apache Doris 1.2.0-rc01, Apache Doris 1.1.3 and Apache Doris 0.15.0 RC04 versions. Compared with Apache Doris 1.1.3, the overall performance of Apache Doris 1.2.0-rc01 has been improved by nearly 3 times, and by nearly 11 times compared with Apache Doris 0.15.0 RC04.","date":"2022-11-22","author":"Apache Doris","tags":["Tech Sharing"],"image":"/images/tech-sharing.png"},"unlisted":false,"prevItem":{"title":"Apache Doris 1.2 star-schema-benchmark performance test report","permalink":"/blog/ssb"},"nextItem":{"title":"Apache Doris announced the official release of version 1.1.4","permalink":"/blog/release-note-1.1.4"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n# TPC-H Benchmark\\n\\nTPC-H is a decision support benchmark (Decision Support Benchmark), which consists of a set of business-oriented special query and concurrent data modification. The data that is queried and populates the database has broad industry relevance. This benchmark demonstrates a decision support system that examines large amounts of data, executes highly complex queries, and answers key business questions. The performance index reported by TPC-H is called TPC-H composite query performance index per hour (QphH@Size), which reflects multiple aspects of the system\'s ability to process queries. These aspects include the database size chosen when executing the query, the query processing capability when the query is submitted by a single stream, and the query throughput when the query is submitted by many concurrent users.\\n\\nThis document mainly introduces the performance of Doris on the TPC-H 100G test set.\\n\\n> Note 1: The standard test set including TPC-H is usually far from the actual business scenario, and some tests will perform parameter tuning for the test set. Therefore, the test results of the standard test set can only reflect the performance of the database in a specific scenario. We suggest users use actual business data for further testing.\\n>\\n> Note 2: The operations involved in this document are all tested on CentOS 7.x.\\n\\nOn 22 queries on the TPC-H standard test data set, we conducted a comparison test based on Apache Doris 1.2.0-rc01, Apache Doris 1.1.3 and Apache Doris 0.15.0 RC04 versions. Compared with Apache Doris 1.1.3, the overall performance of Apache Doris 1.2.0-rc01 has been improved by nearly 3 times, and by nearly 11 times compared with Apache Doris 0.15.0 RC04.\\n\\n## 1. Hardware Environment\\n\\n| Hardware           | Configuration Instructions                                   |\\n| -------- | ------------------------------------ |\\n| Number of mMachines | 4 Tencent Cloud Virtual Machine\uFF081FE\uFF0C3BEs\uFF09 |\\n| CPU      | Intel Xeon(Cascade Lake) Platinum 8269CY  16C  (2.5 GHz/3.2 GHz) |\\n| Memory | 64G                                  |\\n| Network | 5Gbps                              |\\n| Disk   | ESSD Cloud Hard Disk  |\\n\\n## 2. Software Environment\\n\\n- Doris Deployed 3BEs and 1FE\\n- Kernel Version: Linux version 5.4.0-96-generic (buildd@lgw01-amd64-051)\\n- OS version: CentOS 7.8\\n- Doris software version: Apache Doris 1.2.0-rc01, Apache Doris 1.1.3 , Apache Doris 0.15.0 RC04\\n- JDK: openjdk version \\"11.0.14\\" 2022-01-18\\n\\n## 3. Test Data Volume\\n\\nThe TPCH 100G data generated by the simulation of the entire test are respectively imported into Apache Doris 1.2.0-rc01, Apache Doris 1.1.3 and Apache Doris 0.15.0 RC04 for testing. The following is the relevant description and data volume of the table.\\n\\n| TPC-H Table Name | Rows        | Size after Import  | Annotation |\\n| :--------------- | :---------- | ---------- | :----- |\\n| REGION           | 5           | 400KB      | Region       |\\n| NATION           | 25          | 7.714 KB   | Nation       |\\n| SUPPLIER         | 1,000,000 | 85.528 MB  | Supplier       |\\n| PART             | 20,000,000  | 752.330 MB | Parts       |\\n| PARTSUPP         | 20,000,000  | 4.375 GB   | Parts Supply       |\\n| CUSTOMER         | 15,000,000  | 1.317 GB   | Customer        |\\n| ORDERS           | 1,500,000,000 | 6.301 GB   | Orders        |\\n| LINEITEM         | 6,000,000,000   | 20.882 GB  | Order Details       |\\n\\n## 4. Test SQL\\n\\nTPCH 22 test query statements : [TPCH-Query-SQL](https://github.com/apache/incubator-doris/tree/master/tools/tpch-tools/queries)\\n\\n**Notice:**\\n\\nThe following four parameters in the above SQL do not exist in Apache Doris 0.15.0 RC04. When executing, please remove:\\n\\n```\\n1. enable_vectorized_engine=true,\\n2. batch_size=4096,\\n3. disable_join_reorder=false\\n4. enable_projection=true\\n```\\n\\n## 5. Test Results\\n\\nHere we use Apache Doris 1.2.0-rc01, Apache Doris 1.1.3 and Apache Doris 0.15.0 RC04 for comparative testing. In the test, we use Query Time(ms) as the main performance indicator. The test results are as follows:\\n\\n| Query    | Apache Doris 1.2.0-rc01 (ms) | Apache Doris 1.1.3 (ms) | Apache Doris 0.15.0 RC04 (ms) |\\n| -------- | --------------------------- | ---------------------- | ---------------------------- |\\n| Q1       | 2.12                        | 3.75                   | 28.63                        |\\n| Q2       | 0.20                        | 4.22                   | 7.88                         |\\n| Q3       | 0.62                        | 2.64                   | 9.39                         |\\n| Q4       | 0.61                        | 1.5                    | 9.3                          |\\n| Q5       | 1.05                        | 2.15                   | 4.11                         |\\n| Q6       | 0.08                        | 0.19                   | 0.43                         |\\n| Q7       | 0.58                        | 1.04                   | 1.61                         |\\n| Q8       | 0.72                        | 1.75                   | 50.35                        |\\n| Q9       | 3.61                        | 7.94                   | 16.34                        |\\n| Q10      | 1.26                        | 1.41                   | 5.21                         |\\n| Q11      | 0.15                        | 0.35                   | 1.72                         |\\n| Q12      | 0.21                        | 0.57                   | 5.39                         |\\n| Q13      | 2.62                        | 8.15                   | 20.88                        |\\n| Q14      | 0.16                        | 0.3                    |                              |\\n| Q15      | 0.30                        | 0.66                   | 1.86                         |\\n| Q16      | 0.38                        | 0.79                   | 1.32                         |\\n| Q17      | 0.65                        | 1.51                   | 26.67                        |\\n| Q18      | 2.28                        | 3.364                  | 11.77                        |\\n| Q19      | 0.20                        | 0.829                  | 1.71                         |\\n| Q20      | 0.21                        | 2.77                   | 5.2                          |\\n| Q21      | 1.17                        | 4.47                   | 10.34                        |\\n| Q22      | 0.46                        | 0.9                    | 3.22                         |\\n| **Total** | **19.64**                   | **51.253**             | **223.33**                   |\\n\\n![image-20220614114351241](/images/tpch.png)\\n\\n- **Result Description**\\n    - The data set corresponding to the test results is scale 100, about 600 million.\\n    - The test environment is configured as the user\'s common configuration, with 4 cloud servers, 16-core 64G SSD, and 1 FE 3 BEs deployment.\\n    - Select the user\'s common configuration test to reduce the cost of user selection and evaluation, but the entire test process will not consume so many hardware resources.\\n    - Apache Doris 0.15 RC04 failed to execute Q14 in the TPC-H test, unable to complete the query.\\n\\n## 6. Environmental Preparation\\n\\nPlease refer to the [official document](https://doris.apache.org/docs/install/deploy-manually/storage-compute-coupled-deploy-manually/) to install and deploy Doris to obtain a normal running Doris cluster (at least 1 FE 1 BE, 1 FE 3 BE is recommended).\\n\\n## 7. Data Preparation\\n\\n### 7.1 Download and Install TPC-H Data Generation Tool\\n\\nExecute the following script to download and compile the [tpch-tools](https://github.com/apache/incubator-doris/tree/master/tools/tpch-tools) tool.\\n\\n```shell\\nsh build-tpch-dbgen.sh\\n```\\n\\nAfter successful installation, the `dbgen` binary will be generated under the `TPC-H_Tools_v3.0.0/` directory.\\n\\n### 7.2 Generating the TPC-H Test Set\\n\\nExecute the following script to generate the TPC-H dataset:\\n\\n```shell\\nsh gen-tpch-data.sh\\n```\\n\\n> Note 1: Check the script help via `sh gen-tpch-data.sh -h`.\\n>\\n> Note 2: The data will be generated under the `tpch-data/` directory with the suffix `.tbl`. The total file size is about 100GB and may need a few minutes to an hour to generate.\\n>\\n> Note 3: A standard test data set of 100G is generated by default.\\n\\n### 7.3 Create Table\\n\\n#### 7.3.1 Prepare the `doris-cluster.conf` File\\n\\nBefore import the script, you need to write the FE\u2019s ip port and other information in the `doris-cluster.conf` file.\\n\\nThe file location is at the same level as `load-tpch-data.sh`.\\n\\nThe content of the file includes FE\'s ip, HTTP port, user name, password and the DB name of the data to be imported:\\n\\n```shell\\n# Any of FE host\\nexport FE_HOST=\'127.0.0.1\'\\n# http_port in fe.conf\\nexport FE_HTTP_PORT=8030\\n# query_port in fe.conf\\nexport FE_QUERY_PORT=9030\\n# Doris username\\nexport USER=\'root\'\\n# Doris password\\nexport PASSWORD=\'\'\\n# The database where TPC-H tables located\\nexport DB=\'tpch1\'\\n```\\n\\n#### Execute the Following Script to Generate and Create TPC-H Table\\n\\n```shell\\nsh create-tpch-tables.sh\\n```\\nOr copy the table creation statement in [create-tpch-tables.sql](https://github.com/apache/incubator-doris/blob/master/tools/tpch-tools/create-tpch-tables.sql) and execute it in Doris.\\n\\n\\n### 7.4 Import Data\\n\\nPlease perform data import with the following command:\\n\\n```shell\\nsh ./load-tpch-data.sh\\n```\\n\\n### 7.5 Check Imported Data\\n\\nExecute the following SQL statement to check that the imported data is consistent with the above data.\\n\\n```sql\\nselect count(*)  from  lineitem;\\nselect count(*)  from  orders;\\nselect count(*)  from  partsupp;\\nselect count(*)  from  part;\\nselect count(*)  from  customer;\\nselect count(*)  from  supplier;\\nselect count(*)  from  nation;\\nselect count(*)  from  region;\\nselect count(*)  from  revenue0;\\n```\\n\\n### 7.6 Query Test\\n\\n#### 7.6.1 Executing Query Scripts\\n\\nExecute the above test SQL or execute the following command\\n\\n```\\n./run-tpch-queries.sh\\n```\\n\\n>Notice:\\n>\\n>1. At present, the query optimizer and statistics functions of Doris are not so perfect, so we rewrite some queries in TPC-H to adapt to the execution framework of Doris, but it does not affect the correctness of the results\\n>\\n>2. Doris\' new query optimizer will be released in future versions\\n>3. Set `set mem_exec_limit=8G` before executing the query\\n\\n#### 7.6.2 Single SQL Execution\\n\\nThe following is the SQL statement used in the test, you can also get the latest SQL from the code base.\\n\\n```SQL\\n--Q1\\nselect /*+SET_VAR(exec_mem_limit=8589934592, parallel_fragment_exec_instance_num=8, enable_vectorized_engine=true, batch_size=4096, disable_join_reorder=false, enable_cost_based_join_reorder=false, enable_projection=false) */\\n    l_returnflag,\\n    l_linestatus,\\n    sum(l_quantity) as sum_qty,\\n    sum(l_extendedprice) as sum_base_price,\\n    sum(l_extendedprice * (1 - l_discount)) as sum_disc_price,\\n    sum(l_extendedprice * (1 - l_discount) * (1 + l_tax)) as sum_charge,\\n    avg(l_quantity) as avg_qty,\\n    avg(l_extendedprice) as avg_price,\\n    avg(l_discount) as avg_disc,\\n    count(*) as count_order\\nfrom\\n    lineitem\\nwhere\\n    l_shipdate <= date \'1998-12-01\' - interval \'90\' day\\ngroup by\\n    l_returnflag,\\n    l_linestatus\\norder by\\n    l_returnflag,\\n    l_linestatus;\\n\\n--Q2\\nselect /*+SET_VAR(exec_mem_limit=8589934592, parallel_fragment_exec_instance_num=1, enable_vectorized_engine=true, batch_size=4096, disable_join_reorder=true, enable_cost_based_join_reorder=false, enable_projection=true) */\\n    s_acctbal,\\n    s_name,\\n    n_name,\\n    p_partkey,\\n    p_mfgr,\\n    s_address,\\n    s_phone,\\n    s_comment\\nfrom\\n    partsupp join\\n    (\\n        select\\n            ps_partkey as a_partkey,\\n            min(ps_supplycost) as a_min\\n        from\\n            partsupp,\\n            part,\\n            supplier,\\n            nation,\\n            region\\n        where\\n            p_partkey = ps_partkey\\n            and s_suppkey = ps_suppkey\\n            and s_nationkey = n_nationkey\\n            and n_regionkey = r_regionkey\\n            and r_name = \'EUROPE\'\\n            and p_size = 15\\n            and p_type like \'%BRASS\'\\n        group by a_partkey\\n    ) A on ps_partkey = a_partkey and ps_supplycost=a_min ,\\n    part,\\n    supplier,\\n    nation,\\n    region\\nwhere\\n    p_partkey = ps_partkey\\n    and s_suppkey = ps_suppkey\\n    and p_size = 15\\n    and p_type like \'%BRASS\'\\n    and s_nationkey = n_nationkey\\n    and n_regionkey = r_regionkey\\n    and r_name = \'EUROPE\'\\n\\norder by\\n    s_acctbal desc,\\n    n_name,\\n    s_name,\\n    p_partkey\\nlimit 100;\\n\\n--Q3\\nselect /*+SET_VAR(exec_mem_limit=8589934592, parallel_fragment_exec_instance_num=8, enable_vectorized_engine=true, batch_size=4096, disable_join_reorder=true, enable_cost_based_join_reorder=false, enable_projection=true, runtime_filter_wait_time_ms=10000) */\\n    l_orderkey,\\n    sum(l_extendedprice * (1 - l_discount)) as revenue,\\n    o_orderdate,\\n    o_shippriority\\nfrom\\n    (\\n        select l_orderkey, l_extendedprice, l_discount, o_orderdate, o_shippriority, o_custkey from\\n        lineitem join orders\\n        where l_orderkey = o_orderkey\\n        and o_orderdate < date \'1995-03-15\'\\n        and l_shipdate > date \'1995-03-15\'\\n    ) t1 join customer c \\n    on c.c_custkey = t1.o_custkey\\n    where c_mktsegment = \'BUILDING\'\\ngroup by\\n    l_orderkey,\\n    o_orderdate,\\n    o_shippriority\\norder by\\n    revenue desc,\\n    o_orderdate\\nlimit 10;\\n\\n--Q4\\nselect /*+SET_VAR(exec_mem_limit=8589934592, parallel_fragment_exec_instance_num=4, enable_vectorized_engine=true, batch_size=4096, disable_join_reorder=true, enable_cost_based_join_reorder=false, enable_projection=true) */\\n    o_orderpriority,\\n    count(*) as order_count\\nfrom\\n    (\\n        select\\n            *\\n        from\\n            lineitem\\n        where l_commitdate < l_receiptdate\\n    ) t1\\n    right semi join orders\\n    on t1.l_orderkey = o_orderkey\\nwhere\\n    o_orderdate >= date \'1993-07-01\'\\n    and o_orderdate < date \'1993-07-01\' + interval \'3\' month\\ngroup by\\n    o_orderpriority\\norder by\\n    o_orderpriority;\\n\\n--Q5\\nselect /*+SET_VAR(exec_mem_limit=8589934592, parallel_fragment_exec_instance_num=8, enable_vectorized_engine=true, batch_size=4096, disable_join_reorder=false, enable_cost_based_join_reorder=false, enable_projection=true) */\\n    n_name,\\n    sum(l_extendedprice * (1 - l_discount)) as revenue\\nfrom\\n    customer,\\n    orders,\\n    lineitem,\\n    supplier,\\n    nation,\\n    region\\nwhere\\n    c_custkey = o_custkey\\n    and l_orderkey = o_orderkey\\n    and l_suppkey = s_suppkey\\n    and c_nationkey = s_nationkey\\n    and s_nationkey = n_nationkey\\n    and n_regionkey = r_regionkey\\n    and r_name = \'ASIA\'\\n    and o_orderdate >= date \'1994-01-01\'\\n    and o_orderdate < date \'1994-01-01\' + interval \'1\' year\\ngroup by\\n    n_name\\norder by\\n    revenue desc;\\n\\n--Q6\\nselect /*+SET_VAR(exec_mem_limit=8589934592, parallel_fragment_exec_instance_num=1, enable_vectorized_engine=true, batch_size=4096, disable_join_reorder=false, enable_cost_based_join_reorder=false, enable_projection=true) */\\n    sum(l_extendedprice * l_discount) as revenue\\nfrom\\n    lineitem\\nwhere\\n    l_shipdate >= date \'1994-01-01\'\\n    and l_shipdate < date \'1994-01-01\' + interval \'1\' year\\n    and l_discount between .06 - 0.01 and .06 + 0.01\\n    and l_quantity < 24;\\n\\n--Q7\\nselect /*+SET_VAR(exec_mem_limit=458589934592, parallel_fragment_exec_instance_num=2, enable_vectorized_engine=true, batch_size=4096, disable_join_reorder=false, enable_cost_based_join_reorder=false, enable_projection=true) */\\n    supp_nation,\\n    cust_nation,\\n    l_year,\\n    sum(volume) as revenue\\nfrom\\n    (\\n        select\\n            n1.n_name as supp_nation,\\n            n2.n_name as cust_nation,\\n            extract(year from l_shipdate) as l_year,\\n            l_extendedprice * (1 - l_discount) as volume\\n        from\\n            supplier,\\n            lineitem,\\n            orders,\\n            customer,\\n            nation n1,\\n            nation n2\\n        where\\n            s_suppkey = l_suppkey\\n            and o_orderkey = l_orderkey\\n            and c_custkey = o_custkey\\n            and s_nationkey = n1.n_nationkey\\n            and c_nationkey = n2.n_nationkey\\n            and (\\n                (n1.n_name = \'FRANCE\' and n2.n_name = \'GERMANY\')\\n                or (n1.n_name = \'GERMANY\' and n2.n_name = \'FRANCE\')\\n            )\\n            and l_shipdate between date \'1995-01-01\' and date \'1996-12-31\'\\n    ) as shipping\\ngroup by\\n    supp_nation,\\n    cust_nation,\\n    l_year\\norder by\\n    supp_nation,\\n    cust_nation,\\n    l_year;\\n\\n--Q8\\n\\nselect /*+SET_VAR(exec_mem_limit=8589934592, parallel_fragment_exec_instance_num=8, enable_vectorized_engine=true, batch_size=4096, disable_join_reorder=true, enable_cost_based_join_reorder=false, enable_projection=true) */\\n    o_year,\\n    sum(case\\n        when nation = \'BRAZIL\' then volume\\n        else 0\\n    end) / sum(volume) as mkt_share\\nfrom\\n    (\\n        select\\n            extract(year from o_orderdate) as o_year,\\n            l_extendedprice * (1 - l_discount) as volume,\\n            n2.n_name as nation\\n        from\\n            lineitem,\\n            orders,\\n            customer,\\n            supplier,\\n            part,\\n            nation n1,\\n            nation n2,\\n            region\\n        where\\n            p_partkey = l_partkey\\n            and s_suppkey = l_suppkey\\n            and l_orderkey = o_orderkey\\n            and o_custkey = c_custkey\\n            and c_nationkey = n1.n_nationkey\\n            and n1.n_regionkey = r_regionkey\\n            and r_name = \'AMERICA\'\\n            and s_nationkey = n2.n_nationkey\\n            and o_orderdate between date \'1995-01-01\' and date \'1996-12-31\'\\n            and p_type = \'ECONOMY ANODIZED STEEL\'\\n    ) as all_nations\\ngroup by\\n    o_year\\norder by\\n    o_year;\\n\\n--Q9\\nselect/*+SET_VAR(exec_mem_limit=37179869184, parallel_fragment_exec_instance_num=2, enable_vectorized_engine=true, batch_size=4096, disable_join_reorder=false, enable_cost_based_join_reorder=false, enable_projection=true, enable_remove_no_conjuncts_runtime_filter_policy=true, runtime_filter_wait_time_ms=100000) */\\n    nation,\\n    o_year,\\n    sum(amount) as sum_profit\\nfrom\\n    (\\n        select\\n            n_name as nation,\\n            extract(year from o_orderdate) as o_year,\\n            l_extendedprice * (1 - l_discount) - ps_supplycost * l_quantity as amount\\n        from\\n            lineitem join orders on o_orderkey = l_orderkey\\n            join[shuffle] part on p_partkey = l_partkey\\n            join[shuffle] partsupp on ps_partkey = l_partkey\\n            join[shuffle] supplier on s_suppkey = l_suppkey\\n            join[broadcast] nation on s_nationkey = n_nationkey\\n        where\\n            ps_suppkey = l_suppkey and \\n            p_name like \'%green%\'\\n    ) as profit\\ngroup by\\n    nation,\\n    o_year\\norder by\\n    nation,\\n    o_year desc;\\n\\n--Q10\\n\\nselect /*+SET_VAR(exec_mem_limit=8589934592, parallel_fragment_exec_instance_num=4, enable_vectorized_engine=true, batch_size=4096, disable_join_reorder=true, enable_cost_based_join_reorder=false, enable_projection=true) */\\n    c_custkey,\\n    c_name,\\n    sum(t1.l_extendedprice * (1 - t1.l_discount)) as revenue,\\n    c_acctbal,\\n    n_name,\\n    c_address,\\n    c_phone,\\n    c_comment\\nfrom\\n    customer,\\n    (\\n        select o_custkey,l_extendedprice,l_discount from lineitem, orders\\n        where l_orderkey = o_orderkey\\n        and o_orderdate >= date \'1993-10-01\'\\n        and o_orderdate < date \'1993-10-01\' + interval \'3\' month\\n        and l_returnflag = \'R\'\\n    ) t1,\\n    nation\\nwhere\\n    c_custkey = t1.o_custkey\\n    and c_nationkey = n_nationkey\\ngroup by\\n    c_custkey,\\n    c_name,\\n    c_acctbal,\\n    c_phone,\\n    n_name,\\n    c_address,\\n    c_comment\\norder by\\n    revenue desc\\nlimit 20;\\n\\n--Q11\\nselect /*+SET_VAR(exec_mem_limit=8589934592, parallel_fragment_exec_instance_num=2, enable_vectorized_engine=true, batch_size=4096, disable_join_reorder=false, enable_cost_based_join_reorder=true, enable_projection=true) */\\n    ps_partkey,\\n    sum(ps_supplycost * ps_availqty) as value\\nfrom\\n    partsupp,\\n    (\\n    select s_suppkey\\n    from supplier, nation\\n    where s_nationkey = n_nationkey and n_name = \'GERMANY\'\\n    ) B\\nwhere\\n    ps_suppkey = B.s_suppkey\\ngroup by\\n    ps_partkey having\\n        sum(ps_supplycost * ps_availqty) > (\\n            select\\n                sum(ps_supplycost * ps_availqty) * 0.000002\\n            from\\n                partsupp,\\n                (select s_suppkey\\n                 from supplier, nation\\n                 where s_nationkey = n_nationkey and n_name = \'GERMANY\'\\n                ) A\\n            where\\n                ps_suppkey = A.s_suppkey\\n        )\\norder by\\n    value desc;\\n\\n--Q12\\n\\nselect /*+SET_VAR(exec_mem_limit=8589934592, parallel_fragment_exec_instance_num=2, enable_vectorized_engine=true, batch_size=4096, disable_join_reorder=false, enable_cost_based_join_reorder=true, enable_projection=true) */\\n    l_shipmode,\\n    sum(case\\n        when o_orderpriority = \'1-URGENT\'\\n            or o_orderpriority = \'2-HIGH\'\\n            then 1\\n        else 0\\n    end) as high_line_count,\\n    sum(case\\n        when o_orderpriority <> \'1-URGENT\'\\n            and o_orderpriority <> \'2-HIGH\'\\n            then 1\\n        else 0\\n    end) as low_line_count\\nfrom\\n    orders,\\n    lineitem\\nwhere\\n    o_orderkey = l_orderkey\\n    and l_shipmode in (\'MAIL\', \'SHIP\')\\n    and l_commitdate < l_receiptdate\\n    and l_shipdate < l_commitdate\\n    and l_receiptdate >= date \'1994-01-01\'\\n    and l_receiptdate < date \'1994-01-01\' + interval \'1\' year\\ngroup by\\n    l_shipmode\\norder by\\n    l_shipmode;\\n\\n--Q13\\nselect /*+SET_VAR(exec_mem_limit=45899345920, parallel_fragment_exec_instance_num=16, enable_vectorized_engine=true, batch_size=4096, disable_join_reorder=true, enable_cost_based_join_reorder=true, enable_projection=true) */\\n    c_count,\\n    count(*) as custdist\\nfrom\\n    (\\n        select\\n            c_custkey,\\n            count(o_orderkey) as c_count\\n        from\\n            orders right outer join customer on\\n                c_custkey = o_custkey\\n                and o_comment not like \'%special%requests%\'\\n        group by\\n            c_custkey\\n    ) as c_orders\\ngroup by\\n    c_count\\norder by\\n    custdist desc,\\n    c_count desc;\\n\\n--Q14\\n\\nselect /*+SET_VAR(exec_mem_limit=8589934592, parallel_fragment_exec_instance_num=8, enable_vectorized_engine=true, batch_size=4096, disable_join_reorder=true, enable_cost_based_join_reorder=true, enable_projection=true, runtime_filter_mode=OFF) */\\n    100.00 * sum(case\\n        when p_type like \'PROMO%\'\\n            then l_extendedprice * (1 - l_discount)\\n        else 0\\n    end) / sum(l_extendedprice * (1 - l_discount)) as promo_revenue\\nfrom\\n    part,\\n    lineitem\\nwhere\\n    l_partkey = p_partkey\\n    and l_shipdate >= date \'1995-09-01\'\\n    and l_shipdate < date \'1995-09-01\' + interval \'1\' month;\\n\\n--Q15\\nselect /*+SET_VAR(exec_mem_limit=8589934592, parallel_fragment_exec_instance_num=8, enable_vectorized_engine=true, batch_size=4096, disable_join_reorder=false, enable_cost_based_join_reorder=true, enable_projection=true) */\\n    s_suppkey,\\n    s_name,\\n    s_address,\\n    s_phone,\\n    total_revenue\\nfrom\\n    supplier,\\n    revenue0\\nwhere\\n    s_suppkey = supplier_no\\n    and total_revenue = (\\n        select\\n            max(total_revenue)\\n        from\\n            revenue0\\n    )\\norder by\\n    s_suppkey;\\n\\n--Q16\\nselect /*+SET_VAR(exec_mem_limit=8589934592, parallel_fragment_exec_instance_num=8, enable_vectorized_engine=true, batch_size=4096, disable_join_reorder=false, enable_cost_based_join_reorder=true, enable_projection=true) */\\n    p_brand,\\n    p_type,\\n    p_size,\\n    count(distinct ps_suppkey) as supplier_cnt\\nfrom\\n    partsupp,\\n    part\\nwhere\\n    p_partkey = ps_partkey\\n    and p_brand <> \'Brand#45\'\\n    and p_type not like \'MEDIUM POLISHED%\'\\n    and p_size in (49, 14, 23, 45, 19, 3, 36, 9)\\n    and ps_suppkey not in (\\n        select\\n            s_suppkey\\n        from\\n            supplier\\n        where\\n            s_comment like \'%Customer%Complaints%\'\\n    )\\ngroup by\\n    p_brand,\\n    p_type,\\n    p_size\\norder by\\n    supplier_cnt desc,\\n    p_brand,\\n    p_type,\\n    p_size;\\n\\n--Q17\\nselect /*+SET_VAR(exec_mem_limit=8589934592, parallel_fragment_exec_instance_num=1, enable_vectorized_engine=true, batch_size=4096, disable_join_reorder=false, enable_cost_based_join_reorder=true, enable_projection=true) */\\n    sum(l_extendedprice) / 7.0 as avg_yearly\\nfrom\\n    lineitem join [broadcast]\\n    part p1 on p1.p_partkey = l_partkey\\nwhere\\n    p1.p_brand = \'Brand#23\'\\n    and p1.p_container = \'MED BOX\'\\n    and l_quantity < (\\n        select\\n            0.2 * avg(l_quantity)\\n        from\\n            lineitem join [broadcast]\\n            part p2 on p2.p_partkey = l_partkey\\n        where\\n            l_partkey = p1.p_partkey\\n            and p2.p_brand = \'Brand#23\'\\n            and p2.p_container = \'MED BOX\'\\n    );\\n\\n--Q18\\n\\nselect /*+SET_VAR(exec_mem_limit=45899345920, parallel_fragment_exec_instance_num=4, enable_vectorized_engine=true, batch_size=4096, disable_join_reorder=true, enable_cost_based_join_reorder=true, enable_projection=true) */\\n    c_name,\\n    c_custkey,\\n    t3.o_orderkey,\\n    t3.o_orderdate,\\n    t3.o_totalprice,\\n    sum(t3.l_quantity)\\nfrom\\ncustomer join\\n(\\n  select * from\\n  lineitem join\\n  (\\n    select * from\\n    orders left semi join\\n    (\\n      select\\n          l_orderkey\\n      from\\n          lineitem\\n      group by\\n          l_orderkey having sum(l_quantity) > 300\\n    ) t1\\n    on o_orderkey = t1.l_orderkey\\n  ) t2\\n  on t2.o_orderkey = l_orderkey\\n) t3\\non c_custkey = t3.o_custkey\\ngroup by\\n    c_name,\\n    c_custkey,\\n    t3.o_orderkey,\\n    t3.o_orderdate,\\n    t3.o_totalprice\\norder by\\n    t3.o_totalprice desc,\\n    t3.o_orderdate\\nlimit 100;\\n\\n--Q19\\n\\nselect /*+SET_VAR(exec_mem_limit=8589934592, parallel_fragment_exec_instance_num=2, enable_vectorized_engine=true, batch_size=4096, disable_join_reorder=false, enable_cost_based_join_reorder=false, enable_projection=true) */\\n    sum(l_extendedprice* (1 - l_discount)) as revenue\\nfrom\\n    lineitem,\\n    part\\nwhere\\n    (\\n        p_partkey = l_partkey\\n        and p_brand = \'Brand#12\'\\n        and p_container in (\'SM CASE\', \'SM BOX\', \'SM PACK\', \'SM PKG\')\\n        and l_quantity >= 1 and l_quantity <= 1 + 10\\n        and p_size between 1 and 5\\n        and l_shipmode in (\'AIR\', \'AIR REG\')\\n        and l_shipinstruct = \'DELIVER IN PERSON\'\\n    )\\n    or\\n    (\\n        p_partkey = l_partkey\\n        and p_brand = \'Brand#23\'\\n        and p_container in (\'MED BAG\', \'MED BOX\', \'MED PKG\', \'MED PACK\')\\n        and l_quantity >= 10 and l_quantity <= 10 + 10\\n        and p_size between 1 and 10\\n        and l_shipmode in (\'AIR\', \'AIR REG\')\\n        and l_shipinstruct = \'DELIVER IN PERSON\'\\n    )\\n    or\\n    (\\n        p_partkey = l_partkey\\n        and p_brand = \'Brand#34\'\\n        and p_container in (\'LG CASE\', \'LG BOX\', \'LG PACK\', \'LG PKG\')\\n        and l_quantity >= 20 and l_quantity <= 20 + 10\\n        and p_size between 1 and 15\\n        and l_shipmode in (\'AIR\', \'AIR REG\')\\n        and l_shipinstruct = \'DELIVER IN PERSON\'\\n    );\\n\\n--Q20\\nselect /*+SET_VAR(exec_mem_limit=8589934592, parallel_fragment_exec_instance_num=2, enable_vectorized_engine=true, batch_size=4096, disable_join_reorder=true, enable_cost_based_join_reorder=true, enable_projection=true, runtime_bloom_filter_size=551943) */\\ns_name, s_address from\\nsupplier left semi join\\n(\\n    select * from\\n    (\\n        select l_partkey,l_suppkey, 0.5 * sum(l_quantity) as l_q\\n        from lineitem\\n        where l_shipdate >= date \'1994-01-01\'\\n            and l_shipdate < date \'1994-01-01\' + interval \'1\' year\\n        group by l_partkey,l_suppkey\\n    ) t2 join\\n    (\\n        select ps_partkey, ps_suppkey, ps_availqty\\n        from partsupp left semi join part\\n        on ps_partkey = p_partkey and p_name like \'forest%\'\\n    ) t1\\n    on t2.l_partkey = t1.ps_partkey and t2.l_suppkey = t1.ps_suppkey\\n    and t1.ps_availqty > t2.l_q\\n) t3\\non s_suppkey = t3.ps_suppkey\\njoin nation\\nwhere s_nationkey = n_nationkey\\n    and n_name = \'CANADA\'\\norder by s_name;\\n\\n--Q21\\nselect /*+SET_VAR(exec_mem_limit=8589934592, parallel_fragment_exec_instance_num=4, enable_vectorized_engine=true, batch_size=4096, disable_join_reorder=true, enable_cost_based_join_reorder=true, enable_projection=true) */\\ns_name, count(*) as numwait\\nfrom\\n  lineitem l2 right semi join\\n  (\\n    select * from\\n    lineitem l3 right anti join\\n    (\\n      select * from\\n      orders join lineitem l1 on l1.l_orderkey = o_orderkey and o_orderstatus = \'F\'\\n      join\\n      (\\n        select * from\\n        supplier join nation\\n        where s_nationkey = n_nationkey\\n          and n_name = \'SAUDI ARABIA\'\\n      ) t1\\n      where t1.s_suppkey = l1.l_suppkey and l1.l_receiptdate > l1.l_commitdate\\n    ) t2\\n    on l3.l_orderkey = t2.l_orderkey and l3.l_suppkey <> t2.l_suppkey  and l3.l_receiptdate > l3.l_commitdate\\n  ) t3\\n  on l2.l_orderkey = t3.l_orderkey and l2.l_suppkey <> t3.l_suppkey \\n\\ngroup by\\n    t3.s_name\\norder by\\n    numwait desc,\\n    t3.s_name\\nlimit 100;\\n\\n--Q22\\n\\nwith tmp as (select\\n                    avg(c_acctbal) as av\\n                from\\n                    customer\\n                where\\n                    c_acctbal > 0.00\\n                    and substring(c_phone, 1, 2) in\\n                        (\'13\', \'31\', \'23\', \'29\', \'30\', \'18\', \'17\'))\\n\\nselect /*+SET_VAR(exec_mem_limit=8589934592, parallel_fragment_exec_instance_num=4,runtime_bloom_filter_size=4194304) */\\n    cntrycode,\\n    count(*) as numcust,\\n    sum(c_acctbal) as totacctbal\\nfrom\\n    (\\n\\tselect\\n            substring(c_phone, 1, 2) as cntrycode,\\n            c_acctbal\\n        from\\n             orders right anti join customer c on  o_custkey = c.c_custkey join tmp on c.c_acctbal > tmp.av\\n        where\\n            substring(c_phone, 1, 2) in\\n                (\'13\', \'31\', \'23\', \'29\', \'30\', \'18\', \'17\')\\n    ) as custsale\\ngroup by\\n    cntrycode\\norder by\\n    cntrycode;\\n```"},{"id":"/release-note-1.1.4","metadata":{"permalink":"/blog/release-note-1.1.4","source":"@site/blog/release-note-1.1.4.md","title":"Apache Doris announced the official release of version 1.1.4","description":"Dear community, Apache Doris team has fixed about 60 issues or performance improvements in version 1.1.4 compared to previous verisons","date":"2022-11-11T00:00:00.000Z","tags":[{"inline":true,"label":"Release Notes","permalink":"/blog/tags/release-notes"}],"hasTruncateMarker":false,"authors":[{"name":"Apache Doris","key":null,"page":null}],"frontMatter":{"title":"Apache Doris announced the official release of version 1.1.4","description":"Dear community, Apache Doris team has fixed about 60 issues or performance improvements in version 1.1.4 compared to previous verisons","date":"2022-11-11","author":"Apache Doris","tags":["Release Notes"],"image":"/images/release-notes.png"},"unlisted":false,"prevItem":{"title":"Apache Doris 1.2 TPC-H performance test report","permalink":"/blog/tpch"},"nextItem":{"title":"Apache Doris announced the official release of version 1.1.3","permalink":"/blog/release-note-1.1.3"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\nIn this release, Doris Team has fixed about 60 issues or performance improvement since 1.1.3. This release is a bugfix release on 1.1 and all users are encouraged to upgrade to this release.\\n\\n\\n# Features\\n\\n- Support obs broker load for Huawei Cloud. [#13523](https://github.com/apache/doris/pull/13523)\\n\\n- SparkLoad support parquet and orc file.[#13438](https://github.com/apache/doris/pull/13438)\\n\\n# Improvements\\n\\n- Do not acquire mutex in metric hook since it will affect query performance during heavy load.[#10941](https://github.com/apache/doris/pull/10941)\\n\\n\\n# BugFix\\n\\n- The where condition does not take effect when spark load loads the file. [#13804](https://github.com/apache/doris/pull/13804)\\n\\n- If function return error result when there is nullable column in vectorized mode. [#13779](https://github.com/apache/doris/pull/13779)\\n\\n- Fix incorrect result when using anti join with other join predicates. [#13743](https://github.com/apache/doris/pull/13743)\\n\\n- BE crash when call function concat(ifnull). [#13693](https://github.com/apache/doris/pull/13693)\\n\\n- Fix planner bug when there is a function in group by clause. [#13613](https://github.com/apache/doris/pull/13613)\\n\\n- Table name and column name is not recognized correctly in lateral view clause. [#13600](https://github.com/apache/doris/pull/13600)\\n\\n- Unknown column when use MV and table alias. [#13605](https://github.com/apache/doris/pull/13605)\\n\\n- JSONReader release memory of both value and parse allocator. [#13513](https://github.com/apache/doris/pull/13513)\\n\\n- Fix allow create mv using to_bitmap() on negative value columns when enable_vectorized_alter_table is true. [#13448](https://github.com/apache/doris/pull/13448)\\n\\n- Microsecond in function from_date_format_str is lost. [#13446](https://github.com/apache/doris/pull/13446)\\n\\n- Sort exprs nullability property may not be right after subsitute using child\'s smap info. [#13328](https://github.com/apache/doris/pull/13328)\\n\\n- Fix core dump on case when have 1000 condition. [#13315](https://github.com/apache/doris/pull/13315)\\n\\n- Fix bug that last line of data lost for stream load. [#13066](https://github.com/apache/doris/pull/13066)\\n\\n- Restore table or partition with the same replication num as before the backup. [#11942](https://github.com/apache/doris/pull/11942)"},{"id":"/release-note-1.1.3","metadata":{"permalink":"/blog/release-note-1.1.3","source":"@site/blog/release-note-1.1.3.md","title":"Apache Doris announced the official release of version 1.1.3","description":"Dear community, Apache Doris team has fixed more than 80 issues or performance improvements in version 1.1.3 compared to previous verisons","date":"2022-10-17T00:00:00.000Z","tags":[{"inline":true,"label":"Release Notes","permalink":"/blog/tags/release-notes"}],"hasTruncateMarker":false,"authors":[{"name":"Apache Doris","key":null,"page":null}],"frontMatter":{"title":"Apache Doris announced the official release of version 1.1.3","description":"Dear community, Apache Doris team has fixed more than 80 issues or performance improvements in version 1.1.3 compared to previous verisons","date":"2022-10-17","author":"Apache Doris","tags":["Release Notes"],"image":"/images/release-notes.png"},"unlisted":false,"prevItem":{"title":"Apache Doris announced the official release of version 1.1.4","permalink":"/blog/release-note-1.1.4"},"nextItem":{"title":"Apache Doris announced the official release of version 1.1.1","permalink":"/blog/release-note-1.1.1"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\nIn this release, Doris Team has fixed more than 80 issues or performance improvement since 1.1.2. This release is a bugfix release on 1.1 and all users are encouraged to upgrade to this release.\\n\\n\\n## Features\\n\\n- Support escape identifiers for sqlserver and postgresql in ODBC table.\\n\\n- Could use Parquet as output file format.\\n\\n## Improvements\\n\\n- Optimize flush policy to avoid small segments. [#12706](https://github.com/apache/doris/pull/12706) [#12716](https://github.com/apache/doris/pull/12716)\\n\\n- Refactor runtime filter to reduce the prepare time. [#13127](https://github.com/apache/doris/pull/13127)\\n\\n- Lots of memory control related issues during query or load process. [#12682](https://github.com/apache/doris/pull/12682) [#12688](https://github.com/apache/doris/pull/12688) [#12708](https://github.com/apache/doris/pull/12708) [#12776](https://github.com/apache/doris/pull/12776) [#12782](https://github.com/apache/doris/pull/12782) [#12791](https://github.com/apache/doris/pull/12791) [#12794](https://github.com/apache/doris/pull/12794) [#12820](https://github.com/apache/doris/pull/12820) [#12932](https://github.com/apache/doris/pull/12932) [#12954](https://github.com/apache/doris/pull/12954) [#12951](https://github.com/apache/doris/pull/12951)\\n\\n## Bug Fixes\\n\\n- Core dump on compaction with largeint. [#10094](https://github.com/apache/doris/pull/10094)\\n\\n- Grouping sets cause be core or return wrong results. [#12313](https://github.com/apache/doris/pull/12313)\\n\\n- PREAGGREGATION flag in orthogonal_bitmap_union_count operator is wrong. [#12581](https://github.com/apache/doris/pull/12581)\\n\\n- Level1Iterator should release iterators in heap and it may cause memory leak. [#12592](https://github.com/apache/doris/pull/12592)\\n\\n- Fix decommission failure with 2 BEs and existing colocation table. [#12644](https://github.com/apache/doris/pull/12644)\\n\\n- BE may core dump because of stack-buffer-overflow when TBrokerOpenReaderResponse too large. [#12658](https://github.com/apache/doris/pull/12658)\\n\\n- BE may OOM during load when error code -238 occurs. [#12666](https://github.com/apache/doris/pull/12666)\\n\\n- Fix wrong child expression of lead function. [#12587](https://github.com/apache/doris/pull/12587)\\n\\n- Fix intersect query failed in row storage code. [#12712](https://github.com/apache/doris/pull/12712)\\n\\n- Fix wrong result produced by curdate()/current_date() function. [#12720](https://github.com/apache/doris/pull/12720)\\n\\n- Fix lateral view explode_split with temp table bug. [#13643](https://github.com/apache/doris/pull/13643)\\n\\n- Bucket shuffle join plan is wrong in two same table. [#12930](https://github.com/apache/doris/pull/12930)\\n\\n- Fix bug that tablet version may be wrong when doing alter and load. [#13070](https://github.com/apache/doris/pull/13070)\\n\\n- BE core when load data using broker with md5sum()/sm3sum(). [#13009](https://github.com/apache/doris/pull/13009)\\n\\n## Upgrade Notes\\n\\nPageCache and ChunkAllocator are disabled by default to reduce memory usage and can be re-enabled by modifying the configuration items `disable_storage_page_cache` and `chunk_reserved_bytes_limit`.\\n\\nStorage Page Cache and Chunk Allocator cache user data chunks and memory preallocation, respectively.\\n\\nThese two functions take up a certain percentage of memory and are not freed. This part of memory cannot be flexibly allocated, which may lead to insufficient memory for other tasks in some scenarios, affecting system stability and availability. Therefore, we disabled these two features by default in version 1.1.3.\\n\\nHowever, in some latency-sensitive reporting scenarios, turning off this feature may lead to increased query latency. If you are worried about the impact of this feature on your business after upgrade, you can add the following parameters to be.conf to keep the same behavior as the previous version.\\n\\n```\\ndisable_storage_page_cache=false\\nchunk_reserved_bytes_limit=10%\\n```\\n\\n* ``disable_storage_page_cache``: Whether to disable Storage Page Cache. version 1.1.2 (inclusive), the default is false, i.e., on. version 1.1.3 defaults to true, i.e., off.\\n* `chunk_reserved_bytes_limit`: Chunk allocator reserved memory size. 1.1.2 (and earlier), the default is 10% of the overall memory. 1.1.3 version default is 209715200 (200MB)."},{"id":"/release-note-1.1.1","metadata":{"permalink":"/blog/release-note-1.1.1","source":"@site/blog/release-note-1.1.1.md","title":"Apache Doris announced the official release of version 1.1.1","description":"Dear community, Apache Doris 1.1.1 is now available, with several enhancements and bug fixes based on 1.1.0\uFF0Cenabling smoother user experience.","date":"2022-09-13T00:00:00.000Z","tags":[{"inline":true,"label":"Release Notes","permalink":"/blog/tags/release-notes"}],"hasTruncateMarker":false,"authors":[{"name":"Apache Doris","key":null,"page":null}],"frontMatter":{"title":"Apache Doris announced the official release of version 1.1.1","description":"Dear community, Apache Doris 1.1.1 is now available, with several enhancements and bug fixes based on 1.1.0\uFF0Cenabling smoother user experience.","date":"2022-09-13","author":"Apache Doris","tags":["Release Notes"],"image":"/images/release-notes.png"},"unlisted":false,"prevItem":{"title":"Apache Doris announced the official release of version 1.1.3","permalink":"/blog/release-note-1.1.3"},"nextItem":{"title":"Apache Doris announced the official release of version 1.1.2","permalink":"/blog/release-note-1.1.2"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n\\n## Features\\n\\n### Support ODBC Sink in Vectorized Engine.\\n\\nThis feature is enabled in non-vectorized engine but it is missed in vectorized engine in 1.1. So that we add back this feature in 1.1.1.\\n\\n### Simple Memtracker for Vectorized Engine.\\n\\nThere is no memtracker in BE for vectorized engine in 1.1, so that the memory is out of control and cause OOM. In 1.1.1, a simple memtracker is added to BE and could control the memory and cancel the query when memory exceeded.\\n\\n## Improvements\\n\\n### Cache decompressed data in page cache.\\n\\nSome data is compressed using bitshuffle and it costs a lot of time to decompress it during query. In 1.1.1, doris will decompress the data that encoded by bitshuffle to accelerate query and we find it could reduce 30% latency for some query in ssb-flat.\\n\\n## Bug Fixes\\n\\n### Fix the problem that could not do rolling upgrade from 1.0.(Serious)\\n\\nThis issue was introduced in version 1.1 and may cause BE core when upgrade BE but not upgrade FE.\\n\\nIf you encounter this problem, you can try to fix it with [#10833](https://github.com/apache/doris/pull/10833).\\n\\n### Fix the problem that some query not fall back to non-vectorized engine, and BE will core.\\n\\nCurrently, vectorized engine could not deal with all sql queries and some queries (like left outer join) will use non-vectorized engine to run. But there are some cases not covered in 1.1. And it will cause be crash.\\n\\n### Compaction not work correctly and cause -235 Error.\\n\\nOne rowset multi segments in uniq key compaction, segments rows will be merged in generic_iterator but merged_rows not increased. Compaction will failed in check_correctness, and make a tablet with too much versions which lead to -235 load error.\\n\\n### Some segment fault cases during query.\\n\\n[#10961](https://github.com/apache/doris/pull/10961) \\n[#10954](https://github.com/apache/doris/pull/10954) \\n[#10962](https://github.com/apache/doris/pull/10962)\\n\\n## Thanks\\n\\nThanks to everyone who has contributed to this release:\\n\\n```\\n@jacktengg\\n@mrhhsg\\n@xinyiZzz\\n@yixiutt\\n@starocean999\\n@morrySnow\\n@morningman\\n@HappenLee\\n```"},{"id":"/release-note-1.1.2","metadata":{"permalink":"/blog/release-note-1.1.2","source":"@site/blog/release-note-1.1.2.md","title":"Apache Doris announced the official release of version 1.1.2","description":"Dear community, Apache Doris team has fixed more than 170 issues or performance improvements in version 1.1.2 compared to previous verisons","date":"2022-09-13T00:00:00.000Z","tags":[{"inline":true,"label":"Release Notes","permalink":"/blog/tags/release-notes"}],"hasTruncateMarker":false,"authors":[{"name":"Apache Doris","key":null,"page":null}],"frontMatter":{"title":"Apache Doris announced the official release of version 1.1.2","description":"Dear community, Apache Doris team has fixed more than 170 issues or performance improvements in version 1.1.2 compared to previous verisons","date":"2022-09-13","author":"Apache Doris","tags":["Release Notes"],"image":"/images/release-notes.png"},"unlisted":false,"prevItem":{"title":"Apache Doris announced the official release of version 1.1.1","permalink":"/blog/release-note-1.1.1"},"nextItem":{"title":"Doris stream load principle analysis","permalink":"/blog/principle-of-Doris-Stream-Load"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n\\n\\nIn this release, Doris Team has fixed more than 170 issues or performance improvement since 1.1.1. This release is a bugfix release on 1.1 and all users are encouraged to upgrade to this release.\\n\\n## Features\\n\\n### New MemTracker\\n\\nIntroduced new MemTracker for both vectorized engine and non-vectorized engine which is more accurate.\\n\\n### Add API for showing current queries and kill query\\n\\n### Support read/write emoji of UTF16 via ODBC Table\\n\\n\\n\\n## Improvements\\n\\n### Data Lake related improvements\\n\\n- Improved HDFS ORC File scan performance about 300%. [#11501](https://github.com/apache/doris/pull/11501)\\n\\n- Support HDFS HA mode when query Iceberg table.\\n\\n- Support query Hive data created by [Apache Tez](https://tez.apache.org/)\\n\\n- Add Ali OSS as Hive external support.\\n\\n### Add support for string and text type in Spark Load\\n\\n\\n### Add reuse block in non-vectorized engine and have 50% performance improvement in some cases. [#11392](https://github.com/apache/doris/pull/11392)\\n\\n### Improve like or regex performance\\n\\n### Disable tcmalloc\'s aggressive_memory_decommit \\n\\nIt will have 40% performance gains in load or query.\\n\\nCurrently it is a config, you can change it by set config `tc_enable_aggressive_memory_decommit`.\\n\\n\\n## Bug Fixes\\n\\n### Some issues about FE that will cause FE failure or data corrupt.\\n\\n- Add reserved disk config to avoid too many reserved BDB-JE files.**(Serious)**   In an HA environment, BDB JE will retains as many reserved files. The BDB-je log doesn\'t delete until approaching a disk limit.\\n\\n- Fix fatal bug in BDB-JE which will cause FE replica could not start correctly or data corrupted.** (Serious)**\\n\\n### Fe will hang on waitFor_rpc during query and BE will hang in high concurrent scenarios.\\n\\n[#12459](https://github.com/apache/doris/pull/12459) [#12458](https://github.com/apache/doris/pull/12458) [#12392](https://github.com/apache/doris/pull/12392)\\n\\n### A fatal issue in vectorized storage engine which will cause wrong result. **(Serious)**\\n\\n[#11754](https://github.com/apache/doris/pull/11754) [#11694](https://github.com/apache/doris/pull/11694)\\n\\n### Lots of planner related issues that will cause BE core or in abnormal state.\\n\\n[#12080](https://github.com/apache/doris/pull/12080) [#12075](https://github.com/apache/doris/pull/12075) [#12040](https://github.com/apache/doris/pull/12040) [#12003](https://github.com/apache/doris/pull/12003) [#12007](https://github.com/apache/doris/pull/12007) [#11971](https://github.com/apache/doris/pull/11971) [#11933](https://github.com/apache/doris/pull/11933) [#11861](https://github.com/apache/doris/pull/11861) [#11859](https://github.com/apache/doris/pull/11859) [#11855](https://github.com/apache/doris/pull/11855) [#11837](https://github.com/apache/doris/pull/11837) [#11834](https://github.com/apache/doris/pull/11834) [#11821](https://github.com/apache/doris/pull/11821) [#11782](https://github.com/apache/doris/pull/11782) [#11723](https://github.com/apache/doris/pull/11723) [#11569](https://github.com/apache/doris/pull/11569)"},{"id":"/principle-of-Doris-Stream-Load","metadata":{"permalink":"/blog/principle-of-Doris-Stream-Load","source":"@site/blog/principle-of-Doris-Stream-Load.md","title":"Doris stream load principle analysis","description":"Stream Load, one of the most commonly used data import methods for Doris users, is a synchronous import method. It allows users to import data into Doris in batch through HTTP access and returns the results of data import.","date":"2022-09-08T00:00:00.000Z","tags":[{"inline":true,"label":"Tech Sharing","permalink":"/blog/tags/tech-sharing"}],"hasTruncateMarker":false,"authors":[{"name":"Apache Doris","key":null,"page":null}],"frontMatter":{"title":"Doris stream load principle analysis","description":"Stream Load, one of the most commonly used data import methods for Doris users, is a synchronous import method. It allows users to import data into Doris in batch through HTTP access and returns the results of data import.","date":"2022-09-08","author":"Apache Doris","tags":["Tech Sharing"],"image":"/images/tech-sharing.png"},"unlisted":false,"prevItem":{"title":"Apache Doris announced the official release of version 1.1.2","permalink":"/blog/release-note-1.1.2"},"nextItem":{"title":"Doris analysis: Doris SQL principle analysis","permalink":"/blog/principle-of-Doris-SQL-parsing"}},"content":"\x3c!-- \\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n**Lead\uFF1A**\\n\\nStream Load, one of the most commonly used data import methods for Doris users, is a synchronous import method. It allows users to import data into Doris in batch through HTTP access and returns the results of data import. The user can not only directly judge whether the data import is successful through the return body of the HTTP request, but also query the results of historical tasks by executing query SQL on the client.\\n\\n#  **Introduction to Stream Load**\\n\\nThe Doris import (Load) function is to import the user\'s original data into the Doris table. And Doris realizes a unified streaming import framework at the bottom. On this basis, Doris provides a very rich import mode to adapt to different data sources and data import requirements. Stream Load is one of the most commonly used data import methods for Doris users. It is a synchronous import method that allows users to import data in CSV format or JSON format into Doris in batch through HTTP access and return the results of data import. User can directly judge whether the data import is successful through the return body of the HTTP request, and can query the results of historical tasks by executing query SQL on the client. In addition, Doris also provides the operation audit function for Stream Load, which can audit the historical Stream Load task information through the audit log. The implementation principle of Stream Load will be deeply analyzed from the aspects of execution process, transaction management, implementation of import plan, data writing and operation audit of Stream Load.\\n\\n# 1 Implementation Process\\n\\nThe user submits the HTTP request of Stream Load to the FE, and the FE will forward the data import request to a BE node through HTTP Redirect, which will be the Coordinator of this Stream Load task. In this process, the FE node receiving the request only provides forwarding service. The BE node as the Coordinator is actually responsible for the entire import job, such as sending transaction requests to the Master FE, obtaining import execution plans from the FE, receiving real-time data, distributing data to other Executor BE nodes, and returning results to the user after data import. The user can also submit the HTTP request of Stream Load directly to a specified BE node, and the node will act as the Coordinator of this Stream Load task. During the Stream Load process, the Executor BE node is responsible for writing data to the storage layer.\\n\\nIn the Coordinator BE, all HTTP requests, including Stream Load requests, are processed through a thread pool. A Stream Load task is uniquely identified by the imported Label. The principle block diagram of Stream Load is shown in Figure 1.\\n\\n![](/images/blogs/principle-of-Doris-Stream-Load/Figure_1_en.png)\\n           \\n\\nThe complete execution process of Stream Load is shown in Figure 2:\\n\\n(1)The user submits the HTTP request of Stream Load to the FE (the user can also directly submit the HTTP request of Stream Load to the Coordinator BE).\\n\\n(2)FE, after receiving the Stream Load request submitted by the user, will perform HTTP Header parsing (including the library, table, Label and other information imported by parsing data), and then perform user authentication. If the HTTP Header is successfully resolved and the user authentication passes, the FE will forward the HTTP request of Stream Load to a BE node, which will be the Coordinator of this Stream Load. Otherwise, the FE will directly return the failure information of Stream Load to the user.\\n\\n(3)After receiving the HTTP request from Stream Load, the Coordinator BE will first perform HTTP Header parsing and data verification, including the file format of the parsed data, the size of the data body, the HTTP timeout, and user authentication. If the Header data verification fails, the Stream Load failure information will be directly returned to the user.\\n\\n(4)After the HTTP Header data verification is passed, the Coordinator BE will send a Begin Transaction request to the FE through Thrift RPC.\\n\\n(5)After the FE receives the Begin Transaction request sent by the Coordinator BE, it will start a transaction and return the Transaction ID to the Coordinator BE.\\n\\n(6)After the Coordinator BE receives the Begin Transaction success information, it will send a request to get the import plan to the FE through Thrift RPC.\\n\\n(7)After receiving the request for obtaining the import plan sent by the Coordinator BE, the FE will generate the import plan for the Stream Load task and return it to the Coordinator BE.\\n\\n(8)After receiving the import plan, the Coordinator BE starts to execute the import plan, including receiving the real-time data from HTTP and distributing the real-time data to other Executor BE through BRPC.\\n\\n(9)After receiving the real-time data distributed by the Coordinator BE, the Executor BE is responsible for writing the data to the storage layer.\\n\\n(10)After the Executor BE completes data writing, the Coordinator BE sends a Commit Transaction request to the FE through Thrift RPC.\\n\\n(11)After receiving the Commit Transaction request sent by the Coordinator BE, the FE will commit transaction, send the Publish Version task to the Executor BE, and wait for the Executor BE to execute the Publish Version.\\n\\n(12)The Executor BE asynchronously executes the Publish Version to change the Rowset generated by data import into a visible data version.\\n\\n(13)After the Publish Version completes normally or the execution timeout, the FE returns the results of the Commit Transaction and the Publish Version to the Coordinator BE.\\n\\n(14)The Coordinator BE returns the final result of Stream Load to the user.\\n\\n![](/images/blogs/principle-of-Doris-Stream-Load/Figure_2_en.png)\\n# 2 Transaction Management\\n\\nDoris ensures the atomicity of data import through Transaction. One Stream Load task corresponds to one transaction. The FE is responsible for the transaction management of Stream Load. The FE receives the Thrift RPC transaction request sent by the Coordinator BE node through the FrontendService. Transaction request types include Begin Transaction, Commit Transaction and Rollback Transaction. The transaction states of Doris include PREPARE, COMMITTED, VISIBLE, and ABORTED. The status flow process of the Stream Load transaction is shown in Figure 3.\\n\\n![](/images/blogs/principle-of-Doris-Stream-Load/Figure_3_en.png)\\n\\nThe Coordinator BE node will send a Begin Transaction request to the FE before data import. The FE will check whether the label requested by the Begin Transaction already exists. If the label does not exist in the system, it will open a new transaction for the current label, assign a Transaction ID to the transaction, and set the transaction status to PREPARE, then returns the Transaction ID and the success information of the Begin Transaction to the Coordinator BE. Otherwise, this transaction may be a repeated data import. The FE returns the Begin Transaction failure message to the Coordinator BE, and the Stream Load task exits.\\n\\nAfter the data is written in all Executor BE nodes, the Coordinator BE node will send a Commit Transaction request to the FE. After receiving the Commit Transaction request, the FE will execute the Commit Transaction and Publish Version operations. First, the FE will judge whether the number of replicas of data successfully written by each Tablet exceeds half of the total number of replicas of the tablet. If the number of replicas of data successfully written by each Tablet exceeds half of the total number of replicas of the Tablet (most of them are successful), the Commit Transaction is successful and the transaction status is set to COMMITTED; Otherwise, the Commit Transaction failure information is returned to the Coordinator BE. The COMMITTED status indicates that the data has been written successfully, but the data is not visible. You need to continue to execute the Publish Version task. After that, the transaction cannot be rolled back.\\n\\nThe FE will have a separate thread to execute the Publish Version on the Transaction with successful Commit. When the Publish Version is executed, the FE will send the Publish Version request to all Executor BE nodes related to the Transaction through Thrift RPC. The Publish Version task is executed asynchronously on each Executor BE node, and the Rowset generated by data import is changed into a visible data version. When all the Publish Version tasks on the Executor BE are successfully executed, the FE will set the transaction status to VISIBLE, and return the Commit Transaction and Publish Version success information to the Coordinator BE. When some Publish Version tasks fail, the FE will repeatedly issue a Publish Version request to the Executor BE node until the previously failed Publish Version task succeeds. If the transaction status has not been set to VISIBLE after a certain timeout period, the FE will return to the Coordinator BE the information that the Commit Transaction was successful but the Publish Version timed out (note that at this time, the data is still written successfully, but it is still invisible, and the user needs to wait for the transaction status to finally become VISIBLE).\\n\\nWhen obtaining the import plan from the FE fails, executing data import fails, or Commit Transaction fails, the Coordinator BE node will send a Rollback Transaction request to the FE to execute transaction rollback. After receiving the transaction rollback request, the FE will set the transaction status to ABORTED, and send a Clear Transaction request to the Executor BE through Thrift RPC. The Clear Transaction task is asynchronously executed at the BE node, marking the Rowset generated by data import as unavailable. These Rowset will be deleted from the BE later. Transactions with COMMITTED status (transactions with Commit Transaction succeeded but Publish Version timed out) cannot be rolled back.\\n\\n# 3 Execution of the import plan\\nIn Doris BE, all execution plans are managed by FragmentMgr, and the execution of each import plan is managed by PlanFragmentExecutor. After the BE obtains the import execution plan from the FE, it will submit the import plan to the thread pool of FragmentMgr for execution. The import execution plan of Stream Load has only one Fragment, including one BrokerScanNode and one OlapTableSink. BrokerScanNode is responsible for reading streaming data in real time and converting the data lines in CSV format or JSON format to the Tuple format of Doris. OlapTableSink is responsible for sending real-time data to the corresponding Executor BE node. The Executor BE node corresponding to each data row is determined by which BE the Tablet where the data row is stored. The Partition and Tablet where the data row is stored can be determined according to the PartitionKey and DistributionKey of the data row. The BE node on which each Tablet and its replica are stored has been determined when the Table or Partition is created.\\n\\nAfter importing the execution plan and submitting it to the thread pool of FragmentMgr, the Stream Load thread will receive the real-time data transmitted through HTTP in chunks and write it to the StreamLoadPipe. The BrokerScanNode will read the real-time data in batches from the StreamLoadPipe. OlapTableSink will send the batch data read by the BrokerScanNode to the Executor BE through BRPC for data writing. After all real-time data is written to the StreamLoadPipe, the Stream Load thread will wait for the import plan to finish.\\n\\nThe PlanFragmentExecutor executes a specific import plan process, which consists of three stages: Prepare, Open, and Close. In the Prepare stage, the import execution plan from the FE is mainly analyzed; In the Open stage, BrokerScanNode and OlapTableSink will be opened. BrokerScanNode is responsible for reading the real-time data of one Batch at a time, and OlapTableSink is responsible for calling BRPC to send the data of each Batch to other Executor BE nodes; In the Close stage, it is responsible for waiting for the data import to end and closing the BrokerScanNode and OlapTableSink. The import execution plan of Stream Load is shown in Figure 4.\\n\\n![](/images/blogs/principle-of-Doris-Stream-Load/Figure_4_en.png)\\n\\nOlapTableSink is responsible for the data distribution of the Stream Load task. Tables in Doris may have Rollup or Materialized view. Each Table and its Rollup and Materialized view are called an Index. In the process of data distribution, the IndexChannel will maintain a data distribution channel of the Index. The Tablet under the Index may have multiple replicas and are distributed on different BE nodes. The NodeChannel will maintain the data distribution channel of an Executor BE node under the IndexChannel. Therefore, the OlapTableSink contains multiple IndexChannel, and each NodeChannel contains multiple NodeChannel, as shown in Figure 5.\\n\\n![](/images/blogs/principle-of-Doris-Stream-Load/Figure_5_en.png)\\n\\nWhen OlapTableSink distributes data, it will read the data Batch obtained by BrokerScanNode row by row, and add the data row to the IndexChannel of each Index. The Partition and Tablet of the data row can be determined according to the PartitionKey and DistributionKey, and then the corresponding Tablet of the data row in other Index can be calculated according to the order of the Tablet in the Partition. Each Tablet may have multiple replicas distributed on different BE nodes. Therefore, in the IndexChannel, each data row will be added to the NodeChannel corresponding to each replica of its Tablet. Each NodeChannel has a send queue. When the new data rows in NodeChannel accumulate to a certain size, they will be added to the send queue as a data Batch. There will be a fixed thread in OlapTableSink to train each NodeChannel under each IndexChannel in turn, and call BRPC to send a data Batch in the sending queue to the corresponding Executor BE. The data distribution process of the Stream Load task is shown in Figure 6.\\n\\n![](/images/blogs/principle-of-Doris-Stream-Load/Figure_6_en.png)\\n\\n# 4 **Data Write**\\n\\nAfter receiving the data Batch sent by the Coordinator BE, the BRPC server of the Executor BE will submit the data writing task to the thread pool for asynchronous execution. In Doris BE, data is written to the storage layer in a hierarchical manner. Each Stream Load task corresponds to a LoadChannel on each Executor BE. The LoadChannel maintains the data writing channel of a Stream Load task and is responsible for the data writing of a Stream Load task on the current Executor BE node, LoadChannel can write the data of a Stream Load task in the current BE node to the storage layer in batches until the Stream Load task is completed. Each LoadChannel is uniquely identified by the load ID, and all LoadChannel on the BE node are managed by LoadChannelMgr. The Table corresponding to a Stream Load task may have multiple Index. Each Index corresponds to a TabletsChannel, which is uniquely identified by the Index ID. Therefore, there will be multiple TabletsChannel under each LoadChannel. The TabletsChannel maintains an Index data writing channel, which is responsible for managing the data writing of all the Tablet under the Index. The TabletsChannel will read the data Batch row by row and write it to the corresponding Tablet through the DeltaWriter. The DeltaWriter maintains a data writing channel of a Tablet, which is uniquely identified by the Tablet ID. it is responsible for receiving the data import of a single Tablet and writing the data into the MemTable corresponding to the tablet. When the MemTable is full, the data in the MemTable will be flushed to the disk and Segment files will be generated. MemTable adopts the data structure of SkipList to temporarily store the data in memory. SkipList will sort the data rows according to the Key of Schema. In addition, if the data model is Aggregate or Unique, MemTable will aggregate data rows with the same Key. The data write channel of the Stream Load task is shown in Figure 7.\\n\\n![](/images/blogs/principle-of-Doris-Stream-Load/Figure_7_en.png)\\n\\nThe Flush operation of MemTable is performed asynchronously by MemtableFlushExecutor. After the MemTable Flush task is submitted to the thread pool, a new MemTable will be generated to receive the subsequent data writing of the current Tablet. When the MemtableFlushExecutor performs data Flush, the RowsetWriter will read out all the data in the MemTable and write out multiple Segment files through the SegmentWriter. The size of each Segment file is no more than 256MB. For a Tablet, each Stream Load task will generate a newRowset. The generated Rowset can contain multiple Segment files. The data writing process of the Stream Load task is shown in Figure 8.\\n\\n![](/images/blogs/principle-of-Doris-Stream-Load/Figure_8_en.png)\\n\\nThe TxnManager on the Executor BE node is responsible for transaction management of Tablet level data import. When the Delta Writer is initialized, the PrepareTransaction will be executed to add the data write transaction of the corresponding Tablet in the current Stream Load task to the TxnManager for management. When the data write Tablet is completed and the DeltaWriter is closed, the Commit Transaction will be executed to add the new Rowset generated by the data import to the TxnManager for management. Note that the TxnManager here is only responsible for the transactions on a single BE, while the transaction management in the FE is responsible for the overall import of transactions.\\n\\nAfter the data import is completed, when the Executor BE executes the Publish Version task issued by the FE, it will execute the Publish Transaction to change the new Rowset generated by the data import into a visible version, and delete the data writing task of the corresponding Tablet in the current Stream Load task from the TxnManager. This means that the data writing transaction of the Tablet in the current Stream Load task ends.\\n\\n# 5 **Stream Load Operation Audit**\\n\\nDoris adds the operation audit function to Stream Load. After each Stream Load task is completed and the results are returned to the user, the Coordinator BE will persistently store the detailed information of this Stream Load task on the local RocksDB. The Master FE periodically pulls the information of the completed Stream Load task from each BE node of the cluster through Thrift RPC, pulls a batch of Stream Load operation records from one BE node at a time, and writes the pulled Stream Load task information into the audit log (fe.audit.log). Each Stream Load task information stored on the BE will be set with an expiration time (TTL), and the expired Stream Load task information will be deleted when RocksDB executes the Compaction. The user can audit the historical Stream Load task information through the FE audit log.\\n\\nWhen the FE writes the pulled Stream Load task information into the Audit log, it will keep a copy in the memory. In order to prevent memory expansion, a fixed number of Stream Load task information will be kept in the memory. As the subsequent data pulling continues, the early Stream Load task information will be gradually eliminated from the FE memory. The user can query the latest Stream Load task information by executing the SHOW STREAM LOAD command at the client.\\n\\n# **Summary**\\n\\nIn this paper, the implementation principle of Stream Load is deeply analyzed from the aspects of execution process, transaction management, implementation of import plan, data writing and operation audit of Stream Load. Stream Load is one of the most commonly used data import methods for Doris users. It is a synchronous import method that allows users to import data into Doris in batch through HTTP access and return the results of data import. The user can not only directly judge whether the data import is successful through the return body of the HTTP request, but also query the results of historical tasks by executing query SQL on the client. Otherwise, Doris also provides the result audit function for Stream Load, which can audit the historical Stream Load task information through the audit log."},{"id":"/principle-of-Doris-SQL-parsing","metadata":{"permalink":"/blog/principle-of-Doris-SQL-parsing","source":"@site/blog/principle-of-Doris-SQL-parsing.md","title":"Doris analysis: Doris SQL principle analysis","description":"This article mainly introduces the principle of Doris SQL parsing.Since there are many types of SQL, this article focuses on the analysis of query SQL. Doris\'s SQL analysis will be explained deeply in the algorithm principle and code implementation.","date":"2022-08-25T00:00:00.000Z","tags":[{"inline":true,"label":"Tech Sharing","permalink":"/blog/tags/tech-sharing"}],"hasTruncateMarker":false,"authors":[{"name":"Apache Doris","key":null,"page":null}],"frontMatter":{"title":"Doris analysis: Doris SQL principle analysis","description":"This article mainly introduces the principle of Doris SQL parsing.Since there are many types of SQL, this article focuses on the analysis of query SQL. Doris\'s SQL analysis will be explained deeply in the algorithm principle and code implementation.","date":"2022-08-25","author":"Apache Doris","tags":["Tech Sharing"],"image":"/images/tech-sharing.png"},"unlisted":false,"prevItem":{"title":"Doris stream load principle analysis","permalink":"/blog/principle-of-Doris-Stream-Load"},"nextItem":{"title":"How Flink\'s real-time writes to Apache Doris ensure both high throughput and low latency","permalink":"/blog/Flink-realtime-write"}},"content":"\x3c!-- \\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n**Lead\uFF1A**\\nThis article mainly introduces the principle of Doris SQL parsing.\\n\\nIt focuses on generating a single-machine logical plan, developing a distributed logical plan, and generating a distributed physical plan. Analyze, SinglePlan, DistributedPlan, and Schedule four parts correspond to the code implementation.\\n\\nFirst, AST will be processed preliminary by Analyze and then optimized by SinglePlan to generate a single-machine query plan. Third, DistributedPlan will split the single-machine query plan into distributed query plans. In the end, the query plan will be sent to machines and executed orderly, which decide by Schedule.\\n\\nSince there are many types of SQL, this article focuses on the analysis of query SQL. Doris\'s SQL analysis will be explained deeply in the algorithm principle and code implementation.\\n\\n# 1. Introduction to Doris\\nDoris is an interactive SQL database based on MPP architecture, mainly used to solve near real-time reports and multi-dimensional analysis. The Doris architecture is straightforward, with only two types of processes.\\n\\n- Frontend\uFF08FE\uFF09: It is mainly responsible for user request access, query parsing and planning, storage and management of metadata, and node management-related work.\\n\\n- Backend\uFF08BE\uFF09: It is mainly responsible for data storage and query plan execution.\\n\\nIn Doris\' storage engine, data will be horizontally divided into several data shards (Tablet, also called data bucket). Each tablet contains several rows of data. Multiple Tablets belong to different partitions logically. A Tablet only belongs to one Partition. And a Partition contains several Tablets. Tablet is the smallest physical storage unit for operations such as data movement, copying, etc.\\n\\n# 2. SQL parsing In Apache Doris\\nSQL parsing in this article refers to **the process of generating a complete physical execution plan after a series of parsing of an SQL statement**.\\n\\nThis process includes the following four steps: lexical analysis, syntax analysis, generating a logical plan, and generating a physical plan.\\n\\n![](/images/blogs/principle-of-Doris-SQL-parsing/Figure_1_en.png)\\n\\n## 2.1 Lexical analysis\\nThe lexical analysis will identify the SQL in the form of a string into tokens, in preparation for the grammatical analysis.\\n```undefined\\nselect ......  from ...... where ....... group by ..... order by ......\\n\\nSQL Tokens could be divided into the following categories:\\n\uFFEE Keywords (select, from, where)\\n\uFFEE operator (+, -, >=)\\n\uFFEE Open/close flag ((, CASE)\\n\uFFEE placeholder (?)\\n\uFFEE Comments\\n\uFFEE space\\n......\\n```\\n## 2.2 Syntax analysis\\nThe syntax analysis will convert the token generated by the lexical analysis into an abstract syntax tree based on the syntax rules, as shown in Figure 2.\\n\\n![](/images/blogs/principle-of-Doris-SQL-parsing/Figure_2_en.png)\\n\\n## 2.3 Logical plan\\nThe logical plan converts the abstract syntax tree into an algebraic relation, which is an operator tree, and each node represents a calculation method for data. The entire tree represents the calculation method and flows direction of data, as shown in Figure 3.\\n\\n ![](/images/blogs/principle-of-Doris-SQL-parsing/Figure_3_en.png)\\n\\n## 2.4 Physical plan\\nThe physical plan is the plan that determines which computing operations are performed on which machines. It will be generated based on the logical plan, the distribution of machines, and the distribution of data.\\n\\nThe SQL parsing of the Doris system also adopts these steps, but it is refined and optimized according to the characteristics of the Doris system structure and the storage method of data to maximize the computing power of the machine.\\n\\n# 3. Design goals\\nThe design goals of the Doris SQL parsing architecture are:\\n\\n1. Maximize Computational Parallelism\\n\\n2. Minimize network transfer of data\\n\\n3. Minimize the amount of data that needs to be scanned\\n\\n# 4. Architecture\\nDoris SQL parsing includes five steps: lexical analysis, syntax analysis, generation of a stand-alone logical plan, generation of a distributed logical plan, and generation of a physical execution plan.\\n\\nIn terms of code implementation, it corresponds to the following five steps: Parse, Analyze, SinglePlan, DistributedPlan, and Schedule, which as shown in Figure 4.\\n\\n![](/images/blogs/principle-of-Doris-SQL-parsing/Figure_4_en.png)\\n\\nThe Parse phase will not be discussed in this article. Analyze will do some pre-processing of the AST. A stand-alone query plan will be optimized by SinglePlan based on the AST. DistributedPlan will split the stand-alone query plan into distributed query plans. Schedule phase will determine which machines the query plan will be sent to for execution.\\n\\n**Since there are many types of SQL, this article focuses on the analysis of query SQL.**\\n\\nFigure 5 shows a simple query SQL parsing implementation in Doris.\\n\\n![](/images/blogs/principle-of-Doris-SQL-parsing/Figure_5_en.png)\\n\\n# 5. Parse Phase\\nIn the Parse stage, JFlex technology is used for lexical analysis, java cup parser technology is used for syntax analysis, and an AST\uFF08Abstract Syntax Tree\uFF09will finally generate. These are existing and mature technologies and will not be introduced in detail here.\\n\\nAST has a tree-like structure, which represents a piece of SQL. Therefore, different types of queries -- select, insert, show, set, alter table, create table, etc. will generate additional data structures after Parse (SelectStmt, InsertStmt, ShowStmt, SetStmt, AlterStmt, AlterTableStmt, CreateTableStmt, etc.). However, they all inherit from Statements and will perform some specific processing according to their own grammar rules. For example: for select type SQL, the SelectStmt structure will be generated after Parse.\\n\\nSelectStmt structure contains SelectList, FromClause, WhereClause, GroupByClause, SortInfo and other structures. These structures contain more basic data structures. For Example, WhereClause contains BetweenPredicate, BinaryPredicate, CompoundPredicate, InPredicate, and so on.\\n\\nAll structures in AST are composed of basic structure expressions--Expr by using various combinations, as shown in Figure 6.\\n\\n![](/images/blogs/principle-of-Doris-SQL-parsing/Figure_6_en.png)\\n# 6. Analyze Phase\\nAnalyze will perform pre-processing and semantic analysis on the abstract syntax tree AST generated in the Parse phase, preparing for the generation of stand-alone logic plans.\\n\\nThe abstract class StatementBase represents the abstract syntax tree. This abstract class contains a most crucial member function--analyze(), which is used to perform what\'s needed to do in Analyze phase.\\n\\nDifferent types of queries (select, insert, show, set, alter table, create table, etc.) will generate different data structures through the Parse stage(SelectStmt, InsertStmt, ShowStmt, SetStmt, AlterStmt, AlterTableStmt, CreateTableStmt, etc.), these data structures inherit From StatementBase, and perform a specific Analysis on a specific type sof SQL by implementing the analyze() function.\\n\\nFor example, a query of select type will be converted into analyze() of the sub-statements SelectList, FromClause, GroupByClause, HavingClause, WhereClause, SortInfo, etc. of select SQL. Then these sub-statements further analyze() their sub-structures, and various scenarios of various types of SQL are analyzed by layer-by-layer iteration. For example, WhereClause will further explore the BetweenPredicate, BinaryPredicate, CompoundPredicate, InPredicate, etc., which it contains.\\n\\n**For query type SQL, Analyze will performs several important steps:**\\n\\n- **Metadata identification and parsing**\uFF1A Identify and parse metadata such as Cluster, Database, Table, Column, etc. involved in SQL, and determine which columns, tables, databases, and clusters need to be calculated.\\n\\n- **SQL correctness check**\uFF1Asuch as the window function cannot DISTINCT, whether the projection column is ambiguous, the where statement cannot contain grouping operations, etc.\\n\\n- **Rewrite SQL simply**\uFF1Afor example, expand select * to select all columns, convert count distinct to bitmap or hll function, etc.\\n\\n- **Function correctness check**\uFF1ACheck whether the functions contained in SQL are consistent with the system-defined procedures, including parameter types, number of parameters, etc.\\n\\n- **Aliasing for Table and Column.**\\n\\n- **Type checking and conversion**\uFF1A For example, when the types on both sides of a binary expression are inconsistent, one of the types needs to be converted (with BIGINT and DECIMAL, the BIGINT type needs to be cast to DECIMAL).\\n\\nAfter analyzing the AST, a rewrite operation will be performed again to simplify or convert it into a unified processing method. A present rewrite algorithm is a rule-based approach. It will rewrite the AST with each rule from bottom to top, based on the tree structure of the AST. If the AST changes after rewriting, analysis and rewrite will start again until there is no change in the AST.\\n\\nFor example: simplification of constant expressions: 1 + 1 + 1 is rewritten as 3, 1 > 2 is rewritten as Flase, etc. Convert some statements into a unified processing method, such as rewriting where in, where exists as semi join, where not in, where not exists as anti join.\\n\\n# 7. Generate stand-alone logical Plan phase\\nAt this stage, algebraic relations will be generated according to the AST abstract syntax tree, also known as the operator number. Each node on the tree is an operator, representing an operation.\\n\\nAs shown in Figure 7, ScanNode represents scan and read operations on a table. HashJoinNode represents the join operation. A hash table of a small table will be constructed in memory, and the large table will be traversed to find the exact value of the join key. Project means the projection operation, which represents the column that needs to be output at the end. Figure 7 shows that only citycode column will output.\\n\\n![](/images/blogs/principle-of-Doris-SQL-parsing/Figure_7_en.png)\\n\\nWithout optimization, the generated relational algebra is very expensive to send to storage and execute.\\n\\nFor query:\\n```sql\\nselect a.siteid, a.pv from table1 a join table2 b on a.siteid = b.siteid where a.citycode=122216 and b.username=\\"test\\" order by a.pv limit 10\\n```\\nAs shown in Figure 8, for unoptimized relational algebra, all columns need to be read out for a series of calculations. In the end, siteid and pv column are selected and output. A large amount of useless column data wastes computing resources.\\n\\nWhen Doris generates algebraic relations, a lot of optimizations are made: the projection columns and query conditions will be put into the scan operation as much as possible.\\n\\n![](/images/blogs/principle-of-Doris-SQL-parsing/Figure_8_en.png)\\n\\n**Specifically, this phase mainly does the following tasks:**\\n\\n- **Slot materialization**\uFF1ADetermine the column that needs to be scanned and calculated for the expression. Such as aggregate function expressions and Group By words of aggregate nodes need to be materialized.\\n\\n- **Projection pushdown**\uFF1ABE only scans the columns that must be read when Scanning.\\n\\n- **Predicate pushdown**\uFF1APush down the filter conditions to the Scan node as much as possible under the premise of semantically correct.\\n\\n- **Partition, bucket cutting**\uFF1AAccording to the information in the filter conditions, determine which partitions and buckets of tablets need to be scanned.\\n\\n- **Join Reorder**\uFF1AFor Inner Join, Doris will adjust the order of the table according to the number of rows--put the large table in the front.\\n\\n- **Sort + Limit optimized to TopN**\uFF1AFor the order by the limit statement, it will be converted into TopN operation nodes, which is convenient for unified processing.\\n\\n- **MaterializedView selection**: The best-materialized view will be selected according to the columns required by the query, the columns for filtering, sorting and Join, the number of rows, the number of columns, and other factors.\\n\\nFigure 9 shows an example of optimization. The optimization of Doris is carried out in generating relational algebra. Generating one will optimize one.\xb7 Projection pushdown: BE only scans the columns that must be read when Scanning.\\n\\n![](/images/blogs/principle-of-Doris-SQL-parsing/Figure_9_en.png)\\n\\n# 8 Generate Distributed Plan Phase\\n\\nAfter the single-machine PlanNode tree is generated, it needs to be split into a distributed PlanFragment tree (PlanFragment is used to represent an independent execution unit) according to the distributed environment. A table\'s data is distributed across multiple hosts could allow some computations to be parallelized.\\n\\nThe primary purpose of this step is to maximize parallelism and data localization. The primary strategy is to split the nodes that can be executed in parallel and create a separate PlanFragment. ExchangeNodes will replace the split nodes to receive data. Finally, a DataSinkNode will be added to the split node to transmit the calculated data to the ExchangeNode for further processing.\\n\\nThis step adopts a recursive method, traverses the entire PlanNode tree from bottom to top, and then creates a PlanFragment for each leaf node on the tree. If the parent node is encountered, splitting the child nodes that can be executed in parallel will be considered.\\n\\nFor query operations, the join operation is the most common.\\n\\n**Doris currently supports four join algorithms:** broadcast join, hash partition join, colocate join, and bucket shuffle join.\\n\\n**broadcast join**\uFF1ASend the small table to each machine where the large table is located and perform a hash join operation. When the amount of data scanned from a table is small, the cost of broadcast join will be calculated, and the method with the smallest cost will be selected by calculating and comparing the cost of hash partitions.\\n\\n**hash partition join**\uFF1AWhen the data scanned from the two tables are both large, hash partition join is generally used. It traverses all the data in the table, calculates the hash value of the key, then modulizes the number of clusters, and whichever machine is selected, the data will be sent to this machine for hash join operation.\\n\\n**colocate join**\uFF1AIf the data distribution of the two tables is specified to be consistent when they are created, the colocate join algorithm will be used when the join key of the two tables is the same as the bucket key. Since the data distribution of the two tables is the same, the hash join operation is equivalent to a local process. It does not involve data transmission, which significantly improves query performance.\\n\\n**bucket shuffle join**\uFF1AWhen the join key is a bucketing key, and only one partition is involved, the bucket shuffle join algorithm is preferred. Since bucketing itself represents a way of dividing data, it only needs to take the hash modulo of the number of buckets from the right table to the left table, so that only one copy of the data in the right table needs to be transmitted over the network, which greatly reduces the network of data transmission, as shown in Figure 10.\\n\\n![](/images/blogs/principle-of-Doris-SQL-parsing/Figure_10_en.png)\\n\\nFigure 11 shows the core process of creating a distributed logical plan with a single-machine logical plan with HashJoinNode.\\n\\n- For PlanNodes, PlanFragments are created bottom-up.\\n\\n- If it is a ScanNode, PlanFragment will be created directly, and the RootPlanNode of the PlanFragment is this ScanNode.\\n\\n- If it is a HashJoinNode, the broadcastCost will be calculated at first, which could provide a reference for selecting boracast join or hash partition join.\\n\\n- Join algorithm will be chosen according to different conditions.\\n\\n- If colocate joins are used, since joins are all local, no splitting is required. Set the left child node of HashJoinNode as the RootPlanNode of leftFragment, and the right child node as the RootPlanNode of rightFragment, share a PlanFragment with leftFragment, and delete rightFragment.\\n\\n- If bucket shuffle join is used, data from the right table needs to be sent to the left table. So first create an ExchangeNode, set the left child node of HashJoinNode as the RootPlanNode of leftFragment, the right child node as this ExchangeNode, share a PlanFragment with leftFragment, and specify the destination of rightFragment data to be sent to this ExchangeNode.\\n\\n- If broadcast join is used, the data from the right table needs to be sent to the left table. So first create an ExchangeNode, set the left child node of HashJoinNode as the RootPlanNode of leftFragment, the right child node as this ExchangeNode, share a PlanFragment with leftFragment, and specify the destination of rightFragment data to be sent to this ExchangeNode.\\n\\n- If hash partition join is used, the data in the left table and the right table must be split, and both left and right nodes need to be split out to create left ExchangeNode and right ExchangeNode respectively. HashJoinNode specifies the left and right nodes as left ExchangeNode and right ExchangeNode. Create a PlanFragment separately and specify RootPlanNode as this HashJoinNode. Finally, specify the data sending destination of leftFragment and rightFragment as left ExchangeNode and right ExchangeNode.\\n\\n![](/images/blogs/principle-of-Doris-SQL-parsing/Figure_11_en.png)\\n\\nFigure 12 is an example after the join operation of two tables is converted into a PlanFragment tree, there are 3 PlanFragments generated. The final output data passes through the ResultSinkNode node.\\n\\n![](/images/blogs/principle-of-Doris-SQL-parsing/Figure_12_en.png)\\n\\n# 9. Schedule phase\\n\\nThis step is to create a distributed physical plan based on the distributed logical plan. will solve the following questions:\\n\\n- Which BE executes which PlanFragment\\n\\n- Which replica to chooes for each Tablet to query\\n\\n- How to perform multi-instance concurrency\\n\\n**Figure 13 shows the core process for creating a distributed physical plan:**\\n\\n**a. Prepare phase**\uFF1ACreate a FragmentExecParams structure for each PlanFragment to represent all the parameters required for PlanFragment execution; if a PlanFragment contains DataSinkNode, the destination PlanFragment for data transmission will be found, and specify the input of FragmentExecParams of the destination PlanFragment as FragmentExecParams of this PlanFragment.\\n\\n**b. computeScanRangeAssignment phase**\uFF1ADifferent processing is performed for different types of joins.\\n\\n- computeScanRangeAssignmentByColocate: For colocate join processing, since the data distribution in the two table buckets of the join is the same, they are based on the bucket join operation, so here is to determine which host is selected for each bucket. When allocating buckets to hosts, try to ensure that the buckets allocated to each host are even.\\n\\n- computeScanRangeAssignmentByBucket: Processing for bucket shuffle join, which is only based on bucket operations, so here is to determine which host is selected for each bucket. When allocating buckets to hosts, it is also necessary to try to ensure that the buckets allocated to each host are even.\\n\\n- computeScanRangeAssignmentByScheduler: Process for other types of joins. Determines which replica of the tablet each scanNode reads. A scanNode will read multiple tablets, and each tablet has various copies. To distribute the scan operation on various machines as much as possible, improve concurrent performance, and reduce IO pressure, Doris uses the Round-Robin algorithm to distribute tablet scans to multiple machines as much as possible. For example, 100 tablets need to be scanned, each tablet has three copies, and ten machines could be used. When allocating, each machine is guaranteed to scan ten tablets.\\n\\n**c.computeFragmentExecParams phase**\uFF1AThis stage determines which BE the PlanFragment is issued to for execution and how to handle instance concurrency. After the scan address of each tablet is determined, FragmentExecParams will generate multiple instances with the address as the dimension. If various addresses are contained in FragmentExecParams, various instances of FInstanceExecParam will be generated. If the concurrency is set, the execution instance of an address will be further split into multiple FInstanceExecParams. There will be some special processing for bucket shuffle join and colocate join, but the basic logic is the same. After FInstanceExecParam is created, a unique ID will be assigned to facilitate tracking information. If FragmentExecParams contains ExchangeNode, the number of senders will be counted to know how many senders\' data needs to be accepted. Finally, FragmentExecParams determines the destinations and fills in the destination address.\\n\\n**d. Create result receiver stage**\uFF1AThe resulting receiver is where the final data needs to be output after the query is completed.\\n\\n**e. to thrift stage**\uFF1ACreate RPC requests based on FInstanceExecParam of all PlanFragments, then send them to the BE side for execution. A complete SQL parsing process is completed.\\n\\n![](/images/blogs/principle-of-Doris-SQL-parsing/Figure_13_en.png)\\n\\nFigure 14 is a simple example. The PlanFrament in the figure contains a ScanNode. The ScanNode scans three tablets. Each tablet has two copies, and the cluster assumes that there are two hosts.\\n\\nThe computeScanRangeAssignment stage determines that replicas 1, 3, 5, 8, 10, and 12 need to be scanned, where replicas 1, 3, and 5 are located on host1, and replicas 8, 10, and 12 are located on host2.\\n\\nIf the global concurrency is set to 1, 2 instances of FInstanceExecParam are created and sent to host1 and host2 for execution. If the global concurrency is set to 3, 3 instances of FInstanceExecParam are created on this host1, and three instances of FInstanceExecParam are created on host2. Each instance scans one replica, equivalent to initiating 6 RPC requests.\\n\\n![](/images/blogs/principle-of-Doris-SQL-parsing/Figure_14_en.png)\\n\\n# 10 Summary\\nThis article first briefly introduces Doris and then introduces the general process of SQL parsing: lexical analysis, syntax analysis, generating logical plans, and generating physical plans. Then, it presents the overall architecture of DorisSQL parsing. In the end, the five processes:  Parse, Analyze, SinglePlan, DistributedPlan, and Schedule are explained in detail, and an in-depth explanation is given of the algorithm principle and code implementation.\\n\\nDoris complies with the standard methods of SQL parsing. Still, according to the underlying storage architecture and distributed characteristics, many optimizations have been made in SQL parsing to achieve maximum parallelism and minimize network transmission, reducing a lot of burden on the SQL execution level."},{"id":"/Flink-realtime-write","metadata":{"permalink":"/blog/Flink-realtime-write","source":"@site/blog/Flink-realtime-write.md","title":"How Flink\'s real-time writes to Apache Doris ensure both high throughput and low latency","description":"With the increasing demand for real-time analysis, the timeliness of data is becoming more and more important to the refined operation of enterprises. With the massive data, real-time data warehouse plays an irreplaceable role in effectively digging out valuable information, quickly obtaining data feedback, helping companies make faster decisions and better product iterations.","date":"2022-07-29T00:00:00.000Z","tags":[{"inline":true,"label":"Tech Sharing","permalink":"/blog/tags/tech-sharing"}],"hasTruncateMarker":false,"authors":[{"name":"Apache Doris","key":null,"page":null}],"frontMatter":{"title":"How Flink\'s real-time writes to Apache Doris ensure both high throughput and low latency","description":"With the increasing demand for real-time analysis, the timeliness of data is becoming more and more important to the refined operation of enterprises. With the massive data, real-time data warehouse plays an irreplaceable role in effectively digging out valuable information, quickly obtaining data feedback, helping companies make faster decisions and better product iterations.","date":"2022-07-29","author":"Apache Doris","tags":["Tech Sharing"],"image":"/images/tech-sharing.png"},"unlisted":false,"prevItem":{"title":"Doris analysis: Doris SQL principle analysis","permalink":"/blog/principle-of-Doris-SQL-parsing"},"nextItem":{"title":"Best practice of Apache Doris in JD","permalink":"/blog/jd"}},"content":"\x3c!-- \\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n\\n\\nWith the increasing demand for real-time analysis, the timeliness of data is becoming more and more important to the refined operation of enterprises. With the massive data, real-time data warehouse plays an irreplaceable role in effectively digging out valuable information, quickly obtaining data feedback, helping companies make faster decisions and better product iterations.\\n\\nIn this situation, Apache Doris stands out as a real-time MPP analytic database, which is high performance and easy to use, and supports various data import methods. Combined with Apache Flink, users can quickly import unstructured data from Kafka and CDC(Change Data Capture) from upstream database like MySQL. Apache Doris also provides sub-second analytic query capabilities, which can effectively satisfy the needs of several real-time scenarios: multi-dimensional analysis, dashboard and data serving etc.\\n# Challenge\\n\\nUsually, there are many challenges to ensure high end-to-end concurrency and low latency for real-time data warehouses , such as:\\n\\n- How to ensure end-to-end data sync in second-level ?\\n\\n- How to quickly ensure data visibility ?\\n\\n- How to solve the problem of small files writing under high concurrency situation?\\n\\n- How to ensure end-to-end Exactly-Once?\\n\\nWithin the challenges above , we conducted an in-depth research on the business scenarios of users using Flink and Doris to build real-time data warehouses . After grasping the pain points of users, we made targeted optimizations in Doris version 1.1 and greatly improved the user experience  and improved the stability. The resource consumption of Doris has also been greatly optimized.\\n\\n# Optimization\\n\\n### Streaming Write\\n\\nThe initial practice of Flink Doris Connector is to cache the data into the memory batch after receiving data.The method of data writing is saving batches, and using parameters such as `batch.size` and `batch.interval` to control the timing of Stream Load writing at the same time.\\n\\nIt usually runs stably when the parameters are reasonable. Whatever the parameters are unreasonable, it would cause frequent Stream Load and compaction untimely, resulting in excessive version errors ( -235 ). On the other hand, when there is too much data, in order to reduce the writing frequency of Stream Load , the setting of `batch.size` too large may also cause OOM.\\n\\n**To solve this problem, we introduce streaming write:**\\n\\n![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/otliigutb8p9l1y6qyp6.png)\\n- After the Flink task starts, the Stream Load Http request will be asynchronously initiated.\\n\\n- When the data is received, it will be continuously transmitted to Doris through the Chunked transfer encoding of Http.\\n\\n- Http request will end at Checkpoint and complete the Stream Load writing . The next Stream Load request will be asynchronously initiated at the same time.\\n\\n- The data will continue to be received and the follow-up process is the same as above.\\n\\nThe pressure on the memory of the batch is avoided since the Chunked mechanism is used to transmit data. And the timing of writing is bound to the Checkpoint, which makes the timing of Stream Load controllable, and provides a basis for the following Exactly-Once semantics.\\n\\n### Exactly-Once\\n\\nExactly-Once means that data will not be reprocessed or lost, even machine or application failure. Flink supports the End-to-End\'s Exactly-Once scenario a long time ago, mainly through the two-phase commit protocol to realize the Exactly-Once semantics of the Sink operator.\\n\\nOn the basis of Flink\'s two-stage submission, with the help of Doris 1.0\'s Stream Load two-stage submission,Flink Doris Connector implements Exactly Once semantics. The specific principles are as follows:\\n\\n- When the Flink task is started, it will initiate a Stream Load PreCommit request. At this time, a transaction will be opened first, and data will be continuously sent to Doris through the Chunked mechanism of Http.\\n\\n![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/ole5tqi91jibzdg9vqep.png)\\n\\n- Http request will be completed when the data writing ends at Checkpoint , and set the transaction status to preCommitted. The data has been written to BE and is invisible to the user at this time.\\n\\n![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/jiieu1eff6smunkr85s5.png)\\n\\n- A Commit request will be initiated after the Checkpoint, and the transaction status will be set to Committed. The data will become visible to the user after request.\\n\\n![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/eaona8eslljmkpaa9324.png)\\n\\n- After the Flink application ends unexpectedly and restarts from Checkpoint, if the last transaction was in the preCommitted state, a rollback request will be initiated and the transaction state will be set to Aborted.\\n\\nBased on the above , Flink Doris Connector can be used to realize real-time data storage without loss or weight.\\n\\n### Second- Level Data Synchronization\\n\\nEnd-to-end second-level data sync and real-time visibility of data in high concurrent write scenarios require Doris to have the following capabilities:\\n\\n- **Transaction Processing Capability**\\n\\nFlink real-time writing interacts with Doris in the form of Stream Load 2pc, which requires Doris to have the corresponding transaction processing capabilities to ensure the basic ACID characteristics, and support Flink\'s second-level data sync in high concurrency scenarios.\\n\\n- **Rapid Aggregation Capability of Data Versions**\\n\\nOne import in Doris will generate one data version. In a high concurrent write scenario, an inevitable impact is that there are too many data versions, and the amount of data imported in a single time will not be too large. The continuous high-concurrency small file writing scenario extremely tests the real-time ability and Doris\' data merging performance, which is not friendly to Doris, and in turn affects the performance of the query. Doris has greatly enhanced the data compaction capability in version 1.1, which can quickly complete the aggregation of new data, avoiding -235 errors and query efficiency problems which are caused by too many versions of sharded data.\\n\\nFirst of all, in Doris 1.1 version, QuickCompaction was introduced, which can actively triggered Compaction when the data version increased. At the same time, by improving the ability to scan fragment meta information, fragments that need to be compacted can be quickly discovered and trigger Compaction. Through active triggering and passive scanning, the real-time problem of data merging is completely solved.\\n\\nFor high-frequency small file Cumulative Compaction, the scheduling and isolation of Compaction tasks is implemented to prevent the heavyweight Base Compaction from affecting the merging of new data.\\n\\nFinally, the strategy of merging small files is optimized by adopting gradient merge method. Each time the files participating in the merging belong to the same data magnitude,which can prevent versions with large differences in size from merging, and gradually merges hierarchically, reducing the number of times a single file is involved in merging, which can greatly save the CPU consumption of the system.\\n\\n![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/ez5qdcpgwjw60g9aacqd.png)\\n\\nDoris version 1.1 has made targeted optimizations for scenarios such as high concurrent import, second-level data sync, and real-time data visibility, which greatly increases the ease of use and stability of the Flink system and Doris system, saves the overall resources of the cluster.\\n\\n# Effect\\n\\n### General Flink High Concurrency Scenarios\\n\\nIn the general scenario of the survey, Flink is used to synchronize unstructured data in upstream Kafka. The data is written to Doris in real time by the Flink Doris Connector after ETL.\\n\\nThe customer scenario is extremely strict here. The upstream maintains a high frequency of 10w per second, and the data needs to be able to complete the upstream and downstream sync within 5s to achieve second-level data visibility. Flink is configured with 20 concurrency, and the Checkpoint interval is 5s. The performance of Doris version 1.1 is quite excellent.\\n\\nSpecifically reflected in the following aspects:\\n\\n- **Compaction Real-Time**\\n\\nData can be merged quickly, the number of tablet data versions is kept below 50, and the compaction score is stable. Compared with the previous -235 problem in high concurrent import scenario, the compaction efficiency is improved more than 10 times.\\n\\n![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/d6enyv1zj68o7myjypnl.png)\\n\\n-  **CPU Resource Consumption**\\n\\nDoris version 1.1 has optimized the strategy for compaction of small files. In high-concurrency import scenarios, CPU resource consumption is reduced by 25%.\\n\\n- **QPS Query Delay is Stable**\\n\\nBy reducing the CPU usage and the number of data versions, the overall order of data has been improved, and the delay of SQL queries will be reduced.\\n\\n### Second-Level Data Synchronization Scenario (Extreme High Pressure)\\n\\nIn single bet and single tablet with 30 concurrent limit stream load pressure test on the client side, data in real-time <1s, the comparison before and after compaction score optimization as below:\\n\\n![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/r01hn8hv6arzbdclknis.png)\\n# Recommendations\\n\\n### Real-Time Data Visualization Scenario\\n\\nFor strict latency requirements scenarios, such as second-level data synchronization, usually mean that a single import file is small, and it is recommended to reduce `cumulative_size_based_promotion_min_size_mbytes `. The default unit is 64 MB, and you can set it to 8 MB manually, which can greatly improve the compaction real-time performance. \\n\\n### High Concurrency Scenario\\n\\nFor high concurrent writing scenarios, you can reduce the frequency of Stream Load by increasing the checkpoint interval. For example, setting checkpoint to 5-10s can not only increase the throughput of Flink tasks, but also reduce the generation of small files and avoid causing compaction more pressure.\\n\\nIn addition, for scenarios that do not require high real-time data, such as minute-level data sync, the checkpoint interval can be increased, such as 5-10 minutes. And the Flink Doris connector can still ensure the integrity of data through the two-stage submission and checkpoint mechanism.\\n\\n# Future planning\\n\\n-  **Real-time Schema Change**\\n\\nWhen accessing data in real time through Flink CDC, the upstream business table will perform the schema change operation, it has to modify the schema manually in Doris and Flink tasks. In the end, the data of the new schema can be synchronized after restart the task . \\n\\nThis way requires human intervention, which will bring a great operation burden to users. In subsequent versions, real-time schema changes will support CDC scenarios, and the upstream schema changes will be synchronized to the downstream in real-time, which will comprehensively improve the efficiency of schema changes.\\n\\n-  **Doris Multi-table Writing**\\n\\nAt present, the Doris Sink operator only supports synchronizing a single table, so for the entire database, it still has to divide the flow manually at the Flink level and write to multiple Doris Sinks, which will increase the difficulty of developers. In subsequent versions, we will support a single Doris Sink to synchronize multiple tables, which greatly simplifies the user\'s operation.\\n\\n-  **Adaptive Compaction Parameter Tuning**\\n\\nAt present, the compaction strategy has many parameters, which can play a good role in most general scenarios, but these strategies still can\'t play an efficient role in some special scenarios. We will continue to optimize in subsequent versions, carry out adaptive compaction tuning for different scenarios, and keep improving data merging efficiency and real-time performance in various scenarios.\\n\\n-  **Single-Copy Compaction**\\n\\nThe current compaction strategy is that each BE is carried out separately. In subsequent versions, we will implement single-copy compaction, and realize compaction tasks by cloning snapshots, reduce system load while reducing about 2/3 compaction tasks of the cluster, leaving more system resources to the user side."},{"id":"/jd","metadata":{"permalink":"/blog/jd","source":"@site/blog/jd.md","title":"Best practice of Apache Doris in JD","description":"This paper mainly discusses how to use Doris for business exploration and practice in the multi-dimensional analysis of real-time and offline data in the large real-time screen of JD customer service in the scenarios of manual consultation, customer event list, after-sales service list, etc.","date":"2022-07-20T00:00:00.000Z","tags":[{"inline":true,"label":"Best Practice","permalink":"/blog/tags/best-practice"}],"hasTruncateMarker":false,"authors":[{"name":"Apache Doris","key":null,"page":null}],"frontMatter":{"title":"Best practice of Apache Doris in JD","description":"This paper mainly discusses how to use Doris for business exploration and practice in the multi-dimensional analysis of real-time and offline data in the large real-time screen of JD customer service in the scenarios of manual consultation, customer event list, after-sales service list, etc.","date":"2022-07-20","author":"Apache Doris","tags":["Best Practice"],"image":"/images/user-jd.jpg"},"unlisted":false,"prevItem":{"title":"How Flink\'s real-time writes to Apache Doris ensure both high throughput and low latency","permalink":"/blog/Flink-realtime-write"},"nextItem":{"title":"Best practice of Apache Doris in Meituan","permalink":"/blog/meituan"}},"content":"\x3c!-- \\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n# **Introduction\uFF1A**\\n\\n\\n\\nApache Doris is an open source MPP analytical database product that not only can get query results in sub-second response time, effectively supporting real-time data analysis, but also supports huge data sets of more than 10PB. Compared with other industry-hot OLAP database systems, the distributed architecture of Apache is very simple. Itsupports elastic scaling and is easy to operate and maintain, saving a lot of labor and time costs. At present, the domestic community is very popular , and there are also many companies which have large scale uses, such as Meituan and Xiaomi,etc. \\n\\n\\n\\nThis paper mainly discusses how to use Doris for business exploration and practice in the multi-dimensional analysis of real-time and offline data in the large real-time screen of JD customer service in the scenarios of manual consultation, customer event list, after-sales service list, etc.\\n\\n\\n\\nIn recent years, with the explosive growth of data volume and the emergence of the demand for online analysis of massive data, traditional relational databases such as MySQL and Oracle have encountered bottlenecks under large data volume, while databases such as Hive and Kylin lack timeliness. So Apache Doris, Apache Druid, ClickHouse and other real-time analytic databases begun to appear, not only to cope with the second-level queries of massive data, but also to meet the real-time and quasi-real-time analysis needs. Offline and real-time computing engines are in full bloom. But for different scenarios and facing different problems, no single engine is a panacea. We hope that this article can give you some inspiration on the application and practice of offline and real-time analytics in JD\'s customer service business, and we hope you will communicate more and give us valuable suggestions.\\n\\n# **JD Customer Service Business Form**\\n\\n\\n\\nAs the entrance to the group\'s services, JD Customer Service provides efficient and reliable protection for users and merchants. JD customer service is responsible for solving users\' problems in a timely manner and providing them with detailed and easy-to-understand instructions and explanations; in order to better understand users\' feedback and the status of products, it is necessary to monitor a series of indicators such as the number of inquiries, pick-up rates, complaints, etc. in real time, and discover problems in a timely manner through ring comparison and year-on-year comparison, in order to better adapt to users\' shopping styles, improve service quality and efficiency, and thus enhance the brand of JD influence.\\n\\n\\n\\n# **Easy OLAP Design**\\n\\n\\n\\n### **01 EasyOLAP Doris Data Import Links**\\n\\n\\n\\nEasyOLAP Doris data sources are mainly real-time Kafka and offline HDFS files. The import of real-time data relies on Routine Load; offline data is mainly imported using Broker Load and Stream Load.\\n\\n\\n\\n![1280X1280](/images/jd03.png)\\n\\n\\n\\nEasyOLAP Doris Data Import Links\\n\\n\\n\\n### **02 EasyOLAP Doris Full Link Monitor**\\n\\n\\n\\nThe EasyOLAP Doris project currently uses the Prometheus + Grafana framework for monitoring. The node_exporter is responsible for collecting machine-level metrics, and Doris automatically spits out FE and BE service-level metrics in Prometheus format. In addition, OLAP Exporter service is deployed to collect Routine Load related metrics, aiming to discover real-time data stream import at the first time and ensure real-time data timeliness.\\n\\n\\n\\n![EasyOLAP Doris monitoring link](/images/jd04.png)\\n\\n\\n\\nEasyOLAP Doris monitoring link\\n\\n![640](/images/jd01.png)\\n\\n### **03 EasyOLAP Doris Primary-Secondary Dual Stream Design**\\n\\n\\n\\nEasyOLAP Doris adopts a dual-write approach for the primary and secondary clusters in order to guarantee the service stability of Level 0 services during the promotion time.\\n\\n\\n\\n![03 EasyOLAP Doris Primary-Secondary Dual Stream Design](/images/jd02.png)\\n\\n\\n\\nEasyOLAP Doris Primary-Secondary Dual Stream Design\\n\\n\\n\\n### **04 EasyOLAP Doris Dynamic Partition Management**\\n\\n\\n\\nAfter analyzing the requirements, the JD OLAP team did some customization of Doris, which involved dynamic partition management. Although the community version already had the function of dynamic partitioning, the function could not retain partitions of a specified time. For the characteristics of JD Group, we have retained historical data of specified time, such as data during 618 and 11.11, which will not be deleted due to dynamic partitioning. The dynamic partition management feature can control the amount of data stored in the cluster, and it is easy to use by the business side without the need to manage partition information manually or with additional code.\\n\\n\\n\\n# **Doris Caching Mechanism**\\n\\n\\n\\n### **01 Demand Scenarios**\\n\\n\\n\\nCommitted to continuously improving user experience, JD Customer Service\'s data analysis pursues the ultimate timeliness. Offline data analysis scenario is write less read more, data is written once and read frequently many times; real-time data analysis scenario, part of the data is not updated historical partition, part of the data is in the updated partition. In most analysis applications, there are the following scenarios:\\n\\n- High concurrency scenario: Doris better support high concurrency, but too high QPS will cause cluster jitter, and a single node can not carry too high QPS;.\\n\\n- Complex queries: JD customer service real-time operation platform monitoring needs to display multi-dimensional complex indicators according to business scenarios, rich indicators display corresponding to a variety of different queries, and data sources from multiple tables . Although the response time of individual queries at milliseconds level , the overall response time may be at the second level.\\n\\n- Repeated queries: if there is no anti-refresh mechanism, repeatedly refreshing the page will lead to the submission of a large number of repeated queries due to delays or hand errors.\\n\\nFor the above scenario, there are solutions at the application layer \u2014\u2014 the query results are put into Redis and the cache is refreshed periodically or manually by the user, but there are some problems\uFF1A\\n\\n- Data inconsistency: can not respond immediately to data updates, and the user may receive results with old data.\\n\\n- Low hit rate: if the data is highly real-time and the cache is frequently invalidated, the hit rate of the cache is low and the load on the system cannot be relieved.\\n\\nAdditional cost: introduction of external components increases system complexity and adds additional cost.\\n\\n\\n\\n### **02 Introduction to Caching Mechanism**\\n\\n\\n\\nThere are three different types of Cache in EasyOLAP Doris, respectively Result Cache, SQL Cache and Partition Cache, depending on the applicable scenario. All three types of caches can be switched on and off by MySQL client commands.\\n\\nThese three caching mechanisms can coexist: which can be turned on at the same time. When querying, the query parser first determines whether the Result Cache is enabled or not, and if the Result Cache is enabled, it first finds out whether the cache exists for the query from the Result Cache, and if the cache fails or does not exist, it directly takes the cached value and returns it to the client. The cache is placed in the memory of each FE node for fast reading.\\n\\nSQL Cache stores and gets the cache according to the signature of SQL, the ID of the partition of the queried table, and the latest version number of the partition. These three together serve as cache conditions. If one of these three conditions is changed, such as SQL statement change or partition version number change after data update, the cache will not be hit. In the case of multiple table joins, the partition update of one of the tables will also result in failure to hit the cache. SQL Cache is more suitable for T+1 update scenarios.\\n\\nPartition Cache is a more fine-grained caching mechanism. Partition cache mainly splits a query into read-only partition and updatable partition in parallel based on partition, read-only partition is cached, updatable partition is not cached, and the corresponding result set is generated n, and then the results of each split subquery are merged. Therefore, if the query N days of data, data update the most recent D days, each day is only a different date range but similar queries, you can use Partition Cache, only need to query D partitions can be, the other parts are from the cache, can effectively reduce the cluster load, shorten the query response time.\\n\\nWhen a query enters Doris, the system will first process the query statement and take it as the key, before executing the query statement, the query analyzer can automatically select the most suitable caching mechanism to ensure that the caching mechanism is used to shorten the query response time in the best case. Then, it checks whether the query result exists in the Cache, and if it does, it gets the data in the cache and returns it to the client; if it does not, it queries normally and stores the query result as Value and the query statement Key in the cache. SQL Cache is more suitable for T+1 scenarios and works well when partition updates are infrequent and SQL statements are repetitive Partition Cache is the least granular cache. When a query statement queries data for a time period, the query statement is split into multiple subqueries. It can shorten the query time and save cluster resources when the data is written to only one partition or partial partition.\\n\\nTo better observe the effectiveness of caching, metrics have been added to Doris\' service metrics, which are monitored visually through Prometheus and Grafana monitoring systems. The metrics include the number of hits for different types of Cache, the hit rate for different types of Cache, the memory size of the Cache, and other metrics.\\n\\n\\n\\n### **03 Caching Mechanism Effect**\\n\\n\\n\\nFor the JD Customer Service Doris main cluster, some services reached 100% CPU usage during 11.11 period without caching on; with Result Cache on, CPU usage was between 30% and 40%. The caching mechanism ensures that the business can get the query results quickly and protects the cluster resources well under high concurrency scenarios.\\n\\n\\n\\n# **Doris\' optimization during the 11.11 sale, 2020**\\n\\n\\n\\n### **01 Import Task Optimization**\\n\\n\\n\\nThe import of real-time data has always been a challenge. Among them, ensuring real-time data and importing stability is the most important. In order to observe the real-time data import situation more intuitively, JD OLAP team developed OLAP Exporter independently to collect real-time data import-related metrics, such as import speed, import backlog and suspended tasks. The import speed and import backlog can be used to determine the status of a real-time import task, and if find a trend of backlog, the sampling tool developed independently can be used to sample and analyze the real-time task. Real-time tasks have three main thresholds to control the submission of tasks, which are the maximum processing interval per batch, the maximum number of processing entries per batch and the maximum amount of data processed per batch, and a task will be submitted as soon as one of these thresholds is reached. By increasing the logs, we found that the task queue in FE was relatively busy, so the parameters were mainly adjusted to make the maximum number of processing entries per batch and the maximum amount of data processed per batch larger, and then the maximum processing interval per batch was adjusted to ensure that the data latency was within twice the maximum processing interval per batch according to the business requirements. Through the sampling tool, the analysis task ensures not only the real-time data, but also the stability of the import. In addition, we also set up alarms to detect abnormalities such as backlog of real-time import tasks and suspension of import tasks in a timely manner.\\n\\n\\n\\n### **02 Monitoring Metrics Optimization**\\n\\n\\n\\nThe monitoring metrics are divided into two main sections, a machine level metrics section and a business level metrics section. In the whole monitoring panel, detailed metrics bring comprehensive data and at the same time make it more difficult to get important metrics. So, to get a better view of important metrics for all clusters, a separate panel is created - 11.11 Important Metrics Summary Panel. The board contains metrics such as BE CPU usage, real-time task consumption backlog rows, TP99, QPS, and so on. The number of metrics is small, but the situation of all clusters can be observed, which can eliminate the trouble of frequent switching in monitoring.\\n\\n\\n\\n### **03 Peripheral Tools Support**\\n\\n\\n\\nIn addition to the sampling tools and OLAP Exporter mentioned above, the JD OLAP team has also developed a series maintenance tools for Doris.\\n\\n\\n\\n1. Import sampling tool: The import sampling tool not only collects the data imported in real time, but also supports adjusting the parameters of the real time import task, or generating creation statements (including the latest loci and other information) for task migration and other operations when the real time import task is paused.\\n\\n   \\n\\n2. Big query tool: Big queries not only cause jitter in cluster BE CPU usage, but also lead to longer response time for other queries. Before the Big Query tool, if you found jitter in cluster CPU, you needed to check the audit logs on all FEs and then do the statistics, which is not only time-consuming but also not intuitive. The Big Query tool is designed to solve the above problem. When the monitoring side finds that the cluster has jitter, you can use the Big Query tool and enter the cluster name and time point to get the total number of queries for different services at that time point, the number of queries with more than 5 seconds, 10 seconds, 20 seconds, the number of queries with huge scanning volume, etc. It is convenient for us to analyze the big queries from different dimensions. The details of the big queries will also be saved in the intermediate file, which can directly get the big queries of different businesses. The whole process only takes a few tens of seconds to a minute to locate the big query that is happening and get the corresponding query statements, which greatly saves time and operation and maintenance costs.\\n\\n   \\n\\n3. Downgrade and recovery tools: In order to ensure the stability of the Level 0 business during the 11.11 promotion, when the cluster pressure exceeds the safety level, it is necessary to downgrade other non-Level 0 businesses, and then restore them to the pre-downgrade settings with one click after the peak period. The degradation mainly involves reducing the maximum number of connections to the service, suspending non-level 0 real-time import tasks, and so on. This greatly increases the ease of operation and improves efficiency.\\n\\n   \\n\\n4. Cluster inspection tool: During 11.11 period, the health inspection of clusters is extremely important. Routine inspections include primary and secondary cluster consistency checks for dual-stream services. In order to ensure that the business can quickly switch to the other cluster when one cluster has problems, it is necessary to ensure that the library tables on both clusters are consistent and the data volume is not too different; check whether the number of copies of the library tables is 3 and whether there are unhealthy Tablet in the cluster; check the machine disk utilization, memory and other machine-level indicators, etc. Check the machine disk utilization, memory and other machine-level metrics, etc.\\n\\n   \\n\\n   # **Summary & Outlook**\\n\\n   \\n\\n   JD Customer Service was introduced to Doris in early 2020, and currently has one standalone cluster and one shared cluster, and is an experienced user of JD OLAP.\\n\\n   \\n\\n   In the business use, we also encountered problems such as task scheduling-related, import task configuration-related and query-related problems, which are driving the JD OLAP team to understand Doris more deeply. We plan to promote the use of materialized views to further improve the efficiency of queries; use Bitmap to support accurate de-duplication of UV and other metrics; use audit logs to make it easier to count large and slow queries; and solve the scheduling problem of real-time import tasks to make them more efficient and stable. In addition, we also plan to optimize table building, create high-quality Rollup or materialized views to improve the smoothness of the application, and accelerate more businesses to the OLAP platform to improve the impact of the application."},{"id":"/meituan","metadata":{"permalink":"/blog/meituan","source":"@site/blog/meituan.md","title":"Best practice of Apache Doris in Meituan","description":"Introduction: This paper mainly introduces a general method and practice of real-time data warehouse construction. The real-time data warehouse aims at end-to-end low latency, SQL standardization, rapid response to changes, and data unification. In practice, the best practice we summarize is: a common real-time production platform + a common interactive real-time analysis engine cooperate with each other to meet real-time and quasi-real-time business scenarios. The two have a reasonable division of labor and complement each other to form an easy-to-develop, easy-to-maintain, and most efficient assembly line, taking into account development efficiency and production costs, and satisfying diverse business needs with a better input-output ratio.","date":"2022-07-20T00:00:00.000Z","tags":[{"inline":true,"label":"Best Practice","permalink":"/blog/tags/best-practice"}],"hasTruncateMarker":false,"authors":[{"name":"Apache Doris","key":null,"page":null}],"frontMatter":{"title":"Best practice of Apache Doris in Meituan","description":"Introduction: This paper mainly introduces a general method and practice of real-time data warehouse construction. The real-time data warehouse aims at end-to-end low latency, SQL standardization, rapid response to changes, and data unification. In practice, the best practice we summarize is: a common real-time production platform + a common interactive real-time analysis engine cooperate with each other to meet real-time and quasi-real-time business scenarios. The two have a reasonable division of labor and complement each other to form an easy-to-develop, easy-to-maintain, and most efficient assembly line, taking into account development efficiency and production costs, and satisfying diverse business needs with a better input-output ratio.","date":"2022-07-20","author":"Apache Doris","tags":["Best Practice"],"image":"/images/best-practice.png"},"unlisted":false,"prevItem":{"title":"Best practice of Apache Doris in JD","permalink":"/blog/jd"},"nextItem":{"title":"Best practice of Apache Doris in Xiaomi Group","permalink":"/blog/xiaomi"}},"content":"\x3c!-- \\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n# Best Practice of Apache Doris in Meituan\\n\\nIntroduction: This paper mainly introduces a general method and practice of real-time data warehouse construction. The real-time data warehouse aims at end-to-end low latency, SQL standardization, rapid response to changes, and data unification. In practice, the best practice we summarize is: a common real-time production platform + a common interactive real-time analysis engine cooperate with each other to meet real-time and quasi-real-time business scenarios. The two have a reasonable division of labor and complement each other to form an easy-to-develop, easy-to-maintain, and most efficient assembly line, taking into account development efficiency and production costs, and satisfying diverse business needs with a better input-output ratio.\\n\\n# real-time scene\\n\\nThere are many scenarios in which real-time data is delivered in Meituan, mainly including these following points:\\n\\n- Operational level: Such as real-time business changes, real-time marketing effects, daily business status and daily real-time business trend analysis, etc.\\n- Production level: such as whether the real-time system is reliable, whether the system is stable, real-time monitoring of the health of the system, etc.\\n- C-end users: For example, search recommendation sorting requires real-time understanding of users\' thoughts, behaviors and characteristics, and recommendation of more concerned content to users.\\n- Risk control: Food delivery and financial technology are used a lot. Real-time risk identification, anti-fraud, abnormal transactions, etc., are all scenarios where a large number of real-time data are applied\\n\\n# Real-time technology and architecture\\n\\n### 1.Real-time computing technology selection\\n\\nAt present, there are many open source real-time technologies, among which Storm, Spark Streaming and Flink are common. The specific selection depends on the business situation of different companies.\\n\\nMeituan Takeaway relies on the overall construction of meituan\'s basic data system. In terms of technology maturity, It used Storm a few years ago, which was irreplaceable in terms of performance stability, reliability and scalability. As Flink becomes more and more mature, it has surpassed Storm in terms of technical performance and framework design advantages. In terms of trends, just like Spark replacing MR, Storm will be gradually replaced by Flink. Of course, there will be a process of migrating from Storm to Flink. We currently have some old tasks still on Storm, and we are constantly promoting task migration.\\n\\nThe comparison between Storm and Flink can refer to the form above.\\n\\n### 2.Real-time Architecture\\n\\n#### \u2460 Lambda Architecture\\n\\nThe Lambda architecture is a relatively classic architecture. In the past, there were not many real-time scenarios, mainly offline. When a real-time scene is attached, the technical ecology is different due to the different timeliness of offline and real- time. The Lambda architecture is equivalent to attaching a real-time production link, which is integrated at the application level, and two-way production is independent of each other.This is also a logical approach to adopt in business applications.\\n\\nThere will be some problems in dual-channel production, such as double processing logic, double development and operation and maintenance, and resources will also become two resource links. Because of these problems,  Kappa architecture has been evolved.\\n\\n#### \u2461 Kappa Architecture\\n\\nThe Kappa architecture is relatively simple in terms of architecture design, unified in production, and a set of logic produces both offline and real time. However, there are relatively large limitations in practical application scenarios. There are few cases in the industry that directly use the Kappa architecture for production and implementation,  and the scene is relatively simple. These problems will also be encountered on our side, and we will also have some thoughts of our own, which will be discussed later.\\n\\n# Business Pain Points\\n\\nIn the take-away business, we also encountered some problems.\\n\\nIn the early stage of the business, in order to meet the business needs, the requirements are generally completed case by case after the requirements are obtained. The business has high real-time requirements. From the perspective of timeliness, there is no opportunity for middle-level precipitation. In the scenario, the business logic is generally directly embedded. This is a simple and effective method that can be imagined. This development mode is relatively common in the early stage of business development.\\n\\nAs shown in the figure above, after getting the data source, it will go through data cleaning, dimension expansion, business logic processing through Storm or Flink, and finally direct business output. Taking this link apart, the data source will repeatedly refer to the same data source, and the operations such as cleaning, filtering, and dimension expansion must be repeated. The only difference is that the code logic of the business is different. IIf there is less business, this model is acceptable, but when the subsequent business volume increases, there will be a situation where whoever develops will be responsible for operation and maintenance, the maintenance workload will increase, and the operations cannot be managed in a unified manner. Moreover, everyone is applying for resources, resulting in a rapid expansion of resource costs, and resources cannot be used intensively and effectively. Therefore, it is necessary to think about how to construct real-time data from the whole data source.\\n\\n# Data features and Application Scenario\\n\\nSo how to build a real-time data warehouse?\\n\\nFirst of all, we need to disassemble this task into what data, what scenarios, and what features these scenarios have in common. For takeaway business scenarios, there are two categories, log class and business category.\\n\\n- Log class: It is characterized by a large amount of data, semi-structured, and deeply nested.Log data has a great feature that once the log stream is formed, it will not change. It will collect all the logs of the platform by means of buried points, and then collect and distribute them uniformly. Just like a tree with really large roots.  The whole process of pushing to the front-end application is just like the process of a tree branching from the root to a branch (the decomposition process from 1 to n). If all businesses search for data from the root, although the path seems to be the shortest, because of the heavy burden,the data retrieval efficiency is low. Log data is generally used for production monitoring and user behavior analysis. The timeliness requirements are relatively high . Generally, the  time window will be 5 minutes or 10 minutes, or up to the current state. The main application is the real-time large screen and real-time features, such as behaviour can immediately perceive the need for waiting every time the user clicks.\\n\\n- Business category: The business class is mainly about business transaction data. Business systems are usually self-contained and distribute data down in the form of Binlog logs. All business systems are transactional, mainly using paradigm modeling methods, which have a structured characteristic and the main part can be seen clearly. However, due to the large number of data tables, multi-table associations are required to express the complete business. So it\'s an integrated machining process from n to 1 .\\n\\nSeveral difficulties faced by business real-time processing:\\n\\n- Diversity of business: Business processes are constantly changing from the beginning to the end, such as from ordering -> payment -> delivery. The business database is changed  on the original basis,and Binlog will generate a lot of changed logs. Business analysis is more focused on the end state, which leads to the problem of data retraction calculation, such as placing an order at 10 o\'clock and canceling it at 13 o\'clock, but hoping to subtract the canceled order at 10 o\'clock.\\n\\n- Business integration: Business analysis data usually cannot be expressed by a single subject, and often many tables are associated to obtain the desired information. When confluent alignment of data is performed in real-time streaming, it often requires large cache processing and is complicated.\\n\\n- The analysis is batch, and the processing process is streaming: for a single data, no analysis can be formed, so the analysis object must be batch, and the data processing is one by one.\\n\\nThe scenarios of log classes and business classes generally exist at the same time and are intertwined. Whether it is Lambda architecture or Kappa architecture, a single application will have some problems, so it is more meaningful to choose the architecture and practice according to the scenario.\\n\\n# Architecture Design of Real-time Data Warehouse\\n\\n### 1.Real-time Architecture: Exploration of Stream-Batch Combination\\n\\nBased on the above problems, we have our own thinking and ideas\uFF0Cit is to deal with different business scenarios through the combination of flow and batch.\\n\\nAs shown in the figure above, the data is collected from the log to the message queue, and then to the ETL process of the data stream. The construction of the basic data stream is unified. Afterwards, for log real-time features, real-time large-screen applications use real-time stream computing. Real-time OLAP batch processing is used for Binlog business analysis.\\n\\nWhat are the Pain Points of Stream Processing Analysis Business? For the paradigm business, both Storm and Flink require a large amount of external memory to achieve business alignment between data streams, which requires a lot of computing resources. Due to the limitation of external memory, the window limitation strategy must be carried out, and may eventually discard some data as a result. After calculation, it is generally stored in Redis as query support, and KV storage has many limitations in dealing with analytical query scenarios.\\n\\nHow to achieve real-time OLAP? Is there a real-time computing engine with its own storage, when the real-time data is entered,it can flexibly and freely calculate within a certain range, and has a certain data carrying capacity, and supports analysis of query responses at the same time? With the development of technology, the current MPP engine is developing very rapidly, and its performance is also improving rapidly, so there is a new possibility in this scenario, just like the Doris engine we use here.\\n\\nThis idea has been practiced in the industry and has become an important exploration direction. For example, Alibaba\'s real-time OLAP solution based on ADB, etc.\\n\\n### 2.Architecture Design of Real-time Data Warehouse\\n\\nFrom the perspective of the entire real-time data warehouse architecture, the first thing to consider is how to manage all real-time data, how to effectively integrate resources, and how to construct data.\\n\\nIn terms of methodology, the real-time and offline are very similar to each other. In the early stage of offline data warehouse, it is also case by case. Consider how to govern it when the scale of data increases to a certain amount. We all know that layering is a very effective way of data governing. So, on the issue of how to manage the real-time data warehouse, the first consideration is also the hierarchical processing logic, as follows:\\n\\n- Data source: At the data source level, offline and real-time data sources are consistent. They are mainly divided into log classes and business classes. Log classes include user logs, DB logs, and server logs.\\n\\n- Real-time detail layer: At the detail level, in order to solve the problem of repeated construction, a unified construction should be carried out.Using the offline data warehouse model to build a unified basic detailed data layer, managed according to the theme, the purpose of the detail layer is to provide directly available data downstream, so the basic layer should be processed uniformly, such as cleaning, filtering, and dimension expansion.\\n\\n- Aggregation layer: The summary layer can directly calculate the result through the concise operator of Flink or Storm. And form a summary of indicators, all indicators are processed at the summary layer, and everyone manages and constructs according to unified specifications, forming a reusable summary result.\\n\\nIn conclusion, from the perspective of the construction of the entire real-time data warehouse,first of all, the data construction needs to be layered, build the framework first, and set the specifications includs  what extent each layer is processed and how each layer is used.The definition of specifications facilitates standardized processing in production.Due to the need to ensure timeliness, don\'t design too many layers when designing.For scenarios with high real-time requirements, you can basically refer to the left side of the figure above. For batch processing requirements, you can import from the real-time detail layer to the real-time OLAP engine, and perform fast retraction calculations based on the OLAP engine\'s own calculation and query capabilities, as shown in the data flow on the right side of the figure above.\\n\\n# Real-time platform construction\\n\\nAfter the architecture is determined, the next consideration is how to build a platform.The construction of the real-time platform is completely attached to the real-time data warehouse management.\\n\\nFirst, abstract the functions and abstract them into components, so that standardized production can be achieved, and systematic guarantees can be further constructed. For the basic processing layer cleaning, filtering, confluence, dimension expansion, conversion, encryption, screening and other functions can be abstracted, and the base layer builds a directly usable data result stream in this componentized way. How to meet diverse needs and how to be compatible with users are the problems that we need to figure out. In this case it may occur problems with redundant processing. In terms of storage, real-time data does not have a history and will not consume too much storage. This redundancy is acceptable.The production efficiency can be improved by means of redundancy, which is an ideological application of changing space for time.\\n\\nThrough the processing of the base layer, all data is deposited in the IDL layer, and written to the base layer of the OLAP engine at the same time, and then the real-time summary layer is calculated. Based on Storm, Flink or Doris, multi-dimensional summary indicators are produced to form a unified summary layer for unified storage distribution.\\n\\nWhen these functions are available, system capabilities such as metadata management, indicator management, data security, SLA, and data quality will be gradually built.\\n\\n### 1.Real-time base layer functions\\n\\nThe construction of the real-time base layer needs to solve some problems.\\n\\nThe first is the problem of repeated reading of a stream. When a Binlog is called, it exists in the form of a DB package. Users may only use one of the tables. If everyone wants to use it, there may be a problem that everyone needs to access this stream. The solution can be deconstructed according to different businesses, restored to the basic data flow layer, made into a paradigm structure according to the needs of the business, and integrated with the theme construction according to the modeling method of the data warehouse.\\n\\nSecondly, we need to encapsulate components, such as basic layer cleaning, filtering, and dimension expansion . Users can write logic by a very simple expression. Trans part is more flexible. For example, converting from one value to another value, for this custom logic expression, we also open custom components, which can develop custom scripts through Java or Python for data processing.\\n\\n### 2.Real-time feature production capabilities\\n\\nFeature production can be expressed logically through SQL syntax, and the underlying logic is adapted, and transparently transmitted to the computing engine, shielding the user\'s dependence on the computing engine.Just like for offline scenarios, currently large companies rarely develop through code, unless there are some special cases, so they can basically be expressed in SQL.\\n\\nAt the functional level, the idea of indicator management is integrated. Atomic indicators, derived indicators, standard calculation apertures, dimension selection, window settings and other operations can be configured in a configurable way.In this way, the production logic can be uniformly parsed and packaged uniformly.\\n\\nAnother question,with the same source code a lot of SQL is written, and each submission will have a data stream which is a waste of resources.Our solution is to produce dynamic metrics through the same data stream, so that metrics can be added dynamically without stopping the service.\\n\\nSo, during the construction of the real-time platform, engineers should consider more about how to use resources more effectively and which links can use resources more economically.\\n\\n### 3.SLA construction\\n\\nSLA mainly solves two problems, one is about the end-to-end SLA, the other is  about the SLA of job productivity. We adopt the method of burying points + reporting.Because the real-time stream is relatively large, the burying point should be as simple as possible, do not bury too many things,can express the business information is enough.The output of each job is reported to the SLA monitoring platform in a unified manner, and the required information is reported at each job point through a unified interface, and finally the end-to-end SLA can be counted.\\n\\nIn real-time production, because the process is very long, it is impossible to control all links, but it can control the efficiency of its own operations, so job SLA is also essential.\\n\\n### 4.Real-time OLAP solution\\n\\nProblems:\\n\\n- Binlog business restoration is complex\uFF1AThere are many changes in the business, and changes at a certain point in time are required. Therefore, sorting and data storage are required, which consumes a lot of memory and CPU resources.\\n\\n- Binlog business association is complex\uFF1AIn stream computing, the relationship between streams and streams is very difficult to express business logic.\\n\\nsolutions\uFF1A\\n\\nTo solve the problem through the OLAP engine with computing power, there is no need to logically map a data stream, and only the problem of real-time and stable data storage needs to be solved.\\n\\nWe use Doris as a high-performance OLAP engine here.Due to the need for derivative calculations between the results generated by the business data and the results, Doris can quickly restore the business by using the unique model or the aggregation model, and can also perform aggregation at the summary layer while restoring the business,and is also designed for reuse.The application layer can be physical or logical view.\\n\\nThis mode focuses on solving the business rollback calculation. For example, when the business state changes, the value needs to be changed at a certain point in history. The cost of using flow calculation in this scenario is very high. The OLAP mode can solve this problem very well.\\n\\n# Real-time use cases\\n\\nIn the end, we use a case to illustrate.For example, merchants want to offer discounts to users based on the number of historical orders placed by users. Merchants need to see how many orders they have placed in history. They must have historical T+1 data and real-time data today.This scenario is a typical Lambda architecture,You can design a partition table in Doris, one is the historical partition, and the other is the today partition. The historical partition can be produced offline. Today\'s indicators can be calculated in real time and written to today\'s partition. When querying, a simple summary.\\n\\nThis scenario seems relatively simple, but the difficulty lies in the fact that many simple problems will become complicated after the number of merchants increases.Therefore, in the future, we will use more business input to precipitate more business scenarios, abstract them to form a unified production plan and function, and support diversified business needs with minimized real-time computing resources, which is also what needs to be achieved in the future. \\n\\nThat\'s all for today, thank you.\\n\\n### about the author:\\n\\nZhu Liang, more than 5 years experience in data warehouse construction in traditional industries, 6 years experience in Internet data warehouse, technical direction involves offline, real-time data warehouse management, systematic capacity building, OLAP system and engine, big data related technologies, focusing on OLAP,and real-time technology frontier development trends.The business direction involves ad hoc query, operation analysis, strategy report product, user portrait, crowd recommendation, experimental evaluation, etc."},{"id":"/xiaomi","metadata":{"permalink":"/blog/xiaomi","source":"@site/blog/xiaomi.md","title":"Best practice of Apache Doris in Xiaomi Group","description":"In order to improve the query performance of the Xiaomi growth analysis platform and reduce the operation and maintenance costs, Xiaomi Group introduced Apache Doris in September 2019. In the past two and a half years, **Apache Doris has been widely used in Xiaomi Group,** **such as business growth analytic platform, realtime dashboards for all business groups,  finance analysis, user profile analysis, advertising reports, A/B testing platform and so on.** This article will share the best practice of Apache Doris in Xiaomi Group.","date":"2022-07-20T00:00:00.000Z","tags":[{"inline":true,"label":"Best Practice","permalink":"/blog/tags/best-practice"}],"hasTruncateMarker":false,"authors":[{"name":"Apache Doris","key":null,"page":null}],"frontMatter":{"title":"Best practice of Apache Doris in Xiaomi Group","description":"In order to improve the query performance of the Xiaomi growth analysis platform and reduce the operation and maintenance costs, Xiaomi Group introduced Apache Doris in September 2019. In the past two and a half years, **Apache Doris has been widely used in Xiaomi Group,** **such as business growth analytic platform, realtime dashboards for all business groups,  finance analysis, user profile analysis, advertising reports, A/B testing platform and so on.** This article will share the best practice of Apache Doris in Xiaomi Group.","date":"2022-07-20","author":"Apache Doris","tags":["Best Practice"],"image":"/images/best-practice.png"},"unlisted":false,"prevItem":{"title":"Best practice of Apache Doris in Meituan","permalink":"/blog/meituan"},"nextItem":{"title":"Apache Doris announced the official release of version 1.1.0","permalink":"/blog/release-note-1.1.0"}},"content":"\x3c!-- \\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n\\n# Background\\n\\nIn order to improve the query performance of the Xiaomi growth analysis platform and reduce the operation and maintenance costs, Xiaomi Group introduced Apache Doris in September 2019. In the past two and a half years, **Apache Doris has been widely used in Xiaomi Group,** **such as business growth analytic platform, realtime dashboards for all business groups,  finance analysis, user profile analysis, advertising reports, A/B testing platform and so on.** This article will share the best practice of Apache Doris in Xiaomi Group. \\n\\n# Business Practice\\n\\nThe typical business practices of Apache Doris in Xiaomi are as follows:\\n\\n## 01 User Access\\n\\nData Factory is a one-stop data development platform developed by Xiaomi for data developers and data analysts. This platform supports data sources such as Doris, Hive, Kudu, Iceberg, ES, Talso, TiDB, MySQL, etc. It also supports computing engines such as Flink, Spark,  Presto,etc.\\n\\nInside Xiaomi, users need to access the Doris service through the data factory. Users need to register in the data factory and complete the approval for building the database. The Doris operation and maintenance classmates will connect according to the descriptions of the business scenarios and data usage expectations submitted by users in the data factory. After completing the access approval, users can use the Doris service to perform operations such as visual table creation and data import in the data factory.\\n\\n## 02 Data import\\n\\nIn Xiaomi\'s business, the two most common ways to import data into Doris are Stream Load and Broker Load. User data will be divided into real-time data and offline data, and users\' real-time and offline data will generally be written to Talos first (Talos is a distributed, high-throughput message queue developed by Xiaomi). The offline data from Talos will be sink to HDFS, and then imported to Doris through the data factory. Users can directly submit Broker Load tasks in the data factory to import large batches of data on HDFS into Doris, In addition, you can run the SparkSQL command in the data factory to query data from Hive, Import the data found in SparkSQL into Doris through Spark-doris-Connector, and encapsulate Stream Load at the bottom layer of Spark-doris-Connector. Real-time data from Talos is generally imported into Doris in two ways. One is to first perform ETL on the data through Flink, and then import small batches of data to Doris through.Flink- Doris-connector encapsulates the Stream Load at the bottom layer. Another way is to import small batches of data into Doris through Stream Load encapsulated by Spark Streaming at regular intervals.\\n\\n## 03 Data Query\\n\\nDoris users of Xiaomi generally analyze and query Doris and display the results through the ShuJing platform.ShuJing is a general-purpose BI analysis tool developed by Xiaomi. Users can query and visualize Doris through ShuJing  platform, and realize user behavior analysis (in order to meet the needs of business event analysis, retention analysis, funnel analysis, path analysis and other behavior analysis needs, We added corresponding UDF and UDAF ) and user profile analysis for Doris.\\n\\n## 04 Compaction Tuning\\n\\nFor Doris, each data import will generate a data version under the relevant data shard (Tablet) of the storage layer, and the Compaction mechanism will asynchronously merge the smaller data versions generated by the import (the detailed principle of the Compaction mechanism can be Refer to the previous article \\"Doris Compaction Mechanism Analysis\\").\\n\\nXiaomi has many high-frequency, high-concurrency, near-real-time import business scenarios, and a large number of small versions will be generated in a short period of time. If Compaction does not merge data versions in time, it will cause version accumulation.On the one hand, too many minor versions will increase the pressure on metadata, and on the other hand, too many versions will affect query performance.In Xiaomi\'s usage scenarios, many tables use the Unique and Aggregate data models, and the query performance is heavily dependent on whether Compaction can merge data versions in time.In our business scenario, the query performance was reduced by tens of times due to delayed version merging, thus affecting online services.When a Compaction happens, it consumes CPU, memory, and disk I/O resources. Too much compaction will take up too many machine resources, affect query performance, and may cause OOM.\\n\\n**In response to this problem of Compaction, we first start from the business side and guide users through the following aspects:**\\n\\n- Set reasonable partitions and buckets for tables to avoid generating too many data fragments.\\n\\n- Standardize the user\'s data import operation, reduce the frequency of data import, increase the amount of data imported in a single time, and reduce the pressure of Compaction.\\n\\n- Avoid using delete operations too much.The delete operation will generate a delete version under the relevant data shard in the storage layer.The Cumulative Compaction task will be truncated when the delete version is encountered. This task can only merge the data version after the Cumulative Point and before the delete version, move the Cumulative Point to the delete version, and hand over the delete version to the subsequent Base Compaction task. to process. If you use the delete operation too much, too many delete versions will be generated under the Tablet, which will cause the Cumulative Compaction task to slow down the progress of version merging. Using the delete operation does not actually delete the data from the disk, but records the deletion conditions in the delete version. When the data is queried, the deleted data will be filtered out by Merge-On-Read. Only the delete version is merged by the Base Compaction task. After that, the data to be deleted by the delete operation can be cleared from the disk as expired data with the Stale Rowset. If you need to delete the data of an entire partition, you can use the truncated partition operation instead of the delete operation.\\n\\n**Second, we tuned Compaction from the operation and maintenance side:**\\n\\n- According to different business scenarios, different Compaction parameters (Compaction strategy, number of threads, etc.) are configured for different clusters.\\n\\n- Appropriately lowers the priority of the Base Compaction task and increases the priority of the Cumulative Compaction task, because the Base Compaction task takes a long time to execute and has serious write amplification problems, while the Cumulative Compaction task executes faster and can quickly merge a large number of small versions.\\n\\n- Version backlog alarm, dynamic adjustment of Compaction parameters.When the Compaction Producer produces Compaction tasks, it will update the corresponding metric.It records the value of the largest Compaction Score on the BE node. You can check the trend of this indicator through Grafana to determine whether there is a version backlog. In addition, we have added a Version backlog alert.In order to facilitate the adjustment of Compaction parameters, we have optimized the code level to support dynamic adjustment of the Compaction strategy and the number of Compaction threads at runtime, avoiding the need to restart the process when adjusting the Compaction parameters.\\n\\n- Supports manual triggering of the Compaction task of the specified Table and data shards under the specified Partition, and improves the Compaction priority of the specified Table and data shards under the specified Partition.\\n\\n# Monitoring and Alarm Management\\n\\n## 01 Monitoring System\\n\\nPrometheus will regularly pull Metrics metrics from Doris\'s FE and BE and display them in the Grafana monitoring panel.The service metadata based on QingZhou Warehouse will be automatically registered in Zookeeper, and Prometheus will regularly pull the latest cluster metadata information from Zookeeper and display it dynamically in the Grafana monitoring panel.\uFF08Qingzhou Data Warehouse is a data warehouse constructed by the Qingzhou platform based on the operation data of Xiaomi\'s full-scale big data service. It consists of 2 base tables and 30+ dimension tables.Covers the whole process data such as resources, server cmdb, cost, process status and so on when big data components are running\uFF09We have also added statistics and display boards for common troubleshooting data such as Doris large query list, real-time write data volume, data import transaction numbers, etc. in Grafana.In Grafana, we also added statistics and display boards for common troubleshooting data such as the Doris big query list, the amount of real-time data written, and the number of data import transactions, so that alarms can be linked. When the cluster is abnormal, Doris\' operation and maintenance students can locate the cause of the cluster failure in the shortest time.\\n\\n## 02  Falcon \\n\\nFalcon is a monitoring and alarm system widely used inside Xiaomi.Because Doris provides a relatively complete metrics interface, which can easily provide monitoring functions based on Prometheus and Grafana, we only use Falcon\'s alarm function in the Doris service.For different levels of faults in Doris, we define alarms as three levels of P0, P1 and P2:\\n\\n- P2 alarm (alarm level is low): single node failure alarm.When a single node indicator or process status is abnormal, an alarm is generally issued as a P2 level.The alarm information is sent to the members of the alarm group in the form of Xiaomi Office messages.(Xiaomi Office is a privatized deployment product of ByteDance Feishu in Xiaomi, and its functions are similar to Feishu.)\\n\\n- P1 alarm (alarm level is higher):In a short period of time (within 3 minutes), the cluster will issue a P1 level alarm if there are short-term exceptions such as increased query delay and abnormal writing,etc.The alarm information is sent to the members of the alarm group in the form of Xiaomi Office messages.P1 level alarms require Oncall engineers to respond and provide feedback.\\n\\n- P0 alarm (alarm level is high):In a long period of time (more than 3 minutes), the cluster will issue a P0 level alarm if there are exceptions such as increased query delay and abnormal writing,etc.Alarm information is sent in the form of Xiaomi office messages and phone alarms.P0 level alarm requires Oncall engineers to respond within 1 minute and coordinate resources for failure recovery and review preparation.\\n\\n## 03  Cloud-Doris \\n\\ncloud-Doris is a data collection component developed by Xiaomi for the internal Doris service. Its main capability is to detect the availability of the Doris service and collect the cluster indicator data of internal concern.For example, Cloud-Doris can periodically simulate users reading and writing to the Doris system to detect the availability of services.If the cluster has abnormal availability, it will be alerted through Falcon.Collect user\'s read and write data, and then generate user bill.Collect information such as table-level data volume, unhealthy copies, and oversized Tablets, and send alarms to abnormal information through Falcon.\\n\\n## 04 QingZhou inspection\\n\\nFor chronic hidden dangers such as capacity, user growth, resource allocation, etc., we use the unified QingZhou big data service inspection platform for inspection and reporting.The inspection generally consists of two parts:Service-specific inspections and basic indicator inspections.Among them, the service-specific inspection refers to the indicators that are unique to each big data service and cannot be used universally.For Doris, it mainly includes: Quota, number of shard copies, number of single table columns, number of table partitions, etc.By increasing the inspection method, the chronic hidden dangers that are difficult to be alarmed in advance can be well avoided, which provides support for the failure-free major festivals.\\n\\n# Failure Recovery\\n\\nWhen an online cluster fails, the first principle should be to quickly restore services.If the cause of the failure is clear, handle it according to the specific cause and restore the service.If the cause of the failure is not clear, you should try restarting the process as soon as you keep the snapshot to restore the service.\\n\\n## 01 Access Failures Handling\\n\\nDoris uses Xiaomi LVS as the access layer, which is similar to the LB service of open source or public cloud, and provides layer 4 or layer 7 traffic load scheduling capability.After Doris binds a reasonable port,Generally speaking, if an abnormality occurs in a single FE node, it will be automatically kicked out, and the service can be restored without the user\'s perception, and an alarm will be issued for the abnormal node.Of course, for FE faults that cannot be processed in a short time, we will first adjust the weight of the faulty node to 0 or delete the abnormal node from LVS first to prevent unpredictable problems caused by process detection exceptions.\\n\\n## 02 Node Failure Handling\\n\\nFor FE node failures, if the cause of the failure cannot be quickly located, it is generally necessary to keep thread snapshots and memory snapshots and restart the process.\\n\\n```undefined\\njstack \u8FDB\u7A0BID >> \u5FEB\u7167\u6587\u4EF6\u540D.jstack\\n```\\n\\nSave a memory snapshot of FE with the command:\\n\\n```undefined\\njmap -dump:live,format=b,file=\u5FEB\u7167\u6587\u4EF6\u540D.heap \u8FDB\u7A0BID\\n```\\n\\nIn the case of version upgrade or some unexpected scenarios, the image of the FE node may have abnormal metadata, and the abnormal metadata may be synchronized to other FE, resulting in all FE not working.Once a failed image is discovered, the fastest recovery option is to use Recovery mode to stop FE elections and replace the failed image with the backup image.Of course, it is not easy to backup images all the time.Since this failure is common in cluster upgrades, we recommend adding simple local image backup logic to the cluster upgrade procedure.Ensure that a copy of the current and latest image data will be retained before each upgrade starts the FE process.For BE node failure, if the process crashes, a core file will be generated, and minos will automatically pull the process;If the task is stuck, you need to restart the process after retaining the thread snapshot with the following command:\\n\\n```undefined\\npstack \u8FDB\u7A0BID >> \u5FEB\u7167\u6587\u4EF6\u540D.pstack\\n```\\n\\n# Concluding Remarks\\n\\nApache Doris has been widely used by Xiaomi since the first use of open source software Apache Doris by Xiaomi Group in September 2019.At present, it has served dozens of businesses of Xiaomi, with dozens of clusters and hundreds of nodes, and a set of data ecology with Apache Doris as the core has been formed within Xiaomi.In order to improve the efficiency of operation and maintenance, Xiaomi has also developed a complete set of automated management and operation and maintenance systems around Doris.With the increasing number of services, Doris also exposed some problems. For example, there was no better resource isolation mechanism in the past version, and services would affect each other. In addition, system monitoring needs to be further improved.With the rapid development of the community, more and more small partners have participated in the community construction, the vectorized engine has been transformed, the transformation of the query optimizer is in full swing, and Apache Doris is gradually maturing."},{"id":"/release-note-1.1.0","metadata":{"permalink":"/blog/release-note-1.1.0","source":"@site/blog/release-note-1.1.0.md","title":"Apache Doris announced the official release of version 1.1.0","description":"Dear community, we are pleased to announce that we have officially released Apache Doris 1.1 on July 14, 2022! This is the first release version after Apache Doris graduated from the Apache incubator and became an Apache Top-Level Project.","date":"2022-07-14T00:00:00.000Z","tags":[{"inline":true,"label":"Release Notes","permalink":"/blog/tags/release-notes"}],"hasTruncateMarker":false,"authors":[{"name":"Apache Doris","key":null,"page":null}],"frontMatter":{"title":"Apache Doris announced the official release of version 1.1.0","description":"Dear community, we are pleased to announce that we have officially released Apache Doris 1.1 on July 14, 2022! This is the first release version after Apache Doris graduated from the Apache incubator and became an Apache Top-Level Project.","date":"2022-07-14","author":"Apache Doris","tags":["Release Notes"],"image":"/images/release-notes.png"},"unlisted":false,"prevItem":{"title":"Best practice of Apache Doris in Xiaomi Group","permalink":"/blog/xiaomi"},"nextItem":{"title":"Announcing open source realtime analytical database Apache Doris as a top-level project","permalink":"/blog/Annoucing"}},"content":"Dear community, we are pleased to announce that we have officially released Apache Doris 1.1 on July 14, 2022! This is the first release version after Apache Doris graduated from the Apache incubator and became an Apache Top-Level Project.\\n\\nIn version 1.1, we realized the full vectorization of the computing layer and storage layer, and officially enabled the vectorized execution engine as a stable function. All queries are executed by the vectorized execution engine by default, and the performance is 3-5 times higher than the previous version. It increases the ability to access the external tables of Apache Iceberg and supports federated query of data in Doris and Iceberg, and expands the analysis capabilities of Apache Doris on the data lake; on the basis of the original LZ4, the ZSTD compression algorithm is added , further improves the data compression rate; fixed many performance and stability problems in previous versions, greatly improving system stability. Downloading and using is recommended.\\n\\n## Upgrade Notes\\n\\n### The vectorized execution engine is enabled by default\\n\\nIn version 1.0, we introduced the vectorized execution engine as an experimental feature and Users need to manually enable it when executing queries by configuring the session variables through `set batch_size = 4096` and `set enable_vectorized_engine = true` .\\n\\nIn version 1.1, we officially fully enabled the vectorized execution engine as a stable function. The session variable `enable_vectorized_engine` is set to true by default. All queries are executed by default through the vectorized execution engine.\\n\\n### BE Binary File Renaming\\n\\nBE binary file has been renamed from palo_be to doris_be . Please pay attention to modifying the relevant scripts if you used to rely on process names for cluster management and other operations.\\n\\n### Segment storage format upgrade\\n\\nThe storage format of earlier versions of Apache Doris was Segment V1. In version 0.12, we had implemented Segment V2 as a new storage format, which introduced Bitmap indexes, memory tables, page cache, dictionary compression, delayed materialization and many other features. Starting from version 0.13, the default storage format for newly created tables is Segment V2, while maintaining compatibility with the Segment V1 format.\\n\\nIn order to ensure the maintainability of the code structure and reduce the additional learning and development costs caused by redundant historical codes, we have decided to no longer support the Segment v1 storage format from the next version. It is expected that this part of the code will be deleted in the Apache Doris 1.2 version, and all users who are still using the Segment V1 storage format must complete the data format conversion in version 1.1. Please refer to the following link for the operation manual:\\n\\n[https://doris.apache.org/zh-CN/docs/1.0/administrator-guide/segment-v2-usage](https://doris.apache.org/zh-CN/docs/1.0/administrator-guide/segment-v2-usage)\\n\\n### Normal Upgrade\\n\\nFor normal upgrade operations, you can perform rolling upgrades according to the cluster upgrade documentation on the official website.\\n\\n[https://doris.apache.org/zh-CN/docs/admin-manual/cluster-management/upgrade](https://doris.apache.org/zh-CN/docs/admin-manual/cluster-management/upgrade)\\n\\n## Features\\n\\n### Support random distribution of data [experimental]\\n\\nIn some scenarios (such as log data analysis), users may not be able to find a suitable bucket key to avoid data skew, so the system needs to provide additional distribution methods to solve the problem.\\n\\nTherefore, when creating a table you can set `DISTRIBUTED BY random BUCKET number`to use random distribution, the data will be randomly written to a single tablet when importing to reduce the data fanout during the loading process. And reduce resource overhead and improve system stability.\\n\\n### Support for creating Iceberg external tables[experimental]\\n\\nIceberg external tables provide Apache Doris with direct access to data stored in Iceberg. Through Iceberg external tables, federated queries on data stored in local storage and Iceberg can be implemented, which saves tedious data loading work, simplifies the system architecture for data analysis, and performs more complex analysis operations.\\n\\nIn version 1.1, Apache Doris supports creating Iceberg external tables and querying data, and supports automatic synchronization of all table schemas in the Iceberg database through the REFRESH command.\\n\\n### Added ZSTD compression algorithm\\n\\nAt present, the data compression method in Apache Doris is uniformly specified by the system, and the default is LZ4. For some scenarios that are sensitive to data storage costs, the original data compression ratio requirements cannot be met.\\n\\nIn version 1.1, users can set \\"compression\\"=\\"zstd\\" in the table properties to specify the compression method as ZSTD when creating a table. In the 25GB 110 million lines of text log test data, the highest compression rate is nearly 10 times, which is 53% higher than the original compression rate, and the speed of reading data from disk and decompressing it is increased by 30%.\\n\\n## Improvements\\n\\n### More comprehensive vectorization support\\n\\nIn version 1.1, we implemented full vectorization of the compute and storage layers, including:\\n\\nImplemented vectorization of all built-in functions\\n\\nThe storage layer implements vectorization and supports dictionary optimization for low-cardinality string columns\\n\\nOptimized and resolved numerous performance and stability issues with the vectorization engine.\\n\\nWe tested the performance of Apache Doris version 1.1 and version 0.15 on the SSB and TPC-H standard test datasets:\\n\\nOn all 13 SQLs in the SSB test data set, version 1.1 is better than version 0.15, and the overall performance is improved by about 3 times, which solves the problem of performance degradation in some scenarios in version 1.0;\\n\\nOn all 22 SQLs in the TPC-H test data set, version 1.1 is better than version 0.15, the overall performance is improved by about 4.5 times, and the performance of some scenarios is improved by more than ten times;\\n\\n![](/images/blogs/1.1.0/release-note-1.1.0-SSB.png)\\n\\n<p align=\'center\'>SSB Benchmark</p>\\n\\n![](/images/blogs/1.1.0/release-note-1.1.0-TPC-H.png)\\n\\n<p align=\'center\'>TPC-H Benchmark</p>\\n\\n**Performance test report**\\n\\n[https://doris.apache.org/zh-CN/docs/benchmark/ssb](https://doris.apache.org/zh-CN/docs/benchmark/ssb)\\n\\n[https://doris.apache.org/zh-CN/docs/benchmark/tpch](https://doris.apache.org/zh-CN/docs/benchmark/tpch)\\n\\n### Compaction logic optimization and real-time guarantee\\n\\nIn Apache Doris, each commit will generate a data version. In high concurrent write scenarios, -235 errors are prone to occur due to too many data versions and untimely compaction, and query performance will also decrease accordingly.\\n\\nIn version 1.1, we introduced QuickCompaction, which will actively trigger compaction when the data version increases. At the same time, by improving the ability to scan fragment metadata, it can quickly find fragments with too many data versions and trigger compaction. Through active triggering and passive scanning, the real-time problem of data merging is completely solved.\\n\\nAt the same time, for high-frequency small file cumulative compaction, the scheduling and isolation of compaction tasks is implemented to prevent the heavyweight base compaction from affecting the merging of new data.\\n\\nFinally, for the merging of small files, the strategy of merging small files is optimized, and the method of gradient merging is adopted. Each time the files participating in the merging belong to the same data magnitude, it prevents versions with large differences in size from merging, and gradually merges hierarchically. , reducing the number of times a single file participates in merging, which can greatly save the CPU consumption of the system.\\n\\nWhen the data upstream maintains a write frequency of 10w per second (20 concurrent write tasks, 5000 rows per job, and checkpoint interval of 1s), version 1.1 behaves as follows:\\n\\n-   Quick data consolidation: Tablet version remains below 50 and compaction score is stable. Compared with the -235 problem that frequently occurred during high concurrent writing in the previous version, the compaction merge efficiency has been improved by more than 10 times.\\n\\n-   Significantly reduced CPU resource consumption: The strategy has been optimized for small file Compaction. In the above scenario of high concurrent writing, CPU resource consumption is reduced by 25%;\\n\\n-   Stable query time consumption: The overall orderliness of data is improved, and the fluctuation of query time consumption is greatly reduced. The query time consumption during high concurrent writing is the same as that of only querying, and the query performance is improved by 3-4 times compared with the previous version.\\n\\n### Read efficiency optimization for Parquet and ORC files\\n\\nBy adjusting arrow parameters, arrow\'s multi-threaded read capability is used to speed up Arrow\'s reading of each row_group, and it is modified to SPSC model to reduce the cost of waiting for the network through prefetching. After optimization, the performance of Parquet file import is improved by 4 to 5 times.\\n\\n### Safer metadata Checkpoint\\n\\nBy double-checking the image files generated after the metadata checkpoint and retaining the function of historical image files, the problem of metadata corruption caused by image file errors is solved.\\n\\n## Bugfix\\n\\n### Fix the problem that the data cannot be queried due to the missing data version.(Serious)\\n\\nThis issue was introduced in version 1.0 and may result in the loss of data versions for multiple replicas.\\n\\n### Fix the problem that the resource isolation is invalid for the resource usage limit of loading tasks (Moderate)\\n\\nIn 1.1, the broker load and routine load will use Backends with specified resource tags to do the load.\\n\\n### Use HTTP BRPC to transfer network data packets over 2GB (Moderate)\\n\\nIn the previous version, when the data transmitted between Backends through BRPC exceeded 2GB,\\nit may cause data transmission errors.\\n\\n## Others\\n\\n### Disabling Mini Load\\n\\nThe `/_load` interface is disabled by default, please use `the /_stream_load` interface uniformly.\\nOf course, you can re-enable it by turning off the FE configuration item `disable_mini_load`.\\n\\nThe Mini Load interface will be completely removed in version 1.2.\\n\\n### Completely disable the SegmentV1 storage format\\n\\nData in SegmentV1 format is no longer allowed to be created. Existing data can continue to be accessed normally.\\nYou can use the `ADMIN SHOW TABLET STORAGE FORMAT` statement to check whether the data in SegmentV1 format\\nstill exists in the cluster. And convert to SegmentV2 through the data conversion command\\n\\nAccess to SegmentV1 data will no longer be supported in version 1.2.\\n\\n### Limit the maximum length of String type\\n\\nIn previous versions, String types were allowed a maximum length of 2GB.\\nIn version 1.1, we will limit the maximum length of the string type to 1MB. Strings longer than this length cannot be written anymore.\\nAt the same time, using the String type as a partitioning or bucketing column of a table is no longer supported.\\n\\nThe String type that has been written can be accessed normally.\\n\\n### Fix fastjson related vulnerabilities\\n\\nUpdate to Canal version to fix fastjson security vulnerability.\\n\\n### Added `ADMIN DIAGNOSE TABLET` command\\n\\nUsed to quickly diagnose problems with the specified tablet.\\n\\n## Download to Use\\n\\n### Download Link\\n\\n[hhttps://doris.apache.org/download](https://doris.apache.org/download)\\n\\n### Feedback\\n\\nIf you encounter any problems with use, please feel free to contact us through GitHub discussion forum or Dev e-mail group anytime.\\n\\nGitHub Forum: [https://github.com/apache/doris/discussions](https://github.com/apache/doris/discussions)\\n\\nMailing list: [dev@doris.apache.org](dev@doris.apache.org)\\n\\n## Thanks\\n\\nThanks to everyone who has contributed to this release:\\n\\n```\\n\\n@adonis0147\\n\\n@airborne12\\n\\n@amosbird\\n\\n@aopangzi\\n\\n@arthuryangcs\\n\\n@awakeljw\\n\\n@BePPPower\\n\\n@BiteTheDDDDt\\n\\n@bridgeDream\\n\\n@caiconghui\\n\\n@cambyzju\\n\\n@ccoffline\\n\\n@chenlinzhong\\n\\n@daikon12\\n\\n@DarvenDuan\\n\\n@dataalive\\n\\n@dataroaring\\n\\n@deardeng\\n\\n@Doris-Extras\\n\\n@emerkfu\\n\\n@EmmyMiao87\\n\\n@englefly\\n\\n@Gabriel39\\n\\n@GoGoWen\\n\\n@gtchaos\\n\\n@HappenLee\\n\\n@hello-stephen\\n\\n@Henry2SS\\n\\n@hewei-nju\\n\\n@hf200012\\n\\n@jacktengg\\n\\n@jackwener\\n\\n@Jibing-Li\\n\\n@JNSimba\\n\\n@kangshisen\\n\\n@Kikyou1997\\n\\n@kylinmac\\n\\n@Lchangliang\\n\\n@leo65535\\n\\n@liaoxin01\\n\\n@liutang123\\n\\n@lovingfeel\\n\\n@luozenglin\\n\\n@luwei16\\n\\n@luzhijing\\n\\n@mklzl\\n\\n@morningman\\n\\n@morrySnow\\n\\n@nextdreamblue\\n\\n@Nivane\\n\\n@pengxiangyu\\n\\n@qidaye\\n\\n@qzsee\\n\\n@SaintBacchus\\n\\n@SleepyBear96\\n\\n@smallhibiscus\\n\\n@spaces-X\\n\\n@stalary\\n\\n@starocean999\\n\\n@steadyBoy\\n\\n@SWJTU-ZhangLei\\n\\n@Tanya-W\\n\\n@tarepanda1024\\n\\n@tianhui5\\n\\n@Userwhite\\n\\n@wangbo\\n\\n@wangyf0555\\n\\n@weizuo93\\n\\n@whutpencil\\n\\n@wsjz\\n\\n@wunan1210\\n\\n@xiaokang\\n\\n@xinyiZzz\\n\\n@xlwh\\n\\n@xy720\\n\\n@yangzhg\\n\\n@Yankee24\\n\\n@yiguolei\\n\\n@yinzhijian\\n\\n@yixiutt\\n\\n@zbtzbtzbt\\n\\n@zenoyang\\n\\n@zhangstar333\\n\\n@zhangyifan27\\n\\n@zhannngchen\\n\\n@zhengshengjun\\n\\n@zhengshiJ\\n\\n@zingdle\\n\\n@zuochunwei\\n\\n@zy-kkk\\n```"},{"id":"/Annoucing","metadata":{"permalink":"/blog/Annoucing","source":"@site/blog/Annoucing.md","title":"Announcing open source realtime analytical database Apache Doris as a top-level project","description":"Apache Doris is a modern, high-performance and real-time analytical database based on MPP. It is well known for its high-performance and easy-to-use. It can return query results under massive data within only sub-seconds. It can support not only high concurrent point query scenarios, but also complex analysis scenarios with high throughput. Based on this, Apache Doris can be well applied in many business fields, such as multi-dimensional reporting, user portrait, ad-hoc query, real-time dashboard and so on.","date":"2022-06-16T00:00:00.000Z","tags":[{"inline":true,"label":"Top News","permalink":"/blog/tags/top-news"}],"hasTruncateMarker":false,"authors":[{"name":"morningman","key":null,"page":null}],"frontMatter":{"title":"Announcing open source realtime analytical database Apache Doris as a top-level project","description":"Apache Doris is a modern, high-performance and real-time analytical database based on MPP. It is well known for its high-performance and easy-to-use. It can return query results under massive data within only sub-seconds. It can support not only high concurrent point query scenarios, but also complex analysis scenarios with high throughput. Based on this, Apache Doris can be well applied in many business fields, such as multi-dimensional reporting, user portrait, ad-hoc query, real-time dashboard and so on.","date":"2022-06-16","author":"morningman","tags":["Top News"],"image":"/images/live-streaming.png"},"unlisted":false,"prevItem":{"title":"Apache Doris announced the official release of version 1.1.0","permalink":"/blog/release-note-1.1.0"},"nextItem":{"title":"Apache Doris(Incubating) announced 1.0.0 release","permalink":"/blog/release-note-1.0.0"}},"content":"Apache Doris is a modern, high-performance and real-time analytical database based on MPP. It is well known for its high-performance and easy-to-use. It can return query results under massive data within only sub-seconds. It can support not only high concurrent point query scenarios, but also complex analysis scenarios with high throughput. Based on this, Apache Doris can be well applied in many business fields, such as multi-dimensional reporting, user portrait, ad-hoc query, real-time dashboard and so on.\\n\\nApache Doris was first born in the Palo Project within Baidu\'s advertising report business and officially opened source in 2017. It was donated by Baidu to Apache foundation for incubation in July 2018, and then incubated and operated by members of the podling project management committee (PPMC) under the guidance of Apache incubator mentors.\\n\\nWe are very proud that Doris graduated from Apache incubator successfully. It is an important milestone. In the whole incubating period, with the guidance of Apache Way and the help of incubator mentors, we learned how to develop our project and community in Apache Way, and have achieved great growth in this process.\\n\\nAt present, Apache Doris community has gathered more than 300 contributors from nearly 100 enterprises in different industries, and the number of active contributors per month is close to 100. During the incubation period, Apache Doris released a total of 8 major versions and completed many major functions, including storage engine upgrade, vectorization execution engine and so on, and released 1.0 version. It is the strength of these open source contributors that makes Apache Doris achieve today\'s results.\\n\\nAt the same time, Apache Doris now has a wide range of users in China and even around the world. Up to now, Apache Doris has been applied in the production environment of more than 500 enterprises around the world. Among the top 50 Internet companies in China by market value or valuation, more than 80% are long-term users of Apache Doris, including Baidu, Meituan, Xiaomi, JD, ByteDance, Tencent, Kwai, Netease, Sina, 360 and other well-known companies. It also has rich applications in some traditional industries, such as finance, energy, manufacturing, telecommunications and other fields.\\n\\nYou can quickly build a simple, easy-to-use and powerful data analysis platform based on Apache Doris, which is very easy to start, and the learning cost is very low. In addition, the distributed architecture of Apache Doris is very simple, which can greatly reduce the workload of system operation and maintenance. This is also the key factor for more and more users to choose Apache Doris.\\n\\nAs a mature analytical database project, Apache Doris has the following advantages:\\n\\n-   Excellent performance: it is equipped with an efficient column storage engine, which not only reduces the amount of data scanning, but also implements an ultra-high data compression ratio. At the same time, Doris also provides a rich index structure to speed up data reading and filtering. Using the partition and bucket pruning function, Doris can support ultra-high concurrency of online service business, and a single node can support up to thousands of QPS. Further, Apache Doris combines the vectorization execution engine to give full play to the modern CPU parallel computing power, supplemented by intelligent materialized view technology to accelerate pre-aggregation, and can simultaneously carry out planning based and cost based query optimization through the query optimizer. Through the above methods, Doris can reach ultimate query performance.\\n\\n-   Easy to use: it supports ANSI SQL syntax, including single table aggregation, sorting, filtering and multi table join, sub query, etc. it also supports complex SQL syntax such as window function and grouping set. At the same time, users can expand system functions through UDF, UDAF and other user-defined functions. In addition, Apache Doris is also compatible with MySQL protocol. Users can access Doris through various client tools and support seamless connection with BI tools.\\n\\n-   Streamlined architecture: the system has only two modules \u2014\u2014 frontend (FE) and backend (BE). The FE node is responsible for the access of user requests, the analysis of query plans, metadata storage and cluster management, and the BE node is responsible for the implementation of data storage and query plans. It is a complete distributed database management system. Users can run the Apache Doris cluster without installing any third-party management and control components, and the deployment and upgrade process are very simple. At the same time, any module can support horizontal expansion, and the cluster can be expanded up to hundreds of nodes, supporting the storage of more than 10PB of ultra large scale data.\\n\\n-   Scalability and reliability: it supports the storage of multiple replicas of data. The cluster is able to self-healing. Its own distributed management framework can automatically manage the distribution, repair and balance of data replicas. When the replicas are damaged, the system can automatically perceive and repair them. When a node is expanded, it can be completed with only one SQL command, and the data replicas will be automatically rebalanced among nodes without manual intervention or operation. Whether it is capacity expansion, capacity reduction, single node failure or upgrading, the system does not need to stop running, and can normally provide stable and reliable online services.\\n\\n-   Ecological enrichment: It provides rich data synchronisation methods, supports fast loading of data from localhost, Hadoop, Flink, Spark, Kafka, SeaTunnel and other systems, and can also directly access data in MySQL, PostgreSQL, Oracle, S3, Hive, Iceberg, Elasticsearch and other systems without data replication. At the same time, the data stored in Doris can also be read by Spark and Flink, and can be output to the upstream data application for display and analysis.\\n\\nGraduation is not the ultimate goal, it is the starting point of a new journey. In the past, our goal of launching Doris was to provide more people with better data analysis tools and solve their data analysis problems. Becoming an Apache top-level project is not only an affirmation of the hard work of all contributors to the Apache Doris community in the past, but also means that we have established a strong, prosperous and sustainable open source community under the guidance of Apache Way.In the future, we will continue to operate the community in the Way of Apache. I believe we will attract more excellent open source contributors to participate in the community, and the community will further grow with the help of all contributors.\\n\\nApache Doris will carry out more challenging and meaningful work in the future, including new query optimizer, support for Lakehouse integration, and architecture evolution for cloud infrastructure. More open source technology enthusiasts are welcome to join the Apache Doris community and grow together.\\n\\nOnce again, we sincerely thank all contributors who participated in the construction of Apache Doris community and all users who use Apache Doris and constantly put forward improvement suggestions. At the same time, we also thank our incubator mentors, IPMC members and friends in various open source project communities who have continuously encouraged, supported and helped us all the way.\\n\\n**Apache Doris GitHub:**\\n\\n[https://github.com/apache/doris](https://github.com/apache/doris)\\n\\n**Apache Doris website:**\\n\\n[http://doris.apache.org](http://doris.apache.org)\\n\\n**Please contact us via:**\\n\\n[dev@doris.apache.org.](dev@doris.apache.org.)\\n\\n**See How to subscribe:**\\n\\n[https://doris.apache.org/community/subscribe-mail-list](https://doris.apache.org/community/subscribe-mail-list/)"},{"id":"/release-note-1.0.0","metadata":{"permalink":"/blog/release-note-1.0.0","source":"@site/blog/release-note-1.0.0.md","title":"Apache Doris(Incubating) announced 1.0.0 release","description":"Dear community friends, we are happy to announce that Apache Doris (incubating) has officially released the 1.0 Release version on April 18, 2022!","date":"2022-04-18T00:00:00.000Z","tags":[{"inline":true,"label":"Release Notes","permalink":"/blog/tags/release-notes"}],"hasTruncateMarker":false,"authors":[{"name":"Apache Doris","key":null,"page":null}],"frontMatter":{"title":"Apache Doris(Incubating) announced 1.0.0 release","description":"Dear community friends, we are happy to announce that Apache Doris (incubating) has officially released the 1.0 Release version on April 18, 2022!","date":"2022-04-18","author":"Apache Doris","tags":["Release Notes"],"image":"/images/release-notes.png"},"unlisted":false,"prevItem":{"title":"Announcing open source realtime analytical database Apache Doris as a top-level project","permalink":"/blog/Annoucing"},"nextItem":{"title":"Apache Doris(Incubating) annoucned 0.15.0 release","permalink":"/blog/release-note-0.15.0"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n# Apache Doris(Incubating) 1.0.0 Release\\n\\nDear community friends, after several months, we are happy to announce that Apache Doris (incubating) has officially released the 1.0 Release version on April 18, 2022! **This is the first 1-bit version of Apache Doris since it was incubated by the Apache Foundation, and it is also the version with the largest refactoring of the core code of Apache Doris so far****! **With **114 Contributors** committing **over 660 optimizations and fixes** for Apache Doris, thank you to everyone who makes Apache Doris even better!\\n\\nIn version 1.0, we introduced important functions such as vectorized execution engine, Hive external table, Lateral View syntax and Table Function table function, Z-Order data index, Apache SeaTunnel plug-in, etc., and added support for synchronous update and deletion of data in Flink CDC. Support, optimize many problems in the process of data import and query, and comprehensively enhance the query performance, ease of use, stability and other special effects of Apache Doris. Welcome to download and use! Click \\"**Read the original text**\\" at the end of the article to go directly to the download address.\\n\\nEvery day that has not been published, there are countless contributors behind it, who dare not stop for half a minute. Here we would like to especially thank the small partners from SIG (Special Interest Group) such as **vectorized execution engine, query optimizer, and visual operation and maintenance platform**. Since the establishment of the Apache Doris Community SIG group in August 2021, data from more than ten companies including Baidu, Meituan, Xiaomi, JD, Shuhai, ByteDance, Tencent, NetEase, Alibaba, PingCAP, Nebula Graph, etc. Ten contributors** joined the SIG as the first members, and for the first time completed the development of such major functions as the vectorized execution engine, query optimizer, and Doris Manager visual monitoring operation and maintenance platform in the form of open source collaboration of special groups. **During more than half a year, conducting technical research and sharing dozens of times, holding nearly 100 remote meetings, accumulatively submitting hundreds of Commits, involving more than 100,000 lines of code**, it is precisely because of their contributions , only the 1.0 version came out, let us once again express our most sincere thanks for their hard work!\\n\\nAt the same time, the number of Apache Doris contributors has exceeded 300, the number of monthly active contributors has exceeded 60, and the average weekly number of Commits submitted in recent weeks has also exceeded 80. The scale and activity of developers gathered by the community There has been a huge improvement. We are very much looking forward to having more small partners participate in the community contribution, and work with us to build Apache Doris into the world\'s top analytical database. We also hope that all small partners can reap valuable growth with us. If you would like to participate in the community, please contact us via the developer email dev@doris.apache.org.\\n\\nWe welcome you to contact us with any questions during the use process through GitHub Discussion or Dev mail group, and we look forward to your participation in community discussions and construction.\\n\\n\\n## Important update \\n\\n### Vectorized Execution Engine [Experimental]\\n\\nIn the past, the SQL execution engine of Apache Doris was designed based on the row-based memory format and the traditional volcano model. There was unnecessary overhead in performing SQL operator and function operations, which led to the limited efficiency of the Apache Doris execution engine, which did not Adapt to the architecture of modern CPUs. The goal of the vectorized execution engine is to replace the current row-based SQL execution engine of Apache Doris, fully release the computing power of modern CPUs, break through the performance limitations on the SQL execution engine, and exert extreme performance.\\n\\nBased on the characteristics of modern CPUs and the execution characteristics of the volcano model, the vectorized execution engine redesigned the SQL execution engine in the columnar storage system:\\n\\n- Reorganized the data structure of memory, replaced Tuple with Column, improved Cache affinity, branch prediction and prefetch memory friendliness during calculation\\n- Type judgment is performed in batches. In this batch, the type determined during type judgment is used, and the virtual function cost of type judgment of each line is allocated to the batch level.\\n- Through batch-level type judgment, virtual function calls are eliminated, allowing the compiler to have the opportunity for function inlining and SIMD optimization\\n\\nThis greatly improves the efficiency of the CPU when executing SQL and improves the performance of SQL queries.\\n\\nIn Apache Doris version 1.0, enabling the vectorized execution engine with set batch_size = 4096 and set enable_vectorized_engine = true can significantly improve query performance in most cases. Under the SSB and OnTime standard test datasets, the overall performance of the two scenarios of multi-table association and wide-column query is improved by 3 times and 2.6 times respectively.\\n\\n![](/images/blogs/1.0/1.0.0-1.png)\\n\\n![](/images/blogs/1.0/1.0.0-2.png)\\n\\n### Lateral View Grammar [Experimental]\\n\\nThrough Lateral View syntax, we can use Table Function table functions such as explode_bitmap, explode_split, explode_jaon_array, etc., to expand bitmap, String or Json Array from one column into multiple rows, so that the expanded data can be further processed (such as Filter, Join, etc.) .\\n\\n### Hive External Table [Experimental]\\n\\nHive External Table provides users with the ability to directly access Hive tables through Doris. External tables save the tedious data import work, and can use Doris\'s own OLAP capabilities to solve data analysis problems of Hive tables. The current version supports connecting Hive data sources to Doris, and supports federated queries through data in Doris and Hive data sources for more complex analysis operations.\\n\\n### Support Z-Order data sorting format\\n\\nApache Doris data is sorted and stored according to the prefix column, so when the prefix query condition is included, fast data search can be performed on the sorted data, but if the query condition is not a prefix column, the data sorting feature cannot be used for fast data search. The above problems can be solved by Z-Order Indexing. In version 1.0, we have added the Z-Order data sorting format, which can play a good filtering effect in the scenario of kanban multi-column query and accelerate the filtering performance of non-prefix column conditions. .\\n\\n### Support for Apache SeaTunnel (Incubating) plugin\\n\\nApache SeaTunnel is a high-performance distributed data integration framework built on Apache Spark and Apache Flink. In the 1.0 version of Apache Doris, we have added the SaeTunnel plugin, users can use Apache SeaTunnel for synchronization and ETL between multiple data sources.\\n\\n### New Function\\n\\nMore bitmap functions are supported, see the function manual for details:\\n\\n- bitmap_max\\n- bitmap_and_not\\n- bitmap_and_not_count\\n- bitmap_has_all\\n- bitmap_and_count\\n- bitmap_or_count\\n- bitmap_xor_count\\n- bitmap_subset_limit\\n- sub_bitmap\\n\\nSupport national secret algorithm SM3/SM4;\\n\\n\\n\\n> **Note**: The functions marked [Experimental] above are experimental functions. We will continue to optimize and iterate on the above functions in subsequent versions, and further improve them in subsequent versions. If you have any questions or comments during use, please feel free to contact us\\n\\n### Important Optimization\\n\\n### Features Optimization\\n\\n* Reduced the number of segment files generated when importing in large batches to reduce Compaction pressure.\\n* Transfer data through BRPC\'s attachment function to reduce serialization and deserialization overhead during query.\\n* Support to directly return binary data of HLL/BITMAP type for external analysis of business.\\n* Optimize and reduce the probability of OVERCROWDED and NOT_CONNECTED errors in BRPC, and enhance system stability.\\n* Enhance the fault tolerance of import.\\n* Support to update and delete data synchronously through Flink CDC.\\n* Support adaptive Runtime Filter.\\n* Significantly reduce the memory footprint of insert into operations\\n\\n\\n### Usability Improvements\\n\\n* Routine Load supports displaying the current offset delay number and other status.\\n* Added statistics on peak memory usage of queries in FE audit log.\\n* Added missing version information to Compaction URL results to facilitate troubleshooting.\\n* Support marking BE as non-queryable or non-importable to quickly screen problem nodes.\\n\\n### Important Bug Fixes\\n\\n* Fixed several query errors.\\n* Fixed some scheduling logic issues in Broker Load.\\n* Fix the problem that the metadata cannot be loaded due to the STREAM keyword.\\n* Fixed Decommission not executing correctly.\\n* Fix the problem that -102 error may occur when operating Schema Change operation in some cases.\\n* Fix the problem that using String type may cause BE to crash in some cases.\\n\\n### Other\\n\\n* Added Minidump function; easy to locate when problems occur\\n\\n## Changelog\\n\\nFor detailed Release Note, please check the link:\\n\\nhttps://github.com/apache/incubator-doris/issues/8549\\n\\n## Thanks  \\n\\nThe release of Apache Doris(incubating) 1.0 Release version is inseparable from the support of all community users. I would like to express my gratitude to all community contributors who participated in version design, development, testing and discussion. They are:\\n\\n```\\n@924060929\\n@adonis0147\\n@Aiden-Dong\\n@aihai\\n@airborne12\\n@Alibaba-HZY\\n@amosbird\\n@arthuryangcs\\n@awakeljw\\n@bingzxy\\n@BiteTheDDDDt\\n@blackstar-baba\\n@caiconghui\\n@CalvinKirs\\n@cambyzju\\n@caoliang-web\\n@ccoffline\\n@chaplinthink\\n@chovy-3012\\n@ChPi\\n@DarvenDuan\\n@dataalive\\n@dataroaring\\n@dh-cloud\\n@dohongdayi\\n@dongweizhao\\n@drgnchan\\n@e0c9\\n@EmmyMiao87\\n@englefly\\n@eyesmoons\\n@freemandealer\\n@Gabriel39\\n@gaodayue\\n@GoGoWen\\n@Gongruixiao\\n@gwdgithubnom\\n@HappenLee\\n@Henry2SS\\n@hf200012\\n@htyoung\\n@jacktengg\\n@jackwener\\n@JNSimba\\n@Keysluomo\\n@kezhenxu94\\n@killxdcj\\n@lihuigang\\n@littleeleventhwolf\\n@liutang123\\n@liuzhuang2017\\n@lonre\\n@lovingfeel\\n@luozenglin\\n@luzhijing\\n@MeiontheTop\\n@mh-boy\\n@morningman\\n@mrhhsg\\n@Myasuka\\n@nimuyuhan\\n@obobj\\n@pengxiangyu\\n@qidaye\\n@qzsee\\n@renzhimin7\\n@Royce33\\n@SleepyBear96\\n@smallhibiscus\\n@sodamnsure\\n@spaces-X\\n@sparklezzz\\n@stalary\\n@steadyBoy\\n@tarepanda1024\\n@THUMarkLau\\n@tianhui5\\n@tinkerrrr\\n@ucasfl\\n@Userwhite\\n@vinson0526\\n@wangbo\\n@wangshuo128\\n@wangyf0555\\n@weajun\\n@weizuo93\\n@whutpencil\\n@WindyGao\\n@wunan1210\\n@xiaokang\\n@xiaokangguo\\n@xiedeyantu\\n@xinghuayu007\\n@xingtanzjr\\n@xinyiZzz\\n@xtr1993\\n@xu20160924\\n@xuliuzhe\\n@xuzifu666\\n@xy720\\n@yangzhg\\n@yiguolei\\n@yinzhijian\\n@yjant\\n@zbtzbtzbt\\n@zenoyang\\n@zh0122\\n@zhangstar333\\n@zhannngchen\\n@zhengshengjun\\n@zhengshiJ\\n@ZhikaiZuo\\n@ztgoto\\n@zuochunwei\\n```"},{"id":"/release-note-0.15.0","metadata":{"permalink":"/blog/release-note-0.15.0","source":"@site/blog/release-note-0.15.0.md","title":"Apache Doris(Incubating) annoucned 0.15.0 release","description":"\u201CDear Community","date":"2021-11-29T00:00:00.000Z","tags":[{"inline":true,"label":"Release Notes","permalink":"/blog/tags/release-notes"}],"hasTruncateMarker":false,"authors":[{"name":"Apache Doris","key":null,"page":null}],"frontMatter":{"title":"Apache Doris(Incubating) annoucned 0.15.0 release","description":"\u201CDear Community","we are pleased to announce the release of Apache Doris(Incubating) on November 29":null,"2021! Nearly 700 optimizations and fixes have been submitted by 99 contributors to Apache Doris":null,"and we\'d like to express our sincere gratitude to all of them!\u201D":null,"date":"2021-11-29","author":"Apache Doris","tags":["Release Notes"],"image":"/images/release-notes.png"},"unlisted":false,"prevItem":{"title":"Apache Doris(Incubating) announced 1.0.0 release","permalink":"/blog/release-note-1.0.0"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n# Apache Doris(Incubating) 0.15.0 Release\\n\\nDear Community, After months of polishing, we are pleased to announce the release of Apache Doris(Incubating) on November 29, 2021! Nearly 700 optimizations and fixes have been submitted by 99 contributors to Apache Doris, and we\'d like to express our sincere gratitude to all of them!\\n\\nIn the 0.15.0 Release, we have added many new features to optimize Apache Doris\'s query performance, ease of use, and stability: a new resource division and isolation feature that allows users to divide BE nodes in a cluster into resource groups by means of resource tags, enabling unified management of online and offline services and resource isolation; the addition of Runtime Filter and Join Reorder functions have been added to significantly improve the query efficiency of multi-table Join scenarios, with a 2-10 times performance improvement under the Star Schema Benchmark test data set; new import method Binlog Load enables Doris to incrementally synchronize the CDC of data update operations in MySQL; support for String column type The new import method, Binlog Load, allows Doris to incrementally synchronize the CDC of MySQL for data update operations; supports String column type with a maximum length of 2GB; supports List partitioning to create partitions by enumerating values; supports Update statements on the Unique Key model; Spark-Doris-Connector supports data writing to Doris ... ...and many more important features, welcome to download and use.\\n\\nWe welcome you to contact us via GitHub Discussion or the Dev email group if you have any questions during use, and we look forward to your participation in community discussions and building.\\n\\n## High Lights\\n\\n### Resource Segregation and Isolation\\n\\nYou can divide BE nodes in a Doris cluster into resource groups by using resource tags, allowing you to manage online and offline operations and isolate resources at the node level.\\nYou can also control the resource overhead of individual queries by limiting the CPU and memory overhead and complexity of individual query tasks, thus reducing the resource hogging problem between different queries.\\n\\n### Performance Optimization\\n\\n* The Runtime Filter feature can significantly improve query efficiency in most Join scenarios by using the Join Key column condition of the right table in the Join algorithm to filter the data in the left table. For example, you can get 2-10 times performance improvement under Star Schema Benchmark (TPCH\'s streamlined test set).\\n\\n* The Join Reorder feature can automatically help adjust the order of joins in SQL by using a cost model to help achieve optimal join efficiency.\\nIt can be enabled via the session variable `set enable_cost_based_join_reorder=true`.\\n\\n### New features\\n\\n* Support synchronizing MySQL binlog data directly to Canal Server.\\n* Support String column type, support up to 2GB.\\n* Support List partitioning, you can create partitions for enumerated values.\\n* Support transactional Insert statement function. You can import data in bulk by begin ; insert ; insert;, ... You can import data in bulk by begin ; insert ; insert ;, ... ;.\\n* Support Update statement function on Unique Key model. You can execute Update Set where statement on Unique Key model table.\\n* Support SQL blocking list function. You can block some SQL execution by regular, hash value matching, etc.\\n* Support LDAP login authentication.\\n\\n### Extended Features\\n\\n* Support Flink-Doris-Connector.\\n* Support for DataX doriswriter plugin.\\n* Spark-Doris-Connector support for data writing to Doris.\\n\\n## Feature Optimization \\n\\n### Query\\n\\n* Support for computing all constant expressions in the SQL query planning phase using BE\'s functional computing power.\\n\\n### Import\\n\\n* Support for specifying multi-byte row separators or invisible separators when importing text format files.\\n* Supports importing compressed format files via Stream Load.\\n* Stream Load supports importing Json data in multi-line format.\\n\\n### Export\\n\\n* Support Export export function to specify where filter. Supports exporting files with multi-byte row separators. Support export to local files.\\n* Export export function supports exporting only specified columns.\\n* Supports exporting the result set to local disk via outfile statement and writing the exported marker file after exporting.\\n\\n### Ease of use\\n\\n* Dynamic partitioning function supports creating and keeping specified historical partitions, and supports automatic hot and cold data migration settings.\\n* Supports displaying queries, imported schedules and Profiles using a visual tree structure at the command line.\\n* Support to record and view Stream Load operation logs.\\n* When consuming Kafka data via Routine Load, you can specify the time point for consumption.\\n* Supports exporting Routine Load creation statements by show create routine load function.\\n* Support to start and stop all Routine Load jobs with one click by pause/resume all routine load command.\\n* Supports modifying the Broker List and Topic of Routine Load by alter routine load statement.\\n* Support create table as select function.\\n* Support modify column comments and table comments by alter table command.\\n* show tablet status to add table creation time and data update time.\\n* Support show data skew command to check the data volume distribution of a table to troubleshoot data skewing problems.\\n* Support show/clean trash command to check the disk occupation of BE file recycle bin and clear it actively.\\n* Support show view statement to show which views a table is referenced by.\\n\\n### New functions\\n\\n* `bitmap_min`, `bit_length`\\n* `yearweek`, `week`, `makedate`\\n* `percentile` exact percentile function\\n* `json_array`, `json_object`, `json_quote`\\n* Support for creating custom public keys for the `AES_ENCRYPT` and `AES_DECRYPT` functions.\\n* Support for creating function aliases to combine multiple functions by `create alias function`.\\n\\n### Other\\n\\n* Support for accessing the ES exterior of the SSL connection protocol.\\n* Support specifying the number of hotspot partitions in the dynamic partition property, which will be stored in SSD disks.\\n* Support importing Json format data via Broker Load.\\n* Supports accessing HDFS directly through libhdfs3 library for data import and export without the Broker process.\\n* select into outfile function supports exporting Parquet file format and parallel export.\\n* ODBC external table support for SQLServer. \\n\\n## \u81F4\u8C22  \\n\\nThe release of Apache Doris (incubating) 0.15.0 Release is made possible by the support of all community users. We would like to thank all the community contributors who participated in the design, development, testing, and discussion of the release, namely.\\n\\n* [@924060929](https://github.com/924060929)\\n* [@acelyc111](https://github.com/acelyc111)\\n* [@Aimiyoo](https://github.com/Aimiyoo)\\n* [@amosbird](https://github.com/amosbird)\\n* [@arthur-zhang](https://github.com/arthur-zhang)\\n* [@azurenake](https://github.com/azurenake)\\n* [@BiteTheDDDDt](https://github.com/BiteTheDDDDt)\\n* [@caiconghui](https://github.com/caiconghui)\\n* [@caneGuy](https://github.com/caneGuy)\\n* [@caoliang-web](https://github.com/caoliang-web)\\n* [@ccoffline](https://github.com/ccoffline)\\n* [@chaplinthink](https://github.com/chaplinthink)\\n* [@chovy-3012](https://github.com/chovy-3012)\\n* [@ChPi](https://github.com/ChPi)\\n* [@copperybean](https://github.com/copperybean)\\n* [@crazyleeyang](https://github.com/crazyleeyang)\\n* [@dh-cloud](https://github.com/dh-cloud)\\n* [@DinoZhang](https://github.com/DinoZhang)\\n* [@dixingxing0](https://github.com/dixingxing0)\\n* [@dohongdayi](https://github.com/dohongdayi)\\n* [@e0c9](https://github.com/e0c9)\\n* [@EmmyMiao87](https://github.com/EmmyMiao87)\\n* [@eyesmoons](https://github.com/eyesmoons)\\n* [@francisoliverlee](https://github.com/francisoliverlee)\\n* [@Gabriel39](https://github.com/Gabriel39)\\n* [@gaodayue](https://github.com/gaodayue)\\n* [@GoGoWen](https://github.com/GoGoWen)\\n* [@HappenLee](https://github.com/HappenLee)\\n* [@harveyyue](https://github.com/harveyyue)\\n* [@Henry2SS](https://github.com/Henry2SS)\\n* [@hf200012](https://github.com/hf200012)\\n* [@huangmengbin](https://github.com/huangmengbin)\\n* [@huozhanfeng](https://github.com/huozhanfeng)\\n* [@huzk8](https://github.com/huzk8)\\n* [@hxianshun](https://github.com/hxianshun)\\n* [@ikaruga4600](https://github.com/ikaruga4600)\\n* [@JameyWoo](https://github.com/JameyWoo)\\n* [@Jennifer88huang](https://github.com/Jennifer88huang)\\n* [@JinLiOnline](https://github.com/JinLiOnline)\\n* [@jinyuanlu](https://github.com/jinyuanlu)\\n* [@JNSimba](https://github.com/JNSimba)\\n* [@killxdcj](https://github.com/killxdcj)\\n* [@kuncle](https://github.com/kuncle)\\n* [@liutang123](https://github.com/liutang123)\\n* [@luozenglin](https://github.com/luozenglin)\\n* [@luzhijing](https://github.com/luzhijing)\\n* [@MarsXDM](https://github.com/MarsXDM)\\n* [@mh-boy](https://github.com/mh-boy)\\n* [@mk8310](https://github.com/mk8310)\\n* [@morningman](https://github.com/morningman)\\n* [@Myasuka](https://github.com/Myasuka)\\n* [@nimuyuhan](https://github.com/nimuyuhan)\\n* [@pan3793](https://github.com/pan3793)\\n* [@PatrickNicholas](https://github.com/PatrickNicholas)\\n* [@pengxiangyu](https://github.com/pengxiangyu)\\n* [@pierre94](https://github.com/pierre94)\\n* [@qidaye](https://github.com/qidaye)\\n* [@qzsee](https://github.com/qzsee)\\n* [@shiyi23](https://github.com/shiyi23)\\n* [@smallhibiscus](https://github.com/smallhibiscus)\\n* [@songenjie](https://github.com/songenjie)\\n* [@spaces-X](https://github.com/spaces-X)\\n* [@stalary](https://github.com/stalary)\\n* [@stdpain](https://github.com/stdpain)\\n* [@Stephen-Robin](https://github.com/Stephen-Robin)\\n* [@Sunt-ing](https://github.com/Sunt-ing)\\n* [@Taaang](https://github.com/Taaang)\\n* [@tarepanda1024](https://github.com/tarepanda1024)\\n* [@tianhui5](https://github.com/tianhui5)\\n* [@tinkerrrr](https://github.com/tinkerrrr)\\n* [@TobKed](https://github.com/TobKed)\\n* [@ucasfl](https://github.com/ucasfl)\\n* [@Userwhite](https://github.com/Userwhite)\\n* [@vinson0526](https://github.com/vinson0526)\\n* [@wangbo](https://github.com/wangbo)\\n* [@wangliansong](https://github.com/wangliansong)\\n* [@wangshuo128](https://github.com/wangshuo128)\\n* [@weajun](https://github.com/weajun)\\n* [@weihongkai2008](https://github.com/weihongkai2008)\\n* [@weizuo93](https://github.com/weizuo93)\\n* [@WindyGao](https://github.com/WindyGao)\\n* [@wunan1210](https://github.com/wunan1210)\\n* [@wuyunfeng](https://github.com/wuyunfeng)\\n* [@xhmz](https://github.com/xhmz)\\n* [@xiaokangguo](https://github.com/xiaokangguo)\\n* [@xiaoxiaopan118](https://github.com/xiaoxiaopan118)\\n* [@xinghuayu007](https://github.com/xinghuayu007)\\n* [@xinyiZzz](https://github.com/xinyiZzz)\\n* [@xuliuzhe](https://github.com/xuliuzhe)\\n* [@xxiao2018](https://github.com/xxiao2018)\\n* [@xy720](https://github.com/xy720)\\n* [@yangzhg](https://github.com/yangzhg)\\n* [@yx91490](https://github.com/yx91490)\\n* [@zbtzbtzbt](https://github.com/zbtzbtzbt)\\n* [@zenoyang](https://github.com/zenoyang)\\n* [@zh0122](https://github.com/zh0122)\\n* [@zhangboya1](https://github.com/zhangboya1)\\n* [@zhangstar333](https://github.com/zhangstar333)\\n* [@zuochunwei](https://github.com/zuochunwei)"}]}}')}}]);