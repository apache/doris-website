"use strict";(self.webpackChunkdoris_website=self.webpackChunkdoris_website||[]).push([["639134"],{819832:function(e,n,i){i.r(n),i.d(n,{default:()=>h,frontMatter:()=>d,metadata:()=>r,assets:()=>c,toc:()=>l,contentTitle:()=>a});var r=JSON.parse('{"id":"ai/vector-search/overview","title":"Overview | Vector Search","description":"In generative AI applications, relying solely on a large model\'s internal parameter \u201Cmemory\u201D has clear limitations: (1) the model\u2019s knowledge becomes ","source":"@site/versioned_docs/version-4.x/ai/vector-search/overview.md","sourceDirName":"ai/vector-search","slug":"/ai/vector-search/overview","permalink":"/docs/4.x/ai/vector-search/overview","draft":false,"unlisted":false,"tags":[],"version":"4.x","lastUpdatedAt":1770477659000,"frontMatter":{"title":"Overview | Vector Search","language":"en","description":"In generative AI applications, relying solely on a large model\'s internal parameter \u201Cmemory\u201D has clear limitations: (1) the model\u2019s knowledge becomes ","sidebar_label":"Overview"},"sidebar":"docs","previous":{"title":"Relevance Scoring","permalink":"/docs/4.x/ai/text-search/scoring"},"next":{"title":"HNSW","permalink":"/docs/4.x/ai/vector-search/hnsw"}}'),t=i("785893"),s=i("250065");let d={title:"Overview | Vector Search",language:"en",description:"In generative AI applications, relying solely on a large model's internal parameter \u201Cmemory\u201D has clear limitations: (1) the model\u2019s knowledge becomes ",sidebar_label:"Overview"},a="Overview",c={},l=[{value:"Approximate Nearest Neighbor Search",id:"approximate-nearest-neighbor-search",level:2},{value:"Approximate Range Search",id:"approximate-range-search",level:2},{value:"Compound Search",id:"compound-search",level:2},{value:"ANN Search with Additional Filters",id:"ann-search-with-additional-filters",level:2},{value:"Session Variables Related to ANN Search",id:"session-variables-related-to-ann-search",level:2},{value:"Vector Quantization",id:"vector-quantization",level:2},{value:"Performance Tuning",id:"performance-tuning",level:2},{value:"Query Performance",id:"query-performance",level:3},{value:"Use Prepared Statements",id:"use-prepared-statements",level:3},{value:"Reduce Segment Count",id:"reduce-segment-count",level:3},{value:"Reduce Rowset Count",id:"reduce-rowset-count",level:3},{value:"Keep ANN Index in Memory",id:"keep-ann-index-in-memory",level:3},{value:"parallel_pipeline_task_num = 1",id:"parallel_pipeline_task_num--1",level:3},{value:"enable_profile = false",id:"enable_profile--false",level:3},{value:"Python SDK",id:"python-sdk",level:2},{value:"Usage Limitations",id:"usage-limitations",level:2}];function o(e){let n={a:"a",br:"br",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.a)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"overview",children:"Overview"})}),"\n",(0,t.jsx)(n.p,{children:"In generative AI applications, relying solely on a large model's internal parameter \u201Cmemory\u201D has clear limitations: (1) the model\u2019s knowledge becomes outdated and cannot cover the latest information; (2) directly asking the model to \u201Cgenerate\u201D answers increases the risk of hallucinations. This gives rise to RAG (Retrieval-Augmented Generation). The key task of RAG is not to have the model fabricate answers from nothing, but to retrieve the Top-K most relevant information chunks from an external knowledge base and feed them to the model as grounding context."}),"\n",(0,t.jsx)(n.p,{children:"To achieve this, we need a mechanism to measure semantic relatedness between a user query and documents in the knowledge base. Vector representations are a standard tool: by encoding both queries and documents into semantic vectors, we can use vector similarity to measure relevance. With the advancement of pretrained language models, generating high-quality embeddings has become mainstream. Thus, the retrieval stage of RAG becomes a typical vector similarity search problem: from a large vector collection, find the K vectors most similar to the query (i.e., candidate knowledge pieces)."}),"\n",(0,t.jsx)(n.p,{children:"Vector retrieval in RAG is not limited to text; it naturally extends to multimodal scenarios. In a multimodal RAG system, images, audio, video, and other data types can also be encoded into vectors for retrieval and then supplied to the generative model as context. For example, if a user uploads an image, the system can first retrieve related descriptions or knowledge snippets, then generate explanatory content. In medical QA, RAG can retrieve patient records and literature to support more accurate diagnostic suggestions."}),"\n",(0,t.jsx)(n.h2,{id:"approximate-nearest-neighbor-search",children:"Approximate Nearest Neighbor Search"}),"\n",(0,t.jsx)(n.p,{children:"From version 4.0, Apache Doris officially supports ANN search. No additional data type is introduced: vectors are stored as fixed-length arrays. For distance-based indexing a new index type, ANN, is implemented based on Faiss."}),"\n",(0,t.jsxs)(n.p,{children:["Using the common ",(0,t.jsx)(n.a,{href:"http://corpus-texmex.irisa.fr/",children:"SIFT"})," dataset as an example, you can create a table like this:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:'CREATE TABLE sift_1M (\n  id int NOT NULL,\n  embedding array<float>  NOT NULL  COMMENT "",\n  INDEX ann_index (embedding) USING ANN PROPERTIES(\n      "index_type"="hnsw",\n      "metric_type"="l2_distance",\n      "dim"="128",\n      "quantizer"="flat"\n  )\n) ENGINE=OLAP\nDUPLICATE KEY(id) COMMENT "OLAP"\nDISTRIBUTED BY HASH(id) BUCKETS 1\nPROPERTIES (\n  "replication_num" = "1"\n);\n'})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["index_type: ",(0,t.jsx)(n.code,{children:"hnsw"})," means using the ",(0,t.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/Hierarchical_navigable_small_world",children:"Hierarchical Navigable Small World algorithm"})]}),"\n",(0,t.jsxs)(n.li,{children:["metric_type: ",(0,t.jsx)(n.code,{children:"l2_distance"})," means using L2 distance as the distance function"]}),"\n",(0,t.jsxs)(n.li,{children:["dim: ",(0,t.jsx)(n.code,{children:"128"})," means the vector dimension is 128"]}),"\n",(0,t.jsxs)(n.li,{children:["quantizer: ",(0,t.jsx)(n.code,{children:"flat"})," means each vector dimension is stored as original float32"]}),"\n"]}),"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Parameter"}),(0,t.jsx)(n.th,{children:"Required"}),(0,t.jsx)(n.th,{children:"Supported/Options"}),(0,t.jsx)(n.th,{children:"Default"}),(0,t.jsx)(n.th,{children:"Description"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"index_type"})}),(0,t.jsx)(n.td,{children:"Yes"}),(0,t.jsx)(n.td,{children:"hnsw only"}),(0,t.jsx)(n.td,{children:"(none)"}),(0,t.jsx)(n.td,{children:"ANN index algorithm. Currently only HNSW supported."})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"metric_type"})}),(0,t.jsx)(n.td,{children:"Yes"}),(0,t.jsxs)(n.td,{children:[(0,t.jsx)(n.code,{children:"l2_distance"}),", ",(0,t.jsx)(n.code,{children:"inner_product"})]}),(0,t.jsx)(n.td,{children:"(none)"}),(0,t.jsx)(n.td,{children:"Vector similarity/distance metric. L2 = Euclidean; inner_product can approximate cosine if vectors are normalized."})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"dim"})}),(0,t.jsx)(n.td,{children:"Yes"}),(0,t.jsx)(n.td,{children:"Positive integer (> 0)"}),(0,t.jsx)(n.td,{children:"(none)"}),(0,t.jsx)(n.td,{children:"Vector dimension. All imported vectors must match or an error is raised."})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"max_degree"})}),(0,t.jsx)(n.td,{children:"No"}),(0,t.jsx)(n.td,{children:"Positive integer"}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"32"})}),(0,t.jsx)(n.td,{children:"HNSW M (max neighbors per node). Affects index memory and search performance."})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"ef_construction"})}),(0,t.jsx)(n.td,{children:"No"}),(0,t.jsx)(n.td,{children:"Positive integer"}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"40"})}),(0,t.jsx)(n.td,{children:"HNSW efConstruction (candidate queue size during build). Larger gives better quality but slower build."})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"quantizer"})}),(0,t.jsx)(n.td,{children:"No"}),(0,t.jsxs)(n.td,{children:[(0,t.jsx)(n.code,{children:"flat"}),", ",(0,t.jsx)(n.code,{children:"sq8"}),", ",(0,t.jsx)(n.code,{children:"sq4"}),", ",(0,t.jsx)(n.code,{children:"pq"})]}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"flat"})}),(0,t.jsxs)(n.td,{children:["Vector encoding/quantization: ",(0,t.jsx)(n.code,{children:"flat"})," = raw; ",(0,t.jsx)(n.code,{children:"sq8"}),"/",(0,t.jsx)(n.code,{children:"sq4"})," = scalar quantization (8/4 bit), ",(0,t.jsx)(n.code,{children:"pq"})," = product quantization to reduce memory."]})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"pq_m"})}),(0,t.jsx)(n.td,{children:"Required when 'quantizer=pq'"}),(0,t.jsx)(n.td,{children:"Positive integer"}),(0,t.jsx)(n.td,{children:"(none)"}),(0,t.jsx)(n.td,{children:"Specifies how many subvectors are used (vector dimension dim must be divisible by pq_m)."})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"pq_nbits"})}),(0,t.jsx)(n.td,{children:"Required when 'quantizer=pq'"}),(0,t.jsx)(n.td,{children:"Positive integer"}),(0,t.jsx)(n.td,{children:"(none)"}),(0,t.jsx)(n.td,{children:"The number of bits used to represent each subvector, in faiss pq_nbits is generally required to be no greater than 24."})]})]})]}),"\n",(0,t.jsx)(n.p,{children:"Import via S3 TVF:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:'INSERT INTO sift_1M\nSELECT *\nFROM S3(\n  "uri" = "https://selectdb-customers-tools-bj.oss-cn-beijing.aliyuncs.com/sift_database.tsv",\n  "format" = "csv");\n\nSELECT count(*) FROM sift_1M\n\n+----------+\n| count(*) |\n+----------+\n|  1000000 |\n+----------+\n'})}),"\n",(0,t.jsxs)(n.p,{children:["Using ",(0,t.jsx)(n.code,{children:"l2_distance_approximate"})," / ",(0,t.jsx)(n.code,{children:"inner_product_approximate"})," triggers the ANN index path. The function must match the index ",(0,t.jsx)(n.code,{children:"metric_type"})," exactly (e.g., ",(0,t.jsx)(n.code,{children:"metric_type=l2_distance"})," \u2192 use ",(0,t.jsx)(n.code,{children:"l2_distance_approximate"}),"; ",(0,t.jsx)(n.code,{children:"metric_type=inner_product"})," \u2192 use ",(0,t.jsx)(n.code,{children:"inner_product_approximate"}),"). For ordering: L2 uses ascending distance (smaller is closer); inner product uses descending score (larger is closer)."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:"SELECT id,\n       l2_distance_approximate(\n        embedding,\n        [0,11,77,24,3,0,0,0,28,70,125,8,0,0,0,0,44,35,50,45,9,0,0,0,4,0,4,56,18,0,3,9,16,17,59,10,10,8,57,57,100,105,125,41,1,0,6,92,8,14,73,125,29,7,0,5,0,0,8,124,66,6,3,1,63,5,0,1,49,32,17,35,125,21,0,3,2,12,6,109,21,0,0,35,74,125,14,23,0,0,6,50,25,70,64,7,59,18,7,16,22,5,0,1,125,23,1,0,7,30,14,32,4,0,2,2,59,125,19,4,0,0,2,1,6,53,33,2]\n       ) AS distance\nFROM sift_1M\nORDER BY distance\nLIMIT 10;\n--------------\n\n+--------+----------+\n| id     | distance |\n+--------+----------+\n| 178811 | 210.1595 |\n| 177646 | 217.0161 |\n| 181997 | 218.5406 |\n| 181605 | 219.2989 |\n| 821938 | 221.7228 |\n| 807785 | 226.7135 |\n| 716433 | 227.3148 |\n| 358802 | 230.7314 |\n| 803100 | 230.9112 |\n| 866737 | 231.6441 |\n+--------+----------+\n10 rows in set (0.02 sec)\n"})}),"\n",(0,t.jsxs)(n.p,{children:["To compare with exact ground truth, use ",(0,t.jsx)(n.code,{children:"l2_distance"})," or ",(0,t.jsx)(n.code,{children:"inner_product"})," (without the ",(0,t.jsx)(n.code,{children:"_approximate"})," suffix). In this example, exact search takes ~290 ms:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"10 rows in set (0.29 sec)\n"})}),"\n",(0,t.jsx)(n.p,{children:"With the ANN index, query latency drops from ~290 ms to ~20 ms in this example."}),"\n",(0,t.jsx)(n.p,{children:"ANN indexes are built at segment granularity. In distributed tables, each segment returns its local TopN; then the TopN operator merges results across tablets and segments to produce the global TopN."}),"\n",(0,t.jsx)(n.p,{children:"Note on ordering:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["For ",(0,t.jsx)(n.code,{children:"metric_type = l2_distance"}),", smaller distance = closer vectors \u2192 use ",(0,t.jsx)(n.code,{children:"ORDER BY dist ASC"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:["For ",(0,t.jsx)(n.code,{children:"metric_type = inner_product"}),", larger value = closer vectors \u2192 use ",(0,t.jsx)(n.code,{children:"ORDER BY dist DESC"})," to obtain TopN via the index."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"approximate-range-search",children:"Approximate Range Search"}),"\n",(0,t.jsx)(n.p,{children:"Beyond the common TopN nearest neighbor search (returning the closest N records), another typical pattern is threshold-based range search. Instead of returning a fixed number of results, it returns all points whose distance to the target vector satisfies a predicate (>, >=, <, <=). For example, you might want vectors whose distance is greater than or less than a threshold. This is useful when you need candidates that are \u201Csufficiently similar\u201D or \u201Csufficiently dissimilar.\u201D In recommendation systems you might retrieve items that are close but not identical to improve diversity; in anomaly detection you look for points far from the normal distribution."}),"\n",(0,t.jsx)(n.p,{children:"Example SQL:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:"SELECT count(*)\nFROM   sift_1M\nWHERE  l2_distance_approximate(\n        embedding,\n        [0,11,77,24,3,0,0,0,28,70,125,8,0,0,0,0,44,35,50,45,9,0,0,0,4,0,4,56,18,0,3,9,16,17,59,10,10,8,57,57,100,105,125,41,1,0,6,92,8,14,73,125,29,7,0,5,0,0,8,124,66,6,3,1,63,5,0,1,49,32,17,35,125,21,0,3,2,12,6,109,21,0,0,35,74,125,14,23,0,0,6,50,25,70,64,7,59,18,7,16,22,5,0,1,125,23,1,0,7,30,14,32,4,0,2,2,59,125,19,4,0,0,2,1,6,53,33,2])\n        > 300 \n--------------\n\n+----------+\n| count(*) |\n+----------+\n|   999271 |\n+----------+\n1 row in set (0.19 sec)\n"})}),"\n",(0,t.jsxs)(n.p,{children:["These range-based vector searches are also accelerated by the ANN index: the index first narrows candidates, then approximate distances are computed, reducing cost and improving latency. Supported predicates: ",(0,t.jsx)(n.code,{children:">"}),", ",(0,t.jsx)(n.code,{children:">="}),", ",(0,t.jsx)(n.code,{children:"<"}),", ",(0,t.jsx)(n.code,{children:"<="}),"."]}),"\n",(0,t.jsx)(n.h2,{id:"compound-search",children:"Compound Search"}),"\n",(0,t.jsx)(n.p,{children:"Compound Search combines an ANN TopN search with a range predicate in the same SQL statement, returning the TopN results that also satisfy a distance constraint."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:"SELECT id,\n       l2_distance_approximate(\n        embedding, [0,11,77,24,3,0,0,0,28,70,125,8,0,0,0,0,44,35,50,45,9,0,0,0,4,0,4,56,18,0,3,9,16,17,59,10,10,8,57,57,100,105,125,41,1,0,6,92,8,14,73,125,29,7,0,5,0,0,8,124,66,6,3,1,63,5,0,1,49,32,17,35,125,21,0,3,2,12,6,109,21,0,0,35,74,125,14,23,0,0,6,50,25,70,64,7,59,18,7,16,22,5,0,1,125,23,1,0,7,30,14,32,4,0,2,2,59,125,19,4,0,0,2,1,6,53,33,2]) as dist\nFROM sift_1M\nWHERE l2_distance_approximate(\n        embedding, [0,11,77,24,3,0,0,0,28,70,125,8,0,0,0,0,44,35,50,45,9,0,0,0,4,0,4,56,18,0,3,9,16,17,59,10,10,8,57,57,100,105,125,41,1,0,6,92,8,14,73,125,29,7,0,5,0,0,8,124,66,6,3,1,63,5,0,1,49,32,17,35,125,21,0,3,2,12,6,109,21,0,0,35,74,125,14,23,0,0,6,50,25,70,64,7,59,18,7,16,22,5,0,1,125,23,1,0,7,30,14,32,4,0,2,2,59,125,19,4,0,0,2,1,6,53,33,2])\n        > 300\nORDER BY dist limit 10\n--------------\n\n+--------+----------+\n| id     | dist     |\n+--------+----------+\n| 243590 |  300.005 |\n| 549298 | 300.0317 |\n| 429685 | 300.0533 |\n| 690172 | 300.0916 |\n| 123410 | 300.1333 |\n| 232540 | 300.1649 |\n| 547696 | 300.2066 |\n| 855437 | 300.2782 |\n| 589017 | 300.3048 |\n| 930696 | 300.3381 |\n+--------+----------+\n10 rows in set (0.12 sec)\n"})}),"\n",(0,t.jsx)(n.p,{children:"A key question is whether predicate filtering happens before or after TopN. If predicates filter first and TopN is applied on the reduced set, it\u2019s pre-filtering; otherwise, it\u2019s post-filtering. Post-filtering can be faster but may dramatically reduce recall. Doris uses pre-filtering to preserve recall."}),"\n",(0,t.jsx)(n.p,{children:"Doris can accelerate both phases with the index. However, if the first phase (range filter) is too selective, indexing both phases can hurt recall. Doris adaptively decides whether to use the index twice based on predicate selectivity and index type."}),"\n",(0,t.jsx)(n.h2,{id:"ann-search-with-additional-filters",children:"ANN Search with Additional Filters"}),"\n",(0,t.jsx)(n.p,{children:"This refers to applying other predicates before the ANN TopN and returning the TopN under those constraints."}),"\n",(0,t.jsx)(n.p,{children:"Example with a small 8-D vector and a text filter:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:'CREATE TABLE ann_with_fulltext (\n  id int NOT NULL,\n  embedding array<float> NOT NULL,\n  comment String NOT NULL,\n  value int NULL,\n  INDEX idx_comment(`comment`) USING INVERTED PROPERTIES("parser" = "english") COMMENT \'inverted index for comment\',\n  INDEX ann_embedding(`embedding`) USING ANN PROPERTIES("index_type"="hnsw","metric_type"="l2_distance","dim"="8")\n) DUPLICATE KEY (`id`) \nDISTRIBUTED BY HASH(`id`) BUCKETS 1\nPROPERTIES("replication_num"="1");\n'})}),"\n",(0,t.jsxs)(n.p,{children:["Insert sample data and search only within rows where ",(0,t.jsx)(n.code,{children:"comment"})," contains \u201Cmusic\u201D:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:"INSERT INTO ann_with_fulltext VALUES\n(1, [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8], 'this is about music', 10),\n(2, [0.2,0.1,0.5,0.3,0.9,0.4,0.7,0.1], 'sports news today',   20),\n(3, [0.9,0.8,0.7,0.6,0.5,0.4,0.3,0.2], 'latest music trend',  30),\n(4, [0.05,0.06,0.07,0.08,0.09,0.1,0.2,0.3], 'politics update',40);\n\nSELECT id, comment,\n       l2_distance_approximate(embedding, [0.1,0.1,0.2,0.2,0.3,0.3,0.4,0.4]) AS dist\nFROM ann_with_fulltext\nWHERE comment MATCH_ANY 'music'       -- Filter using inverted index\nORDER BY dist ASC                     -- Ann topn calculation after predicates evaluate.\nLIMIT 2;\n\n+------+---------------------+----------+\n| id   | comment             | dist     |\n+------+---------------------+----------+\n|    1 | this is about music | 0.663325 |\n|    3 | latest music trend  | 1.280625 |\n+------+---------------------+----------+\n2 rows in set (0.04 sec)\n"})}),"\n",(0,t.jsx)(n.p,{children:"To ensure TopN can be accelerated via the vector index, all predicate columns should have appropriate secondary indexes (e.g., an inverted index)."}),"\n",(0,t.jsx)(n.h2,{id:"session-variables-related-to-ann-search",children:"Session Variables Related to ANN Search"}),"\n",(0,t.jsx)(n.p,{children:"Beyond build-time parameters for HNSW, you can pass search-time parameters via session variables:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"hnsw_ef_search: EF search parameter. Controls max length of the candidate queue; larger = higher accuracy, higher latency. Default 32."}),"\n",(0,t.jsx)(n.li,{children:"hnsw_check_relative_distance: Whether to enable relative distance checking to improve accuracy. Default true."}),"\n",(0,t.jsx)(n.li,{children:"hnsw_bounded_queue: Whether to use a bounded priority queue to optimize performance. Default true."}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"vector-quantization",children:"Vector Quantization"}),"\n",(0,t.jsx)(n.p,{children:"With FLAT encoding, an HNSW index (raw vectors plus graph structure) may consume large amounts of memory. HNSW must be fully resident in memory to function, so memory can become a bottleneck at large scale."}),"\n",(0,t.jsx)(n.p,{children:"Scalar quantization (SQ) compresses float32 storage to reduce memory. Product quantization (PQ) reduces memory overhead by compressing high-dimensional vectors into smaller subvectors and quantizing each subvector independently. For scalar quantization, Doris currently supports two scalar quantization schemes: INT8 and INT4 (SQ8 / SQ4). Example using SQ8:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:'CREATE TABLE sift_1M (\n  id int NOT NULL,\n  embedding array<float>  NOT NULL  COMMENT "",\n  INDEX ann_index (embedding) USING ANN PROPERTIES(\n      "index_type"="hnsw",\n      "metric_type"="l2_distance",\n      "dim"="128",\n      "quantizer"="sq8"\n  )\n) ENGINE=OLAP\nDUPLICATE KEY(id) COMMENT "OLAP"\nDISTRIBUTED BY HASH(id) BUCKETS 1\nPROPERTIES (\n  "replication_num" = "1"\n);\n'})}),"\n",(0,t.jsx)(n.p,{children:"On 768-D Cohere-MEDIUM-1M and Cohere-LARGE-10M datasets, SQ8 reduces index size to roughly one third compared to FLAT."}),"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Dataset"}),(0,t.jsx)(n.th,{children:"Dim"}),(0,t.jsx)(n.th,{children:"Storage/Index Scheme"}),(0,t.jsx)(n.th,{children:"Total Disk"}),(0,t.jsx)(n.th,{children:"Data Part"}),(0,t.jsx)(n.th,{children:"Index Part"}),(0,t.jsx)(n.th,{children:"Notes"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Cohere-MEDIUM-1M"}),(0,t.jsx)(n.td,{children:"768D"}),(0,t.jsx)(n.td,{children:"Doris (FLAT)"}),(0,t.jsx)(n.td,{children:"5.647 GB (2.533 + 3.114)"}),(0,t.jsx)(n.td,{children:"2.533 GB"}),(0,t.jsx)(n.td,{children:"3.114 GB"}),(0,t.jsx)(n.td,{children:"1M vectors"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Cohere-MEDIUM-1M"}),(0,t.jsx)(n.td,{children:"768D"}),(0,t.jsx)(n.td,{children:"Doris SQ INT8"}),(0,t.jsx)(n.td,{children:"3.501 GB (2.533 + 0.992)"}),(0,t.jsx)(n.td,{children:"2.533 GB"}),(0,t.jsx)(n.td,{children:"0.992 GB"}),(0,t.jsx)(n.td,{children:"INT8 symmetric quantization"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Cohere-MEDIUM-1M"}),(0,t.jsx)(n.td,{children:"768D"}),(0,t.jsx)(n.td,{children:"Doris PQ(pq_m=384,pq_nbits=8)"}),(0,t.jsx)(n.td,{children:"3.149 GB (2.535 + 0.614)"}),(0,t.jsx)(n.td,{children:"2.535 GB"}),(0,t.jsx)(n.td,{children:"0.614 GB"}),(0,t.jsx)(n.td,{children:"product quantization"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Cohere-LARGE-10M"}),(0,t.jsx)(n.td,{children:"768D"}),(0,t.jsx)(n.td,{children:"Doris (FLAT)"}),(0,t.jsx)(n.td,{children:"56.472 GB (25.328 + 31.145)"}),(0,t.jsx)(n.td,{children:"25.328 GB"}),(0,t.jsx)(n.td,{children:"31.145 GB"}),(0,t.jsx)(n.td,{children:"10M vectors"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Cohere-LARGE-10M"}),(0,t.jsx)(n.td,{children:"768D"}),(0,t.jsx)(n.td,{children:"Doris SQ INT8"}),(0,t.jsx)(n.td,{children:"35.016 GB (25.329 + 9.687)"}),(0,t.jsx)(n.td,{children:"25.329 GB"}),(0,t.jsx)(n.td,{children:"9.687 GB"}),(0,t.jsx)(n.td,{children:"INT8 quantization"})]})]})]}),"\n",(0,t.jsx)(n.p,{children:"Quantization introduces extra build-time overhead because each distance computation must decode quantized values. For 128-D vectors, build time increases with row count; SQ vs. FLAT can be up to ~10\xd7 slower to build."}),"\n",(0,t.jsx)(n.p,{children:"Similarly, Doris also supports product quantization, but note that when using PQ, additional parameters need to be provided:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"pq_m"}),": Indicates how many sub-vectors to split the original high-dimensional vector into (vector dimension dim must be divisible by pq_m)."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"pq_nbits"}),": Indicates the number of bits for each sub-vector quantization, which determines the size of each subspace codebook, in faiss pq_nbits is generally required to be no greater than 24."]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Note that PQ quantization requires sufficient data during the training, the number of training points needing to be at least as large as the number of clusters (n >= 2 ^ pq_nbits)."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:'CREATE TABLE sift_1M (\n  id int NOT NULL,\n  embedding array<float>  NOT NULL  COMMENT "",\n  INDEX ann_index (embedding) USING ANN PROPERTIES(\n      "index_type"="hnsw",\n      "metric_type"="l2_distance",\n      "dim"="128",\n      "quantizer"="pq",    -- Specify using PQ for quantization\n      "pq_m"="2",          -- Required when using PQ, indicates splitting high-dimensional vector into pq_m low-dimensional sub-vectors\n      "pq_nbits"="2"       -- Required when using PQ, indicates the number of bits for each subspace codebook\n  )\n) ENGINE=OLAP\nDUPLICATE KEY(id) COMMENT "OLAP"\nDISTRIBUTED BY HASH(id) BUCKETS 1\nPROPERTIES (\n  "replication_num" = "1"\n);\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"ANN-SQ-BUILD_COSTS",src:i(895314).Z+"",width:"1280",height:"588"})}),"\n",(0,t.jsx)(n.h2,{id:"performance-tuning",children:"Performance Tuning"}),"\n",(0,t.jsx)(n.p,{children:"Vector search is a typical secondary-index point lookup scenario. For high QPS and low latency, consider the following:"}),"\n",(0,t.jsx)(n.p,{children:"With tuning, on hardware FE 32C 64GB + BE 32C 64GB, Doris can reach 3000+ QPS (dataset: Cohere-MEDIUM-1M)."}),"\n",(0,t.jsx)(n.h3,{id:"query-performance",children:"Query Performance"}),"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Concurrency"}),(0,t.jsx)(n.th,{children:"Scheme"}),(0,t.jsx)(n.th,{children:"QPS"}),(0,t.jsx)(n.th,{children:"Avg Latency (s)"}),(0,t.jsx)(n.th,{children:"P99 (s)"}),(0,t.jsx)(n.th,{children:"CPU Usage"}),(0,t.jsx)(n.th,{children:"Recall"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"240"}),(0,t.jsx)(n.td,{children:"Doris"}),(0,t.jsx)(n.td,{children:"3340.4399"}),(0,t.jsx)(n.td,{children:"0.071368168"}),(0,t.jsx)(n.td,{children:"0.163399825"}),(0,t.jsx)(n.td,{children:"40%"}),(0,t.jsx)(n.td,{children:"91.00%"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"240"}),(0,t.jsx)(n.td,{children:"Doris SQ INT8"}),(0,t.jsx)(n.td,{children:"3188.6359"}),(0,t.jsx)(n.td,{children:"0.074728852"}),(0,t.jsx)(n.td,{children:"0.160370195"}),(0,t.jsx)(n.td,{children:"40%"}),(0,t.jsx)(n.td,{children:"88.26%"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"240"}),(0,t.jsx)(n.td,{children:"Doris SQ INT4"}),(0,t.jsx)(n.td,{children:"2818.2291"}),(0,t.jsx)(n.td,{children:"0.084663868"}),(0,t.jsx)(n.td,{children:"0.174826815"}),(0,t.jsx)(n.td,{children:"43%"}),(0,t.jsx)(n.td,{children:"80.38%"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"240"}),(0,t.jsx)(n.td,{children:"Doris brute force"}),(0,t.jsx)(n.td,{children:"3.6787"}),(0,t.jsx)(n.td,{children:"25.554878826"}),(0,t.jsx)(n.td,{children:"29.363227973"}),(0,t.jsx)(n.td,{children:"100%"}),(0,t.jsx)(n.td,{children:"100.00%"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"480"}),(0,t.jsx)(n.td,{children:"Doris"}),(0,t.jsx)(n.td,{children:"4155.7220"}),(0,t.jsx)(n.td,{children:"0.113387271"}),(0,t.jsx)(n.td,{children:"0.261086075"}),(0,t.jsx)(n.td,{children:"60%"}),(0,t.jsx)(n.td,{children:"91.00%"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"480"}),(0,t.jsx)(n.td,{children:"Doris SQ INT8"}),(0,t.jsx)(n.td,{children:"3833.1130"}),(0,t.jsx)(n.td,{children:"0.123040214"}),(0,t.jsx)(n.td,{children:"0.276912867"}),(0,t.jsx)(n.td,{children:"50%"}),(0,t.jsx)(n.td,{children:"88.26%"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"480"}),(0,t.jsx)(n.td,{children:"Doris SQ INT4"}),(0,t.jsx)(n.td,{children:"3431.0538"}),(0,t.jsx)(n.td,{children:"0.137636995"}),(0,t.jsx)(n.td,{children:"0.281631249"}),(0,t.jsx)(n.td,{children:"57%"}),(0,t.jsx)(n.td,{children:"80.38%"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"480"}),(0,t.jsx)(n.td,{children:"Doris brute force"}),(0,t.jsx)(n.td,{children:"3.6787"}),(0,t.jsx)(n.td,{children:"25.554878826"}),(0,t.jsx)(n.td,{children:"29.363227973"}),(0,t.jsx)(n.td,{children:"100%"}),(0,t.jsx)(n.td,{children:"100.00%"})]})]})]}),"\n",(0,t.jsx)(n.h3,{id:"use-prepared-statements",children:"Use Prepared Statements"}),"\n",(0,t.jsx)(n.p,{children:"Modern embedding models often output 768-D or higher vectors. If you inline a 768-D literal into SQL, parsing time can exceed execution time. Use prepared statements. Currently Doris does not support MySQL client prepare commands directly; use JDBC:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["Enable server-side prepared statements in the JDBC URL:",(0,t.jsx)(n.br,{}),"\n",(0,t.jsx)(n.code,{children:"jdbc:mysql://127.0.0.1:9030/demo?useServerPrepStmts=true"})]}),"\n",(0,t.jsxs)(n.li,{children:["Use PreparedStatement with placeholders (",(0,t.jsx)(n.code,{children:"?"}),") and reuse it."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"reduce-segment-count",children:"Reduce Segment Count"}),"\n",(0,t.jsxs)(n.p,{children:["ANN indexes are built per segment. Too many segments cause overhead. Ideally each tablet should have no more than ~5 segments for an ANN-indexed table. Adjust ",(0,t.jsx)(n.code,{children:"write_buffer_size"})," and ",(0,t.jsx)(n.code,{children:"vertical_compaction_max_segment_size"})," in ",(0,t.jsx)(n.code,{children:"be.conf"})," (e.g., both to 10737418240)."]}),"\n",(0,t.jsx)(n.h3,{id:"reduce-rowset-count",children:"Reduce Rowset Count"}),"\n",(0,t.jsxs)(n.p,{children:["Same motivation as reducing segments: minimize scheduling overhead. Each load creates a rowset, so prefer stream load or ",(0,t.jsx)(n.code,{children:"INSERT INTO SELECT"})," for batched ingestion."]}),"\n",(0,t.jsx)(n.h3,{id:"keep-ann-index-in-memory",children:"Keep ANN Index in Memory"}),"\n",(0,t.jsxs)(n.p,{children:["Current ANN algorithms are memory-based. If a segment\u2019s index is not in memory, a disk I/O occurs. Set ",(0,t.jsx)(n.code,{children:"enable_segment_cache_prune=false"})," in ",(0,t.jsx)(n.code,{children:"be.conf"})," to keep ANN indexes resident."]}),"\n",(0,t.jsx)(n.h3,{id:"parallel_pipeline_task_num--1",children:"parallel_pipeline_task_num = 1"}),"\n",(0,t.jsxs)(n.p,{children:["ANN TopN queries return very few rows from each scanner, so high pipeline task parallelism is unnecessary. Set ",(0,t.jsx)(n.code,{children:"parallel_pipeline_task_num = 1"}),"."]}),"\n",(0,t.jsx)(n.h3,{id:"enable_profile--false",children:"enable_profile = false"}),"\n",(0,t.jsx)(n.p,{children:"Disable query profiling for ultra latency-sensitive queries."}),"\n",(0,t.jsx)(n.h2,{id:"python-sdk",children:"Python SDK"}),"\n",(0,t.jsx)(n.p,{children:"In the era of AI, Python has become the mainstream language for data processing and intelligent application development. To make it easier for developers to use Doris's vector search capabilities in Python environments, some community contributors have developed Python SDKs for Doris."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"https://github.com/uchenily/doris_vector_search",children:"https://github.com/uchenily/doris_vector_search"}),": Optimized for vector distance retrieval, this is currently the highest-performance Doris vector search Python SDK available."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"usage-limitations",children:"Usage Limitations"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["The ANN index column must be a NOT NULL ",(0,t.jsx)(n.code,{children:"Array<Float>"}),", and every imported vector must match the declared ",(0,t.jsx)(n.code,{children:"dim"}),", otherwise an error is thrown."]}),"\n",(0,t.jsx)(n.li,{children:"ANN index is only supported on DuplicateKey table model."}),"\n",(0,t.jsxs)(n.li,{children:["Doris uses pre-filter semantics (predicates applied before ANN TopN). If predicates include columns without secondary indexes that can precisely locate rows (e.g., no inverted index), Doris falls back to brute force to preserve correctness.\nExample:\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:"SELECT id, l2_distance_approximate(embedding, [xxx]) AS distance\nFROM sift_1M\nWHERE round(id) > 100\nORDER BY distance LIMIT 10;\n"})}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["Although ",(0,t.jsx)(n.code,{children:"id"})," is a key, without a secondary index (such as an inverted index), its predicate is applied after index analysis, so Doris falls back to brute force to honor pre-filter semantics."]}),"\n",(0,t.jsxs)(n.ol,{start:"4",children:["\n",(0,t.jsxs)(n.li,{children:["If the distance function in SQL does not match the metric type defined in the index DDL, Doris cannot use the ANN index for TopN\u2014even if you call ",(0,t.jsx)(n.code,{children:"l2_distance_approximate"})," / ",(0,t.jsx)(n.code,{children:"inner_product_approximate"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:["For metric type ",(0,t.jsx)(n.code,{children:"inner_product"}),", only ",(0,t.jsx)(n.code,{children:"ORDER BY inner_product_approximate(...) DESC LIMIT N"})," (DESC required) can be accelerated by the ANN index."]}),"\n",(0,t.jsxs)(n.li,{children:["The first parameter of ",(0,t.jsx)(n.code,{children:"xxx_approximate()"})," must be a ColumnArray, and the second must be a CAST or ArrayLiteral. Reversing them triggers brute-force search."]}),"\n"]})]})}function h(e={}){let{wrapper:n}={...(0,s.a)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(o,{...e})}):o(e)}},895314:function(e,n,i){i.d(n,{Z:function(){return r}});let r=i.p+"assets/images/ann-index-quantization-build-time-cb4b5d55cb382352c87a843008e155ce.jpg"},250065:function(e,n,i){i.d(n,{Z:function(){return a},a:function(){return d}});var r=i(667294);let t={},s=r.createContext(t);function d(e){let n=r.useContext(s);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:d(e.components),r.createElement(s.Provider,{value:n},e.children)}}}]);