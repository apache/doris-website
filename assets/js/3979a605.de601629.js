"use strict";(self.webpackChunkdoris_website=self.webpackChunkdoris_website||[]).push([["731446"],{759049:function(e,t,n){n.r(t),n.d(t,{default:()=>h,frontMatter:()=>o,metadata:()=>a,assets:()=>d,toc:()=>l,contentTitle:()=>i});var a=JSON.parse('{"id":"data-operate/import/data-source/bigquery","title":"BigQuery","description":"During the process of migrating BigQuery, it is usually necessary to use object storage as an intermediate medium. The core process is as follows: First, use BigQuery\'s Export statement to export data to GCS (Google Cloud Storage); then, use Doris\'s S3 Load function to read data from the object storage and load it into Doris. For details, please refer to S3 Load.","source":"@site/docs/data-operate/import/data-source/bigquery.md","sourceDirName":"data-operate/import/data-source","slug":"/data-operate/import/data-source/bigquery","permalink":"/docs/dev/data-operate/import/data-source/bigquery","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"title":"BigQuery","language":"en"},"sidebar":"docs","previous":{"title":"Snowflake","permalink":"/docs/dev/data-operate/import/data-source/snowflake"},"next":{"title":"Redshift","permalink":"/docs/dev/data-operate/import/data-source/redshift"}}'),r=n("785893"),s=n("250065");let o={title:"BigQuery",language:"en"},i=void 0,d={},l=[{value:"Considerations",id:"considerations",level:2},{value:"Data type mapping",id:"data-type-mapping",level:2},{value:"1. Create Table",id:"1-create-table",level:2},{value:"2. Export BigQuery Data",id:"2-export-bigquery-data",level:2},{value:"3. Load Data to Doris",id:"3-load-data-to-doris",level:2}];function c(e){let t={a:"a",code:"code",em:"em",h2:"h2",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,s.a)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsxs)(t.p,{children:["During the process of migrating BigQuery, it is usually necessary to use object storage as an intermediate medium. The core process is as follows: First, use BigQuery's ",(0,r.jsx)(t.a,{href:"https://cloud.google.com/bigquery/docs/exporting-data",children:"Export"})," statement to export data to GCS (Google Cloud Storage); then, use Doris's S3 Load function to read data from the object storage and load it into Doris. For details, please refer to ",(0,r.jsx)(t.a,{href:"/docs/dev/data-operate/import/data-source/amazon-s3",children:"S3 Load"}),"."]}),"\n",(0,r.jsx)(t.h2,{id:"considerations",children:"Considerations"}),"\n",(0,r.jsxs)(t.ol,{children:["\n",(0,r.jsxs)(t.li,{children:["Before the migration, it is necessary to select Doris' ",(0,r.jsx)(t.a,{href:"/docs/dev/table-design/data-model/overview",children:"Data Model"}),", as well as the strategies for ",(0,r.jsx)(t.a,{href:"/docs/dev/table-design/data-partitioning/dynamic-partitioning",children:"Partitioning"})," and ",(0,r.jsx)(t.a,{href:"/docs/dev/table-design/data-partitioning/data-bucketing",children:"Bucketing"})," according to the table structure of BigQuery. For more table creation strategies, please refer to ",(0,r.jsx)(t.a,{href:"/docs/dev/data-operate/import/load-best-practices",children:"Load Best Practices"}),"."]}),"\n",(0,r.jsx)(t.li,{children:"When BigQuery exports data in JSON type, it does not support exporting in Parquet format. You can export it in JSON format instead."}),"\n",(0,r.jsx)(t.li,{children:"When BigQuery exports data of the Time type, it is necessary to export it after casting it to the String type."}),"\n"]}),"\n",(0,r.jsx)(t.h2,{id:"data-type-mapping",children:"Data type mapping"}),"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",(0,r.jsxs)(t.table,{children:[(0,r.jsx)(t.thead,{children:(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.th,{children:"BigQuery"}),(0,r.jsx)(t.th,{children:"Doris"}),(0,r.jsx)(t.th,{children:"Comment"})]})}),(0,r.jsxs)(t.tbody,{children:[(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"Array"}),(0,r.jsx)(t.td,{children:"Array"}),(0,r.jsx)(t.td,{})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"BOOLEAN"}),(0,r.jsx)(t.td,{children:"BOOLEAN"}),(0,r.jsx)(t.td,{})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"DATE"}),(0,r.jsx)(t.td,{children:"DATE"}),(0,r.jsx)(t.td,{})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"DATETIME/TIMESTAMP"}),(0,r.jsx)(t.td,{children:"DATETIME"}),(0,r.jsx)(t.td,{})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"JSON"}),(0,r.jsx)(t.td,{children:"JSON"}),(0,r.jsx)(t.td,{})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"INT64"}),(0,r.jsx)(t.td,{children:"BIGINT"}),(0,r.jsx)(t.td,{})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"NUMERIC"}),(0,r.jsx)(t.td,{children:"DECIMAL"}),(0,r.jsx)(t.td,{})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"FLOAT64"}),(0,r.jsx)(t.td,{children:"DOUBLE"}),(0,r.jsx)(t.td,{})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"STRING"}),(0,r.jsx)(t.td,{children:"VARCHAR/STRING"}),(0,r.jsx)(t.td,{children:"VARCHAR maximum length is 65535"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"STRUCT"}),(0,r.jsx)(t.td,{children:"STRUCT"}),(0,r.jsx)(t.td,{})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"TIME"}),(0,r.jsx)(t.td,{children:"STRING"}),(0,r.jsx)(t.td,{})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"OTHER"}),(0,r.jsx)(t.td,{children:"UNSUPPORTED"}),(0,r.jsx)(t.td,{})]})]})]}),"\n",(0,r.jsx)(t.h2,{id:"1-create-table",children:"1. Create Table"}),"\n",(0,r.jsx)(t.p,{children:"When migrating a BigQuery table to Doris, it is necessary to create a Doris table first."}),"\n",(0,r.jsx)(t.p,{children:"Suppose we already have the following table and data in BigQuery."}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-SQL",children:"CREATE OR REPLACE TABLE test.sales_data (\n    order_id      INT64,\n    customer_name STRING,\n    order_date    DATE,\n    amount        NUMERIC(10,2),\n    country       STRING\n)\nPARTITION BY  order_date\n\n\nINSERT INTO test.sales_data (order_id, customer_name, order_date, amount, country) VALUES\n(1, 'Alice', '2025-04-08', 99.99, 'USA'),\n(2, 'Bob', '2025-04-08', 149.50, 'Canada'),\n(3, 'Charlie', '2025-04-09', 75.00, 'UK'),\n(4, 'Diana', '2025-04-10', 200.00, 'Australia');\n"})}),"\n",(0,r.jsx)(t.p,{children:"According to this table structure, a Doris primary key partitioned table can be created. The partition field should be the same as that in BigQuery, and the table should be partitioned on a daily basis."}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-sql",children:'CREATE TABLE `sales_data` (\n  order_id      INT,\n  order_date    DATE NOT NULL,\n  customer_name VARCHAR(128),\n  amount        DECIMAL(10,2),\n  country       VARCHAR(48)\n) ENGINE=OLAP\nUNIQUE KEY(`order_id`,`order_date`)\nPARTITION BY RANGE(`order_date`) (\nPARTITION p20250408 VALUES [(\'2025-04-08\'), (\'2025-04-09\')),\nPARTITION p20250409 VALUES [(\'2025-04-09\'), (\'2025-04-10\')),\nPARTITION p20250410 VALUES [(\'2025-04-10\'), (\'2025-04-11\'))\n)\nDISTRIBUTED BY HASH(`order_id`) BUCKETS 16\nPROPERTIES (\n "dynamic_partition.enable" = "true",\n "dynamic_partition.time_unit" = "DAY",\n "dynamic_partition.end" = "5",\n "dynamic_partition.prefix" = "p",\n "dynamic_partition.buckets" = "16",\n "replication_num" = "1"\n);\n'})}),"\n",(0,r.jsx)(t.h2,{id:"2-export-bigquery-data",children:"2. Export BigQuery Data"}),"\n",(0,r.jsxs)(t.p,{children:["2.1. ",(0,r.jsx)(t.strong,{children:"Export to GCS Parquet format file through Export method"})]}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-sql",children:"EXPORT DATA\n  OPTIONS (\n    uri = 'gs://mybucket/export/sales_data/*.parquet',\n    format = 'PARQUET')\nAS (\n  SELECT *\n  FROM test.sales_data \n);\n"})}),"\n",(0,r.jsxs)(t.p,{children:["2.2. ",(0,r.jsx)(t.strong,{children:"View the exported files on GCS"})]}),"\n",(0,r.jsxs)(t.p,{children:["The above command will export the data of sales_data to GCS, and each partition will generate one or more files with increasing file names. For details, please refer to ",(0,r.jsx)(t.a,{href:"https://cloud.google.com/bigquery/docs/exporting-data#exporting_data_into_one_or_more_files",children:"exporting-data"}),", as follows:"]}),"\n",(0,r.jsx)(t.p,{children:(0,r.jsx)(t.img,{alt:"gcs_export",src:n(240610).Z+"",width:"2036",height:"536"})}),"\n",(0,r.jsx)(t.h2,{id:"3-load-data-to-doris",children:"3. Load Data to Doris"}),"\n",(0,r.jsxs)(t.p,{children:["S3 Load is an asynchronous data load method. After execution, Doris actively pulls data from the data source. The data source supports object storage compatible with the S3 protocol, including (",(0,r.jsx)(t.a,{href:"/docs/dev/data-operate/import/data-source/amazon-s3",children:"AWS S3"}),"\uFF0C",(0,r.jsx)(t.a,{href:"/docs/dev/data-operate/import/data-source/google-cloud-storage",children:"GCS"}),"\uFF0C",(0,r.jsx)(t.a,{href:"/docs/dev/data-operate/import/data-source/azure-storage",children:"AZURE"}),"\uFF0Cetc)"]}),"\n",(0,r.jsxs)(t.p,{children:["This method is suitable for scenarios involving large volumes of data that require asynchronous processing in the background. For data imports that need to be handled synchronously, refer to  ",(0,r.jsx)(t.a,{href:"/docs/dev/data-operate/import/data-source/amazon-s3#load-with-tvf",children:"TVF Load"}),"\u3002"]}),"\n",(0,r.jsx)(t.p,{children:(0,r.jsxs)(t.em,{children:["Note: For ",(0,r.jsx)(t.strong,{children:"Parquet/ORC format files that contain complex types (Struct/Array/Map)"}),", TVF Load must be used."]})}),"\n",(0,r.jsxs)(t.p,{children:["3.1. ",(0,r.jsx)(t.strong,{children:"Loading data from a single file"})]}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-sql",children:'LOAD LABEL sales_data_2025_04_08\n(\n    DATA INFILE("s3://mybucket/export/sales_data/000000000000.parquet")\n    INTO TABLE sales_data\n    FORMAT AS "parquet"\n    (order_id, order_date, customer_name, amount, country)\n)\nWITH S3\n(\n    "provider" = "GCP",\n    "s3.endpoint" = "storage.asia-southeast1.rep.googleapis.com",  \n    "s3.region" = "asia-southeast1",\n    "s3.access_key" = "<ak>",\n    "s3.secret_key" = "<sk>"\n);\n'})}),"\n",(0,r.jsxs)(t.p,{children:["3.2. ",(0,r.jsx)(t.strong,{children:"Check Load Status via SHOW LOAD"})]}),"\n",(0,r.jsx)(t.p,{children:"Since S3 Load import is submitted asynchronously, you can check the status of a specific label using SHOW LOAD:"}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-yaml",children:'mysql> show load where label = "label_sales_data_2025_04_08"\\G\n*************************** 1. row ***************************\n        JobId: 17956078\n        Label: label_sales_data_2025_04_08\n        State: FINISHED\n      Progress: 100.00% (1/1)\n          Type: BROKER\n      EtlInfo: unselected.rows=0; dpp.abnorm.ALL=0; dpp.norm.ALL=2\n      TaskInfo: cluster:storage.asia-southeast1.rep.googleapis.com; timeout(s):3600; max_filter_ratio:0.0; priority:NORMAL\n      ErrorMsg: NULL\n    CreateTime: 2025-04-10 17:50:53\n  EtlStartTime: 2025-04-10 17:50:54\nEtlFinishTime: 2025-04-10 17:50:54\nLoadStartTime: 2025-04-10 17:50:54\nLoadFinishTime: 2025-04-10 17:50:54\n          URL: NULL\n    JobDetails: {"Unfinished backends":{"5eec1be8612d4872-91040ff1e7208a4f":[]},"ScannedRows":2,"TaskNumber":1,"LoadBytes":91,"All backends":{"5eec1be8612d4872-91040ff1e7208a4f":[10022]},"FileNumber":1,"FileSize":1620}\nTransactionId: 766228\n  ErrorTablets: {}\n          User: root\n      Comment: \n1 row in set (0.00 sec)\n'})}),"\n",(0,r.jsxs)(t.p,{children:["3.3. ",(0,r.jsx)(t.strong,{children:"Handle Load Errors"})]}),"\n",(0,r.jsx)(t.p,{children:"When there are multiple load tasks, you can use the following statement to query the dates and reasons for data load failures."}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-yaml",children:'mysql> show load where state=\'CANCELLED\' and label like "label_test%"\\G\n*************************** 1. row ***************************\n        JobId: 18312384\n        Label: label_test123\n        State: CANCELLED\n      Progress: 100.00% (3/3)\n          Type: BROKER\n      EtlInfo: unselected.rows=0; dpp.abnorm.ALL=4; dpp.norm.ALL=0\n      TaskInfo: cluster:storage.asia-southeast1.rep.googleapis.com; timeout(s):14400; max_filter_ratio:0.0; priority:NORMAL\n      ErrorMsg: type:ETL_QUALITY_UNSATISFIED; msg:quality not good enough to cancel\n    CreateTime: 2025-04-15 17:32:59\n  EtlStartTime: 2025-04-15 17:33:02\nEtlFinishTime: 2025-04-15 17:33:02\nLoadStartTime: 2025-04-15 17:33:02\nLoadFinishTime: 2025-04-15 17:33:02\n          URL: http://10.16.10.6:28747/api/_load_error_log?file=__shard_2 error_log_insert_stmt_7602ccd7c3a4854-95307efca7bfe342_7602ccd7c3a4854_95307efca7bfe342\n    JobDetails: {"Unfinished backends":{"7602ccd7c3a4854-95307efca7bfe341":[]},"ScannedRows":4,"TaskNumber":1,"LoadBytes":188,"All backends":{"7602ccd7c3a4854-95307efca7bfe341":[10022]},"FileNumber":3,"FileSize":4839}\nTransactionId: 769213\n  ErrorTablets: {}\n          User: root\n      Comment: \n'})}),"\n",(0,r.jsxs)(t.p,{children:["As shown in the example above, the issue is a ",(0,r.jsx)(t.strong,{children:"data quality error"}),"(ETL_QUALITY_UNSATISFIED). To view the detailed error, you need to visit the URL provided in the result. For example, the data exceeded the defined length of the country column in the table schema:"]}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-python",children:'[root@VM-10-6-centos ~]$ curl "http://10.16.10.6:28747/api/_load_error_log?file=__shard_2/error_log_insert_stmt_7602ccd7c3a4854-95307efca7bfe342_7602ccd7c3a4854_95307efca7bfe342"\nReason: column_name[country], the length of input is too long than schema. first 32 bytes of input str: [USA] schema length: 1; actual length: 3; . src line []; \nReason: column_name[country], the length of input is too long than schema. first 32 bytes of input str: [Canada] schema length: 1; actual length: 6; . src line []; \nReason: column_name[country], the length of input is too long than schema. first 32 bytes of input str: [UK] schema length: 1; actual length: 2; . src line []; \nReason: column_name[country], the length of input is too long than schema. first 32 bytes of input str: [Australia] schema length: 1; actual length: 9; . src line [];\n'})}),"\n",(0,r.jsxs)(t.p,{children:["For data quality errors, if you want to allow skipping erroneous records, you can set a fault tolerance rate in the Properties section of the S3 Load task. For details, refer to ",(0,r.jsx)(t.a,{href:"/docs/dev/data-operate/import/import-way/broker-load-manual#related-configurations",children:"Load Configuration Parameters"}),"\u3002"]}),"\n",(0,r.jsxs)(t.p,{children:["3.4. ",(0,r.jsx)(t.strong,{children:"Loading data from multiple files"})]}),"\n",(0,r.jsx)(t.p,{children:"When migrating a large volume of historical data, it is recommended to use a batch load strategy. Each batch corresponds to one or a few partitions in Doris. It is recommended to keep the data size under 100GB per batch to reduce system load and lower the cost of retries in case of load failures."}),"\n",(0,r.jsxs)(t.p,{children:["You can refer to the script ",(0,r.jsx)(t.a,{href:"https://github.com/apache/doris/blob/master/samples/load/shell/s3_load_file_demo.sh",children:"s3_load_file_demo.sh"}),", which can split the file list under the specified directory on the object storage and submit multiple S3 Load tasks to Doris in batches to achieve the effect of batch load."]})]})}function h(e={}){let{wrapper:t}={...(0,s.a)(),...e.components};return t?(0,r.jsx)(t,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},240610:function(e,t,n){n.d(t,{Z:function(){return a}});let a=n.p+"assets/images/gcs_export-6ffcad224f7655c9a5d9460255b23e07.png"},250065:function(e,t,n){n.d(t,{Z:function(){return i},a:function(){return o}});var a=n(667294);let r={},s=a.createContext(r);function o(e){let t=a.useContext(s);return a.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function i(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),a.createElement(s.Provider,{value:t},e.children)}}}]);