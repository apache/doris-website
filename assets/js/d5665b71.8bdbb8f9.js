"use strict";(self.webpackChunkdoris_website=self.webpackChunkdoris_website||[]).push([["137226"],{524481:function(e,n,r){r.r(n),r.d(n,{default:()=>a,frontMatter:()=>l,metadata:()=>s,assets:()=>o,toc:()=>d,contentTitle:()=>c});var s=JSON.parse('{"id":"ai/text-search/scoring","title":"Relevance Scoring","description":"Text search scoring measures how relevant each row in a table is to a given query text.","source":"@site/versioned_docs/version-4.x/ai/text-search/scoring.md","sourceDirName":"ai/text-search","slug":"/ai/text-search/scoring","permalink":"/docs/4.x/ai/text-search/scoring","draft":false,"unlisted":false,"tags":[],"version":"4.x","lastUpdatedAt":1770477659000,"frontMatter":{"title":"Relevance Scoring","language":"en","description":"Text search scoring measures how relevant each row in a table is to a given query text."},"sidebar":"docs","previous":{"title":"Custom Analyzer","permalink":"/docs/4.x/ai/text-search/custom-analyzer"},"next":{"title":"Overview","permalink":"/docs/4.x/ai/vector-search/overview"}}'),t=r("785893"),i=r("250065");let l={title:"Relevance Scoring",language:"en",description:"Text search scoring measures how relevant each row in a table is to a given query text."},c=void 0,o={},d=[{value:"Overview",id:"overview",level:2},{value:"BM25 Algorithm",id:"bm25-algorithm",level:2},{value:"Formula",id:"formula",level:3},{value:"Using Scoring in Doris",id:"using-scoring-in-doris",level:2},{value:"Supported Index Types",id:"supported-index-types",level:3},{value:"Supported Query Types",id:"supported-query-types",level:3},{value:"Query Pushdown Rules",id:"query-pushdown-rules",level:3},{value:"Example",id:"example",level:2},{value:"Result Interpretation",id:"result-interpretation",level:2}];function h(e){let n={code:"code",h2:"h2",h3:"h3",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.a)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsxs)(n.p,{children:["Text search scoring measures how relevant each row in a table is to a given query text.\nWhen executing a query that includes full-text search predicates (such as ",(0,t.jsx)(n.code,{children:"MATCH_ANY"})," or ",(0,t.jsx)(n.code,{children:"MATCH_ALL"}),"), Doris computes a numeric score for each row, representing its degree of match with the query.\nThis score can be used for ranking query results, so that rows most relevant to the query appear first."]}),"\n",(0,t.jsxs)(n.p,{children:["Doris currently uses the ",(0,t.jsx)(n.strong,{children:"BM25 (Best Matching 25)"})," algorithm for text relevance scoring."]}),"\n",(0,t.jsx)(n.h2,{id:"bm25-algorithm",children:"BM25 Algorithm"}),"\n",(0,t.jsx)(n.p,{children:"BM25 is a probabilistic relevance algorithm that evaluates how well a record matches the query terms by considering term frequency, inverse document frequency, and record length.\nCompared with the traditional TF-IDF model, BM25 provides greater robustness and tunability, effectively balancing score differences between long and short text."}),"\n",(0,t.jsx)(n.h3,{id:"formula",children:"Formula"}),"\n",(0,t.jsx)(n.p,{children:"The core BM25 scoring formula is:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"score = IDF \xd7 (tf \xd7 (k1 + 1)) / (tf + k1 \xd7 (1 - b + b \xd7 |d| / avgdl))\n"})}),"\n",(0,t.jsx)(n.p,{children:"Where:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"tf"})," \u2013 the frequency of a query term in the current row"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"IDF"})," \u2013 inverse document frequency, indicating how rare the term is across all rows"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"|d|"})," \u2013 the length of the current row (number of tokens after analysis)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"avgdl"})," \u2013 the average row length in the table"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"k1"}),", ",(0,t.jsx)(n.strong,{children:"b"})," \u2013 algorithm tuning parameters"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Default parameters:"})}),"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Parameter"}),(0,t.jsx)(n.th,{children:"Default"}),(0,t.jsx)(n.th,{children:"Description"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"k1"})}),(0,t.jsx)(n.td,{children:"1.2"}),(0,t.jsx)(n.td,{children:"Controls how strongly term frequency affects the score."})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"b"})}),(0,t.jsx)(n.td,{children:"0.75"}),(0,t.jsx)(n.td,{children:"Controls the strength of length normalization."})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"boost"})}),(0,t.jsx)(n.td,{children:"1.0"}),(0,t.jsx)(n.td,{children:"Optional query-level weighting factor."})]})]})]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Supporting statistics:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"IDF = log(1 + (N - n + 0.5) / (n + 0.5))\navgdl = total_terms / total_rows\n"})}),"\n",(0,t.jsx)(n.p,{children:"Where:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"N"})," \u2013 total number of rows in the table"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"n"})," \u2013 number of rows that contain the query term"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"The final score of a row is the sum of the BM25 scores for all query terms."}),"\n",(0,t.jsx)(n.h2,{id:"using-scoring-in-doris",children:"Using Scoring in Doris"}),"\n",(0,t.jsx)(n.h3,{id:"supported-index-types",children:"Supported Index Types"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Tokenized inverted index"})," \u2013 supports BM25 scoring."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Non-tokenized inverted index"})," \u2013 supports only exact matching; scoring is not calculated."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"supported-query-types",children:"Supported Query Types"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"MATCH_ANY"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"MATCH_ALL"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"MATCH_PHRASE"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"MATCH_PHRASE_PREFIX"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"SEARCH"})}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"query-pushdown-rules",children:"Query Pushdown Rules"}),"\n",(0,t.jsx)(n.p,{children:"To enable scoring pushdown into the inverted index engine, the following conditions must be met:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["The ",(0,t.jsx)(n.code,{children:"SELECT"})," clause includes the ",(0,t.jsx)(n.code,{children:"score()"})," function."]}),"\n",(0,t.jsxs)(n.li,{children:["The ",(0,t.jsx)(n.code,{children:"WHERE"})," clause contains at least one ",(0,t.jsx)(n.code,{children:"MATCH_*"})," predicate."]}),"\n",(0,t.jsxs)(n.li,{children:["The query is a Top-N query with an ",(0,t.jsx)(n.code,{children:"ORDER BY"})," clause based on the score result."]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"example",children:"Example"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:"SELECT *,\n       score() AS relevance\nFROM search_demo\nWHERE content MATCH_ANY 'text search test'\nORDER BY relevance DESC\nLIMIT 10;\n"})}),"\n",(0,t.jsx)(n.p,{children:"This query returns the top 10 rows most relevant to the search terms, ranked by BM25 score."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:"+------+-----------------------------------+---------+--------------+-----------+\n| id   | content                           | author  | publish_date | relevance |\n+------+-----------------------------------+---------+--------------+-----------+\n|    1 | Full text search engine test demo | Alice   | 2024-01-01   |  2.915228 |\n|    7 | Text processing techniques        | Grace   | 2024-01-07   |  1.341931 |\n|    5 | Performance test framework        | Eve     | 2024-01-05   |  1.341931 |\n|    3 | Advanced search algorithms        | Charlie | 2024-01-03   |  1.341931 |\n+------+-----------------------------------+---------+--------------+-----------+\n"})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"result-interpretation",children:"Result Interpretation"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Score range"})," \u2013 BM25 scores are positive and unbounded. Only relative magnitude matters."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Multiple terms"})," \u2013 For multi-term queries, the total score is the sum of all term scores."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Length effect"})," \u2013 Shorter rows generally receive higher scores when containing the same terms."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"No matching terms"})," \u2013 If none of the query terms appear in the table, the score is ",(0,t.jsx)(n.code,{children:"0"}),"."]}),"\n"]})]})}function a(e={}){let{wrapper:n}={...(0,i.a)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(h,{...e})}):h(e)}},250065:function(e,n,r){r.d(n,{Z:function(){return c},a:function(){return l}});var s=r(667294);let t={},i=s.createContext(t);function l(e){let n=s.useContext(i);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:l(e.components),s.createElement(i.Provider,{value:n},e.children)}}}]);