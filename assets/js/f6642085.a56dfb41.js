"use strict";(self.webpackChunkdoris_website=self.webpackChunkdoris_website||[]).push([["152466"],{730567:function(e,n,o){o.r(n),o.d(n,{default:()=>h,frontMatter:()=>s,metadata:()=>t,assets:()=>a,toc:()=>c,contentTitle:()=>l});var t=JSON.parse('{"id":"ecosystem/observability/loongcollector","title":"LoongCollector (iLogtail) Doris Flusher","description":"LoongCollector (iLogtail) is an open-source, high-performance log collection and processing framework originating from Alibaba Cloud.","source":"@site/versioned_docs/version-2.1/ecosystem/observability/loongcollector.md","sourceDirName":"ecosystem/observability","slug":"/ecosystem/observability/loongcollector","permalink":"/docs/2.1/ecosystem/observability/loongcollector","draft":false,"unlisted":false,"tags":[],"version":"2.1","lastUpdatedAt":1770477659000,"frontMatter":{"title":"LoongCollector (iLogtail) Doris Flusher","language":"en","description":"LoongCollector (iLogtail) is an open-source, high-performance log collection and processing framework originating from Alibaba Cloud."},"sidebar":"docs","previous":{"title":"FluentBit","permalink":"/docs/2.1/ecosystem/observability/fluentbit"},"next":{"title":"Langfuse on Doris","permalink":"/docs/2.1/ecosystem/observability/langfuse"}}'),i=o("785893"),r=o("250065");let s={title:"LoongCollector (iLogtail) Doris Flusher",language:"en",description:"LoongCollector (iLogtail) is an open-source, high-performance log collection and processing framework originating from Alibaba Cloud."},l="LoongCollector (iLogtail) Doris Flusher",a={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Installation",id:"installation",level:2},{value:"Download from Official Website",id:"download-from-official-website",level:3},{value:"Compile from Source Code",id:"compile-from-source-code",level:3},{value:"Configuration",id:"configuration",level:2},{value:"Usage Examples",id:"usage-examples",level:2},{value:"TEXT Log Collection Example",id:"text-log-collection-example",level:3},{value:"JSON Log Collection Example",id:"json-log-collection-example",level:3}];function d(e){let n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,r.a)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"loongcollector-ilogtail-doris-flusher",children:"LoongCollector (iLogtail) Doris Flusher"})}),"\n",(0,i.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.a,{href:"https://github.com/alibaba/loongcollector",children:"LoongCollector (iLogtail)"})," is an open-source, high-performance log collection and processing framework originating from Alibaba Cloud. Before version 3.0, it was named Logtail/iLogtail. It supports custom output plugins to write data into storage systems, and the LoongCollector Doris Flusher is the plugin for outputting data to Doris."]}),"\n",(0,i.jsx)(n.p,{children:"The Doris Flusher calls the Doris Stream Load HTTP interface to write data into Doris in real-time, providing capabilities such as multi-threaded concurrency, failure retries, custom Stream Load formats and parameters, and output write speed monitoring."}),"\n",(0,i.jsx)(n.p,{children:"There are three main steps to use the Doris Flusher:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Install LoongCollector"}),"\n",(0,i.jsx)(n.li,{children:"Configure the Doris output address and other parameters"}),"\n",(0,i.jsx)(n.li,{children:"Start LoongCollector to write data into Doris in real-time"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"installation",children:"Installation"}),"\n",(0,i.jsx)(n.h3,{id:"download-from-official-website",children:"Download from Official Website"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"wget https://apache-doris-releases.oss-cn-beijing.aliyuncs.com/extension/loongcollector-linux-amd64.tar.gz\n"})}),"\n",(0,i.jsx)(n.h3,{id:"compile-from-source-code",children:"Compile from Source Code"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-shell",children:"# Clone the repository\ngit clone https://github.com/alibaba/loongcollector.git\ncd loongcollector\ngit submodule update --init\n\n# Build LoongCollector\nmake all\ncd output\n"})}),"\n",(0,i.jsx)(n.h2,{id:"configuration",children:"Configuration"}),"\n",(0,i.jsx)(n.p,{children:"The configuration for the LoongCollector Doris Flusher Plugin is as follows:"}),"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Configuration"}),(0,i.jsx)(n.th,{children:"Description"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"Addresses"})}),(0,i.jsx)(n.td,{children:'Stream Load HTTP addresses, formatted as a string array with one or more elements, each element being host:port. For example: ["http://fe1:8030", "http://fe2:8030"]'})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"Database"})}),(0,i.jsx)(n.td,{children:"The Doris database name to write into"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"Table"})}),(0,i.jsx)(n.td,{children:"The Doris table name to write into"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"Authentication.PlainText.Username"})}),(0,i.jsx)(n.td,{children:"Doris username, this user needs to have import permissions for the corresponding Doris database and table"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"Authentication.PlainText.Password"})}),(0,i.jsx)(n.td,{children:"Doris user's password"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"LoadProperties"})}),(0,i.jsxs)(n.td,{children:["Doris Stream Load header parameters, formatted as a map. For example: ",(0,i.jsx)(n.code,{children:'LoadProperties: {"format": "json", "read_json_by_line": "true"}'})]})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"LogProgressInterval"})}),(0,i.jsx)(n.td,{children:"Time interval for outputting speed in logs, unit is seconds, default is 10, setting to 0 can disable this type of logging"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"GroupCommit"})}),(0,i.jsx)(n.td,{children:'Group commit mode, optional values are "sync", "async", or "off", default is "off"'})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"Concurrency"})}),(0,i.jsx)(n.td,{children:"Number of goroutines for concurrent data sending, default is 1 (synchronous mode)"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"QueueCapacity"})}),(0,i.jsx)(n.td,{children:"Task queue capacity in asynchronous mode, default is 1024"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"Convert.Protocol"})}),(0,i.jsx)(n.td,{children:"Data conversion protocol, default is custom_single"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"Convert.Encoding"})}),(0,i.jsx)(n.td,{children:"Data conversion encoding, default is json"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"Convert.TagFieldsRename"})}),(0,i.jsx)(n.td,{children:"Rename one or more fields from tags"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"Convert.ProtocolFieldsRename"})}),(0,i.jsx)(n.td,{children:"Rename protocol fields, protocol field options can only be: contents, tags, time"})]})]})]}),"\n",(0,i.jsx)(n.h2,{id:"usage-examples",children:"Usage Examples"}),"\n",(0,i.jsx)(n.h3,{id:"text-log-collection-example",children:"TEXT Log Collection Example"}),"\n",(0,i.jsx)(n.p,{children:"This example demonstrates TEXT log collection using Doris FE logs as an example."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"1. Data"})}),"\n",(0,i.jsx)(n.p,{children:"FE log files are typically located at the fe/log/fe.log file under the Doris installation directory. They are typical Java program logs, including fields such as timestamp, log level, thread name, code location, and log content. Not only do they contain normal logs, but also exception logs with stacktraces, which are multiline. Log collection and storage need to combine the main log and stacktrace into a single log entry."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"2024-07-08 21:18:01,432 INFO (Statistics Job Appender|61) [StatisticsJobAppender.runAfterCatalogReady():70] Stats table not available, skip\n2024-07-08 21:18:53,710 WARN (STATS_FETCH-0|208) [StmtExecutor.executeInternalQuery():3332] Failed to run internal SQL: OriginStatement{originStmt='SELECT * FROM __internal_schema.column_statistics WHERE part_id is NULL  ORDER BY update_time DESC LIMIT 500000', idx=0}\norg.apache.doris.common.UserException: errCode = 2, detailMessage = tablet 10031 has no queryable replicas. err: replica 10032's backend 10008 does not exist or not alive\n        at org.apache.doris.planner.OlapScanNode.addScanRangeLocations(OlapScanNode.java:931) ~[doris-fe.jar:1.2-SNAPSHOT]\n        at org.apache.doris.planner.OlapScanNode.computeTabletInfo(OlapScanNode.java:1197) ~[doris-fe.jar:1.2-SNAPSHOT]\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"2. Table Creation"})}),"\n",(0,i.jsx)(n.p,{children:"The table structure includes fields such as the log's creation time, collection time, hostname, log file path, log type, log level, thread name, code location, and log content."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'CREATE TABLE `doris_log` (\n  `log_time` datetime NULL COMMENT \'log content time\',\n  `collect_time` datetime NULL COMMENT \'log agent collect time\',\n  `host` text NULL COMMENT \'hostname or ip\',\n  `path` text NULL COMMENT \'log file path\',\n  `type` text NULL COMMENT \'log type\',\n  `level` text NULL COMMENT \'log level\',\n  `thread` text NULL COMMENT \'log thread\',\n  `position` text NULL COMMENT \'log code position\',\n  `message` text NULL COMMENT \'log message\',\n  INDEX idx_host (`host`) USING INVERTED COMMENT \'\',\n  INDEX idx_path (`path`) USING INVERTED COMMENT \'\',\n  INDEX idx_type (`type`) USING INVERTED COMMENT \'\',\n  INDEX idx_level (`level`) USING INVERTED COMMENT \'\',\n  INDEX idx_thread (`thread`) USING INVERTED COMMENT \'\',\n  INDEX idx_position (`position`) USING INVERTED COMMENT \'\',\n  INDEX idx_message (`message`) USING INVERTED PROPERTIES("parser" = "unicode", "support_phrase" = "true") COMMENT \'\'\n) ENGINE=OLAP\nDUPLICATE KEY(`log_time`)\nCOMMENT \'OLAP\'\nPARTITION BY RANGE(`log_time`) ()\nDISTRIBUTED BY RANDOM BUCKETS 10\nPROPERTIES (\n"replication_num" = "1",\n"dynamic_partition.enable" = "true",\n"dynamic_partition.time_unit" = "DAY",\n"dynamic_partition.start" = "-7",\n"dynamic_partition.end" = "1",\n"dynamic_partition.prefix" = "p",\n"dynamic_partition.buckets" = "10",\n"dynamic_partition.create_history_partition" = "true",\n"compaction_policy" = "time_series"\n);\n'})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"3. LoongCollector Configuration"})}),"\n",(0,i.jsx)(n.p,{children:"The LoongCollector configuration file consists of 3 main parts:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"inputs - responsible for reading raw data"}),"\n",(0,i.jsx)(n.li,{children:"processors - responsible for data transformation"}),"\n",(0,i.jsx)(n.li,{children:"flushers - responsible for data output"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["Configuration file location: ",(0,i.jsx)(n.code,{children:"conf/continuous_pipeline_config/local/"}),"\nCreate configuration file: ",(0,i.jsx)(n.code,{children:"loongcollector_doris_log.yaml"})]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:'enable: true\n\ninputs:\n  # 1. inputs section is responsible for reading raw data\n  # file_log input is an input plugin that can configure the log file path to read\n  # Using multiline configuration to append lines not starting with timestamp to the previous line,\n  # achieving the effect of merging stacktrace with the main log\n  - Type: input_file\n    FilePaths:\n      - /path/fe.log\n    Multiline:\n      Mode: custom\n      StartPattern: \'^\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\'\n\nprocessors:\n  # 2. processors section is responsible for data transformation\n  # processor_regex is a commonly used data transformation plugin that extracts fields using regular expressions\n  - Type: processor_regex\n    SourceKey: content\n    Regex: \'(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2},\\d{3}) ([A-Z]+) \\(([^\\)]*)\\) \\[([^\\]]*)\\] (.*)\'\n    Keys:\n      - log_time\n      - level\n      - thread\n      - position\n      - message\n  # Add extra fields\n  - Type: processor_add_fields\n    Fields:\n      type: fe.log\n    IgnoreIfExist: false\n\nflushers:\n  # 3. flushers section is responsible for data output\n  # flusher_doris outputs data to Doris using the Stream Load HTTP interface\n  # The LoadProperties parameter specifies the Stream Load data format as JSON\n  - Type: flusher_doris\n    Addresses:\n      - "http://fe_ip:http_port"\n    Database: log_db\n    Table: doris_log\n    Authentication:\n      PlainText:\n        Username: root\n        Password: ""\n    LoadProperties:\n      format: json\n      read_json_by_line: "true"\n      load_to_single_tablet: "true"\n      columns: "log_time,collect_time,host,path,type,level,thread,position,message,log_time=replace(log_time,\',\',\'.\'),collect_time=from_unixtime(collect_time)"\n    Convert:\n      Protocol: custom_single_flatten\n      Encoding: json\n      TagFieldsRename:\n        host.ip: host\n        log.file.path: path\n      ProtocolFieldsRename:\n        time: collect_time\n    LogProgressInterval: 10\n'})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"4. Running LoongCollector"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"nohup ./loongcollector > stdout.log 2> stderr.log &\n\n# By default, speed information is logged every 10 seconds, including data volume since startup (MB and ROWS), total speed (MB/s and R/s), and speed for the last 10 seconds\ntotal 11 MB 18978 ROWS, total speed 0 MB/s 632 R/s, last 10 seconds speed 1 MB/s 1897 R/s\n"})}),"\n",(0,i.jsx)(n.h3,{id:"json-log-collection-example",children:"JSON Log Collection Example"}),"\n",(0,i.jsx)(n.p,{children:"This example demonstrates JSON log collection using data from the GitHub events archive."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"1. Data"})}),"\n",(0,i.jsxs)(n.p,{children:["The GitHub events archive contains archived data of GitHub user actions, formatted as JSON. It can be downloaded from ",(0,i.jsx)(n.a,{href:"https://www.gharchive.org/",children:"https://www.gharchive.org/"}),", for example, the data for January 1, 2024, at 3 PM."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"wget https://data.gharchive.org/2024-01-01-15.json.gz\n\n"})}),"\n",(0,i.jsx)(n.p,{children:"Below is a sample of the data. Normally, each piece of data is on a single line, but for ease of display, it has been formatted here."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'{\n  "id": "37066529221",\n  "type": "PushEvent",\n  "actor": {\n    "id": 46139131,\n    "login": "Bard89",\n    "display_login": "Bard89",\n    "gravatar_id": "",\n    "url": "https://api.github.com/users/Bard89",\n    "avatar_url": "https://avatars.githubusercontent.com/u/46139131?"\n  },\n  "repo": {\n    "id": 780125623,\n    "name": "Bard89/talk-to-me",\n    "url": "https://api.github.com/repos/Bard89/talk-to-me"\n  },\n  "payload": {\n    "repository_id": 780125623,\n    "push_id": 17799451992,\n    "size": 1,\n    "distinct_size": 1,\n    "ref": "refs/heads/add_mvcs",\n    "head": "f03baa2de66f88f5f1754ce3fa30972667f87e81",\n    "before": "85e6544ede4ae3f132fe2f5f1ce0ce35a3169d21"\n  },\n  "public": true,\n  "created_at": "2024-04-01T23:00:00Z"\n}\n'})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"2. Table Creation"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'CREATE DATABASE log_db;\nUSE log_db;\n\nCREATE TABLE github_events\n(\n  `created_at` DATETIME,\n  `id` BIGINT,\n  `type` TEXT,\n  `public` BOOLEAN,\n  `actor` VARIANT,\n  `repo` VARIANT,\n  `payload` TEXT,\n  INDEX `idx_id` (`id`) USING INVERTED,\n  INDEX `idx_type` (`type`) USING INVERTED,\n  INDEX `idx_actor` (`actor`) USING INVERTED,\n  INDEX `idx_host` (`repo`) USING INVERTED,\n  INDEX `idx_payload` (`payload`) USING INVERTED PROPERTIES("parser" = "unicode", "support_phrase" = "true")\n)\nENGINE = OLAP\nDUPLICATE KEY(`created_at`)\nPARTITION BY RANGE(`created_at`) ()\nDISTRIBUTED BY RANDOM BUCKETS 10\nPROPERTIES (\n"replication_num" = "1",\n"inverted_index_storage_format"= "v2",\n"compaction_policy" = "time_series",\n"enable_single_replica_compaction" = "true",\n"dynamic_partition.enable" = "true",\n"dynamic_partition.create_history_partition" = "true",\n"dynamic_partition.time_unit" = "DAY",\n"dynamic_partition.start" = "-30",\n"dynamic_partition.end" = "1",\n"dynamic_partition.prefix" = "p",\n"dynamic_partition.buckets" = "10",\n"dynamic_partition.replication_num" = "1"\n);\n'})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"3. LoongCollector Configuration"})}),"\n",(0,i.jsx)(n.p,{children:"This configuration differs from the previous TEXT log collection in the following ways:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"input_file uses JSON mode for parsing, LoongCollector will parse each line of text as JSON format"}),"\n",(0,i.jsx)(n.li,{children:"No complex processor plugins are used because JSON data already has structured fields"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["Configuration file location: ",(0,i.jsx)(n.code,{children:"conf/continuous_pipeline_config/local/"}),"\nCreate configuration file: ",(0,i.jsx)(n.code,{children:"loongcollector_doris_log.yaml"})]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:'enable: true\n\ninputs:\n  # file_log input reads JSON format log files\n  - Type: input_file\n    FilePaths:\n      - /path/2024-01-01-15.json\n\nprocessors:\n  # Parse content, only expand the first level (actor, repo remain as JSON strings for VARIANT type usage)\n  - Type: processor_json\n    SourceKey: content\n    KeepSource: false\n    ExpandDepth: 1\n    ExpandConnector: ""\n\nflushers:\n  # flusher_doris outputs data to Doris\n  - Type: flusher_doris\n    Addresses:\n      - "http://fe_ip:http_port"\n    Database: log_db\n    Table: github_events\n    Authentication:\n      PlainText:\n        Username: root\n        Password: ""\n    LoadProperties:\n      format: json\n      read_json_by_line: "true"\n      load_to_single_tablet: "true"\n    Convert:\n      Protocol: custom_single_flatten\n      Encoding: json\n    LogProgressInterval: 10\n    Concurrency: 3\n'})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"4. Running LoongCollector"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"nohup ./loongcollector > stdout.log 2> stderr.log &\n"})})]})}function h(e={}){let{wrapper:n}={...(0,r.a)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},250065:function(e,n,o){o.d(n,{Z:function(){return l},a:function(){return s}});var t=o(667294);let i={},r=t.createContext(i);function s(e){let n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);